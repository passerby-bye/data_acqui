[
{"title": "BaxBench: Can LLMs Generate Correct and Secure Backends?", "authors": "Mark Vero , Niels Mündler , Viktor Chibotaru , Veselin Raychev , Maximilian Baader , Nikola Jovanović , Jingxuan He , Martin Vechev", "abstract": "Automatic program generation has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 62% on code correctness; (ii) on average, we could successfully execute security exploits on around …", "poster_file": "44337.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44337.png?t=1752426178.286824", "page_url": "https://icml.cc/virtual/2025/poster/44337", "openreview_url": "https://openreview.net/forum?id=il3KRr4H9u"},
{"title": "Neural Encoding and Decoding at Scale", "authors": "Yizi Zhang , Yanchen Wang , Mehdi Azabou , Alexandre Andre , Zixuan Wang , Hanrui Lyu , International Brain Laboratory , Eva Dyer , Department of Statistics Liam Paninski , Cole Hurwitz", "abstract": "Recent work has demonstrated that large-scale, multi-animal models are powerful tools for characterizing the relationship between neural activity and behavior. Current large-scale approaches, however, focus exclusively on either predicting neural activity from behavior (encoding) or predicting behavior from neural activity (decoding), limiting their ability to capture the bidirectional relationship between neural activity and behavior. To bridge this gap, we introduce a multimodal, multi-task model that enables simultaneous Neural Encoding and Decoding at Scale (NEDS). Central to our approach is a novel multi-task-masking strategy, which alternates between neural, behavioral, within-modality, and cross-modality masking. We pretrain our method on the International Brain Laboratory (IBL) repeated site dataset, which includes recordings from 83 animals performing the visual decision-making task. In comparison to other large-scale modeling approaches, we demonstrate that NEDS achieves state-of-the-art performance for both encoding and decoding when pretrained on multi-animal data and then fine-tuned on new animals. Surprisingly, NEDS's learned embeddings exhibit emergent properties: even without explicit training, they are highly predictive of the brain regions in each recording. Altogether, our approach is a step towards a foundation model of the brain that enables seamless translation between neural activity and behavior.", "poster_file": "43679.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43679.png?t=1750995116.290287", "page_url": "https://icml.cc/virtual/2025/poster/43679", "openreview_url": "https://openreview.net/forum?id=vOdz3zhSCj"},
{"title": "Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective", "authors": "Yujin Oh , Pengfei Jin , Sangjoon Park , Sekeun Kim , Siyeop yoon , Jin Kim , Kyungsang Kim , Xiang Li , Quanzheng Li", "abstract": "Ensuring fairness in medical image segmentation is critical due to biases in imbalanced clinical data acquisition caused by demographic attributes (e.g., age, sex, race) and clinical factors (e.g., disease severity). To address these challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired by optimal control theory. We provide a comprehensive analysis of its underlying mechanisms and clarify dMoE's role in adapting to heterogeneous distributions in medical image segmentation. Furthermore, we integrate dMoE into multiple network architectures, demonstrating its broad applicability across diverse medical image analysis tasks. By incorporating demographic and clinical factors, dMoE achieves state-of-the-art performance on two 2D benchmark datasets and a 3D in-house dataset. Our results highlight the effectiveness of dMoE in mitigating biases from imbalanced distributions, offering a promising approach to bridging control theory and medical image segmentation within fairness learning paradigms. The source code is available at https://github.com/tvseg/dMoE.", "poster_file": "46110.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46110.png?t=1752646703.5557678", "page_url": "https://icml.cc/virtual/2025/poster/46110", "openreview_url": "https://openreview.net/forum?id=BUONdewsBa"},
{"title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective", "authors": "Seungwook Han , Jinyeop Song , Jeff Gore , Pulkit Agrawal", "abstract": "Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding  inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.", "poster_file": "46660.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46660.png?t=1752092639.1074073", "page_url": "https://icml.cc/virtual/2025/poster/46660", "openreview_url": "https://openreview.net/forum?id=0ysC6VS0y3"},
{"title": "Non-stationary Diffusion For Probabilistic Time Series Forecasting", "authors": "Weiwei Ye , Zhuopeng Xu , Ning Gui", "abstract": "Due to the dynamics of underlying physics and external influences, the uncertainty of time series often varies over time. However, existing Denoising Diffusion Probabilistic Models (DDPMs) often fail to capture this non-stationary nature, constrained by their constant variance assumption from the additive noise model (ANM). In this paper, we innovatively utilize the Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of ANM. A diffusion-based probabilistic forecasting framework, termed Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of modeling the changing pattern of uncertainty. Specifically, NsDiff combines a denoising diffusion-based conditional generative model with a pre-trained conditional mean and variance estimator, enabling adaptive endpoint distribution modeling. Furthermore, we propose an uncertainty-aware noise schedule, which dynamically adjusts the noise levels to accurately reflect the data uncertainty at each step and integrates the time-varying variances into the diffusion process. Extensive experiments conducted on nine real-world and synthetic datasets demonstrate the superior performance of NsDiff compared to existing approaches. Code is available at https://github.com/wwy155/NsDiff.", "poster_file": "44783.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44783.png?t=1748927019.4836056", "page_url": "https://icml.cc/virtual/2025/poster/44783", "openreview_url": "https://openreview.net/forum?id=afpc1MFMYU"},
{"title": "Towards Practical Defect-Focused Automated Code Review", "authors": "Junyi Lu , Lili Jiang , Xiaojia Li , Jianbing Fang , Fengjun Zhang , Li Yang , Chun Zuo", "abstract": "The complexity of code reviews has driven efforts to automate review comments, but prior approaches oversimplify this task by treating it as snippet-level code-to-text generation and relying on text similarity metrics like BLEU for evaluation. These methods overlook repository context, real-world merge request evaluation, and defect detection, limiting their practicality. To address these issues, we explore the full automation pipeline within the online recommendation service of a company with nearly 400 million daily active users, analyzing industry-grade C++ codebases comprising hundreds of thousands of lines of code. We identify four key challenges: 1) capturing relevant context, 2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and 4) integrating human workflows. To tackle these, we propose 1) code slicing algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a filtering mechanism for FAR reduction, and 4) a novel prompt design for better human interaction. Our approach, validated on real-world merge requests from historical fault reports, achieves a 2× improvement over standard LLMs and a 10× gain over previous baselines. While the presented results focus on C++, the underlying framework design leverages language-agnostic principles (e.g., AST-based analysis), suggesting potential for broader applicability.", "poster_file": "44165.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44165.png?t=1750852215.7687666", "page_url": "https://icml.cc/virtual/2025/poster/44165", "openreview_url": "https://openreview.net/forum?id=mEV0nvHcK3"},
{"title": "Monte Carlo Tree Diffusion for System 2 Planning", "authors": "Jaesik Yoon , Hyeonseo Cho , Doojin Baek , Yoshua Bengio , Sungjin Ahn", "abstract": "Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)—whose performance naturally improves with inference-time computation scaling—standard diffusion‐based planners offer only limited avenues for the scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree‐structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long‐horizon tasks show that MCTD outperforms diffusion baselines, yielding higher‐quality solutions as inference-time computation increases.", "poster_file": "44944.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44944.png?t=1750379985.5529594", "page_url": "https://icml.cc/virtual/2025/poster/44944", "openreview_url": "https://openreview.net/forum?id=XrCbBdycDc"},
{"title": "Position: We Need An Algorithmic Understanding of Generative AI", "authors": "Oliver Eberle , Thomas McGee , Hamza Giaffar , Taylor Webb , Ida Momennejad", "abstract": "What algorithms do LLMs actually learn and use to solve problems? Studies addressing this question are sparse, as research priorities are focused on improving performance through scale, leaving a theoretical and empirical gap in understanding emergent algorithms. This position paper proposes AlgEval: a framework for systematic research into the algorithms that LLMs learn and use. AlgEval aims to uncover algorithmic primitives, reflected in latent representations, attention, and inference-time compute, and their algorithmic composition to solve task-specific problems. We highlight potential methodological paths and a case study toward this goal, focusing on emergent search algorithms. Our case study illustrates both the formation of top-down hypotheses about candidate algorithms, and bottom-up tests of these hypotheses via circuit-level analysis of attention patterns and hidden states. The rigorous, systematic evaluation of how LLMs actually solve tasks provides an alternative to resource-intensive scaling, reorienting the field toward a principled understanding of underlying computations. Such algorithmic explanations offer a pathway to human-understandable interpretability, enabling comprehension of the model's internal reasoning performance measures. This can in turn lead to more sample-efficient methods for training and improving performance, as well as novel architectures for end-to-end and multi-agent systems.", "poster_file": "40121.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/40121.png?t=1752140103.1147573", "page_url": "https://icml.cc/virtual/2025/poster/40121", "openreview_url": "https://openreview.net/forum?id=eax2ixyeQL"},
{"title": "Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream", "authors": "Abdulkadir Gokce , Martin Schrimpf", "abstract": "When trained on large-scale object classification datasets, certain artificial neural network models begin to approximate core object recognition behaviors and neural response patterns in the primate brain. While recent machine learning advances suggest that scaling compute, model size, and dataset size improves task performance, the impact of scaling on brain alignment remains unclear. In this study, we explore scaling laws for modeling the primate visual ventral stream by systematically evaluating over 600 models trained under controlled conditions on benchmarks spanning V1, V2, V4, IT and behavior. We find that while behavioral alignment continues to scale with larger models, neural alignment saturates. This observation remains true across model architectures and training datasets, even though models with stronger inductive biases and datasets with higher-quality images are more compute-efficient. Increased scaling is especially beneficial for higher-level visual areas, where small models trained on few samples exhibit only poor alignment. Our results suggest that while scaling current architectures and datasets might suffice for alignment with human core object recognition behavior, it will not yield improved models of the brain's visual ventral stream, highlighting the need for novel strategies in building brain models.", "poster_file": "44987.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44987.png?t=1752422604.6563747", "page_url": "https://icml.cc/virtual/2025/poster/44987", "openreview_url": "https://openreview.net/forum?id=WxY61MmHYo"},
{"title": "Rapid Overfitting of Multi-Pass SGD in Stochastic Convex Optimization", "authors": "Shira Vansover-Hager , Tomer Koren , Roi Livni", "abstract": "We study the out-of-sample performance of multi-pass stochastic gradient descent (SGD) in the fundamental stochastic convex optimization (SCO) model. While one-pass SGD is known to achieve an optimal $\\Theta(1/\\sqrt{n})$ excess population loss given a sample of size $n$, much less is understood about the multi-pass version of the algorithm which is widely used in practice. Somewhat surprisingly, we show that in the general non-smooth case of SCO, just a few epochs of SGD can already hurt its out-of-sample performance significantly and lead to overfitting. In particular, using a step size $\\eta = \\Theta(1/\\sqrt{n})$, which gives the optimal rate after one pass, can lead to population loss as large as $\\Omega(1)$ after just one additional pass. More generally, we show that the population loss from the second pass onward is of the order $\\Theta(1/(\\eta T) + \\eta \\sqrt{T})$, where $T$ is the total number of steps. These results reveal a certain phase-transition in the out-of-sample behavior of SGD after the first epoch, as well as a sharp separation between the rates of overfitting in the smooth and non-smooth cases of SCO. Additionally, we extend our results to with-replacement SGD, proving that the same asymptotic bounds hold after $O(n \\log n)$ steps. …", "poster_file": "45321.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45321.png?t=1752075456.9651892", "page_url": "https://icml.cc/virtual/2025/poster/45321", "openreview_url": "https://openreview.net/forum?id=Qq5h78Eshy"},
{"title": "Convergence of Mean-Field Langevin Stochastic Descent-Ascent for Distributional Minimax Optimization", "authors": "Zhangyi Liu , Feng Liu , Rui Gao , Shuang Li", "abstract": "We study convergence properties of the discrete-time Mean-Field Langevin Stochastic Descent-Ascent (MFL-SDA) algorithm for solving distributional minimax optimization. These problems arise in various applications, such as zero-sum games, generative adversarial networks and distributionally robust learning. Despite the significance of MFL-SDA in these contexts, the discrete-time convergence rate remains underexplored.To address this gap, we establish a last-iterate convergence rate of $O(\\frac{1}{\\epsilon}\\log\\frac{1}{\\epsilon})$ for MFL-SDA. This rate is nearly optimal when compared to the complexity lower bound of its Euclidean counterpart. This rate also matches the complexity of mean-field Langevin stochastic gradient descent for distributional minimization and the outer-loop iteration complexity of an existing double-loop algorithm for distributional minimax problems.By leveraging an elementary analysis framework that avoids PDE-based techniques, we overcome previous limitations and achieve a faster convergence rate.", "poster_file": "43696.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43696.png?t=1751922170.187396", "page_url": "https://icml.cc/virtual/2025/poster/43696", "openreview_url": "https://openreview.net/forum?id=v4DWXM93VV"},
{"title": "Efficiently Vectorized MCMC on Modern Accelerators", "authors": "Hugh Dance , Pierre Glaser , Peter Orbanz , Ryan P. Adams", "abstract": "With the advent of automatic vectorization tools (e.g., JAX's vmap), writing multi-chain MCMC algorithms is often now as simple as invoking those tools on single-chain code. Whilst convenient, for various MCMC algorithms this results in a synchronization problem---loosely speaking, at each iteration all chains running in parallel must wait until the last chain has finished drawing its sample. In this work, we show how to design single-chain MCMC algorithms in a way that avoids synchronization overheads when vectorizing with tools like vmap, by using the framework of finite state machines (FSMs). Using a simplified model, we derive an exact theoretical form of the obtainable speed-ups using our approach, and use it to make principled recommendations for optimal algorithm design. We implement several popular MCMC algorithms as FSMs, including Elliptical Slice Sampling, HMC-NUTS, and Delayed Rejection, demonstrating speed-ups of up to an order of magnitude in experiments.", "poster_file": "45532.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45532.png?t=1754312985.0300448", "page_url": "https://icml.cc/virtual/2025/poster/45532", "openreview_url": "https://openreview.net/forum?id=Mlmpf4Izrj"},
{"title": "PCEvolve: Private Contrastive Evolution for Synthetic Dataset Generation via Few-Shot Private Data and Generative APIs", "authors": "Jianqing Zhang , Yang Liu , Jie Fu , Yang Hua , Tianyuan Zou , Jian Cao , Qiang Yang", "abstract": "The rise of generative APIs has fueled interest in privacy-preserving synthetic data generation. While the Private Evolution (PE) algorithm generates Differential Privacy (DP) synthetic images using diffusion model APIs, it struggles with few-shot private data due to the limitations of its DP-protected similarity voting approach. In practice, the few-shot private data challenge is particularly prevalent in specialized domains like healthcare and industry. To address this challenge, we propose a novel API-assisted algorithm, Private Contrastive Evolution (PCEvolve), which iteratively mines inherent inter-class contrastive relationships in few-shot private data beyond individual data points and seamlessly integrates them into an adapted Exponential Mechanism (EM) to optimize DP’s utility in an evolution loop. We conduct extensive experiments on four specialized datasets, demonstrating that PCEvolve outperforms PE and other API-assisted baselines. These results highlight the potential of leveraging API access with private data for quality evaluation, enabling the generation of high-quality DP synthetic images and paving the way for more accessible and effective privacy-preserving generative API applications. Our code is available at https://github.com/TsingZ0/PCEvolve.", "poster_file": "45741.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45741.png?t=1751005708.420475", "page_url": "https://icml.cc/virtual/2025/poster/45741", "openreview_url": "https://openreview.net/forum?id=IKCfxWtTsu"},
{"title": "An Analysis for Reasoning Bias of Language Models with Small Initialization", "authors": "Junjie Yao , zhongwang zhang , Zhi-Qin John Xu", "abstract": "Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.", "poster_file": "46492.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46492.png?t=1750919949.5554066", "page_url": "https://icml.cc/virtual/2025/poster/46492", "openreview_url": "https://openreview.net/forum?id=4HQaMUYWAT"},
{"title": "Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger", "authors": "Qi Yang , Chenghao Zhang , Lubin Fan , Kun Ding , Jieping Ye , Shiming Xiang", "abstract": "Recent advancements in Large Vision Language Models (LVLMs) have significantly improved performance in Visual Question Answering (VQA) tasks through multimodal Retrieval-Augmented Generation (RAG). However, existing methods still face challenges, such as the scarcity of knowledge with reasoning examples and erratic responses from retrieved knowledge. To address these issues, in this study, we propose a multimodal RAG framework, termed RCTS, which enhances LVLMs by constructing a Reasoning Context-enriched knowledge base and a Tree Search re-ranking method. Specifically, we introduce a self-consistent evaluation mechanism to enrich the knowledge base with intrinsic reasoning patterns.  We further propose a Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples.  This ensures that LVLMs can leverage high-quality contextual reasoning for better and more consistent responses. Extensive experiments demonstrate that our framework achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods. It highlights the effectiveness of our knowledge base and re-ranking method in improving LVLMs.", "poster_file": "46017.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46017.png?t=1752457884.975464", "page_url": "https://icml.cc/virtual/2025/poster/46017", "openreview_url": "https://openreview.net/forum?id=DJcEoC9JpQ"},
{"title": "Position: Human Baselines in Model Evaluations Need Rigor and Transparency (With Recommendations & Reporting Checklist)", "authors": "Kevin Wei , Patricia Paskov , Sunishchal Dev , Michael Byun , Anka Reuel , Xavier Roberts-Gaal , Rachel Calcott , Evie Coxon , Chinmay Deshpande", "abstract": "**In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end.** Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve \"super-human\" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: [https://github.com/kevinlwei/human-baselines](https://github.com/kevinlwei/human-baselines).", "poster_file": "40115.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/40115.png?t=1750941031.9824317", "page_url": "https://icml.cc/virtual/2025/poster/40115", "openreview_url": "https://openreview.net/forum?id=gwhPvu97Gm"},
{"title": "Do We Really Need Message Passing in Brain Network Modeling?", "authors": "Liang Yang , Yuwei Liu , Jiaming Zhuo , Di Jin , Chuan Wang , Zhen Wang , Xiaochun Cao", "abstract": "Brain network analysis plays a critical role in brain disease prediction and diagnosis. Graph mining tools have made remarkable progress. Graph neural networks (GNNs) and Transformers, which rely on the message-passing scheme, recently dominated this field due to their powerful expressive ability on graph data. Unfortunately, by considering brain network construction using pairwise Pearson’s coefficients between any pairs of ROIs, model analysis and experimental verification reveal that *the message-passing under both GNNs and Transformers can NOT be fully explored and exploited*. Surprisingly, this paper observes the significant performance and efficiency enhancements of the Hadamard product compared to the matrix product, which is the matrix form of message passing, in processing the brain network. Inspired by this finding, a novel Brain Quadratic Network (BQN) is proposed by incorporating quadratic networks, which possess better universal approximation properties. Moreover, theoretical analysis demonstrates that BQN implicitly performs community detection along with representation learning. Extensive evaluations verify the superiority of the proposed BQN compared to the message-passing-based brain network modeling. Source code is available at [https://github.com/LYWJUN/BQN-demo](https://github.com/LYWJUN/BQN-demo).", "poster_file": "45642.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45642.png?t=1749799320.2105956", "page_url": "https://icml.cc/virtual/2025/poster/45642", "openreview_url": "https://openreview.net/forum?id=KRosBwvhDx"},
{"title": "MCU: An Evaluation Framework for Open-Ended Game Agents", "authors": "Xinyue Zheng , Haowei Lin , Kaichen He , Zihao Wang , Qiang Fu , Haobo Fu , Zilong Zheng , Yitao Liang", "abstract": "Developing AI agents capable of interacting with open-world environments to solve diverse tasks is a compelling challenge. However, evaluating such open-ended agents remains difficult, with current benchmarks facing scalability limitations. To address this, we introduce \\textit{Minecraft Universe} (MCU), a comprehensive evaluation framework set within the open-world video game Minecraft. MCU incorporates three key components: (1) an expanding collection of 3,452 composable atomic tasks that encompasses 11 major categories and 41 subcategories of challenges; (2) a task composition mechanism capable of generating infinite diverse tasks with varying difficulty; and (3) a general evaluation framework that achieves 91.5\\% alignment with human ratings for open-ended task assessment. Empirical results reveal that even state-of-the-art foundation agents struggle with the increasing diversity and complexity of tasks. These findings highlight the necessity of MCU as a robust benchmark to drive progress in AI agent development within open-ended environments. Our evaluation code and scripts are available at https://github.com/CraftJarvis/MCU.", "poster_file": "44393.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44393.png?t=1752041432.6053023", "page_url": "https://icml.cc/virtual/2025/poster/44393", "openreview_url": "https://openreview.net/forum?id=hrdLhNDAzp"},
{"title": "Doubly Robust Conformalized Survival Analysis with Right-Censored Data", "authors": "Matteo Sesia , vladimir svetnik", "abstract": "We present a conformal inference method for constructing lower prediction bounds for survival times from right-censored data, extending recent approaches designed for more restrictive type-I censoring scenarios. The proposed method imputes unobserved censoring times using a machine learning model, and then analyzes the imputed data using a survival model calibrated via weighted conformal inference. This approach is theoretically supported by an asymptotic double robustness property. Empirical studies on simulated and real data demonstrate that our method leads to relatively informative predictive inferences and is especially robust in challenging settings where the survival model may be inaccurate.", "poster_file": "46585.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46585.png?t=1751916087.9853523", "page_url": "https://icml.cc/virtual/2025/poster/46585", "openreview_url": "https://openreview.net/forum?id=2PWn1LtCwP"},
{"title": "TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation", "authors": "Hyeongwon Jang , Changhun Kim , Eunho Yang", "abstract": "Recent explainable artificial intelligence (XAI) methods for time series primarily estimate point-wise attribution magnitudes, while overlooking the directional impact on predictions, leading to suboptimal identification of significant points. Our analysis shows that conventional Integrated Gradients (IG) effectively capture critical points with both positive and negative impacts on predictions. However, current evaluation metrics fail to assess this capability, as they inadvertently cancel out opposing feature contributions. To address this limitation, we propose novel evaluation metrics—Cumulative Prediction Difference (CPD) and Cumulative Prediction Preservation(CPP)—to systematically assess whether attribution methods accurately identify significant positive and negative points in time series XAI. Under these metrics, conventional IG outperforms recent counterparts. However, directly applying IG to time series data may lead to suboptimal outcomes, as generated paths ignore temporal relationships and introduce out-of-distribution samples. To overcome these challenges, we introduce TIMING, which enhances IG by incorporating temporal awareness while maintaining its theoretical properties. Extensive experiments on synthetic and real-world time series benchmarks demonstrate that TIMING outperforms existing timeseries XAI baselines. Our code is available at https://github.com/drumpt/TIMING.", "poster_file": "43941.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43941.png?t=1752247854.2104716", "page_url": "https://icml.cc/virtual/2025/poster/43941", "openreview_url": "https://openreview.net/forum?id=qOgKMqv9T7"},
{"title": "Robust ML Auditing using Prior Knowledge", "authors": "Jade Garcia Bourrée , Augustin Godinot , Sayan Biswas , Anne-Marie Kermarrec , Erwan Le Merrer , Gilles Tredan , Martijn de Vos , Milos Vujasinovic", "abstract": "Among the many technical challenges to enforcing AI regulations, one crucial yet underexplored problem is the risk of audit manipulation.This manipulation occurs when a platform deliberately alters its answers to a regulator to pass an audit without modifying its answers to other users.In this paper, we introduce a novel approach to manipulation-proof auditing by taking into account the auditor's prior knowledge of the task solved by the platform. We first demonstrate that regulators must not rely on public priors (e.g. a public dataset), as platforms could easily fool the auditor in such cases. We then formally establish the conditions under which an auditor can prevent audit manipulations using prior knowledge about the ground truth. Finally, our experiments with two standard datasets illustrate the maximum level of unfairness a platform can hide before being detected as malicious.Our formalization and generalization of manipulation-proof auditing with a prior opens up new research directions for more robust fairness audits.", "poster_file": "46156.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46156.png?t=1751308451.8033564", "page_url": "https://icml.cc/virtual/2025/poster/46156", "openreview_url": "https://openreview.net/forum?id=AiaVCVDuxF"},
{"title": "No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks", "authors": "Attila Szász , Balázs Bánhelyi , Mark Jelasity", "abstract": "The ultimate goal of verification is to guarantee the safety of deployed neural networks. Here, we claim that all the state-of-the-art verifiers we are aware of fail to reach this goal. Our key insight is that theoretical soundness (bounding the full-precision output while computing with floating point) does not imply practical soundness (bounding the floating point output in a potentially stochastic environment). We prove this observation for the approaches that are currently used to achieve provable theoretical soundness, such as interval analysis and its variants. We also argue that achieving practical soundness is significantly harder computationally. We support our claims empirically as well by evaluating several well-known verification methods. To mislead the verifiers, we create adversarial networks that detect and exploit features of the deployment environment, such as the order and precision of floating point operations. We demonstrate that all the tested verifiers are vulnerable to our new deployment-specific attacks, which proves that they are not practically sound.", "poster_file": "44704.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44704.png?t=1750927503.9364269", "page_url": "https://icml.cc/virtual/2025/poster/44704", "openreview_url": "https://openreview.net/forum?id=c16m2kUTLZ"},
{"title": "Generalized Random Forests Using Fixed-Point Trees", "authors": "David Fleischer , David A Stephens , Archer Yang", "abstract": "We propose a computationally efficient alternative to generalized random forests (GRFs) for estimating heterogeneous effects in large dimensions. While GRFs rely on a gradient-based splitting criterion, which in large dimensions is computationally expensive and unstable, our method introduces a fixed-point approximation that eliminates the need for Jacobian estimation. This gradient-free approach preserves GRF’s theoretical guarantees of consistency and asymptotic normality while significantly improving computational efficiency. We demonstrate that our method achieves a speedup of multiple times over standard GRFs without compromising statistical accuracy. Experiments on both simulated and real-world data validate our approach. Our findings suggest that the proposed method is a scalable alternative for localized effect estimation in machine learning and causal inference applications.", "poster_file": "46613.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46613.png?t=1752730420.5971837", "page_url": "https://icml.cc/virtual/2025/poster/46613", "openreview_url": "https://openreview.net/forum?id=1w0Zp99dnX"},
{"title": "Weakly-Supervised Contrastive Learning for Imprecise Class Labels", "authors": "Zi-Hao Zhou , Jun-Jie Wang , Tong Wei , Min-Ling Zhang", "abstract": "Contrastive learning has achieved remarkable success in learning effective representations, with supervised contrastive learning often outperforming self-supervised approaches. However, in real-world scenarios, data annotations are often ambiguous or inaccurate, meaning that class labels may not reliably indicate whether two examples belong to the same class. This limitation restricts the applicability of supervised contrastive learning. To address this challenge, we introduce the concept of ``continuous semantic similarity'' to define positive and negative pairs. Instead of directly relying on imprecise class labels, we measure the semantic similarity between example pairs, which quantifies how closely they belong to the same category by iteratively refining weak supervisory signals. Based on this concept, we propose a graph-theoretic framework for weakly-supervised contrastive learning, where semantic similarity serves as the graph weights. Our framework is highly versatile and can be applied to many weakly-supervised learning scenarios. We demonstrate its effectiveness through experiments in two common settings, i.e., noisy label and partial label learning, where existing methods can be easily integrated to significantly improve performance. Theoretically, we establish an error bound for our approach, showing that it can approximate supervised contrastive learning under mild conditions. The implementation code is available at [https://github.com/Speechless-10308/WSC](https://github.com/Speechless-10308/WSC).", "poster_file": "44934.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44934.png?t=1751251635.635625", "page_url": "https://icml.cc/virtual/2025/poster/44934", "openreview_url": "https://openreview.net/forum?id=Y19ngWhN0b"},
{"title": "Towards Better-than-2 Approximation for Constrained Correlation Clustering", "authors": "Andreas Kalavas , Evangelos Kipouridis , Nithin Varma", "abstract": "In the Correlation Clustering problem, we are given an undirected graph and are tasked with computing a clustering (partition of the nodes) that minimizes the sum of the number of edges across different clusters and the number of non-edges within clusters. In the constrained version of this problem, the goal is to compute a clustering that satisfies additional hard constraints mandating certain pairs to be in the same cluster and certain pairs to be in different clusters. Constrained Correlation Clustering is APX-Hard, and the best known approximation factor is 3 (van Zuylen et al. [SODA '07]). In this work, we show that in order to obtain a better-than-2 approximation, solving the (exponentially large) Constrained Cluster LP would be sufficient.[The peer-reviewed version of this article claimed an efficient algorithm for solving the Constrained Cluster LP. An error in the proof, that the authors discovered after the review process, led them to revise the results to be conditional on the existence of a valid LP solution.]", "poster_file": "45533.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45533.png?t=1752141036.909242", "page_url": "https://icml.cc/virtual/2025/poster/45533", "openreview_url": "https://openreview.net/forum?id=MkCnPNOLMk"},
{"title": "MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding", "authors": "Zhicheng Zhang , Wuyou Xia , Chenxi Zhao , Zhou Yan , Xiaoqiang Liu , Yongjie Zhu , Wenyu Qin , Pengfei Wan , Di ZHANG , Jufeng Yang", "abstract": "Multimodal large language models (MLLMs) recently showed strong capacity in integrating data among multiple modalities, empowered by generalizable attention architecture. Advanced methods predominantly focus on language-centric tuning while less exploring multimodal tokens mixed through attention, posing challenges in high-level tasks that require fine-grained cognition and emotion understanding. In this work, we identify the attention deficit disorder problem in multimodal learning, caused by inconsistent cross-modal attention and layer-by-layer decayed attention activation. To address this, we propose a novel attention mechanism, termed MOdular Duplex Attention (MODA), simultaneously conducting the inner-modal refinement and inter-modal interaction. MODA employs a correct-after-align strategy to effectively decouple modality alignment from cross-layer token mixing. In the alignment phase, tokens are mapped to duplex modality spaces based on the basis vectors, enabling the interaction between visual and language modality. Further, the correctness of attention scores is ensured through adaptive masked attention, which enhances the model's flexibility by allowing customizable masking patterns for different modalities. Extensive experiments on 21 benchmark datasets verify the effectiveness of MODA in perception, cognition, and emotion tasks.", "poster_file": "46210.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46210.png?t=1752342312.5219991", "page_url": "https://icml.cc/virtual/2025/poster/46210", "openreview_url": "https://openreview.net/forum?id=9hd5WA6QCn"},
{"title": "Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting", "authors": "Jan Schuchardt , Mina Dalirrooyfard , Jed Guzelkabaagac , Anderson Schneider , Yuriy Nevmyvaka , Stephan Günnemann", "abstract": "Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential. The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD). However, we observe in this work that the formal guarantees of DP-SGD are incompatible with time series specific tasks like forecasting, since they rely on the *privacy amplification* attained by training on small, unstructured batches sampled from an unstructured dataset. In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows. We theoretically analyze the privacy amplification attained by this *structured subsampling* to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees. Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models. Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees.", "poster_file": "44722.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44722.png?t=1752160816.6877706", "page_url": "https://icml.cc/virtual/2025/poster/44722", "openreview_url": "https://openreview.net/forum?id=bkauyuzBN4"},
{"title": "CVE-Bench: A Benchmark for AI Agents’ Ability to Exploit Real-World Web Application Vulnerabilities", "authors": "Yuxuan Zhu , Antony Kellermann , Dylan Bowman , Philip Li , Akul Gupta , Adarsh Danda , Richard Fang , Conner Jensen , Eric Ihli , Jason Benn , Jet Geronimo , Avi Dhir , Sudhit Rao , Kaicheng Yu , Twm Stone , Daniel Kang", "abstract": "Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture-the-Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized exper-tise to reproduce exploits and a systematic approach to evaluating unpredictable attacks. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our experiments show that the state-of-the-art agent framework can exploit up to 13% of the vulnerabilities.", "poster_file": "46522.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46522.png?t=1751851631.0326378", "page_url": "https://icml.cc/virtual/2025/poster/46522", "openreview_url": "https://openreview.net/forum?id=3pk0p4NGmQ"},
{"title": "ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation", "authors": "Yupeng Hou , Jianmo Ni , Zhankui He , Noveen Sachdeva , Wang-Cheng Kang , Ed Chi , Julian McAuley , Derek Cheng", "abstract": "Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as a *set* of item features. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Our code is available at: https://github.com/google-deepmind/action_piece.", "poster_file": "44439.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44439.png?t=1752363352.8430848", "page_url": "https://icml.cc/virtual/2025/poster/44439", "openreview_url": "https://openreview.net/forum?id=h2oNQOzbc5"},
{"title": "Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models", "authors": "Yinhong Liu , Zhijiang Guo , Tianya Liang , Ehsan Shareghi , Ivan Vulić , Nigel Collier", "abstract": "Large Language Models (LLMs) are expected to be predictable and trustworthy to support reliable decision-making systems. Yet current LLMs often show inconsistencies in their judgments. In this work, we examine \\textit{logical preference consistency} as a foundational requirement for building more dependable LLM systems, ensuring stable and coherent decision-making while minimizing erratic or contradictory outputs.To quantify the logical preference consistency, we propose a universal evaluation framework based on three fundamental properties: *transitivity*, *commutativity* and *negation invariance*.Through extensive experimentation across diverse LLMs, we demonstrate that these properties serve as strong indicators of judgment robustness.Furthermore, we introduce a data refinement and augmentation technique, REPAIR, that enhances logical consistency while maintaining alignment with human preferences. Finally, we show that improving consistency leads to better performance in LLM-driven logic-based algorithms, reinforcing stability and coherence in decision-making systems.", "poster_file": "45083.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45083.png?t=1752616856.9968169", "page_url": "https://icml.cc/virtual/2025/poster/45083", "openreview_url": "https://openreview.net/forum?id=V61nluxFlR"},
{"title": "Graph Diffusion for Robust Multi-Agent Coordination", "authors": "Xianghua Zeng , Hang Su , Zhengyi Wang , Zhiyuan LIN", "abstract": "Offline multi-agent reinforcement learning (MARL) struggles to estimate out-of-distribution states and actions due to the absence of real-time environmental feedback. While diffusion models show promise in addressing these challenges, their application primarily focuses on independently diffusing the historical trajectories of individual agents, neglecting crucial multi-agent coordination dynamics and reducing policy robustness in dynamic environments. In this paper, we propose MCGD, a novel Multi-agent Coordination framework based on Graph Diffusion models to improve the effectiveness and robustness of collaborative policies. Specifically, we begin by constructing a sparse coordination graph that includes continuous node attributes and discrete edge attributes to effectively identify the underlying dynamics of multi-agent interactions. Next, we derive transition probabilities between edge categories and present adaptive categorical diffusion to capture the structure diversity of multi-agent coordination. Leveraging this coordination structure, we define neighbor-dependent forward noise and develop anisotropic diffusion to enhance the action diversity of each agent. Extensive experiments across various multi-agent environments demonstrate that MCGD significantly outperforms existing state-of-the-art baselines in coordination performance and policy robustness in dynamic environments.", "poster_file": "45189.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45189.png?t=1749001928.910642", "page_url": "https://icml.cc/virtual/2025/poster/45189", "openreview_url": "https://openreview.net/forum?id=T5IZ32ImAB"},
{"title": "LOCATE 3D: Real-World Object Localization via Self-Supervised Learning in 3D", "authors": "Paul McVay , Sergio Arnaud , Ada Martin , Arjun Majumdar , Krishna Murthy Jatavallabhula , Phillip Thomas , Ruslan Partsey , Daniel Dugas , Abha Gejji , Alexander Sax , Vincent-Pierre Berges , Mikael Henaff , Ayush Jain , Ang Cao , Ishita Prasad , Mrinal Kalakrishnan , Michael Rabbat , Nicolas Ballas , Mahmoud Assran , Oleksandr Maksymets , Aravind Rajeswaran , Franziska Meier", "abstract": "We present LOCATE 3D, a model for localizing objects in 3D scenes from referring expressions like \"the small coffee table between the sofa and the lamp.\" LOCATE 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model. Code, models and dataset can be found at the project website: locate3d.atmeta.com", "poster_file": "45895.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45895.png?t=1752351270.5740788", "page_url": "https://icml.cc/virtual/2025/poster/45895", "openreview_url": "https://openreview.net/forum?id=FKi6yjXwCN"},
{"title": "Position: The Categorization of Race in ML is a Flawed Premise", "authors": "Miriam Doh , Benedikt Höltgen , Piera Riccio , Nuria Oliver", "abstract": "This position paper critiques the reliance on rigid racial taxonomies in machine learning, exposing their U.S.-centric nature and lack of global applicability—particularly in Europe, where race categories are not commonly used. These classifications oversimplify racial identity, erasing the experiences of mixed-race individuals and reinforcing outdated essentialist views that contradict the social construction of race. We suggest research agendas in machine learning that move beyond categorical variables to better address discrimination and social inequality.", "poster_file": "40122.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/40122.png?t=1751793145.27045", "page_url": "https://icml.cc/virtual/2025/poster/40122", "openreview_url": "https://openreview.net/forum?id=eI8KegpPyX"},
{"title": "Visual and Domain Knowledge for Professional-level Graph-of-Thought Medical Reasoning", "authors": "Rina Bao , Shilong Dong , Zhenfang Chen , Sheng He , Patricia Ellen Grant , Yangming Ou", "abstract": "Medical Visual Question Answering (MVQA) requires AI models to answer questions related to medical images, offering significant potential to assist medical professionals in evaluating and diagnosing diseases, thereby improving early interventions. However, existing MVQA datasets primarily focus on basic questions regarding visual perception and pattern recognition, without addressing the more complex questions that are critical in clinical diagnosis and decision-making. This paper introduces a new benchmark designed for professional-level medical reasoning, simulating the decision-making process. We achieve this by collecting MRI and clinical data related to Hypoxic-Ischemic Encephalopathy, enriched with expert annotations and insights. Building on this data, we generate clinical question-answer pairs and MRI interpretations to enable comprehensive diagnosis, interpretation, and prediction of neurocognitive outcomes. Our evaluation of current large vision-language models (LVLMs) shows limited performance on this benchmark, highlighting both the challenges of the task and the importance of this benchmark for advancing medical AI. Furthermore, we propose a novel ``Clinical Graph of Thoughts\" model, which integrates domain-specific medical knowledge and clinical reasoning processes with the interpretive abilities of LVLMs. The model demonstrates promising results, achieving around 15\\% absolute gain on the most important neurocognitive outcome task, while the benchmark still reveals substantial opportunities for further research innovation.", "poster_file": "43761.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43761.png?t=1752649694.9848278", "page_url": "https://icml.cc/virtual/2025/poster/43761", "openreview_url": "https://openreview.net/forum?id=tnyxtaSve5"},
{"title": "Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning", "authors": "Shuai Yi , Yixiong Zou , Yuhua Li , Ruixuan Li", "abstract": "Vision Transformer (ViT) has achieved remarkable success due to its large-scale pretraining on general domains, but it still faces challenges when applying it to downstream distant domains that have only scarce training data, which gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired by Self-Attention's insensitivity to token orders, we find an interesting phenomenon neglected in current works: disrupting the continuity of image tokens (i.e., making pixels not smoothly transited across patches) in ViT leads to a noticeable performance decline in the general (source) domain but only a marginal decrease in downstream target domains. This questions the role of image tokens' continuity in ViT's generalization under large domain gaps. In this paper, we delve into this phenomenon for an interpretation. We find continuity aids ViT in learning larger spatial patterns, which are harder to transfer than smaller ones, enlarging domain distances. Meanwhile, it implies that only smaller patterns within each patch could be transferred under extreme domain gaps. Based on this interpretation, we further propose a simple yet effective method for CDFSL that better disrupts the continuity of image tokens, encouraging the model to rely less on large patterns and more on smaller ones. Extensive experiments show the effectiveness …", "poster_file": "45416.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45416.png?t=1751980216.0145645", "page_url": "https://icml.cc/virtual/2025/poster/45416", "openreview_url": "https://openreview.net/forum?id=OpineZj5bj"},
{"title": "Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection", "authors": "Matteo Zecchin , Sangwoo Park , Osvaldo Simeone", "abstract": "We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter selection procedure that provides finite-sample statistical guarantees on the population risk of AI models. Unlike the existing learn-then-test (LTT) technique, which relies on conventional p-value-based multiple hypothesis testing (MHT), aLTT implements sequential data-dependent MHT with early termination by leveraging e-processes. As a result, aLTT can reduce the number of testing rounds, making it particularly well-suited for scenarios in which testing is costly or presents safety risks. Apart from maintaining statistical validity, in applications such as online policy selection for offline reinforcement learning and prompt engineering, aLTT is shown to achieve the same performance as LTT while requiring only a fraction of the testing rounds.", "poster_file": "45663.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45663.png?t=1752387704.5853503", "page_url": "https://icml.cc/virtual/2025/poster/45663", "openreview_url": "https://openreview.net/forum?id=JxnOZwFNcU"},
{"title": "RE-Bench: Evaluating Frontier AI R&D Capabilities of Language Model Agents against Human Experts", "authors": "Hjalmar Wijk , Tao Lin , Joel Becker , Sami Jawhar , Neev Parikh , Thomas Broadley , Lawrence Chan , Michael Chen , Joshua Clymer , Jai Dhyani , Elena Ericheva , Katharyn Garcia , Brian Goodrich , Nikola Jurkovic , Megan Kinniment , Aron Lajko , Seraphina Nix , Lucas Jun Koba Sato , William Saunders , Maksym Taran , Ben West , Elizabeth Barnes", "abstract": "Frontier AI safety policies highlight automation of AI research and development (R&D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, V1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-$k$ with varying time budgets and agent designs, and find that the best AI agents achieve a score 4× higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2× the score of the top AI agent when both are given 32 total hours (across different attempts).", "poster_file": "46519.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46519.png?t=1751126394.46102", "page_url": "https://icml.cc/virtual/2025/poster/46519", "openreview_url": "https://openreview.net/forum?id=3rB0bVU6z6"},
{"title": "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry", "authors": "Chi-Ning Chou , Hang Le , Yichen Wang , SueYeon Chung", "abstract": "Integrating task-relevant information into neural representations is a fundamental ability of both biological and artificial intelligence systems. Recent theories have categorized learning into two regimes: the rich regime, where neural networks actively learn task-relevant features, and the lazy regime, where networks behave like random feature models. Yet this simple lazy–rich dichotomy overlooks a diverse underlying taxonomy of feature learning, shaped by differences in learning algorithms, network architectures, and data properties. To address this gap, we introduce an analysis framework to study feature learning via the geometry of neural representations. Rather than inspecting individual learned features, we characterize how task-relevant representational manifolds evolve throughout the learning process. We show, in both theoretical and empirical settings, that as networks learn features, task-relevant manifolds untangle, with changes in manifold geometry revealing distinct learning stages and strategies beyond the lazy–rich dichotomy. This framework provides novel insights into feature learning across neuroscience and machine learning, shedding light on structural inductive biases in neural circuits and the mechanisms underlying out-of-distribution generalization.", "poster_file": "44480.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44480.png?t=1751304530.4435544", "page_url": "https://icml.cc/virtual/2025/poster/44480", "openreview_url": "https://openreview.net/forum?id=gKdjHLrHDS"},
{"title": "P(all-atom) Is Unlocking New Path For Protein Design", "authors": "Wei Qu , Jiawei Guan , Rui Ma , kezhai , weikun wu , haobo Wang", "abstract": "We introduce Pallatom, an innovative protein generation model capable of producing protein structures with all-atom coordinates. Pallatom directly learns and models the joint distribution $P(\\textit{structure}, \\textit{seq})$ by focusing on $P(\\textit{all-atom})$, effectively addressing the interdependence between sequence and structure in protein generation. To achieve this, we propose a novel network architecture specifically designed for all-atom protein generation. Our model employs a dual-track framework that tokenizes proteins into token-level and atomic-level representations, integrating them through a multi-layer decoding process with \"traversing\" representations and recycling mechanism. We also introduce the $\\texttt{atom14}$ representation method, which unifies the description of unknown side-chain coordinates, ensuring high fidelity between the generated all-atom conformation and its physical structure. Experimental results demonstrate that Pallatom excels in key metrics of protein design, including designability, diversity, and novelty, showing significant improvements across the board. Our model not only enhances the accuracy of protein generation but also exhibits excellent sampling efficiency, paving the way for future applications in larger and more complex systems.", "poster_file": "43525.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43525.png?t=1749963791.9066968", "page_url": "https://icml.cc/virtual/2025/poster/43525", "openreview_url": "https://openreview.net/forum?id=yXRixu0ONY"},
{"title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models", "authors": "Mahir Labib Dihan , Tanvir Hassan , Md Tanvir Parvez , Md Hasebul Hasan , Almash Alam , Muhammad Aamir Cheema , Mohammed Eunus Ali , Md Rizwan Parvez", "abstract": "Recent advancements in foundation models have improved autonomous tool usage and reasoning, but their capabilities in map-based reasoning remain underexplored. To address this, we introduce MapEval, a benchmark designed to assess foundation models across three distinct tasks—textual, API-based, and visual reasoning—through 700 multiple-choice questions spanning 180 cities and 54 countries, covering spatial relationships, navigation, travel planning, and real-world map interactions. Unlike prior benchmarks that focus on simple location queries, MapEval requires models to handle long-context reasoning, API interactions and visual map analysis, making it the most comprehensive evaluation framework for geospatial AI.  On evaluation of 30 foundation models, including Claude-3.5-Sonnet, GPT-4o, Gemini-1.5-Pro, none surpasses 67% accuracy, with open-source models performing significantly worse and all models lagging over 20% behind human performance. These results expose critical gaps in spatial inference, as models struggle with distances, directions, route planning, and place-specific reasoning, highlighting the need for better geospatial AI to bridge the gap between foundation models and real-world navigation.", "poster_file": "44415.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44415.png?t=1752511457.179138", "page_url": "https://icml.cc/virtual/2025/poster/44415", "openreview_url": "https://openreview.net/forum?id=hS2Ed5XYRq"},
{"title": "SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming", "authors": "Hong-Ming Chiu , Hao Chen , Huan Zhang , Richard Zhang", "abstract": "Neural network verifiers based on linear bound propagation scale impressively to massive models but can be surprisingly loose when neuron coupling is crucial. Conversely, semidefinite programming (SDP) verifiers capture inter-neuron coupling naturally, but their cubic complexity restricts them to only small models. In this paper, we propose SDP-CROWN, a novel hybrid verification framework that combines the tightness of SDP relaxations with the scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new linear bound---derived via SDP principles---that explicitly captures $\\ell_{2}$-norm-based inter-neuron coupling while adding only one extra parameter per layer. This bound can be integrated seamlessly into any linear bound-propagation pipeline, preserving the inherent scalability of such methods yet significantly improving tightness. In theory, we prove that our inter-neuron bound can be up to a factor of $\\sqrt{n}$ tighter than traditional per-neuron bounds. In practice, when incorporated into the state-of-the-art $\\alpha$-CROWN verifier, we observe markedly improved verification performance on large models with up to 65 thousand neurons and 2.47 million parameters, achieving tightness that approaches that of costly SDP-based methods.", "poster_file": "46410.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46410.png?t=1752445765.8850422", "page_url": "https://icml.cc/virtual/2025/poster/46410", "openreview_url": "https://openreview.net/forum?id=5liHhkgvAn"},
{"title": "Independence Tests for Language Models", "authors": "Sally Zhu , Ahmed Ahmed , Rohith Kuditipudi , Percy Liang", "abstract": "Motivated by liability and intellectual property concerns over open-weight models we consider the following problem: given the weights of two models, can we test whether they were trained independently---i.e., from independent random initializations? We consider two settings: *constrained* and *unconstrained*. In the constrained setting, we make assumptions about model architecture and training and propose statistical tests that yield exact p-values with respect to the null hypothesis that the models are trained from independent random initializations. We compute the p-values by simulating exchangeable copies of each model under our assumptions and comparing various similarity measures between the original two models versus these copies. We report p-values on pairs of 21 open-weight models (210 total pairs) and find we correctly identify all pairs of non-independent models. In the unconstrained setting we make none of the prior assumptions and allow for adversarial evasion attacks that do not change model output. We thus propose a new test which matches hidden activations between two models, which is robust to these transformations and to changes in model architecture and can also identify specific non-independent components of models. Though we no longer obtain exact p-values from this test, empirically we find it reliably distinguishes non-independent models like …", "poster_file": "44127.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44127.png?t=1752431036.1876252", "page_url": "https://icml.cc/virtual/2025/poster/44127", "openreview_url": "https://openreview.net/forum?id=mzSwYvwYdC"},
{"title": "PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling", "authors": "Avery Ma , Yangchen Pan , Amir-massoud Farahmand", "abstract": "Many-shot jailbreaking circumvents the safety alignment of LLMs by exploiting their ability to process long input sequences. To achieve this, the malicious target prompt is prefixed with hundreds of fabricated conversational exchanges between the user and the model. These exchanges are randomly sampled from a pool of unsafe question-answer pairs, making it appear as though the model has already complied with harmful instructions. In this paper, we present PANDAS: a hybrid technique that improves many-shot jailbreaking by modifying these fabricated dialogues with Positive Affirmations, Negative Demonstrations, and an optimized Adaptive Sampling method tailored to the target prompt's topic. We also introduce ManyHarm, a dataset of harmful question–answer pairs, and demonstrate through extensive experiments that PANDAS significantly outperforms baseline methods in long-context scenarios. Through attention analysis, we provide insights into how long-context vulnerabilities are exploited and show how PANDAS further improves upon many-shot jailbreaking.", "poster_file": "43847.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43847.png?t=1751081691.2253635", "page_url": "https://icml.cc/virtual/2025/poster/43847", "openreview_url": "https://openreview.net/forum?id=sEBfiF8JBu"},
{"title": "Learning Soft Sparse Shapes for Efficient Time-Series Classification", "authors": "Zhen Liu , Yicheng Luo , Boyuan Li , Emadeldeen Eldele , Min Wu , Qianli Ma", "abstract": "Shapelets are discriminative subsequences (or shapes) with high interpretability in time series classification. Due to the time-intensive nature of shapelet discovery, existing shapelet-based methods mainly focus on selecting discriminative shapes while discarding others to achieve candidate subsequence sparsification. However, this approach may exclude beneficial shapes and overlook the varying contributions of shapelets to classification performance. To this end, we propose a Soft sparse Shapes (SoftShape) model for efficient time series classification. Our approach mainly introduces soft shape sparsification and soft shape learning blocks. The former transforms shapes into soft representations based on classification contribution scores, merging lower-scored ones into a single shape to retain and differentiate all subsequence information. The latter facilitates intra- and inter-shape temporal pattern learning, improving model efficiency by using sparsified soft shapes as inputs. Specifically, we employ a learnable router to activate a subset of class-specific expert networks for intra-shape pattern learning. Meanwhile, a shared expert network learns inter-shape patterns by converting sparsified shapes into sequences. Extensive experiments show that SoftShape outperforms state-of-the-art methods and produces interpretable results.", "poster_file": "46130.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46130.png?t=1750476139.0266078", "page_url": "https://icml.cc/virtual/2025/poster/46130", "openreview_url": "https://openreview.net/forum?id=B9DOjtj9xK"},
{"title": "Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred Datapoints", "authors": "Sam Bowyer , Laurence Aitchison , Desi Ivanova", "abstract": "Rigorous statistical evaluations of large language models (LLMs), including valid error bars and significance testing, are essential for meaningful and reliable performance assessment. Currently, when such statistical measures are reported, they typically rely on the Central Limit Theorem (CLT). In this position paper, we argue that while CLT-based methods for uncertainty quantification are appropriate when benchmarks consist of thousands of examples, they fail to provide adequate uncertainty estimates for LLM evaluations that rely on smaller, highly specialized benchmarks. In these small-data settings, we demonstrate that CLT-based methods perform very poorly, usually dramatically underestimating uncertainty (i.e. producing error bars that are too small). We give recommendations for alternative frequentist and Bayesian methods that are both easy to implement and more appropriate in these increasingly common scenarios.", "poster_file": "40132.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/40132.png?t=1751884008.6283898", "page_url": "https://icml.cc/virtual/2025/poster/40132", "openreview_url": "https://openreview.net/forum?id=YhZ2PY2nZa"},
{"title": "Covered Forest: Fine-grained generalization analysis of graph neural networks", "authors": "Antonis Vasileiou , Ben Finkelshtein , Floris Geerts , Ron Levie , Christopher Morris", "abstract": "The expressive power of message-passing graph neural networks (MPNNs) is reasonably well understood, primarily through combinatorial techniques from graph isomorphism testing. However, MPNNs' generalization abilities---making meaningful predictions beyond the training set---remain less explored. Current generalization analyses often overlook graph structure, limit the focus to specific aggregation functions, and assume the impractical, hard-to-optimize $0$-$1$ loss function. Here, we extend recent advances in graph similarity theory to assess the influence of graph structure, aggregation, and loss functions on MPNNs' generalization abilities. Our empirical study supports our theoretical insights, improving our understanding of MPNNs' generalization properties.", "poster_file": "43559.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43559.png?t=1750061006.9827547", "page_url": "https://icml.cc/virtual/2025/poster/43559", "openreview_url": "https://openreview.net/forum?id=xvLVYrYQ8a"},
{"title": "Better to Teach than to Give: Domain Generalized Semantic Segmentation via Agent Queries with Diffusion Model Guidance", "authors": "Fan Li , Xuan Wang , Min Qi , Zhaoxiang Zhang , yuelei xu", "abstract": "Domain Generalized Semantic Segmentation (DGSS) trains a model on a labeled source domain to generalize to unseen target domains with consistent contextual distribution and varying visual appearance.Most existing methods rely on domain randomization or data generation but struggle to capture the underlying scene distribution, resulting in the loss of useful semantic information. Inspired by the diffusion model's capability to generate diverse variations within a given scene context, we consider harnessing its rich prior knowledge of scene distribution to tackle the challenging DGSS task.In this paper, we propose a novel agent \\textbf{Query}-driven learning framework based on \\textbf{Diff}usion model guidance for DGSS, named QueryDiff. Our recipe comprises three key ingredients: (1) generating agent queries from segmentation features to aggregate semantic information about instances within the scene; (2) learning the inherent semantic distribution of the scene through agent queries guided by diffusion features; (3) refining segmentation features using optimized agent queries for robust mask predictions.Extensive experiments across various settings demonstrate that our method significantly outperforms previous state-of-the-art methods. Notably, it enhances the model's ability to generalize effectively to extreme domains, such as cubist art styles. Code is available at https://github.com/FanLiHub/QueryDiff.", "poster_file": "44281.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44281.png?t=1750926968.6238697", "page_url": "https://icml.cc/virtual/2025/poster/44281", "openreview_url": "https://openreview.net/forum?id=jvP1wbD0xh"},
{"title": "Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models", "authors": "Ulzee An , Moonseong Jeong , Simon Lee , Aditya Gorla , Yuzhe Yang , Sriram Sankararaman", "abstract": "Current challenges in developing foundational models for volumetric imaging data, such as magnetic resonance imaging (MRI), stem from the computational complexity of state-of-the-art architectures in high dimensions and curating sufficiently large datasets of volumes.To address these challenges, we introduce Raptor (Random Planar Tensor Reduction), a train-free method for generating semantically rich embeddings for volumetric data. Raptor leverages a frozen 2D foundation model, pretrained on natural images, to extract visual tokens from individual cross-sections of medical volumes. These tokens are then spatially compressed using random projections, significantly reducing computational complexity while retaining rich semantic information. Extensive experiments on 10 diverse medical volume tasks verify the superior performance of Raptor over state-of-the-art methods, including those pretrained exclusively on medical volumes (+3 SuPreM, +6 MISFM, +10 Merlin, +13 VoCo, and +14 SLIViT), while entirely bypassing the need for costly training. Our results highlight Raptor's effectiveness and versatility as a foundation for advancing deep learning-based methods for medical volumes (code: github.com/sriramlab/raptor).", "poster_file": "46452.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46452.png?t=1752432232.3020704", "page_url": "https://icml.cc/virtual/2025/poster/46452", "openreview_url": "https://openreview.net/forum?id=4yHWV3B6g4"},
{"title": "Nonparametric Teaching for Graph Property Learners", "authors": "Chen Zhang , Weixin Bu , Zeyi Ren , Zhengwu Liu , Yik-Chung WU , Ngai Wong", "abstract": "Inferring properties of graph-structured data, *e.g.*, the solubility of molecules, essentially involves learning the implicit mapping from graphs to their properties. This learning process is often costly for graph property learners like Graph Convolutional Networks (GCNs). To address this, we propose a paradigm called Graph Nonparametric Teaching (GraNT) that reinterprets the learning process through a novel nonparametric teaching perspective. Specifically, the latter offers a theoretical framework for teaching implicitly defined (*i.e.*, nonparametric) mappings via example selection. Such an implicit mapping is realized by a dense set of graph-property pairs, with the GraNT teacher selecting a subset of them to promote faster convergence in GCN training. By analytically examining the impact of graph structure on parameter-based gradient descent during training, and recasting the evolution of GCNs—shaped by parameter updates—through functional gradient descent in nonparametric teaching, we show *for the first time* that teaching graph property learners (*i.e.*, GCNs) is consistent with teaching structure-aware nonparametric learners. These new findings readily commit GraNT to enhancing learning efficiency of the graph property learner, showing significant reductions in training time for graph-level regression (-36.62\\%), graph-level classification (-38.19\\%), node-level regression (-30.97\\%) and node-level classification (-47.30\\%), all while maintaining its generalization performance.", "poster_file": "43614.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43614.png?t=1747738489.7948136", "page_url": "https://icml.cc/virtual/2025/poster/43614", "openreview_url": "https://openreview.net/forum?id=wbvshlfyB0"},
{"title": "Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance", "authors": "Linxi Zhao , Yihe Deng , Weitong Zhang , Quanquan Gu", "abstract": "The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs to rectify the outputs of LVLMs. However, these approaches require either costly training or fine-tuning, or API access to proprietary LLMs for post-generation correction. In response to these limitations, we propose Mitigating hallucinAtion via image-gRounded guIdaNcE (MARINE), a framework that is both training-free and API-free. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across 5 popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations. We release our code at https://github.com/Linxi-ZHAO/MARINE.", "poster_file": "43644.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43644.png?t=1752076403.4017875", "page_url": "https://icml.cc/virtual/2025/poster/43644", "openreview_url": "https://openreview.net/forum?id=w0xYx9CJhY"},
{"title": "Understanding and Mitigating Memorization in Generative Models via Sharpness of Probability Landscapes", "authors": "Dongjae Jeon , Dueun Kim , Albert No", "abstract": "In this paper, we introduce a geometric framework to analyze memorization in diffusion models through the sharpness of the log probability density. We mathematically justify a previously proposed score-difference-based memorization metric by demonstrating its effectiveness in quantifying sharpness. Additionally, we propose a novel memorization metric that captures sharpness at the initial stage of image generation in latent diffusion models, offering early insights into potential memorization. Leveraging this metric, we develop a mitigation strategy that optimizes the initial noise of the generation process using a sharpness-aware regularization term.", "poster_file": "45941.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45941.png?t=1751948501.2067964", "page_url": "https://icml.cc/virtual/2025/poster/45941", "openreview_url": "https://openreview.net/forum?id=EW2JR5aVLm"},
{"title": "Algorithms with Calibrated Machine Learning Predictions", "authors": "Judy Hanwen Shen , Ellen Vitercik , Anders Wikum", "abstract": "The field of *algorithms with predictions* incorporates machine learning advice in the design of online algorithms to improve real-world performance. A central consideration is the extent to which predictions can be trusted—while existing approaches often require users to specify an aggregate trust level, modern machine learning models can provide estimates of prediction-level uncertainty. In this paper, we propose *calibration* as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the *ski rental* and *online job scheduling* problems. For ski rental, we design an algorithm that achieves near-optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions.", "poster_file": "45433.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45433.png?t=1751678448.8562975", "page_url": "https://icml.cc/virtual/2025/poster/45433", "openreview_url": "https://openreview.net/forum?id=Obet2x6GNl"},
{"title": "scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data", "authors": "Olga Ovcharenko , Florian Barkmann , Philip Toma , Imant Daunhawer , Julia Vogt , Sebastian Schelter , Valentina Boeva", "abstract": "Self-supervised learning (SSL) has proven to be a powerful approach for extracting biologically meaningful representations from single-cell data. To advance our understanding of SSL methods applied to single-cell data, we present scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL methods. Our evaluation spans nine datasets and focuses on three common downstream tasks: batch correction, cell type annotation, and missing modality prediction. Furthermore, we systematically assess various data augmentation strategies. Our analysis reveals task-specific trade-offs: the specialized single-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at uni-modal batch correction, while generic SSL methods, such as VICReg and SimCLR, demonstrate superior performance in cell typing and multi-modal data integration. Random masking emerges as the most effective augmentation technique across all tasks, surpassing domain-specific augmentations. Notably, our results indicate the need for a specialized single-cell multi-modal data integration framework. scSSL-Bench provides a standardized evaluation platform and concrete recommendations for applying SSL to single-cell analysis, advancing the convergence of deep learning and single-cell genomics.", "poster_file": "44286.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44286.png?t=1751836510.2955432", "page_url": "https://icml.cc/virtual/2025/poster/44286", "openreview_url": "https://openreview.net/forum?id=jnPHZqcUdn"},
{"title": "Is Complex Query Answering Really Complex?", "authors": "Cosimo Gregucci , Bo Xiong , Daniel Hernández , Lorenzo Loconte , Pasquale Minervini , Steffen Staab , Antonio Vergari", "abstract": "Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum as a challenging reasoning task.In this paper, we show that the current benchmarks for CQA might not be as *complex* as we think, as the way they are built distorts our perception of progress in this field.For example, we find that in these benchmarks most queries (up to 98% for some query types) can be reduced to simpler problems, e.g., link prediction, where only one link needs to be predicted.The performance of state-of-the-art CQA models decreses significantly when such models are evaluated on queries that cannot be reduced to easier types.Thus, we propose a set of more challenging benchmarks composed of queries that *require* models to reason over multiple hops and better reflect the construction of real-world KGs.In a systematic empirical investigation, the new benchmarks show that current methods leave much to be desired from current CQA methods.", "poster_file": "45905.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45905.png?t=1752074202.8243787", "page_url": "https://icml.cc/virtual/2025/poster/45905", "openreview_url": "https://openreview.net/forum?id=F8NTPAz5HH"},
{"title": "From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models", "authors": "Etowah Adams , Liam Bai , Minji Lee , Yiyang Yu , Mohammed AlQuraishi", "abstract": "Protein language models (pLMs) are powerful predictors of protein structure and function, learning through unsupervised training on millions of protein sequences. pLMs are thought to capture common motifs in protein sequences, but the specifics of pLM features are not well understood. Identifying these features would not only shed light on how pLMs work, but potentially uncover novel protein biology––studying the model to study the biology. Motivated by this, we train sparse autoencoders (SAEs) on the residual stream of a pLM, ESM-2. By characterizing SAE features, we determine that pLMs use a combination of generic features and family-specific features to represent a protein. In addition, we demonstrate how known sequence determinants of properties such as thermostability and subcellular localization can be identified by linear probing of SAE features. For predictive features without known functional associations, we hypothesize their role in unknown mechanisms and provide visualization tools to aid their interpretation. Our study gives a better understanding of the limitations of pLMs, and demonstrates how SAE features can be used to help generate hypotheses for biological mechanisms. We release our code, model weights, and feature visualizer.", "poster_file": "43465.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43465.png?t=1752180147.0966928", "page_url": "https://icml.cc/virtual/2025/poster/43465", "openreview_url": "https://openreview.net/forum?id=zdOGBRQEbz"},
{"title": "Decision Making under the Exponential Family: Distributionally Robust Optimisation with Bayesian Ambiguity Sets", "authors": "Charita Dellaporta , Patrick O'Hara , Theodoros Damoulas", "abstract": "Decision making under uncertainty is challenging as the data-generating process (DGP) is often unknown. Bayesian inference proceeds by estimating the DGP through posterior beliefs on the model’s parameters. However, minimising the expected risk under these beliefs can lead to suboptimal decisions due to model uncertainty or limited, noisy observations. To address this, we introduce Distributionally Robust Optimisation with Bayesian Ambiguity Sets (DRO-BAS) which hedges against model uncertainty by optimising the worst-case risk over a posterior-informed ambiguity set. We provide two such sets, based on the posterior expectation (DRO-BAS(PE)) or the posterior predictive (DRO-BAS(PP)) and prove that both admit, under conditions, strong dual formulations leading to efficient single-stage stochastic programs which are solved with a sample average approximation. For DRO-BAS(PE), this covers all conjugate exponential family members while for DRO-BAS(PP) this is shown under conditions on the predictive's moment generating function. Our DRO-BAS formulations outperform existing Bayesian DRO on the Newsvendor problem and achieve faster solve times with comparable robustness on the Portfolio problem.", "poster_file": "43906.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43906.png?t=1751968685.7588665", "page_url": "https://icml.cc/virtual/2025/poster/43906", "openreview_url": "https://openreview.net/forum?id=r9HlTuCQfr"},
{"title": "Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner", "authors": "Chunhui Zhang , Zhongyu Ouyang , Kwonjoon Lee , Nakul Agarwal , Sean Houlihan , Soroush Vosoughi , Shao-Yuan Lo", "abstract": "Theory-of-mind (ToM) enables humans to infer mental states—such as beliefs, desires, and intentions—forming the foundation of social cognition. Existing computational ToM methods rely on structured workflows with ToM-specific priors or deep model fine-tuning but struggle with scalability in multimodal environments. They remain trapped within the gravitational pull of multi-step planning complexity, failing to generalize as task demands increase. To overcome these limitations, we propose a scalable Bayesian ToM planner. It breaks down ToM complexity into stepwise Bayesian updates. Meanwhile, weak-to-strong control specializes smaller LMs to refine ToM-specific likelihood estimation, transferring their ToM reasoning behavior to larger LMs (7B to 405B) for social and world knowledge integration. This synergistic approach enables scalability, aligning large-model inference with human mental states with Bayesian principles. Extensive experiments demonstrate a 4.6% improvement in accuracy over state-of-the-art methods on multimodal ToM benchmarks, including unseen scenarios, establishing a new standard for modeling human mental states in complex environments.", "poster_file": "46576.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46576.png?t=1752484977.1543176", "page_url": "https://icml.cc/virtual/2025/poster/46576", "openreview_url": "https://openreview.net/forum?id=2dz6psiiA0"},
{"title": "Learning Parametric Distributions from Samples and Preferences", "authors": "Marc Jourdan , Gizem Yüce , Nicolas Flammarion", "abstract": "Recent advances in language modeling have underscored the role of preference feedback in enhancing model performance. This paper investigates the conditions under which preference feedback improves parameter estimation in classes of continuous parametric distributions. In our framework, the learner observes pairs of samples from an unknown distribution along with their relative preferences depending on the same unknown parameter. We show that preferences-based M-estimators achieve a better asymptotic variance than sample-only M-estimators, further improved by deterministic preferences. Leveraging the hard constraints revealed by deterministic preferences, we propose an estimator achieving an estimation error scaling of $\\mathcal{O}(1/n)$---a significant improvement over the $\\Theta(1/\\sqrt{n})$ rate attainable with samples alone. Next, we establish a lower bound that matches this accelerated rate; up to problem-dependent constants. While the assumptions underpinning our analysis are restrictive, they are satisfied by notable cases such as Gaussian or Laplace distributions for preferences based on the log-probability reward.", "poster_file": "45822.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45822.png?t=1751964141.9269783", "page_url": "https://icml.cc/virtual/2025/poster/45822", "openreview_url": "https://openreview.net/forum?id=GbJqQsIwJu"},
{"title": "FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields", "authors": "Gwanhyeong Koo , Sunjae Yoon , Younghwan Lee , Ji Woo Hong , Chang Yoo", "abstract": "Drag-based editing allows precise object manipulation through point-based control, offering user convenience. However, current methods often suffer from a geometric inconsistency problem by focusing exclusively on matching user-defined points, neglecting the broader geometry and leading to artifacts or unstable edits. We propose FlowDrag, which leverages geometric information for more accurate and coherent transformations. Our approach constructs a 3D mesh from the image, using an energy function to guide mesh deformation based on user-defined drag points. The resulting mesh displacements are projected into 2D and incorporated into a UNet denoising process, enabling precise handle-to-target point alignment while preserving structural integrity. Additionally, existing drag-editing benchmarks provide no ground truth, making it difficult to assess how accurately the edits match the intended transformations. To address this, we present VFD (VidFrameDrag) benchmark dataset, which provides ground-truth frames using consecutive shots in a video dataset. FlowDrag outperforms existing drag-based editing methods on both VFD Bench and DragBench.", "poster_file": "43848.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43848.png?t=1752477709.4670188", "page_url": "https://icml.cc/virtual/2025/poster/43848", "openreview_url": "https://openreview.net/forum?id=sDK6bSmHgM"},
{"title": "Flopping for FLOPs: Leveraging Equivariance for Computational Efficiency", "authors": "Georg Bökman , David Nordström , Fredrik Kahl", "abstract": "Incorporating geometric invariance into neural networks enhances parameter efficiency but typically increases computational costs.This paper introduces new equivariant neural networksthat preserve symmetry while maintaining a comparable number of floating-point operations (FLOPs) per parameter to standard non-equivariant networks. We focus on horizontal mirroring (flopping) invariance, common in many computer vision tasks.The main idea is to parametrize the feature spaces in terms of mirror-symmetric and mirror-antisymmetric features, i.e., irreps of the flopping group.This decomposes the linear layers to be block-diagonal, requiring half the number of FLOPs.Our approach reduces both FLOPs and wall-clock time,providing a practical solution for efficient, scalable symmetry-aware architectures.", "poster_file": "45974.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45974.png?t=1750173362.9416957", "page_url": "https://icml.cc/virtual/2025/poster/45974", "openreview_url": "https://openreview.net/forum?id=DzLP43CbiX"},
{"title": "Local Identifying Causal Relations in the Presence of Latent Variables", "authors": "Zheng Li , Zeyu Liu , Feng Xie , Hao Zhang , Chunchen LIU , zhi geng", "abstract": "We tackle the problem of identifying whether a variable is the cause of a specified target using observational data. State-of-the-art causal learning algorithms that handle latent variables typically rely on identifying the global causal structure, often represented as a partial ancestral graph (PAG), to infer causal relationships. Although effective, these approaches are often redundant and computationally expensive when the focus is limited to a specific causal relationship. In this work, we introduce novel local characterizations that are necessary and sufficient for various types of causal relationships between two variables, enabling us to bypass the need for global structure learning. Leveraging these local insights, we develop efficient and fully localized algorithms that accurately identify causal relationships from observational data. We theoretically demonstrate the soundness and completeness of our approach. Extensive experiments on benchmark networks and real-world datasets further validate the effectiveness and efficiency of our method.", "poster_file": "45459.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45459.png?t=1750994642.8750315", "page_url": "https://icml.cc/virtual/2025/poster/45459", "openreview_url": "https://openreview.net/forum?id=O6q2BHK1BL"},
{"title": "Reducing Variance of Stochastic Optimization for Approximating Nash Equilibria in Normal-Form Games", "authors": "Linjian Meng , Wubing Chen , Wenbin Li , Tianpei Yang , Youzhi Zhang , Yang Gao", "abstract": "Nash equilibrium (NE) plays an important role in game theory. How to efficiently compute an NE in NFGs is challenging due to its complexity and non-convex optimization property. Machine Learning (ML), the cornerstone of modern artificial intelligence, has demonstrated remarkable empirical performance across various applications including non-convex optimization. To leverage non-convex stochastic optimization techniques from ML for approximating an NE, various loss functions have been proposed. Among these, only one loss function is unbiased, allowing for unbiased estimation under the sampled play. Unfortunately, this loss function suffers from high variance, which degrades the convergence rate. To improve the convergence rate by mitigating the high variance associated with the existing unbiased loss function, we propose a novel surrogate loss function named Nash Advantage Loss (NAL). NAL is theoretically proved unbiased and exhibits significantly lower variance than the existing unbiased loss function. Experimental results demonstrate that the algorithm minimizing NAL achieves a significantly faster empirical convergence rates compared to other algorithms, while also reducing the variance of estimated loss value by several orders of magnitude.", "poster_file": "45762.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45762.png?t=1750409785.743339", "page_url": "https://icml.cc/virtual/2025/poster/45762", "openreview_url": "https://openreview.net/forum?id=Hp53p5AU7X"},
{"title": "Position: Algebra Unveils Deep Learning - An Invitation to Neuroalgebraic Geometry", "authors": "Giovanni Luca Marchetti , Vahid Shahverdi , Stefano Mereta , Matthew Trager , Kathlén Kohn", "abstract": "In this position paper, we promote the study of function spaces parameterized by machine learning models through the lens of algebraic geometry. To this end, we focus on algebraic models, such as neural networks with polynomial activations, whose associated function spaces are semi-algebraic varieties. We outline a dictionary between algebro-geometric invariants of these varieties, such as dimension, degree, and singularities, and fundamental aspects of machine learning, such as sample complexity, expressivity, training dynamics, and implicit bias. Along the way, we review the literature and discuss ideas beyond the algebraic domain. This work lays the foundations of a research direction bridging algebraic geometry and deep learning, that we refer to as neuroalgebraic geometry.", "poster_file": "40106.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/40106.png?t=1750945257.7018886", "page_url": "https://icml.cc/virtual/2025/poster/40106", "openreview_url": "https://openreview.net/forum?id=mzc1KPkIMJ"},
{"title": "Position: In-House Evaluation Is Not Enough. Towards Robust Third-Party Evaluation and Flaw Disclosure for General-Purpose AI", "authors": "Shayne Longpre , Kevin Klyman , Ruth Elisabeth Appel , Sayash Kapoor , Rishi Bommasani , Michelle Sahar , Sean McGregor , Avijit Ghosh , Borhane Blili-Hamelin , Nathan Butters , Alondra Nelson , Amit Elazari , Andrew Sellars , Casey Ellis , Dane Sherrets , Dawn Song , Harley Geiger , Ilona Cohen , Lauren McIlvenny , Madhulika Srikumar , Mark Jaycox , Markus Anderljung , Nadine Johnson , Nicholas Carlini , Nicolas Miailhe , Nik Marda , Peter Henderson , Rebecca Portnoff , Rebecca Weiss , Victoria Westerhoff , Yacine Jernite , Rumman Chowdhury , Percy Liang , Arvind Narayanan", "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems remain seriously underdeveloped, lagging far behind more established fields like software security. Based on a collaboration between experts from the fields of software security, machine learning, law, social science, and policy, we identify key gaps in the evaluation and reporting of flaws in GPAI systems. We call for three interventions to advance system safety. First, we propose using standardized AI flaw reports and rules of engagement for researchers in order to ease the process of submitting, reproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system providers adopt broadly-scoped flaw disclosure programs, borrowing from bug bounties, with legal safe harbors to protect researchers. Third, we advocate for the development of improved infrastructure to coordinate distribution of flaw reports across the many stakeholders who may be impacted. These interventions are increasingly urgent, as evidenced by the prevalence of jailbreaks and other flaws that can transfer across different providers' GPAI systems. By promoting robust reporting and coordination in the AI ecosystem, these proposals could significantly improve the safety, security, and accountability of GPAI systems.", "poster_file": "40170.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/40170.png?t=1751933980.6469772", "page_url": "https://icml.cc/virtual/2025/poster/40170", "openreview_url": "https://openreview.net/forum?id=ACzL62Jp4E"},
{"title": "Score-of-Mixture Training: One-Step Generative Model Training Made Simple via Score Estimation of Mixture Distributions", "authors": "Tejas Jayashankar , Jongha (Jon) Ryu , Gregory Wornell", "abstract": "We propose *Score-of-Mixture Training* (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the$\\alpha$-skew Jensen–Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels.Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call *Score-of-Mixture Distillation* (SMD).It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64×64 show that SMT/SMD are competitive with and can even outperform existing methods.", "poster_file": "43459.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43459.png?t=1752127153.7314074", "page_url": "https://icml.cc/virtual/2025/poster/43459", "openreview_url": "https://openreview.net/forum?id=zk5k2NQcEA"},
{"title": "PASS: Private Attributes Protection with Stochastic Data Substitution", "authors": "Yizhuo Chen , Chun-Fu (Richard) Chen , Hsiang Hsu , Shaohan Hu , Tarek Abdelzaher", "abstract": "The growing Machine Learning (ML) services require extensive collections of user data, which may inadvertently include people's private information irrelevant to the services. Various studies have been proposed to protect private attributes by removing them from the data while maintaining the utilities of the data for downstream tasks. Nevertheless, as we theoretically and empirically show in the paper, these methods reveal severe vulnerability because of a common weakness rooted in their adversarial training based strategies. To overcome this limitation, we propose a novel approach, PASS, designed to stochastically substitute the original sample with another one according to certain probabilities, which is trained with a novel loss function soundly derived from information-theoretic objective defined for utility-preserving private attributes protection. The comprehensive evaluation of PASS on various datasets of different modalities, including facial images, human activity sensory signals, and voice recording datasets, substantiates PASS's effectiveness and generalizability.", "poster_file": "44888.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44888.png?t=1751045575.355588", "page_url": "https://icml.cc/virtual/2025/poster/44888", "openreview_url": "https://openreview.net/forum?id=Yv416IYTFp"},
{"title": "Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws", "authors": "Xiyuan Wei , Ming Lin , Fanjiang Ye , Fengguang Song , Liangliang Cao , My T. Thai , Tianbao Yang", "abstract": "This paper formalizes an emerging learning paradigm that uses a trained model as a reference to guide and enhance the training of a target model through strategic data selection or weighting, named **model steering**. While ad-hoc methods have been used in various contexts, including the training of large foundation models,  its underlying principles remain insufficiently understood, leading to sub-optimal performance. In this work, we propose a theory-driven framework for model steering called **DRRho risk minimization**, which is rooted in Distributionally Robust Optimization (DRO). Through a generalization analysis, we provide theoretical insights into why this approach improves generalization and data efficiency compared to training without a reference model. To the best of our knowledge, this is the first time such theoretical insights are provided for the new learning paradigm, which significantly enhance our understanding and practice of model steering.  Building on these insights and the connection between contrastive learning and DRO, we introduce a novel method for Contrastive Language-Image Pretraining (CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments validate the theoretical insights, reveal a superior scaling law compared to CLIP without a reference model, and demonstrate its strength over existing heuristic approaches. Code is released at [github.com/Optimization-AI/DRRho-CLIP](https://github.com/Optimization-AI/DRRho-CLIP)", "poster_file": "45349.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45349.png?t=1752598374.436938", "page_url": "https://icml.cc/virtual/2025/poster/45349", "openreview_url": "https://openreview.net/forum?id=QC4dfobOLQ"},
{"title": "Provable Benefits of Unsupervised Pre-training and Transfer Learning via Single-Index Models", "authors": "Taj Jones-McCormick , Aukosh Jagannath , Subhabrata Sen", "abstract": "Unsupervised pre-training and transfer learning are commonly used techniques to initialize training algorithms for neural networks, particularly in settings with limited labeled data. In this paper, we study the effects of unsupervised pre-training and transfer learning on the sample complexity of high-dimensional supervised learning. Specifically, we consider the problem of training a single-layer neural network via online stochastic gradient descent. We establish that pre-training and transfer learning (under concept shift) reduce sample complexity by polynomial factors (in the dimension) under very general assumptions. We also uncover some surprising settings where pre-training grants exponential improvement over random initialization in terms of sample complexity.", "poster_file": "44817.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44817.png?t=1751495580.510627", "page_url": "https://icml.cc/virtual/2025/poster/44817", "openreview_url": "https://openreview.net/forum?id=a7UM5c1CEa"},
{"title": "Learning Safety Constraints for Large Language Models", "authors": "Xin Chen , Yarden As , Andreas Krause", "abstract": "Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP–short for Safety Polytope–a geometric approach to LLM safety, that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space.", "poster_file": "45876.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45876.png?t=1752452544.4006884", "page_url": "https://icml.cc/virtual/2025/poster/45876", "openreview_url": "https://openreview.net/forum?id=Ffpc7vx6qq"},
{"title": "Ad-Hoc Human-AI Coordination Challenge", "authors": "Tin Dizdarevic , Ravi Hammond , Tobias Gessler , Anisoara Calinescu , Jonathan Cook , Matteo Gallici , Andrei Lupu , Jakob Foerster", "abstract": "Achieving seamless coordination between AI agents and humans is crucial for real-world applications, yet it remains a significant open challenge. Hanabi is a cooperative card game featuring imperfect information, constrained communication, theory of mind requirements, and coordinated action -- making it an ideal testbed for human-AI coordination. However, its use for human-AI interaction has been limited by the challenges of human evaluation. In this work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to overcome the constraints of costly and difficult-to-reproduce human evaluations. We develop \\textit{human proxy agents} on a large-scale human dataset that serve as robust, cheap, and reproducible human-like evaluation partners in AH2AC2. To encourage the development of data-efficient methods, we open-source a dataset of 3,079 games, deliberately limiting the amount of available human gameplay data. We present baseline results for both two- and three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy agents through a controlled evaluation system rather than releasing them publicly. The code is available at \\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.", "poster_file": "45867.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45867.png?t=1752585454.700118", "page_url": "https://icml.cc/virtual/2025/poster/45867", "openreview_url": "https://openreview.net/forum?id=FuGps5Zyia"},
{"title": "$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting", "authors": "Xingjian Wu , Xiangfei Qiu , Hongfan Gao , Jilin Hu , Bin Yang , Chenjuan Guo", "abstract": "Probabilistic Time Series Forecasting (PTSF) plays a crucial role in decision-making across various fields, including economics, energy, and transportation. Most existing methods excell at short-term forecasting, while overlooking the hurdles of Long-term Probabilistic Time Series Forecasting (LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have a significant adverse effect on prediction accuracy, and make generative models inefficient by increasing the cost of each iteration. To overcome these limitations, we introduce $K^2$VAE, an efficient VAE-based generative model that leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system, and devises a KalmanNet to refine predictions and model uncertainty in such linear system, which reduces error accumulation in long-term forecasting. Extensive experiments demonstrate that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term PTSF, providing a more efficient and accurate solution.", "poster_file": "46346.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46346.png?t=1751550209.7217135", "page_url": "https://icml.cc/virtual/2025/poster/46346", "openreview_url": "https://openreview.net/forum?id=71Mm8GDGYd"},
{"title": "Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios", "authors": "xihong yang , Siwei Wang , Fangdi Wang , Jiaqi Jin , Suyuan Liu , Yue Liu , En Zhu , Xinwang Liu , Yueming Jin", "abstract": "Leveraging the powerful representation learning capabilities, deep multi-view clustering methods have demonstrated reliable performance by effectively integrating multi-source information from diverse views in recent years. Most existing methods rely on the assumption of clean views. However, noise is pervasive in real-world scenarios, leading to a significant degradation in performance. To tackle this problem, we propose a novel multi-view clustering framework for the automatic identification and rectification of noisy data, termed AIRMVC. Specifically, we reformulate noisy identification as an anomaly identification problem using GMM. We then design a hybrid rectification strategy to mitigate the adverse effects of noisy data based on the identification results. Furthermore, we introduce a noise-robust contrastive mechanism to generate reliable representations. Additionally, we provide a theoretical proof demonstrating that these representations can discard noisy information, thereby improving the performance of downstream tasks. Extensive experiments on six benchmark datasets demonstrate that AIRMVC outperforms state-of-the-art algorithms in terms of robustness in noisy scenarios. The code of AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github.", "poster_file": "44366.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44366.png?t=1750851573.1751878", "page_url": "https://icml.cc/virtual/2025/poster/44366", "openreview_url": "https://openreview.net/forum?id=iFOXz5H2gB"},
{"title": "Determining Layer-wise Sparsity for Large Language Models Through a Theoretical Perspective", "authors": "Weizhong Huang , Yuxin Zhang , Xiawu Zheng , Fei Chao , Rongrong Ji", "abstract": "In this paper, we address the challenge of determining the layer-wise sparsity rates of large language models (LLMs) through a theoretical perspective. Specifically, we identify a critical issue of **\"reconstruction error explosion\"** in existing LLMs sparsification methods. This refers to the cumulative effect of reconstruction errors throughout the sparsification process, where errors from earlier layers propagate and amplify in subsequent layers. As a result, the overall reconstruction error increases significantly, leading to a substantial degradation in model performance. Through theoretical analysis, we derive a simple yet effective approach to layer-wise sparsity allocation that mitigates this issue. Our method uses a monotonically increasing arithmetic progression, reducing the process of determining sparsity rates for multiple layers to the determination of a single common difference hyperparameter. Remarkably, this allows for the optimal layer-wise sparsity rates to be identified with just a few trials. Both our theoretical analysis and experimental results demonstrate that this sparsity allocation scheme is near optimal. Extensive experiments show that our method significantly improves the performance of sparse LLMs across various architectures, outperforming existing layer-wise sparsity methods. Furthermore, it enhances the performance of various compression techniques and is applicable to vision and multimodal models. Notably, our method achieves a reduction …", "poster_file": "44037.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44037.png?t=1751277656.71015", "page_url": "https://icml.cc/virtual/2025/poster/44037", "openreview_url": "https://openreview.net/forum?id=otNB7BzsiR"},
{"title": "FlashTP: Fused, Sparsity-Aware Tensor Product for Machine Learning Interatomic Potentials", "authors": "Seung Lee , Hojoon Kim , Yutack Park , Dawoon Jeong , Seungwu Han , Yeonhong Park , Jae W. Lee", "abstract": "Machine Learning Interatomic Potentials (MLIPs) enable efficient molecular dynamics (MD) simulations with high accuracy. While equivariant MLIPs achieve state-of-the-art accuracy, they face significant computational bottlenecks centered around their Tensor-Product layer, which account for up to 75\\% of training time and cause substantial memory overhead. We present FlashTP, a highly optimized tensor-product library that addresses these inefficiencies through kernel fusion, sparse computation, and path-aggregated execution. FlashTP achieves up to 41.6$\\times$ and 60.8$\\times$ kernel speedups over _e3nn_ and NVIDIA cuEquivariance, respectively. For SevenNet-l3i5, it delivers 4.2$\\times$ and 3.5$\\times$ speedup while reducing peak memory usage by 6.3$\\times$ and 6.2$\\times$ for inference and training, respectively. The code is available at https://github.com/SNU-ARC/flashTP.", "poster_file": "43610.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43610.png?t=1752199279.6431923", "page_url": "https://icml.cc/virtual/2025/poster/43610", "openreview_url": "https://openreview.net/forum?id=wiQe95BPaB"},
{"title": "When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network", "authors": "Dong Xiao , Guangyao Chen , Peixi Peng , Yangru Huang , Yifan Zhao , Yongxing Dai , Yonghong Tian", "abstract": "Anomaly detection is essential for the safety and reliability of autonomous driving systems. Current methods often focus on detection accuracy but neglect response time, which is critical in time-sensitive driving scenarios. In this paper, we introduce real-time anomaly detection for autonomous driving, prioritizing both minimal response time and high accuracy. We propose a novel multimodal asynchronous hybrid network that combines event streams from event cameras with image data from RGB cameras. Our network utilizes the high temporal resolution of event cameras through an asynchronous Graph Neural Network and integrates it with spatial features extracted by a CNN from RGB images. This combination effectively captures both the temporal dynamics and spatial details of the driving environment, enabling swift and precise anomaly detection. Extensive experiments on benchmark datasets show that our approach outperforms existing methods in both accuracy and response time, achieving millisecond-level real-time performance.", "poster_file": "44056.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44056.png?t=1751627330.8024464", "page_url": "https://icml.cc/virtual/2025/poster/44056", "openreview_url": "https://openreview.net/forum?id=oYyaVSqEFu"},
{"title": "Leveraging Diffusion Model as Pseudo-Anomalous Graph Generator for Graph-Level Anomaly Detection", "authors": "Jinyu Cai , Yunhe Zhang , Fusheng Liu , See-Kiong Ng", "abstract": "A fundamental challenge in graph-level anomaly detection (GLAD) is the scarcity of anomalous graph data, as the training dataset typically contains only normal graphs or very few anomalies. This imbalance hinders the development of robust detection models. In this paper, we propose **A**nomalous **G**raph **Diff**usion (AGDiff), a framework that explores the potential of diffusion models in generating pseudo-anomalous graphs for GLAD. Unlike existing diffusion-based methods that focus on modeling data normality, AGDiff leverages the latent diffusion framework to incorporate subtle perturbations into graph representations, thereby generating pseudo-anomalous graphs that closely resemble normal ones. By jointly training a classifier to distinguish these generated graph anomalies from normal graphs, AGDiff learns more discriminative decision boundaries. The shift from solely modeling normality to explicitly generating and learning from pseudo graph anomalies enables AGDiff to effectively identify complex anomalous patterns that other approaches might overlook. Comprehensive experimental results demonstrate that the proposed AGDiff significantly outperforms several state-of-the-art GLAD baselines.", "poster_file": "44832.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44832.png?t=1752123951.491606", "page_url": "https://icml.cc/virtual/2025/poster/44832", "openreview_url": "https://openreview.net/forum?id=Zm2M92TZyO"},
{"title": "Elucidating the Design Space of Multimodal Protein Language Models", "authors": "Cheng-Yen Hsieh , Xinyou Wang , Daiheng Zhang , Dongyu Xue , Fei YE , Shujian Huang , Zaixiang Zheng , Quanquan Gu", "abstract": "Multimodal protein language models (PLMs) integrate sequence and token-based structural information, serving as a powerful foundation for protein modeling, generation, and design. However, the reliance on tokenizing 3D structures into discrete tokens causes substantial loss of fidelity about fine-grained structural details and correlations. In this paper, we systematically elucidate the design space of multimodal PLMs to overcome their limitations. We identify tokenization loss and inaccurate structure token predictions by the PLMs as major bottlenecks.To address these, our proposed design space covers improved generative modeling, structure-aware architectures and representation learning, and data exploration. Our advancements approach finer-grained supervision, demonstrating that token-based multimodal PLMs can achieve robust structural modeling.The effective design methods dramatically improve the structure generation diversity, and notably, folding abilities of our 650M model by reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B baselines and on par with the specialized folding models.Project page and code: https://bytedance.github.io/dplm/dplm-2.1.", "poster_file": "44312.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44312.png?t=1752522583.3403563", "page_url": "https://icml.cc/virtual/2025/poster/44312", "openreview_url": "https://openreview.net/forum?id=jEcQP3lGlq"},
{"title": "LotteryCodec: Searching the Implicit Representation in a Random Network for Low-Complexity Image Compression", "authors": "Haotian Wu , Gongpu Chen , Pier Luigi Dragotti , Deniz Gunduz", "abstract": "We introduce and validate the lottery codec hypothesis, which states that untrained subnetworks within randomly initialized networks can serve as synthesis networks for overfitted image compression, achieving rate-distortion (RD) performance comparable to trained networks. This hypothesis leads to a new paradigm for image compression by encoding image statistics into the network substructure. Building on this hypothesis, we propose LotteryCodec, which overfits a binary mask to an individual image, leveraging an over-parameterized and randomly initialized network shared by the encoder and the decoder. To address over-parameterization challenges and streamline subnetwork search, we develop a rewind modulation mechanism that improves the RD performance. LotteryCodec outperforms VTM and sets a new state-of-the-art in single-image compression. LotteryCodec also enables adaptive decoding complexity through adjustable mask ratios, offering flexible compression solutions for diverse device constraints and application requirements.", "poster_file": "46198.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46198.png?t=1752508749.8679366", "page_url": "https://icml.cc/virtual/2025/poster/46198", "openreview_url": "https://openreview.net/forum?id=9u5hPIcr6j"},
{"title": "Rethink GraphODE Generalization within Coupled Dynamical System", "authors": "Guancheng Wan , Zijie Huang , Wanjia Zhao , Xiao Luo , Yizhou Sun , Wei Wang", "abstract": "Coupled dynamical systems govern essential phenomena across physics, biology, and engineering, where components interact through complex dependencies. While Graph Ordinary Differential Equations (GraphODE) offer a powerful framework to model these systems, their **generalization** capabilities degrade severely under limited observational training data due to two fundamental flaws: (i) the entanglement of static attributes and dynamic states in the initialization process, and (ii) the reliance on context-specific coupling patterns during training, which hinders performance in unseen scenarios. In this paper, we propose a Generalizable GraphODE with disentanglement and regularization (GREAT) to address these challenges. Through systematic analysis via the Structural Causal Model, we identify backdoor paths that undermine generalization and design two key modules to mitigate their effects. The *Dynamic-Static Equilibrium Decoupler (DyStaED)* disentangles static and dynamic states via orthogonal subspace projections, ensuring robust initialization. Furthermore, the *Causal Mediation for Coupled Dynamics (CMCD)* employs variational inference to estimate latent causal factors, reducing spurious correlations and enhancing universal coupling dynamics. Extensive experiments across diverse dynamical systems demonstrate that ours outperforms state-of-the-art methods within both in-distribution and out-of-distribution.", "poster_file": "44104.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44104.png?t=1751603968.8654017", "page_url": "https://icml.cc/virtual/2025/poster/44104", "openreview_url": "https://openreview.net/forum?id=nVD7KoU09V"},
{"title": "Discrepancy Minimization in Input-Sparsity Time", "authors": "Yichuan Deng , Xiaoyu Li , Zhao Song , OMRI WEINSTEIN", "abstract": "A recent work by [Larsen, SODA 2023] introduced a faster combinatorial alternative to Bansal's SDP algorithm for finding a coloring $x \\in \\\\{-1, 1\\\\}^n$ that approximately minimizes the discrepancy $\\mathrm{disc}(A, x) := \\\\| A x \\\\|_{\\infty}$ of a real-valued $m \\times n$ matrix $A$. Larsen's algorithm runs in $\\widetilde{O}(mn^2)$ time compared to Bansal's $\\widetilde{O}(mn^{4.5})$-time algorithm, with a slightly weaker logarithmic approximation ratio in terms of the hereditary discrepancy of $A$ [Bansal, FOCS 2010]. We present a combinatorial $\\widetilde{O}(\\mathrm{nnz}(A) + n^3)$-time algorithm with the same approximation guarantee as Larsen's, optimal for tall matrices where $m = \\mathrm{poly}(n)$. Using a more intricate analysis and fast matrix multiplication, we further achieve a runtime of $\\widetilde{O}(\\mathrm{nnz}(A) + n^{2.53})$, breaking the cubic barrier for square matrices and surpassing the limitations of linear-programming approaches [Eldan and Singh, RS\\&A 2018]. Our algorithm relies on two key ideas: (i) a new sketching technique for finding a projection matrix with a short $\\ell_2$-basis using implicit leverage-score sampling, and (ii) a data structure for efficiently implementing the iterative Edge-Walk partial-coloring algorithm [Lovett and Meka, SICOMP 2015], and using an alternative analysis to enable ``lazy'' batch updates with low-rank corrections. Our results nearly close the computational gap between real-valued and binary …", "poster_file": "45157.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45157.png?t=1752385776.1949587", "page_url": "https://icml.cc/virtual/2025/poster/45157", "openreview_url": "https://openreview.net/forum?id=TmJvacopmV"},
{"title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration", "authors": "Qinglin Zhu , Runcong Zhao , Hanqi Yan , Yulan He , Yudong Chen , Lin Gui", "abstract": "Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution.", "poster_file": "46470.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46470.png?t=1752541563.8642125", "page_url": "https://icml.cc/virtual/2025/poster/46470", "openreview_url": "https://openreview.net/forum?id=4gWE7CMOlH"},
{"title": "Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation", "authors": "Jintao Tong , Ran Ma , Yixiong Zou , Guangyao Chen , Yuhua Li , Ruixuan Li", "abstract": "Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train the model on a source-domain dataset with sufficient samples, and then transfer the model to target-domain datasets where only a few training samples are available for efficient finetuning. There are majorly two challenges in this task: (1) the domain gap and (2) finetuning with scarce data. To solve these challenges, we revisit the adapter-based methods, and discover an intriguing insight not explored in previous works: the adapter not only helps the fine-tuning of downstream tasks but also naturally serves as a domain information decoupler. Then, we delve into this finding for an interpretation, and we find the model's inherent structure could lead to a natural decoupling of domain information. Building upon this insight, we propose the Domain Feature Navigator (DFN), which is a structure-based decoupler instead of loss-based ones like current works, to capture domain-specific information, thereby directing the model's attention towards domain-agnostic knowledge. Moreover, to prevent the potential excessive overfitting of DFN during the source-domain training, we further design the SAM-SVN method to constrain DFN from learning sample-specific knowledge. On target domains, we freeze the model and fine-tune the DFN to learn knowledge specific to target domains. Extensive experiments demonstrate …", "poster_file": "45369.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45369.png?t=1749722423.5264668", "page_url": "https://icml.cc/virtual/2025/poster/45369", "openreview_url": "https://openreview.net/forum?id=Pokj70ZAxJ"},
{"title": "TLLC: Transfer Learning-based Label Completion for Crowdsourcing", "authors": "Wenjun Zhang , Liangxiao Jiang , Chaoqun Li", "abstract": "Label completion serves as a preprocessing approach to handling the sparse crowdsourced label matrix problem, significantly boosting the effectiveness of the downstream label aggregation. In recent advances, worker modeling has been proved to be a powerful strategy to further improve the performance of label completion. However, in real-world scenarios, workers typically annotate only a few instances, leading to insufficient worker modeling and thus limiting the improvement of label completion. To address this issue, we propose a novel transfer learning-based label completion (TLLC) method. Specifically, we first identify all high-confidence instances from the whole crowdsourced data as a source domain and use it to pretrain a Siamese network. The abundant annotated instances in the source domain provide essential knowledge for worker modeling. Then, we transfer the pretrained network to the target domain with the instances annotated by each worker separately, ensuring worker modeling captures unique characteristics of each worker. Finally, we leverage the new embeddings learned by the transferred network to complete each worker’s missing labels. Extensive experiments on several widely used real-world datasets demonstrate the effectiveness of TLLC. Our codes and datasets are available at https://github.com/jiangliangxiao/TLLC.", "poster_file": "46098.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46098.png?t=1750170642.6599708", "page_url": "https://icml.cc/virtual/2025/poster/46098", "openreview_url": "https://openreview.net/forum?id=BkdAnSKNoX"},
{"title": "Not all solutions are created equal: An analytical dissociation of functional and representational similarity in deep linear neural networks", "authors": "Lukas Braun , Erin Grant , Andrew Saxe", "abstract": "A foundational principle of connectionism is that perception, action, and cognition emerge from parallel computations among simple, interconnected units that generate and rely on neural representations. Accordingly, researchers employ multivariate pattern analysis to decode and compare the neural codes of artificial and biological networks, aiming to uncover their functions. However, there is limited analytical understanding of how a network’s representation and function relate, despite this being essential to any quantitative notion of underlying function or functional similarity. We address this question using fully analysable two-layer linear networks and numerical simulations in nonlinear networks. We find that function and representation are dissociated, allowing representational similarity without functional similarity and vice versa. Further, we show that neither robustness to input noise nor the level of generalisation error constrain representations to the task. In contrast, networks robust to parameter noise have limited representational flexibility and must employ task-specific representations. Our findings suggest that representational alignment reflects computational advantages beyond functional alignment alone, with significant implications for interpreting and comparing the representations of connectionist systems", "poster_file": "44890.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44890.png?t=1752684967.989591", "page_url": "https://icml.cc/virtual/2025/poster/44890", "openreview_url": "https://openreview.net/forum?id=YucuAuXMpT"},
{"title": "Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework", "authors": "Terje Mildner , Oliver Hamelijnck , Paris Giampouras , Theodoros Damoulas", "abstract": "We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness to likelihood misspecification. Further, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets.", "poster_file": "45557.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45557.png?t=1752004455.0134497", "page_url": "https://icml.cc/virtual/2025/poster/45557", "openreview_url": "https://openreview.net/forum?id=M7mVzCV6uU"},
{"title": "Efficient and Separate Authentication Image Steganography Network", "authors": "Junchao Zhou , Yao Lu , Jie Wen , Guangming Lu", "abstract": "Image steganography hides multiple images for multiple recipients into a single cover image. All secret images are usually revealed without authentication, which reduces security among multiple recipients. It is elegant to design an authentication mechanism for isolated reception. We explore such mechanism through sufficient experiments, and uncover that additional authentication information will affect the distribution of hidden information and occupy more hiding space of the cover image. This severely decreases effectiveness and efficiency in large-capacity hiding. To overcome such a challenge, we first prove the authentication feasibility within image steganography. Then, this paper proposes an image steganography network collaborating with separate authentication and efficient scheme. Specifically, multiple pairs of lock-key are generated during hiding and revealing. Unlike traditional methods, our method has two stages to make appropriate distribution adaptation between locks and secret images, simultaneously extracting more reasonable primary information from secret images, which can release hiding space of the cover image to some extent. Furthermore, due to separate authentication, fused information can be hidden in parallel with a single network rather than traditional serial hiding with multiple networks, which can largely decrease the model size. Extensive experiments demonstrate that the proposed method achieves more secure, effective, and efficient image …", "poster_file": "44692.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44692.png?t=1750850460.130322", "page_url": "https://icml.cc/virtual/2025/poster/44692", "openreview_url": "https://openreview.net/forum?id=cKaUC1PeJA"},
{"title": "InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective", "authors": "Yuanhong Zhang , Muyao Yuan , Weizhan Zhang , Tieliang Gong , Wen Wen , Jiangyong Ying , Weijie Shi", "abstract": "The Segment Anything Model (SAM), a vision foundation model, exhibits impressive zero-shot capabilities in general tasks but struggles in specialized domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to unleash the potential of SAM in novel scenarios. However, existing PEFT methods for SAM neglect the domain-invariant relations encoded in the pre-trained model. To bridge this gap, we propose InfoSAM, an information-theoretic approach that enhances SAM fine-tuning by distilling and preserving its pre-trained segmentation knowledge. Specifically, we formulate the knowledge transfer process as two novel mutual information-based objectives: (i) to compress the domain-invariant relation extracted from pre-trained SAM, excluding pseudo-invariant information as possible, and (ii) to maximize mutual information between the relational knowledge learned by the teacher (pre-trained SAM) and the student (fine-tuned model).  The proposed InfoSAM establishes a robust distillation framework for PEFT of SAM. Extensive experiments across diverse benchmarks validate InfoSAM's effectiveness in improving SAM family's performance on real-world tasks, demonstrating its adaptability and superiority in handling specialized scenarios. The code and models are available at https://muyaoyuan.github.io/InfoSAM_Page.", "poster_file": "45048.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45048.png?t=1749641564.8059611", "page_url": "https://icml.cc/virtual/2025/poster/45048", "openreview_url": "https://openreview.net/forum?id=VpBBw1bL47"},
{"title": "Not All Wrong is Bad: Using Adversarial Examples for Unlearning", "authors": "Ali Ebrahimpour-Boroojeny , Hari Sundaram , Varun Chandrasekaran", "abstract": "Machine unlearning, where users can request the deletion of a forget dataset, is becoming increasingly important because of numerous privacy regulations. Initial works on \"exact'' unlearning (e.g., retraining) incur large computational overheads. However, while computationally inexpensive, \"approximate'' methods have fallen short of reaching the effectiveness of exact unlearning: models produced fail to obtain comparable accuracy and prediction confidence on both the forget and test (i.e., unseen) dataset. Exploiting this observation, we propose a new unlearning method, Adversarial Machine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA) methods for image classification. AMUN lowers the confidence of the model on the forget samples by fine-tuning the model on their corresponding adversarial examples. Adversarial examples naturally belong to the distribution imposed by the model on the input space; fine-tuning the model on the adversarial examples closest to the corresponding forget samples (a) localizes the changes to the decision boundary of the model around each forget sample and (b) avoids drastic changes to the global behavior of the model, thereby preserving the model's accuracy on test samples. Using AMUN for unlearning a random 10% of CIFAR-10 samples, we observe that even SOTA membership inference attacks cannot do better than random guessing.", "poster_file": "46097.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46097.png?t=1752512579.0512633", "page_url": "https://icml.cc/virtual/2025/poster/46097", "openreview_url": "https://openreview.net/forum?id=BkrIQPREkn"},
{"title": "Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems", "authors": "Shaokun Zhang , Ming Yin , Jieyu Zhang , Jiale Liu , Zhiguang Han , Jingyang Zhang , Beibin Li , Chi Wang , Huazheng Wang , Yiran Chen , Qingyun Wu", "abstract": "Failure attribution in LLM multi-agent systems—identifying the agent and step responsible for task failures—provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems.To support this initiative, we introduce the Who\\&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps.Using the Who\\&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons.  The best method achieves 53.5\\% accuracy in identifying failure-responsible agents but only 14.2\\% in pinpointing failure steps, with some methods performing below random.  Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available in https://github.com/mingyin1/Agents_Failure_Attribution.", "poster_file": "45823.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45823.png?t=1752773606.1475065", "page_url": "https://icml.cc/virtual/2025/poster/45823", "openreview_url": "https://openreview.net/forum?id=GazlTYxZss"},
{"title": "Position: Language model developers should report train-test overlap", "authors": "Andy Zhang , Kevin Klyman , Yifan Mai , Yoav Levine , Yian Zhang , Rishi Bommasani , Percy Liang", "abstract": "Language models are extensively evaluated, but correctly interpreting evaluation results requires knowledge of train-test overlap, which refers to the extent to which the language model is trained on the very data it is being tested on. The public currently lacks adequate information about train-test overlap: most models have no public train-test overlap statistics, and third parties cannot directly measure train-test overlap since they do not have access to the training data. To make this clear, we document the practices of 30 models, finding that just 9 models report train-test overlap: 4 models release training data under open-source licenses, enabling the community to directly measure train-test overlap, and 5 models publish their train-test overlap methodology and statistics. By engaging with language model developers, we provide novel information about train-test overlap for three additional models. Overall, this position paper argues that language model developers should publish train-test overlap statistics and/or training data whenever they report evaluation results on public test sets. We hope our work increases transparency into train-test overlap to increase the community-wide trust in model evaluations.", "poster_file": "40154.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/40154.png?t=1750399792.9459708", "page_url": "https://icml.cc/virtual/2025/poster/40154", "openreview_url": "https://openreview.net/forum?id=J5MmGPWKfb"},
{"title": "Taming Knowledge Conflicts in Language Models", "authors": "Gaotang Li , Yuzhong Chen , Hanghang Tong", "abstract": "Language Models (LMs) often encounter knowledge conflicts when parametric memory contradicts contextual knowledge. Previous works attribute this conflict to the interplay between \"memory heads\" and \"context heads\", attention heads assumed to promote either memory or context exclusively. In this study, we go beyond this fundamental assumption by uncovering a critical phenomenon we term the *superposition of contextual information and parametric memory*, where highly influential attention heads simultaneously contribute to both memory and context. Building upon this insight, we propose Just Run Twice (JuICE), a test-time attention intervention method that steers LMs toward either parametric beliefs or contextual knowledge without requiring fine-tuning. JuICE identifies a set of reliable attention heads and leverages a dual-run approach to mitigate the superposition effects. Extensive experiments across 11 datasets and 6 model architectures demonstrate that JuICE sets the new state-of-the-art performance and robust generalization, achieving significant and consistent improvement across different domains under various conflict types. Finally, we theoretically analyze knowledge conflict and the superposition of contextual information and parametric memory in attention heads, which further elucidates the effectiveness of JuICE in these settings. Our code is available at https://github.com/GaotangLi/JUICE.", "poster_file": "46677.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46677.png?t=1752052768.1831782", "page_url": "https://icml.cc/virtual/2025/poster/46677", "openreview_url": "https://openreview.net/forum?id=0cEZyhHEks"},
{"title": "Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator", "authors": "YuXin Li , Felix Dangel , Derek Tam , Colin Raffel", "abstract": "The diagonal of a model's Fisher Information Matrix (the \"Fisher\") has frequently been used as a way to measure parameter sensitivity.Typically, the Fisher is estimated by computing the squared gradient of the model's outputs with respect to its parameters, averaged over a few hundred or thousand examples — a process which incurs nontrivial computational costs.At the same time, adaptive gradient methods like the ubiquitous Adam optimizer compute a moving average of the squared gradient over the course of training.This paper therefore explores whether an approximation of the Fisher can be obtained \"for free\" by recycling the squared gradient accumulator that has already been computed over the course of training.Through a comprehensive set of experiments covering five applications of the Fisher, we demonstrate that the \"Squisher\" (**Squ**ared gradient accumulator as an approximation of the F**isher**) consistently performs similarly to the Fisher while outperforming baseline methods.Additionally, we clarify the exact differences between the Squisher and the Fisher and provide empirical quantification of their respective impact.", "poster_file": "44175.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44175.png?t=1752601211.6998806", "page_url": "https://icml.cc/virtual/2025/poster/44175", "openreview_url": "https://openreview.net/forum?id=m3zrHhiCCj"},
{"title": "A Closer Look at Multimodal Representation Collapse", "authors": "Abhra Chaudhuri , Anjan Dutta , Tu Bui , Serban Georgescu", "abstract": "We aim to develop a fundamental understanding of modality collapse, a recently observed empirical phenomenon wherein models trained for multimodal fusion tend to rely only on a subset of the modalities, ignoring the rest. We show that modality collapse happens when noisy features from one modality are entangled, via a shared set of neurons in the fusion head, with predictive features from another, effectively masking out positive contributions from the predictive features of the former modality and leading to its collapse. We further prove that cross-modal knowledge distillation implicitly disentangles such representations by freeing up rank bottlenecks in the student encoder, denoising the fusion-head outputs without negatively impacting the predictive features from either modality. Based on the above findings, we propose an algorithm that prevents modality collapse through explicit basis reallocation, with applications in dealing with missing modalities. Extensive experiments on multiple multimodal benchmarks validate our theoretical claims. Project page: https://abhrac.github.io/mmcollapse/.", "poster_file": "45060.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45060.png?t=1750449910.935321", "page_url": "https://icml.cc/virtual/2025/poster/45060", "openreview_url": "https://openreview.net/forum?id=Vf9f7eNX6T"},
{"title": "On the Guidance of Flow Matching", "authors": "Ruiqi Feng , Chenglei Yu , Wenhao Deng , Peiyan Hu , Tailin Wu", "abstract": "Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where generation under energy guidance (abbreviated as guidance in the following) is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance.", "poster_file": "44016.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44016.png?t=1752303489.9100275", "page_url": "https://icml.cc/virtual/2025/poster/44016", "openreview_url": "https://openreview.net/forum?id=pKaNgFzJBy"},
{"title": "Towards Robustness and Explainability of Automatic Algorithm Selection", "authors": "Xingyu Wu , Jibin Wu , Yu Zhou , Liang Feng , KC Tan", "abstract": "Algorithm selection aims to identify the optimal performing algorithm before execution. Existing techniques typically focus on the observed correlations between algorithm performance and meta-features. However, little research has explored the underlying mechanisms of algorithm selection, specifically what characteristics an algorithm must possess to effectively tackle problems with certain feature values. This gap not only limits the explainability but also makes existing models vulnerable to data bias and distribution shift. This paper introduces directed acyclic graph (DAG) to describe this mechanism, proposing a novel modeling paradigm that aligns more closely with the fundamental logic of algorithm selection. By leveraging DAG to characterize the algorithm feature distribution conditioned on problem features, our approach enhances robustness against marginal distribution changes and allows for finer-grained predictions through the reconstruction of optimal algorithm features, with the final decision relying on differences between reconstructed and rejected algorithm features. Furthermore, we demonstrate that, the learned DAG and the proposed counterfactual calculations offer our approach with both model-level and instance-level explainability.", "poster_file": "45808.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45808.png?t=1752157527.5879478", "page_url": "https://icml.cc/virtual/2025/poster/45808", "openreview_url": "https://openreview.net/forum?id=Gp7NfP7Erm"},
{"title": "LipsNet++: Unifying Filter and Controller into a Policy Network", "authors": "Xujie Song , Liangfa Chen , Tong Liu , Wenxuan Wang , Yinuo Wang , Shentao Qin , Yinsong Ma , Jingliang Duan , Shengbo Li", "abstract": "Deep reinforcement learning (RL) is effective for decision-making and control tasks like autonomous driving and embodied AI. However, RL policies often suffer from the action fluctuation problem in real-world applications, resulting in severe actuator wear, safety risk, and performance degradation. This paper identifies the two fundamental causes of action fluctuation: observation noise and policy non-smoothness. We propose LipsNet++, a novel policy network with Fourier filter layer and Lipschitz controller layer to separately address both causes. The filter layer incorporates a trainable filter matrix that automatically extracts important frequencies while suppressing noise frequencies in the observations. The controller layer introduces a Jacobian regularization technique to achieve a low Lipschitz constant, ensuring smooth fitting of a policy function. These two layers function analogously to the filter and controller in classical control theory, suggesting that filtering and control capabilities can be seamlessly integrated into a single policy network. Both simulated and real-world experiments demonstrate that LipsNet++ achieves the state-of-the-art noise robustness and action smoothness. The code and videos are publicly available at https://xjsong99.github.io/LipsNet_v2.", "poster_file": "45632.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45632.png?t=1751002189.6747875", "page_url": "https://icml.cc/virtual/2025/poster/45632", "openreview_url": "https://openreview.net/forum?id=KZo2XhcSg6"},
{"title": "CoPINN: Cognitive Physics-Informed Neural Networks", "authors": "Siyuan Duan , Wenyuan Wu , Peng Hu , Zhenwen Ren , Dezhong Peng , Yuan Sun", "abstract": "Physics-informed neural networks (PINNs) aim to constrain the outputs and gradients of deep learning models to satisfy specified governing physics equations, which have demonstrated significant potential for solving partial differential equations (PDEs). Although existing PINN methods have achieved pleasing performance, they always treat both easy and hard sample points indiscriminately, especially ones in the physical boundaries. This easily causes the PINN model to fall into undesirable local minima and unstable learning, thereby resulting in an Unbalanced Prediction Problem (UPP). To deal with this daunting problem, we propose a novel framework named Cognitive Physical Informed Neural Network (CoPINN) that imitates the human cognitive learning manner from easy to hard. Specifically, we first employ separable subnetworks to encode independent one-dimensional coordinates and apply an aggregation scheme to generate multi-dimensional predicted physical variables. Then, during the training phase, we dynamically evaluate the difficulty of each sample according to the gradient of the PDE residuals. Finally, we propose a cognitive training scheduler to progressively optimize the entire sampling regions from easy to hard, thereby embracing robustness and generalization against predicting physical boundary regions. Extensive experiments demonstrate that our CoPINN achieves state-of-the-art performance, particularly significantly reducing prediction errors in stubborn regions.", "poster_file": "46458.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46458.png?t=1751006307.1343915", "page_url": "https://icml.cc/virtual/2025/poster/46458", "openreview_url": "https://openreview.net/forum?id=4vAa0A98xI"},
{"title": "Policy-labeled Preference Learning: Is Preference Enough for RLHF?", "authors": "Taehyun Cho , Seokhun Ju , Seungyub Han , Dohyeong Kim , Kyungjae Lee , Jungwoo Lee", "abstract": "To design reward that align with human goals, Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent technique for learning reward functions from human preferences and optimizing models using reinforcement learning algorithms. However, existing RLHF methods often misinterpret trajectories as being generated by an optimal policy, causing inaccurate likelihood estimation and suboptimal learning. To address this, we propose Policy-labeled Preference Learning (PPL) within the Direct Preference Optimization (DPO) framework, which resolves these likelihood mismatch problems by modeling human preferences with regret, reflecting the efficiency of executed policies. Additionally, we introduce a contrastive KL regularization term derived from regret-based principles to enhance sequential contrastive learning. Experiments in high-dimensional continuous control environments demonstrate PPL's significant improvements in offline RLHF performance and its effectiveness in online settings.", "poster_file": "43946.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43946.png?t=1750055819.9269214", "page_url": "https://icml.cc/virtual/2025/poster/43946", "openreview_url": "https://openreview.net/forum?id=qLfo1sef50"},
{"title": "Large Language Model-driven Large Neighborhood Search for Large-Scale MILP Problems", "authors": "Huigen Ye , Hua Xu , An Yan , Yaoyang Cheng", "abstract": "Large Neighborhood Search (LNS) is a widely used method for solving large-scale Mixed Integer Linear Programming (MILP) problems. The effectiveness of LNS crucially depends on the choice of the search neighborhood. However, existing strategies either rely on expert knowledge or computationally expensive Machine Learning (ML) approaches, both of which struggle to scale effectively for large problems. To address this, we propose LLM-LNS, a novel Large Language Model (LLM)-driven LNS framework for large-scale MILP problems. Our approach introduces a dual-layer self-evolutionary LLM agent to automate neighborhood selection, discovering effective strategies with scant small-scale training data that generalize well to large-scale MILPs. The inner layer evolves heuristic strategies to ensure convergence, while the outer layer evolves evolutionary prompt strategies to maintain diversity. Experimental results demonstrate that the proposed dual-layer agent outperforms state-of-the-art agents such as FunSearch and EOH. Furthermore, the full LLM-LNS framework surpasses manually designed LNS algorithms like ACP, ML-based LNS methods like CL-LNS, and large-scale solvers such as Gurobi and SCIP. It also achieves superior performance compared to advanced ML-based MILP optimization frameworks like GNN&GBDT and Light-MILPopt, further validating the effectiveness of our approach.", "poster_file": "43770.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43770.png?t=1750044834.862768", "page_url": "https://icml.cc/virtual/2025/poster/43770", "openreview_url": "https://openreview.net/forum?id=teUg2pMrF0"},
{"title": "DPO Meets PPO: Reinforced Token Optimization for RLHF", "authors": "Han Zhong , Zikang Shan , Guhao Feng , Wei Xiong , Xinle Cheng , Li Zhao , Di He , Jiang Bian , Liwei Wang", "abstract": "In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards---a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation. Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \\texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \\texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. We conduct extensive experiments to evaluate \\texttt{RTO} against PPO and …", "poster_file": "45726.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45726.png?t=1752297082.7262132", "page_url": "https://icml.cc/virtual/2025/poster/45726", "openreview_url": "https://openreview.net/forum?id=IfWKVF6LfY"},
{"title": "Prediction models that learn to avoid missing values", "authors": "Lena Stempfle , Anton Matsson , Newton Mwai , Fredrik Johansson", "abstract": "Handling missing values at test time is challenging for machine learning models, especially when aiming for both high accuracy and interpretability. Established approaches often add bias through imputation or excessive model complexity via missingness indicators. Moreover, either method can obscure interpretability, making it harder to understand how the model utilizes the observed variables in predictions. We propose *missingness-avoiding* (MA) machine learning, a general framework for training models to rarely require the values of missing (or imputed) features at test time. We create tailored MA learning algorithms for decision trees, tree ensembles, and sparse linear models by incorporating classifier-specific regularization terms in their learning objectives. The tree-based models leverage contextual missingness by reducing reliance on missing values based on the observed context. Experiments on real-world datasets demonstrate that **MA-DT, MA-LASSO, MA-RF**, and **MA-GBT** effectively reduce the reliance on features with missing values while maintaining predictive performance competitive with their unregularized counterparts. This shows that our framework gives practitioners a powerful tool to maintain interpretability in predictions with test-time missing values.", "poster_file": "43980.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43980.png?t=1750876547.767236", "page_url": "https://icml.cc/virtual/2025/poster/43980", "openreview_url": "https://openreview.net/forum?id=ps3aO9MHJv"},
{"title": "Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting", "authors": "Sunny Sanyal , Hayden Prairie , Rudrajit Das , Ali Kavis , Sujay Sanghavi", "abstract": "Fine-tuning a pre-trained model on a downstream task often degrades its original capabilities, a phenomenon known as \"catastrophic forgetting\". This is especially an issue when one does not have access to the data and recipe used to develop the pre-trained model. Under this constraint, most existing methods for mitigating forgetting are inapplicable. To address this challenge, we propose a *sample weighting scheme for the fine-tuning data* solely based on the pre-trained model's losses. Specifically, we upweight the easy samples on which the pre-trained model's loss is low and vice versa to limit the drift from the pre-trained model. Our approach is orthogonal and yet complementary to existing methods; while such methods mostly operate on parameter or gradient space, we concentrate on the sample space. We theoretically analyze the impact of fine-tuning with our method in a linear setting, showing that it stalls learning in a certain subspace, which inhibits overfitting to the target task. We empirically demonstrate the efficacy of our method on both language and vision tasks. As an example, when fine-tuning Gemma 2 2B on MetaMathQA, our method results in only a $0.8$% drop in accuracy on GSM8K (another math dataset) compared to standard fine-tuning, while preserving $5.4$% …", "poster_file": "46655.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46655.png?t=1752497880.3389258", "page_url": "https://icml.cc/virtual/2025/poster/46655", "openreview_url": "https://openreview.net/forum?id=13HPTmZKbM"},
{"title": "Towards a Mechanistic Explanation of Diffusion Model Generalization", "authors": "Matthew Niedoba , Berend Zwartsenberg , Kevin Murphy , Frank Wood", "abstract": "We propose a simple, training-free mechanism which explains the generalization behaviour of diffusion models. By comparing pre-trained diffusion models to their theoretically optimal empirical counterparts, we identify a shared local inductive bias across a variety of network architectures. From this observation, we hypothesize that network denoisers generalize through localized denoising operations, as these operations approximate the training objective well over much of the training distribution. To validate our hypothesis, we introduce novel denoising algorithms which aggregate local empirical denoisers to replicate network behaviour. Comparing these algorithms to network denoisers across forward and reverse diffusion processes, our approach exhibits consistent visual similarity to neural network outputs, with lower mean squared error than previously proposed methods.", "poster_file": "45759.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45759.png?t=1752172266.9108", "page_url": "https://icml.cc/virtual/2025/poster/45759", "openreview_url": "https://openreview.net/forum?id=Hrp6jRIKdX"},
{"title": "Geometric Representation Condition Improves Equivariant Molecule Generation", "authors": "Zian Li , Cai Zhou , Xiyuan Wang , Xingang Peng , Muhan Zhang", "abstract": "Recent advances in molecular generative models have demonstrated great promise for accelerating scientific discovery, particularly in drug design. However, these models often struggle to generate high-quality molecules, especially in conditional scenarios where specific molecular properties must be satisfied. In this work, we introduce GeoRCG, a general framework to improve molecular generative models by integrating geometric representation conditions with provable theoretical guarantees. We decompose the generation process into two stages: first, generating an informative geometric representation; second, generating a molecule conditioned on the representation. Compared with single-stage generation, the easy-to-generate representation in the first stage guides the second stage generation toward a high-quality molecule in a goal-oriented way. Leveraging EDM and SemlaFlow as base generators, we observe significant quality improvements in unconditional molecule generation on the widely used QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional molecular generation task, our framework achieves an average 50\\% performance improvement over state-of-the-art approaches, highlighting the superiority of conditioning on semantically rich geometric representations. Furthermore, with such representation guidance, the number of diffusion steps can be reduced to as small as 100 while largely preserving the generation quality achieved with 1,000 steps, thereby significantly reducing the generation iterations needed.", "poster_file": "46339.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46339.png?t=1751662067.3096688", "page_url": "https://icml.cc/virtual/2025/poster/46339", "openreview_url": "https://openreview.net/forum?id=79O2XccGXZ"},
{"title": "ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals", "authors": "Utkarsh Saxena , Sayeh Sharify , Kaushik Roy , Xin Wang", "abstract": "Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g.~8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers.  We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama and Qwen2.5 families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33\\% lower perplexity on Wikitext than the next best method SpinQuant, and upto 3X speedup over 16-bit baseline. Anonymous code repository available  at https://anonymous.4open.science/r/project-resq-2142.", "poster_file": "46466.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46466.png?t=1752167865.927488", "page_url": "https://icml.cc/virtual/2025/poster/46466", "openreview_url": "https://openreview.net/forum?id=4qIP1sXcR1"},
{"title": "Exogenous Isomorphism for Counterfactual Identifiability", "authors": "Yikang Chen , Dehui du", "abstract": "This paper investigates $\\sim_{\\mathcal{L}\\_3}$-identifiability, a form of complete counterfactual identifiability within the Pearl Causal Hierarchy (PCH) framework, ensuring that all Structural Causal Models (SCMs) satisfying the given assumptions provide consistent answers to all causal questions. To simplify this problem, we introduce exogenous isomorphism and propose $\\sim_{\\mathrm{EI}}$-identifiability, reflecting the strength of model identifiability required for $\\sim_{\\mathcal{L}\\_3}$-identifiability. We explore sufficient assumptions for achieving $\\sim_{\\mathrm{EI}}$-identifiability in two special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual transport, and Triangular Monotonic SCMs (TM-SCMs), which extend $\\sim_{\\mathcal{L}\\_2}$-identifiability. Our results unify and generalize existing theories, providing theoretical guarantees for practical applications. Finally, we leverage neural TM-SCMs to address the consistency problem in counterfactual reasoning, with experiments validating both the effectiveness of our method and the correctness of the theory.", "poster_file": "46152.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46152.png?t=1751723930.8006814", "page_url": "https://icml.cc/virtual/2025/poster/46152", "openreview_url": "https://openreview.net/forum?id=AnoIgkc6WS"},
{"title": "Learning the RoPEs: Better 2D and 3D Position Encodings with STRING", "authors": "Connor Schenck , Isaac Reid , Mithun Jacob , Alex Bewley , Joshua Ainslie , David Rendleman , Deepali Jain , Mohit Sharma , Kumar Avinava Dubey , Ayzaan Wahid , Sumeet Singh , René Wagner , Tianli Ding , Chuyuan Fu , Arunkumar Byravan , Jacob J Varley , Alexey Gritsenko , Matthias Minderer , Dmitry Kalashnikov , Jonathan Tompson , Vikas Sindhwani , Krzysztof Choromanski", "abstract": "We introduce $\\textbf{STRING}$: Separable Translationally Invariant Position Encodings. STRING extends Rotary Position Encodings, a recently proposed and widely used algorithm in large language models, via a unifying theoretical framework. Importantly, STRING still provides $\\textbf{exact}$ translation invariance, including token coordinates of arbitrary dimensionality, whilst maintaining a low computational footprint. These properties are especially important in robotics, where efficient 3D token representation is key. We integrate STRING into Vision Transformers with RGB(-D) inputs (color plus optional depth), showing substantial gains, e.g. in open-vocabulary object detection and for robotics controllers.We complement our experiments with a rigorous mathematical analysis, proving the universality of our methods. Videos of STRING-based robotics controllers can be found here: https://sites.google.com/view/string-robotics.", "poster_file": "44956.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44956.png?t=1752268317.3711631", "page_url": "https://icml.cc/virtual/2025/poster/44956", "openreview_url": "https://openreview.net/forum?id=XXFBqfwnUp"},
{"title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation", "authors": "jiayue Liu , Zhongchao Yi , Zhengyang Zhou , Qihe Huang , Kuo Yang , Xu Wang , Yang Wang", "abstract": "Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42\\% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.Code available at [https://github.com/Rodger-Lau/SynEVO](https://github.com/Rodger-Lau/SynEVO).", "poster_file": "45354.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45354.png?t=1751705187.90175", "page_url": "https://icml.cc/virtual/2025/poster/45354", "openreview_url": "https://openreview.net/forum?id=Q3rGQUGgWo"},
{"title": "Implicit Language Models are RNNs: Balancing Parallelization and Expressivity", "authors": "Mark Schoene , Babak Rahmani , Heiner Kremer , Fabian Falck , Hitesh Ballani , Jannes Gladrow", "abstract": "State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks. Our code is publicly available at github.com/microsoft/implicit_languagemodels", "poster_file": "46440.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46440.png?t=1751963024.8157492", "page_url": "https://icml.cc/virtual/2025/poster/46440", "openreview_url": "https://openreview.net/forum?id=5EbiopWH6e"},
{"title": "Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data", "authors": "Jeonghye Kim , Yongjae Shin , Whiyoung Jung , Sunghoon Hong , Deunsol Yoon , Youngchul Sung , Kanghoon Lee , Woohyung Lim", "abstract": "Reinforcement learning with offline data suffers from Q-value extrapolation errors. To address this issue, we first demonstrate that linear extrapolation of the Q-function beyond the data range is particularly problematic. To mitigate this, we propose guiding the gradual decrease of Q-values outside the data range, which is achieved through reward scaling with layer normalization (RS-LN) and a penalization mechanism for infeasible actions (PA). By combining RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a range of tasks, demonstrating superior performance compared to state-of-the-art algorithms in both offline training and online fine-tuning on the D4RL benchmark, with notable success in the challenging AntMaze Ultra task.", "poster_file": "45886.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45886.png?t=1752136552.3973079", "page_url": "https://icml.cc/virtual/2025/poster/45886", "openreview_url": "https://openreview.net/forum?id=FSVdEzR4To"},
{"title": "Robust Automatic Modulation Classification with Fuzzy Regularization", "authors": "Xinyan Liang , Ruijie Sang , Yuhua Qian , Qian Guo , Feijiang Li , Liang Du", "abstract": "Automatic Modulation Classification (AMC) serves as a foundational pillar for cognitive radio systems, enabling critical functionalities including dynamic spectrum allocation, non-cooperative signal surveillance, and adaptive waveform optimization. However, practical deployment of AMC faces a fundamental challenge: prediction ambiguity arising from intrinsic similarity among modulation schemes and exacerbated under low signal-to-noise ratio (SNR) conditions. This phenomenon manifests as near-identical probability distributions across confusable modulation types, significantly degrading classification reliability. To address this, we propose Fuzzy Regularization-enhanced AMC (FR-AMC), a novel framework that integrates uncertainty quantification into the classification pipeline. The proposed FR has three features: (1) Explicitly model prediction ambiguity during backpropagation, (2) dynamic sample reweighting through adaptive loss scaling, (3) encourage margin maximization between confusable modulation clusters. Experimental results on benchmark datasets demonstrate that the FR achieves superior classification accuracy and robustness compared to compared methods, making it a promising solution for real-world spectrum management and communication applications.", "poster_file": "46022.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46022.png?t=1750670490.3682723", "page_url": "https://icml.cc/virtual/2025/poster/46022", "openreview_url": "https://openreview.net/forum?id=DDIGCk25BO"},
{"title": "Optimizing Adaptive Attacks against Watermarks for Language Models", "authors": "Abdulrahman Diaa , Toluwani Aremu , Nils Lukas", "abstract": "Large Language Models (LLMs) can be misused to spread unwanted content at scale. Content watermarking deters misuse by hiding messages in content, enabling its detection using a secret *watermarking key*. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content's quality. Many LLM watermarking methods have been proposed, but robustness is tested only against *non-adaptive* attackers who lack knowledge of the watermarking method and can find only suboptimal attacks. We formulate watermark robustness as an objective function and use preference-based optimization to tune *adaptive* attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks evade detection against all surveyed watermarks, (ii) training against *any* watermark succeeds in evading unseen watermarks, and (iii) optimization-based attacks are cost-effective. Our findings underscore the need to test robustness against adaptively tuned attacks. We release our adaptively tuned paraphrasers at <https://github.com/nilslukas/ada-wm-evasion>.", "poster_file": "46148.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46148.png?t=1751390854.5568287", "page_url": "https://icml.cc/virtual/2025/poster/46148", "openreview_url": "https://openreview.net/forum?id=AsODat0dkE"},
{"title": "Training Dynamics of In-Context Learning in Linear Attention", "authors": "Yedi Zhang , Aaditya Singh , Peter Latham , Andrew Saxe", "abstract": "While attention-based models have demonstrated the remarkable ability of in-context learning (ICL), the theoretical understanding of how these models acquired this ability through gradient descent training is still preliminary. Towards answering this question, we study the gradient descent dynamics of multi-head linear self-attention trained for in-context linear regression. We examine two parametrizations of linear self-attention: one with the key and query weights merged as a single matrix (common in theoretical studies), and one with separate key and query matrices (closer to practical settings). For the merged parametrization, we show that the training dynamics has two fixed points and the loss trajectory exhibits a single, abrupt drop. We derive an analytical time-course solution for a certain class of datasets and initialization. For the separate parametrization, we show that the training dynamics has exponentially many fixed points and the loss exhibits saddle-to-saddle dynamics, which we reduce to scalar ordinary differential equations. During training, the model implements principal component regression in context with the number of principal components increasing over training time. Overall, we provide a theoretical description of how ICL abilities evolve during gradient descent training of linear attention, revealing abrupt acquisition or progressive improvements depending on how the key and query …", "poster_file": "44810.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44810.png?t=1751382163.4056323", "page_url": "https://icml.cc/virtual/2025/poster/44810", "openreview_url": "https://openreview.net/forum?id=aFNq67ilos"},
{"title": "Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development", "authors": "Daoyuan Chen , Haibin Wang , Yilun Huang , Ce Ge , Yaliang Li , Bolin Ding , Jingren Zhou", "abstract": "The emergence of multimodal large models has advanced artificial intelligence, introducing unprecedented levels of performance and functionality. However, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resource utilization. In response, we present a new sandbox suite tailored for integrated data-model co-development. This sandbox provides a feedback-driven experimental platform, enabling cost-effective iteration and guided refinement of both data and models. Our proposed ``Probe-Analyze-Refine'' workflow, validated through practical use cases on multimodal tasks such as image-text pre-training with CLIP, image-to-text generation with LLaVA-like models, and text-to-video generation with DiT-based models, yields transferable and notable performance boosts, such as topping the VBench leaderboard. A comprehensive set of over 100 experiments demonstrated the suite's usability and extensibility, while also uncovering insights into the interplay between data quality, diversity, model behavior, and computational costs. All codes, datasets, and models are open-sourced to foster future research and applications that would otherwise be infeasible due to the lack of a dedicated co-development infrastructure.", "poster_file": "43484.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43484.png?t=1752334191.410657", "page_url": "https://icml.cc/virtual/2025/poster/43484", "openreview_url": "https://openreview.net/forum?id=zIGIvysR1H"},
{"title": "When and How Does CLIP Enable Domain and Compositional Generalization?", "authors": "Elias Kempf , Simon Schrodi , Max Argus , Thomas Brox", "abstract": "The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions. However,  key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)? Can it generalize to unseen classes within partially seen domains (compositional generalization)? What factors affect such generalization? To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure. Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain. Through data-centric *and* mechanistic analyses, we find that successful generalization requires the learning of sufficiently shared representations in intermediate layers and circuits.", "poster_file": "45573.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45573.png?t=1750920688.62897", "page_url": "https://icml.cc/virtual/2025/poster/45573", "openreview_url": "https://openreview.net/forum?id=Lktwi30g63"},
{"title": "Instance Correlation Graph-based Naive Bayes", "authors": "Chengyuan Li , Liangxiao Jiang , Wenjun Zhang , Liangjun Yu , Huan Zhang", "abstract": "Due to its simplicity, effectiveness and robustness, naive Bayes (NB) has continued to be one of the top 10 data mining algorithms. To improve its performance, a large number of improved algorithms have been proposed in the last few decades. However, in addition to Gaussian naive Bayes (GNB), there is little work on numerical attributes. At the same time,  none of them takes into account the correlations among instances. To fill this gap, we propose a novel algorithm called instance correlation graph-based naive Bayes (ICGNB). Specifically, it first uses original attributes to construct an instance correlation graph (ICG) to represent the correlations among instances. Then, it employs a variational graph auto-encoder (VGAE) to generate new attributes from the constructed ICG and uses them to augment original attributes.Finally, it weights each augmented attribute to alleviate the attribute redundancy and builds GNB on the weighted attributes. The experimental results on tens of datasets show that ICGNB significantly outperforms its deserved competitors.Our codes and datasets are available at https://github.com/jiangliangxiao/ICGNB.", "poster_file": "44386.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44386.png?t=1750170900.077328", "page_url": "https://icml.cc/virtual/2025/poster/44386", "openreview_url": "https://openreview.net/forum?id=hwTKGdM4TK"},
{"title": "Improving Consistency Models with Generator-Augmented Flows", "authors": "Thibaut Issenhuth , Sangchul Lee , Ludovic Dos Santos , Jean-Yves Franceschi , Chansoo Kim , alain rakotomamonjy", "abstract": "Consistency models imitate the multi-step sampling of score-based diffusion in a single forward pass of a neural network.They can be learned in two ways: consistency distillation and consistency training. The former relies on the true velocity field of the corresponding differential equation, approximated by a pre-trained neural network.In contrast, the latter uses a single-sample Monte Carlo estimate of this velocity field.The related estimation error induces a discrepancy between consistency distillation and training that, we show, still holds in the continuous-time limit.To alleviate this issue, we propose a novel flow that transports noisy data towards their corresponding outputs derived from a consistency model.We prove that this flow reduces the previously identified discrepancy and the noise-data transport cost.Consequently, our method not only accelerates consistency training convergence but also enhances its overall performance. The code is available at https://github.com/thibautissenhuth/consistency_GC.", "poster_file": "45833.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45833.png?t=1752237326.695852", "page_url": "https://icml.cc/virtual/2025/poster/45833", "openreview_url": "https://openreview.net/forum?id=GKqoqGCHTq"},
{"title": "G-Adaptivity: optimised graph-based mesh relocation for finite element methods", "authors": "James Rowbottom , Georg Maierhofer , Teo Deveney , Eike Müller , Alberto Paganini , Katharina Schratz , Pietro Lió , Carola-Bibiane Schönlieb , Chris Budd", "abstract": "We present a novel, and effective, approach to achieve optimal mesh relocation in finite element methods (FEMs). The cost and accuracy of FEMs is critically dependent on the choice of mesh points. Mesh relocation (r-adaptivity) seeks to optimise the mesh geometry to obtain the best solution accuracy at given computational budget. Classical r-adaptivity relies on the solution of a separate nonlinear ``meshing'' PDE to determine mesh point locations. This incurs significant cost at remeshing, and relies on estimates that relate interpolation- and FEM-error. Recent machine learning approaches have focused on the construction of fast surrogates for such classical methods. Instead, our new approach trains a graph neural network (GNN) to determine mesh point locations by directly minimising the FE solution error from the PDE system Firedrake to achieve higher solution accuracy. Our GNN architecture closely aligns the mesh solution space to that of classical meshing methodologies, thus replacing classical estimates for optimality with a learnable strategy. This allows for rapid and robust training and results in an extremely efficient and effective GNN approach to online r-adaptivity. Our method outperforms both classical, and prior ML, approaches to r-adaptive meshing. In particular, it achieves lower FE solution error, whilst retaining the significant …", "poster_file": "43974.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43974.png?t=1751640629.8974917", "page_url": "https://icml.cc/virtual/2025/poster/43974", "openreview_url": "https://openreview.net/forum?id=pyIXyl4qFx"},
{"title": "CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation", "authors": "Aditya Gorla , Ryan Wang , Zhengtong Liu , Ulzee An , Sriram Sankararaman", "abstract": "We present CACTI, a masked autoencoding approach for imputing tabular data that leverages the structure in missingness patterns and contextual information. Our approach employs a novel median truncated copy masking training strategy that encourages the model to learn from empirical patterns of missingness while incorporating semantic relationships between features — captured by column names and text descriptions — to better represent feature dependence. These dual sources of inductive bias enable CACTIto outperform state-of-the-art methods —  an average $R^2$ gain of 7.8\\% over the next best method (13.4%, 6.1%, and 5.3% under missing not at random, at random and completely at random, respectively) — across a diverse range of datasets and missingness conditions. Our results highlight the value of leveraging dataset-specific contextual information and missingness patterns to enhance imputation performance.", "poster_file": "45732.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45732.png?t=1752474103.4803069", "page_url": "https://icml.cc/virtual/2025/poster/45732", "openreview_url": "https://openreview.net/forum?id=IYLNdCII48"},
{"title": "Trusted Multi-View Classification  with Expert Knowledge Constraints", "authors": "Xinyan Liang , Shijie Wang , Yuhua Qian , Qian Guo , Liang Du , Bingbing Jiang , Tingjin Luo , Feijiang Li", "abstract": "Multi-view classification (MVC) based on the Dempster-Shafer theory has gained significant recognition for its reliability in safety-critical applications. However, existing methods predominantly focus on providing confidence levels for decision outcomes without explaining the reasoning behind these decisions. Moreover, the reliance on first-order statistical magnitudes of belief masses often inadequately capture the intrinsic uncertainty within the evidence.  To address these limitations, we propose a novel framework termed Trusted Multi-view Classification Constrained with Expert Knowledge (TMCEK). TMCEK integrates expert knowledge to enhance feature-level interpretability and introduces a distribution-aware subjective opinion mechanism to derive more reliable and realistic confidence estimates. The theoretical superiority of the proposed uncertainty measure over conventional approaches is rigorously established. Extensive experiments conducted on three multi-view datasets for sleep stage classification demonstrate that TMCEK achieves state-of-the-art performance while offering interpretability at both the feature and decision levels. These results position TMCEK as a robust and interpretable solution for MVC in safety-critical domains. The code is available at https://github.com/jie019/TMCEK_ICML2025.", "poster_file": "45140.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45140.png?t=1751266462.563461", "page_url": "https://icml.cc/virtual/2025/poster/45140", "openreview_url": "https://openreview.net/forum?id=U64wEbM7NB"},
{"title": "The Number of Trials Matters in Infinite-Horizon General-Utility Markov Decision Processes", "authors": "Pedro Santos , Alberto Sardinha , Francisco S. Melo", "abstract": "The general-utility Markov decision processes (GUMDPs) framework generalizes the MDPs framework by considering objective functions that depend on the frequency of visitation of state-action pairs induced by a given policy. In this work, we contribute with the first analysis on the impact of the number of trials, i.e., the number of randomly sampled trajectories, in infinite-horizon GUMDPs. We show that, as opposed to standard MDPs, the number of trials plays a key-role in infinite-horizon GUMDPs and the expected performance of a given policy depends, in general, on the number of trials. We consider both discounted and average GUMDPs, where the objective function depends, respectively, on discounted and average frequencies of visitation of state-action pairs.  First, we study policy evaluation under discounted GUMDPs, proving lower and upper bounds on the mismatch between the finite and infinite trials formulations for GUMDPs. Second, we address average GUMDPs, studying how different classes of GUMDPs impact the mismatch between the finite and infinite trials formulations. Third, we provide a set of empirical results to support our claims, highlighting how the number of trajectories and the structure of the underlying GUMDP influence policy evaluation.", "poster_file": "45751.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45751.png?t=1752136960.403208", "page_url": "https://icml.cc/virtual/2025/poster/45751", "openreview_url": "https://openreview.net/forum?id=I4jNAbqHnM"},
{"title": "Causal Attribution Analysis for Continuous Outcomes", "authors": "Shanshan Luo , Yu yixuan , Chunchen LIU , Feng Xie , zhi geng", "abstract": "Previous studies have extensively addressed the attribution problem for binary outcome variables. However, in many practical scenarios, the outcome variable is continuous, and simply binarizing it may result in information loss or biased conclusions. To address this issue, we propose a series of posterior causal estimands for retrospectively evaluating multiple correlated causes from a continuous outcome. These estimands include posterior intervention effects, posterior total causal effects, and posterior natural direct effects. Under assumptions of sequential ignorability, monotonicity, and perfect positive rank, we show that the posterior causal estimands of interest are identifiable and present the corresponding identification equations. We also provide a simple but effective estimation procedure and establish asymptotic properties of the proposed estimators. An artificial hypertension example and a real developmental toxicity dataset are employed to illustrate our method.", "poster_file": "45575.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45575.png?t=1752164406.8276248", "page_url": "https://icml.cc/virtual/2025/poster/45575", "openreview_url": "https://openreview.net/forum?id=Lie2rOCgkh"},
{"title": "The Role of Randomness in Stability", "authors": "Max Hopkins , Shay Moran", "abstract": "Stability is a central property in learning and statistics promising the output of an algorithm $\\mathcal{A}$ does not change substantially when applied to similar datasets $S$ and $S'$. It is an elementary fact that any sufficiently stable algorithm (e.g.\\ one returning the same result with high probability, satisfying privacy guarantees, etc.) must be randomized. This raises a natural question: can we quantify \\textit{how much} randomness is needed for algorithmic stability? We study the randomness complexity of two influential notions of stability in learning: \\textit{replicability} (which promises $\\mathcal{A}$ usually outputs the same result when run over samples from the same distribution), and \\textit{differential privacy} (which promises the output distribution of $\\mathcal{A}$ remains similar under neighboring datasets). In particular, building on the ideas of (Dixon, Pavan, Vander Woude, and Vinodchandran ICML 2024) and (Cannone, Su, and Vadhan ITCS 2024), we prove a \"weak-to-strong\" boosting theorem for stability in these settings: the randomness complexity of a task $\\mathcal{M}$ is tightly controlled by the best replication probability of any \\textit{deterministic} algorithm solving $\\mathcal{M}$, a parameter known as $\\mathcal{M}$'s \"global stability\" (Chase, Moran, Yehudayoff FOCS 2023). Finally, we use this connection to characterize the randomness complexity of PAC Learning: a class has bounded randomness complexity …", "poster_file": "45041.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45041.png?t=1752423683.040108", "page_url": "https://icml.cc/virtual/2025/poster/45041", "openreview_url": "https://openreview.net/forum?id=W2Fe1hT7Ks"},
{"title": "Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss", "authors": "Bo-Han Lai , Pin-Han Huang , Bo-Han Kung , Shang-Tse Chen", "abstract": "Lipschitz neural networks are well-known for providing certified robustness in deep learning. In this paper, we present a novel, efficient Block Reflector Orthogonal (BRO) layer that enhances the capability of orthogonal layers on constructing more expressive Lipschitz neural architectures. In addition, by theoretically analyzing the nature of Lipschitz neural networks, we introduce a new loss function that employs an annealing mechanism to increase margin for most data points. This enables Lipschitz models to provide better certified robustness. By employing our BRO layer and loss function, we design BRONet — a simple yet effective Lipschitz neural network that achieves state-of-the-art certified robustness. Extensive experiments and empirical analysis on CIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms existing baselines. The implementation is available at [GitHub Link](https://github.com/ntuaislab/BRONet).", "poster_file": "45247.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45247.png?t=1751080243.37269", "page_url": "https://icml.cc/virtual/2025/poster/45247", "openreview_url": "https://openreview.net/forum?id=S2K5MyRjrL"},
{"title": "On the Power of Context-Enhanced Learning in LLMs", "authors": "Xingyu Zhu , Abhishek Panigrahi , Sanjeev Arora", "abstract": "We formalize a new concept for LLMs, **context-enhanced learning**. It involves standard gradient-based learning on text except that the context is enhanced with additional data on which no auto-regressive gradients are computed. This setting is a gradient-based analog of usual in-context learning (ICL) and appears in some recent works.Using a multi-step reasoning task, we prove in a simplified setting that context-enhanced learning can be **exponentially more sample-efficient** than standard learning when the model is capable of ICL. At a mechanistic level, we find that the benefit of context-enhancement arises from a more accurate gradient learning signal.We also experimentally demonstrate that it appears hard to detect or recover learning materials that were used in the context during training. This may have implications for data security as well as copyright.", "poster_file": "45811.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45811.png?t=1751924622.9543319", "page_url": "https://icml.cc/virtual/2025/poster/45811", "openreview_url": "https://openreview.net/forum?id=Gn6L4QRKf7"},
{"title": "TimeBase: The Power of Minimalism in Efficient Long-term Time Series Forecasting", "authors": "Qihe Huang , Zhengyang Zhou , Kuo Yang , Zhongchao Yi , Xu Wang , Yang Wang", "abstract": "Long-term time series forecasting (LTSF) has traditionally relied on  large parameters to capture extended temporal dependencies, resulting in substantial computational costs and inefficiencies in both memory usage and processing time. However, time series data, unlike high-dimensional images or text, often exhibit temporal pattern similarity and  low-rank structures, especially in long-term horizons.  By leveraging this structure, models can be guided to focus on more essential, concise temporal data, improving both accuracy and computational efficiency. In this paper, we introduce TimeBase, an ultra-lightweight network  to harness the power of minimalism in LTSF.  TimeBase 1) extracts core basis temporal components and 2) transforms traditional point-level forecasting into efficient segment-level forecasting, achieving optimal utilization of both data and parameters. Extensive experiments on diverse  real-world datasets show that TimeBase achieves remarkable efficiency and secures competitive forecasting performance. Additionally, TimeBase can also serve as a very effective plug-and-play complexity reducer for any patch-based forecasting models. Code is available at \\url{https://github.com/hqh0728/TimeBase}.", "poster_file": "45815.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45815.png?t=1751865963.1001914", "page_url": "https://icml.cc/virtual/2025/poster/45815", "openreview_url": "https://openreview.net/forum?id=GhTdNOMfOD"},
{"title": "Catch Your Emotion: Sharpening Emotion Perception in Multimodal Large Language Models", "authors": "Yiyang Fang , Jian Liang , Wenke Huang , He Li , Kehua Su , Mang Ye", "abstract": "Multimodal large language models (MLLMs) have achieved impressive progress in tasks such as visual question answering and visual understanding, but they still face significant challenges in emotional reasoning. Current methods to enhance emotional understanding typically rely on fine-tuning or manual annotations, which are resource-intensive and limit scalability. In this work, we focus on improving the ability of MLLMs to capture emotions during the inference phase. Specifically, MLLMs encounter two main issues: they struggle to distinguish between semantically similar emotions, leading to misclassification, and they are overwhelmed by redundant or irrelevant visual information, which distracts from key emotional cues. To address these, we propose Sharpening Emotion Perception in MLLMs (SEPM), which incorporates a Confidence-Guided Coarse-to-Fine Inference framework to refine emotion classification by guiding the model through simpler tasks. Additionally, SEPM employs Focus-on-Emotion Visual Augmentation to reduce visual redundancy by directing the attention of models to relevant emotional cues in images. Experimental results demonstrate that SEPM significantly improves MLLM performance on emotion-related tasks, providing a resource-efficient and scalable solution for emotion recognition.", "poster_file": "45730.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45730.png?t=1750837618.555637", "page_url": "https://icml.cc/virtual/2025/poster/45730", "openreview_url": "https://openreview.net/forum?id=IYOksPHJKT"},
{"title": "SAFE: Finding Sparse and Flat Minima to Improve Pruning", "authors": "Dongyeop Lee , Kwanhee Lee , Jinseok Chung , Namhoon Lee", "abstract": "Sparsifying neural networks often suffers from seemingly inevitable performance degradation, and it remains challenging to restore the original performance despite much recent progress.Motivated by recent studies in robust optimization, we aim to tackle this problem by finding subnetworks that are both sparse and flat at the same time.Specifically, we formulate pruning as a sparsity-constrained optimization problem where flatness is encouraged as an objective.We solve it explicitly via an augmented Lagrange dual approach and extend it further by proposing a generalized projection operation, resulting in novel pruning methods called SAFE and its extension, SAFE$^+$.Extensive evaluations on standard image classification and language modeling tasks reveal that SAFE consistently yields sparse networks with improved generalization performance, which compares competitively to well-established baselines.In addition, SAFE demonstrates resilience to noisy data, making it well-suited for real-world conditions.", "poster_file": "46658.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46658.png?t=1751799688.0017884", "page_url": "https://icml.cc/virtual/2025/poster/46658", "openreview_url": "https://openreview.net/forum?id=10l1pGeOcK"},
{"title": "Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning", "authors": "Armin Behnamnia , Gholamali Aminian , Alireza Aghaei , Chengchun Shi , Vincent Tan , Hamid R Rabiee", "abstract": "Off-policy learning and evaluation leverage logged bandit feedback datasets, which contain context, action, propensity score, and feedback for each data point. These scenarios face significant challenges due to high variance and poor performance with low-quality propensity scores and heavy-tailed reward distributions. We address these issues by introducing a novel estimator based on the log-sum-exponential (LSE) operator, which outperforms traditional inverse propensity score estimators. Our LSE estimator demonstrates variance reduction and robustness under heavy-tailed conditions. For off-policy evaluation, we derive upper bounds on the estimator's bias and variance. In the off-policy learning scenario, we establish bounds on the regret—the performance gap between our LSE estimator and the optimal policy—assuming bounded $(1+\\epsilon)$-th moment of weighted reward. Notably, we achieve a convergence rate of $O(n^{-\\epsilon/(1+\\epsilon)})$ for the regret bounds, where $\\epsilon\\in[0,1]$ and $n$ is the size of logged bandit feedback dataset. Theoretical analysis is complemented by comprehensive empirical evaluations in both off-policy learning and evaluation scenarios, confirming the practical advantages of our approach. The code for our estimator is available at the following link: https://github.com/armin-behnamnia/lse-offpolicy-learning .", "poster_file": "45755.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45755.png?t=1753039751.745205", "page_url": "https://icml.cc/virtual/2025/poster/45755", "openreview_url": "https://openreview.net/forum?id=HxCuvx2uUi"},
{"title": "Gridded Transformer Neural Processes for Spatio-Temporal Data", "authors": "Matthew Ashman , Cristiana Diaconu , Eric Langezaal , Adrian Weller , Richard E Turner", "abstract": "Effective modelling of large-scale spatio-temporal datasets is essential for many domains, yet existing approaches often impose rigid constraints on the input data, such as requiring them to lie on fixed-resolution grids. With the rise of foundation models, the ability to process diverse, heterogeneous data structures is becoming increasingly important. Neural processes (NPs), particularly transformer neural processes (TNPs), offer a promising framework for such tasks, but struggle to scale to large spatio-temporal datasets due to the lack of an efficient attention mechanism. To address this, we introduce gridded pseudo-token TNPs which employ specialised encoders and decoders to handle unstructured data and utilise a processor comprising gridded pseudo-tokens with efficient attention mechanisms. Furthermore, we develop equivariant gridded TNPs for applications where exact or approximate translation equivariance is a useful inductive bias, improving accuracy and training efficiency. Our method consistently outperforms a range of strong baselines in various synthetic and real-world regression tasks involving large-scale data, while maintaining competitive computational efficiency. Experiments with weather data highlight the potential of gridded TNPs and serve as just one example of a domain where they can have a significant impact.", "poster_file": "45467.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45467.png?t=1752253167.8667648", "page_url": "https://icml.cc/virtual/2025/poster/45467", "openreview_url": "https://openreview.net/forum?id=O0oe7hPtbl"},
{"title": "Where is the Truth? The Risk of Getting Confounded in a Continual World", "authors": "Florian Peter Busch , Roshni Ramanna Kamath , Rupert Mitchell , Wolfgang Stammer , Kristian Kersting , Martin Mundt", "abstract": "A dataset is confounded if it is most easily solved via a spurious correlation which fails to generalize to new data. In this work, we show that, in a continual learning setting where confounders may vary in time across tasks, the challenge of mitigating the effect of confounders far exceeds the standard forgetting problem normally considered. In particular, we provide a formal description of such continual confounders and identify that, in general, spurious correlations are easily ignored when training for all tasks jointly, but it is harder to avoid confounding when they are considered sequentially. These descriptions serve as a basis for constructing a novel CLEVR-based continually confounded dataset, which we term the ConCon dataset. Our evaluations demonstrate that standard continual learning methods fail to ignore the dataset's confounders. Overall, our work highlights the challenges of confounding factors, particularly in continual learning settings, and demonstrates the need for developing continual learning methods to robustly tackle these.", "poster_file": "44559.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44559.png?t=1752237220.1339896", "page_url": "https://icml.cc/virtual/2025/poster/44559", "openreview_url": "https://openreview.net/forum?id=evb9dNxCN5"},
{"title": "Monte-Carlo Tree Search with Uncertainty Propagation via Optimal Transport", "authors": "Tuan Dam , Pascal Stenger , Lukas Schneider , Joni Pajarinen , Carlo D'Eramo , Odalric-Ambrym Maillard", "abstract": "This paper introduces a novel backup strategy for Monte-Carlo Tree Search (MCTS) tailored for highly stochastic and partially observable Markov decision processes. We adopt a probabilistic approach, modeling both value and action-value nodes as Gaussian distributions, to introduce a novel backup operator that computes value nodes as the Wasserstein barycenter of their action-value children nodes; thus, propagating the uncertainty of the estimate across the tree to the root node. We study our novel backup operator when using a novel combination of $L^1$-Wasserstein barycenter with $\\alpha$-divergence, by drawing a crucial connection to the generalized mean backup operator. We complement our probabilistic backup operator with two sampling strategies, based on optimistic selection and Thompson sampling, obtaining our Wasserstein MCTS algorithm. We provide theoretical guarantees of asymptotic convergence of $\\mathcal{O}(n^{-1/2})$, with $n$ as the number of visited trajectories, to the optimal policy and an empirical evaluation on several stochastic and partially observable environments, where our approach outperforms well-known related baselines.", "poster_file": "46010.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46010.png?t=1752081606.9544914", "page_url": "https://icml.cc/virtual/2025/poster/46010", "openreview_url": "https://openreview.net/forum?id=DUGFTH9W8B"},
{"title": "Probabilistic Factorial Experimental Design for Combinatorial Interventions", "authors": "Divya Shyamal , Jiaqi Zhang , Caroline Uhler", "abstract": "A _combinatorial intervention_, consisting of multiple treatments applied to a single unit with potential interactive effects, has substantial applications in fields such as biomedicine, engineering, and beyond. Given $p$ possible treatments, conducting all possible $2^p$ combinatorial interventions can be laborious and quickly becomes infeasible as $p$ increases. Here we introduce the _probabilistic factorial experimental design_, formalized from how scientists perform lab experiments. In this framework, the experimenter selects a dosage for each possible treatment and applies it to a group of units. Each unit independently receives a random combination of treatments, sampled from a product Bernoulli distribution determined by the dosages. Additionally, the experimenter can carry out such experiments over multiple rounds, adapting the design in an active manner. We address the optimal experimental design problem within a novel intervention model that imposes bounded-degree interactions between treatments. In the passive setting, we provide a closed-form solution for the near-optimal design. Our results prove that a dosage of $\\frac{1}{2}$ for each treatment is optimal up to a factor of $1+O(\\frac{\\ln(n)}{n})$ for estimating any $k$-way interaction model, regardless of $k$, and imply that $O\\big(kp^{3k}\\ln(p)\\big)$ observations are required to accurately estimate this model. For the multi-round setting, we provide a near-optimal acquisition function …", "poster_file": "45285.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45285.png?t=1752681666.3938673", "page_url": "https://icml.cc/virtual/2025/poster/45285", "openreview_url": "https://openreview.net/forum?id=RL6d53a5jj"},
{"title": "Efficient Source-free Unlearning via Energy-Guided Data Synthesis and Discrimination-Aware Multitask Optimization", "authors": "Xiuyuan Wang , Chaochao Chen , Weiming Liu , Xinting Liao , Fan Wang , Xiaolin Zheng", "abstract": "With growing privacy concerns and the enforcement of data protection regulations, machine unlearning has emerged as a promising approach for removing the influence of forget data while maintaining model performance on retain data. However, most existing unlearning methods require access to the original training data, which is often impractical due to privacy policies, storage constraints, and other limitations. This gives rise to the challenging task of source-free unlearning, where unlearning must be accomplished without accessing the original training data. Few existing source-free unlearning methods rely on knowledge distillation and model retraining, which impose substantial computational costs. In this work, we propose the Data Synthesis-based Discrimination-Aware (DSDA) unlearning framework, which enables efficient source-free unlearning in two stages: (1) Accelerated Energy-Guided Data Synthesis (AEGDS), which employs Langevin dynamics to model the training data distribution while integrating Runge–Kutta methods and momentum to enhance efficiency. (2) Discrimination-Aware Multitask Optimization (DAMO), which refines the feature distribution of retain data and mitigates the gradient conflicts among multiple unlearning objectives. Extensive experiments on three benchmark datasets demonstrate that DSDA outperforms existing unlearning methods, validating its effectiveness and efficiency in source-free unlearning.", "poster_file": "43757.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43757.png?t=1751718740.1782064", "page_url": "https://icml.cc/virtual/2025/poster/43757", "openreview_url": "https://openreview.net/forum?id=tqL8gJsuS5"},
{"title": "Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain", "authors": "Gaozheng Pei , Ke Ma , Yingfei Sun , Qianqian Xu , Qingming Huang", "abstract": "The diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. Due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. We turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. We find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. This means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. Meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. Therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. Specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image.For the phase spectrum, we project the phase of the estimated image into a designated …", "poster_file": "46096.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46096.png?t=1751465738.4353688", "page_url": "https://icml.cc/virtual/2025/poster/46096", "openreview_url": "https://openreview.net/forum?id=Bm706VlAtU"},
{"title": "GMAIL: Generative Modality Alignment for generated Image Learning", "authors": "Shentong Mo , Sukmin Yun", "abstract": "Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined \\textit{GMAIL}, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves …", "poster_file": "43745.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/43745.png?t=1751610728.5822399", "page_url": "https://icml.cc/virtual/2025/poster/43745", "openreview_url": "https://openreview.net/forum?id=u6xeKVHS6K"},
{"title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs", "authors": "Greyson Brothers", "abstract": "We investigate the design of pooling methods used to summarize the outputs of transformer embedding models, primarily motivated by reinforcement learning and vision applications. This work considers problems where a subset of the input vectors contains requisite information for a downstream task (signal) while the rest are distractors (noise). By framing pooling as vector quantization with the goal of minimizing signal loss, we demonstrate that the standard methods used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs fluctuates. We then show that an attention-based *adaptive pooling* method can approximate the signal-optimal vector quantizer within derived error bounds for any SNR. Our theoretical results are first validated by supervised experiments on a synthetic dataset designed to isolate the SNR problem, then generalized to standard relational reasoning, multi-agent reinforcement learning, and vision benchmarks with noisy observations, where transformers with adaptive pooling display superior robustness across tasks.", "poster_file": "46284.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/46284.png?t=1752600049.9521952", "page_url": "https://icml.cc/virtual/2025/poster/46284", "openreview_url": "https://openreview.net/forum?id=8JGwoZceQs"},
{"title": "Sharp Generalization for Nonparametric Regression by Over-Parameterized Neural Networks: A Distribution-Free Analysis in Spherical Covariate", "authors": "Yingzhen Yang", "abstract": "Sharp generalization bound for neural networks trained by gradient descent (GD) is of central interest in statistical learning theory and deep learning. In this paper, we consider nonparametric regressionby an over-parameterized two-layer NN trained by GD. We show that, if the neural network is trained by GD with early stopping, then the trained network renders a sharp rate of the nonparametric regression risk of $O(\\epsilon_n^2)$,  which is the same rate as that for the classical kernel regression trained by GD with early stopping, where $\\epsilon_n$ is the critical population rate of the Neural Tangent Kernel (NTK) associated with the network and $n$ is the size of the training data. It is remarked that our result does not require distributional assumptions on the covariate as long as the covariate lies on the unit sphere, in a strong contrast with many existing results which rely on specific distributions such as the spherical uniform data distribution or distributions satisfying certain restrictive conditions.As a special case of our general result, when the eigenvalues of the associated NTKdecay at a rate of $\\lambda_j \\asymp j^{-\\frac{d}{d-1}}$ for $j \\ge 1$ which happens under certain distributional assumption such as the training features follow the spherical uniform distribution, …", "poster_file": "44526.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44526.png?t=1752338927.0326738", "page_url": "https://icml.cc/virtual/2025/poster/44526", "openreview_url": "https://openreview.net/forum?id=fPOkujQBVb"},
{"title": "Feature learning from non-Gaussian inputs: the case of Independent Component Analysis in high dimensions", "authors": "Fabiola Ricci , Lorenzo Bardone , Sebastian Goldt", "abstract": "Deep neural networks learn structured features from complex, non-Gaussian inputs, but the mechanisms behind this process remain poorly understood.   Our work is motivated by the observation that the first-layer filters learnt by deep convolutional neural networks from natural images resemble those learnt by independent component analysis (ICA), a simple unsupervised method that seeks the most non-Gaussian projections of its inputs.   This similarity suggests that ICA provides a simple, yet principled model for studying feature learning.   Here, we leverage this connection to investigate the interplay between data structure and optimisation in feature learning for the most popular ICA algorithm, FastICA, and stochastic gradient descent (SGD), which is used to train deep networks.   We rigorously establish that FastICA requires at least $n\\gtrsim d^4$ samples to recover a single non-Gaussian direction from $d$-dimensional inputs on a simple synthetic data model. We show that vanilla online SGD outperforms FastICA, and prove that the optimal sample complexity $n\\gtrsim d^2$ can be reached by smoothing the loss, albeit in a data-dependent way. We finally demonstrate the existence of a search phase for FastICA on ImageNet, and discuss how the strong non-Gaussianity of said images compensates for the poor sample complexity of FastICA.", "poster_file": "44234.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44234.png?t=1750939487.3140142", "page_url": "https://icml.cc/virtual/2025/poster/44234", "openreview_url": "https://openreview.net/forum?id=kmg7hweySi"},
{"title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation", "authors": "Tianwei Lin , Wenqiao Zhang , Sijing Li , Yuqian Yuan , Binhe Yu , Haoyuan Li , Wanggui He , Hao Jiang , Mengze Li , Song xiaohui , Siliang Tang , Jun Xiao , Hui Lin , Yueting Zhuang , Beng Chin Ooi", "abstract": "We present **HealthGPT**, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained Large Language Models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation **(H-LoRA)** technique, which is complemented by a tailored hierarchical visual perception **(HVP)** approach and a three-stage learning strategy **(TLS)**. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called **VL-Health**. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.", "poster_file": "45007.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/45007.png?t=1750907370.064716", "page_url": "https://icml.cc/virtual/2025/poster/45007", "openreview_url": "https://openreview.net/forum?id=WbP2OwMULq"},
{"title": "STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization", "authors": "Hao Li , Qi Lv , Rui Shao , Xiang Deng , Yinchuan Li , Jianye Hao , Liqiang Nie", "abstract": "Transforming complex actions into discrete skill abstractions has demonstrated strong potential for robotic manipulation.Existing approaches mainly leverage latent variable models, e.g., VQ-VAE, to learn skill abstractions through learned vectors (codebooks), while they suffer from codebook collapse and modeling the causal relationship between learned skills. To address these limitations, we present **S**kill **T**raining with **A**ugmented **R**otation (**STAR**), a framework that advances both skill learning and composition to complete complex behaviors. Specifically, to prevent codebook collapse, we devise rotation-augmented residual skill quantization (RaRSQ).It encodes relative angles between encoder outputs into the gradient flow by rotation-based gradient mechanism. Points within the same skill code are forced to be either pushed apart or pulled closer together depending on gradient directions.Further, to capture the casual relationship between skills, we present causal skill transformer (CST) which explicitly models dependencies between skill representations through an autoregressive mechanism for coherent action generation.Extensive experiments demonstrate the superiority of STAR on both LIBERO benchmark and realworld tasks, with around 12% improvement over the baselines.", "poster_file": "44123.png", "poster_url": "https://icml.cc/media/PosterPDFs/ICML%202025/44123.png?t=1751955749.5637157", "page_url": "https://icml.cc/virtual/2025/poster/44123", "openreview_url": "https://openreview.net/forum?id=n1cqQK4hhC"}
]