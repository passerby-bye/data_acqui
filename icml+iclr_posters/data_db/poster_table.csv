poster_id,table_markdown
1,"| Selection Method | General Knowledge (3 tasks) | Commonsense Reasoning (4 tasks) | Reading Comprehension (2 tasks) | Overall                    | FLOPs |
| ---------------- | --------------------------- | ------------------------------- | ------------------------------- | -------------------------- | ----- |
| Random           | 50.33                       | 36.19                           | 39.09                           | 41.55                      | 7.66  |
| DSIR             | 50.37  $ \uparrow $ 0.04    | 34.01  $ \downarrow $ 2.18      | 38.80  $ \downarrow $ 1.29      | 40.53  $ \downarrow $ 1.02 | 7.66  |
| PPL              | 48.71  $ \downarrow $ 1.62  | 37.72  $ \uparrow $ 1.53        | 38.57  $ \downarrow $ 0.52      | 41.57  $ \uparrow $ 0.02   | 9.51  |
| Semdedup         | 50.99  $ \uparrow $ 0.66    | 36.11  $ \downarrow $ 0.08      | 39.44  $ \uparrow $ 0.35        | 41.81  $ \uparrow $ 0.26   | 8.11  |
| Qurating         | 51.56  $ \uparrow $ 1.23    | 35.93  $ \downarrow $ 0.26      | 39.70  $ \uparrow $ 0.61        | 42.01  $ \uparrow $ 0.46   | 13.66 |
| MATES            | 50.45  $ \uparrow $ 0.12    | 36.06  $ \downarrow $ 0.13      | 39.83  $ \uparrow $ 0.74        | 41.93  $ \uparrow $ 0.38   | 9.81  |
| Quad(ours)       | 52.08  $ \uparrow $ 1.75    | 37.03  $ \uparrow $ 0.84        | 41.07  $ \uparrow $ 1.98        | 42.94  $ \uparrow $ 1.39   | 9.15  |"
2,"| Defence           | Environment | Action Space | Certification |
| ----------------- | ----------- | ------------ | ------------- |
| Deterministic     | Stochastic  | Discrete     | Continuous    |
| COPA [Wu et al.,] | ✓           | ✗            | ✓             |
| Our Method        | ✓           | ✓            | ✓             |"
2,"| Environment | Method         | Noise          | Avg. Cumulative Reward | Action-level Mean Radii |
| ----------- | -------------- | -------------- | ---------------------- | ----------------------- |
| DQN         | C51            | DQN Transition | Trajectory             | C51 Transition          |
| Freeway     | Proposed (RDP) | 0.0            | 20.1                   | 21.3                    |
| 1.0         | 16.9           | 16.1           | 128.1                  | 32.6                    |
| 2.0         | 16.6           | 15.3           | 145.5                  | 58.7                    |
| 3.0         | 16.0           | 15.1           | 160.0                  | 102.4                   |
| COPA        | N/A            | 16.4           | 16.4                   | N/A                     |
| Breakout    | Proposed (RDP) | 0.0            | 385.4                  | 389.3                   |
| 1.0         | 366.6          | 369.0          | 3.4                    | 3.2                     |
| 1.5         | 320.8          | 270.4          | 7.9                    | 7.6                     |
| 2.0         | 268.4          | 102.7          | 17.7                   | 16.9                    |
| COPA        | N/A            | 325.7          | 330.1                  | N/A                     |"
3,"|     | Kinetics50-C | VGGSound-C |
| --- | ------------ | ---------- |
| IQR | UA           | MIS        |
|     |              |            |
| ✓   |              |            |
|     | ✓            |            |
|     |              | ✓          |
| ✓   | ✓            |            |
| ✓   |              | ✓          |
|     | ✓            | ✓          |
| ✓   | ✓            | ✓          |"
3,"| Method | MOSI $ \rightarrow $  SIMS | SIMS $ \rightarrow $  MOSI |
| ------ | -------------------------- | -------------------------- |
| ACC    | F1                         | ACC                        |
| Source | 39.2                       | 39.1                       |
| EATA   | 40.5                       | 41.2                       |
| READ   | 42.0                       | 42.5                       |
| SuMi   | 44.2                       | 44.7                       |"
6,"| Model Type                                                   | Model                      | Overall | Chat | RewardBench | Reasoning |
| ------------------------------------------------------------ | -------------------------- | ------- | ---- | ----------- | --------- |
| Chat-Hard                                                    | Safety                     |         |      |             |           |
| SteerLM Regression                                           | HelpSteer Attributes       | 92.4    | 95.0 | 85.5        | 94.0      |
| Helpfulness Only                                             | 93.0                       | 97.2    | 84.2 | 94.6        | 95.8      |
| Bradley-Terry (from scratch)                                 | Regular                    | 91.5    | 97.5 | 80.3        | 90.5      |
| Margin                                                       | 91.5                       | 98.0    | 78.5 | 94.6        | 94.8      |
| Scaled                                                       | 92.7                       | 97.8    | 83.5 | 93.2        | 96.0      |
| Bradley-Terry (init. with Helpfulness-only Regression Model) | Regular                    | 92.7    | 98.9 | 82.9        | 93.7      |
| Margin                                                       | 93.0                       | 98.3    | 83.8 | 94.0        | 95.8      |
| Scaled                                                       | 93.7                       | 98.0    | 85.7 | 94.3        | 96.7      |
| Scaled + ExPO                                                | 94.1                       | 97.5    | 85.7 | 95.1        | 98.1      |
| External Baselines                                           | Skywork-Reward-Gemma-2-27B | 93.8    | 95.8 | 91.4        | 91.9      |
| TextEval-Llama3.1-70B                                        | 93.5                       | 94.1    | 90.1 | 93.2        | 96.4      |"
6,"| Model Type                 | Model                          | Aligned Metrics        |
| -------------------------- | ------------------------------ | ---------------------- |
| MT Bench (GPT-4-Turbo)     | Mean Response Length (Char.s.) | AlpacaEval 2.0 LC (SE) |
| Offline RLHF               | Regular DPO                    | 8.66                   |
| Margin DPO                 | 8.58                           | 1496.6                 |
| Scaled DPO                 | 8.74                           | 1514.8                 |
| Online RLHF                | PPO                            | 8.74                   |
| REINFORCE                  | 8.98                           | 2199.8                 |
| External Baselines         | Llama-3.1-70B-Instruct         | 8.22                   |
| Llama-3.1-405B-Instruct    | 8.49                           | 1664.7                 |
| Claude-3-5-Sonnet-20240620 | 8.81                           | 1619.9                 |
| GPT-4o-2024-05-13          | 8.74                           | 1752.2                 |"
29,"| Method        | k=2           | k=4     | k=8    |
| ------------- | ------------- | ------- | ------ |
| $ A_{r}(\%) $ | $ A_{f}(\%) $ | MIA (%) | ET (s) |
| Original      | 58.35         | 65.00   | 92.12  |
| Retrained     | 56.23         | 0.00    | 33.21  |
| UNSIR         | 41.87         | 0.00    | 53.02  |
| ADV+IMP       | 41.40         | 3.91    | 36.60  |
| LAU           | 45.28         | 0.50    | 23.30  |
| SCAR          | 52.01         | 13.50   | 28.10  |
| GKT           | 40.57         | 0.00    | 33.65  |
| ISPF          | 44.81         | 0.00    | 24.69  |
| DSDA (ours)   | 49.55         | 0.00    | 33.28  |"
10,"| Dataset                             | Frame    | Scene    | Subject           |
| ----------------------------------- | -------- | -------- | ----------------- |
| Count                               | Geometry | Location | Pos/Pose          |
| HPS (Guzov et al., 2021)            | 300K     | 8        | ✓(3D mesh)        |
| EgoBody (Zhang et al., 2022)        | 153K     | 15       | ✓(3D mesh)        |
| PROX (Hassan et al., 2019)          | 100K     | 12       | ✓(3D mesh)        |
| GIMO (Zheng et al., 2022)           | 129K     | 19       | ✓(3D mesh)        |
| Grand Station (Zhou et al., 2012)   | 50K      | 1        | ✓(Aerial image)   |
| SDD (Robicquet et al., 2016)        | 929K     | 6        | ✓(Aerial image)   |
| ETH (Ess et al., 2007)              | 50K      | 2        | ✓(Aerial image)   |
| THOR (Rudenko et al., 2020)         | 360K     | 3        | ✓(3D point cloud) |
| JRDB (Vendrow et al., 2023)         | 636K     | 30       | ✓(3D point cloud) |
| GTA-IM (Cao et al., 2020)           | 1000K    | 10       | ✓(3D mesh)        |
| HUMANISE (Wang et al., 2022)        | 1200K    | 643      | ✓(3D mesh)        |
| CIRCLE (Araújo et al., 2023)        | 4300K    | 9        | ✓(3D mesh)        |
| THOR-MAGNI (Schreiter et al., 2024) | 1260K    | 4        | ✓(3D mesh)        |
| LocoVR (Ours)                       | 2500K    | 131      | ✓(3D mesh)        |"
10,"| Method                  | Mean              | Max           |
| ----------------------- | ----------------- | ------------- |
| 0m \leq d \leq 3m       | 3m \leq d \leq 6m | 6m \leq d     |
| Ynet (GIMO)             | 0.08\pm 0.003     | 0.22\pm 0.012 |
| Ynet (THOR-MAGNI)       | 0.10\pm 0.003     | 0.30\pm 0.006 |
| Ynet (LocoVR)           | 0.09\pm 0.002     | 0.18\pm 0.004 |
| A* + MAP                | 0.10\pm 0         | 0.27\pm 0.000 |
| A* + DISTMAP            | 0.102\pm 0        | 0.18\pm 0.000 |
| A* + U-Net (GIMO)       | 0.09\pm 0.002     | 0.23\pm 0.006 |
| A* + U-Net (THOR-MAGNI) | 0.07\pm 0.001     | 0.21\pm 0.007 |
| A* + U-Net (LocoVR)     | 0.06\pm 0.001     | 0.12\pm 0.002 |"
10,"| Method                | Goal position error   | Object prediction accuracy |
| --------------------- | --------------------- | -------------------------- |
| $ 0m \leq d \leq 3m $ | $ 3m \leq d \leq 6m $ | $ 6m \leq d $              |
| RANDOM                | 3.70±0.02             | 3.75±0.02                  |
| NEAREST               | 1.76±0.00             | 3.89±0.00                  |
| U-Net (GIMO)          | 1.58±0.32             | 2.47±0.06                  |
| U-Net (THOR-MAGNI)    | 1.82±0.04             | 3.29±0.04                  |
| U-Net (LocoVR)        | 0.83±0.03             | 1.89±0.02                  |"
11,"| Method           | CityRefer-NO | CityRefer-ND | CityAnchor-NO | CityAnchor-ND |
| ---------------- | ------------ | ------------ | ------------- | ------------- |
| Acc@0.25         | Acc@0.50     | Acc@0.25     | Acc@0.50      | Acc@0.25      |
| InstanceRefer    | 4.09         | 3.64         | 1.93          | 1.76          |
| 3DVG-Transformer | 7.73         | 5.69         | 9.64          | 8.12          |
| EDA              | 6.96         | 5.53         | 8.39          | 5.84          |
| CityRefer        | 8.34         | 7.47         | 5.07          | 3.49          |
| CityAnchor       | 50.69        | 46.86        | 53.17         | 50.37         |"
11,"| Id            | Stage I       | Stage II      | Acc@0.50  | Time(s)  |
| ------------- | ------------- | ------------- | --------- | -------- |
| $ E_{x}^{s} $ | $ E_{x}^{g} $ | $ E_{x}^{l} $ | Nei. Enh. | Building |
| a)            | ✓             | ✓             |           |          |
| b)            | ✓             | ✓             | ✓         |          |
| c)            | ✓             | ✓             | ✓         | ✓        |
| d)            |               | ✓             | ✓         | ✓        |
| e)            | ✓             | ✓             | ✓         | ✓        |"
12,"| MODEL    | $ \varepsilon=0.5 $   | $ \varepsilon=1 $     | $ \varepsilon=2 $     | $ \varepsilon=4 $     | $ \varepsilon=8 $     | $ \varepsilon=\infty $ |
| -------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | ---------------------- |
| SIMPLEFF | 0.207  $ \pm $  0.002 | 0.195  $ \pm $  0.003 | 0.193  $ \pm $  0.003 | 0.194  $ \pm $  0.002 | 0.193  $ \pm $  0.003 | 0.136  $ \pm $  0.001  |
| DEEPAR   | 0.157  $ \pm $  0.002 | 0.145  $ \pm $  0.001 | 0.142  $ \pm $  0.001 | 0.141  $ \pm $  0.002 | 0.141  $ \pm $  0.002 | 0.124  $ \pm $  0.001  |
| iTRANS.  | 0.211  $ \pm $  0.004 | 0.193  $ \pm $  0.003 | 0.188  $ \pm $  0.004 | 0.188  $ \pm $  0.004 | 0.188  $ \pm $  0.004 | 0.135  $ \pm $  0.001  |
| DLinear  | 0.204  $ \pm $  0.004 | 0.192  $ \pm $  0.001 | 0.188  $ \pm $  0.003 | 0.188  $ \pm $  0.003 | 0.188  $ \pm $  0.003 | 0.140  $ \pm $  0.000  |
| SEASONAL | 0.251                 | 0.251                 | 0.251                 | 0.251                 | 0.251                 | 0.251                  |
| AUTOETS  | 0.407                 | 0.407                 | 0.407                 | 0.407                 | 0.407                 | 0.407                  |"
13,"| Target LLM   | Seed Question    | Reg-QA | Para-QA    |
| ------------ | ---------------- | ------ | ---------- |
| ASR@1/1(T=0) | ASR@1/100(T=0.5) | 1/1k   | ASR @10/1k |
| GPT 4o       | 0                | 3      | 89         |
| Gemma2 9B    | 0                | 2      | 91         |
| Qwen 72B IT  | 0                | 3      | 89         |
| GPT 3.5      | 29               | 40     | 99         |
| Mixtral 22x8 | 11               | 50     | 96         |
| Mistral 7B   | 35               | 70     | 97         |"
13,"| Attack Method            | No Defense | Remove Non-Dictionary | Synonym Substitution | Smooth LLM |
| ------------------------ | ---------- | --------------------- | -------------------- | ---------- |
| Prompt and Random Search | 93         | 11                    | 5                    | 4          |
| PAIR                     | 71         | 18                    | 21                   | 5          |
| GCG                      | 47         | 9                     | 15                   | 0          |
| ReG-QA (Ours)            | 95         | 88                    | 84                   | 82         |"
13,"| Attack Method            | Log Likelihood ( $ \uparrow $ ) (Min across text chunks, avg across dataset) | Number of characters (avg across dataset) |
| ------------------------ | ---------------------------------------------------------------------------- | ----------------------------------------- |
| Seed question            | -48.17                                                                       | 86.00                                     |
| ReG-QA                   | -54.62                                                                       | 101.00                                    |
| Adv-Prompter             | -105.88                                                                      | 200.49                                    |
| PAIR                     | -58.49                                                                       | 501.73                                    |
| Prompt and Random Search | -368.25                                                                      | 2181.60                                   |
| GCG                      | -144.76                                                                      | 193.15                                    |"
14,"| Datasets               | Methods  | Avg. Err  $ \downarrow $  [cm/°] | Avg.  $ \uparrow $  [5cm, 5°] | Avg.  $ \uparrow $  [2cm, 2°] |
| ---------------------- | -------- | -------------------------------- | ----------------------------- | ----------------------------- |
| 7Scenes                | ACE      | 1.2/0.36                         | 97.1                          | 83.3                          |
| ACE + GS-CPR (ours)    | 0.8/0.27 | 100                              | 93.1                          |                               |
| Marepo                 | 2.9/1.04 | 84                               | 33.7                          |                               |
| Marepo + GS-CPR (ours) | 0.9/0.28 | 99.4                             | 89.6                          |                               |
| 12Scenes               | ACE      | 0.7/0.26                         | 100                           | 97.2                          |
| ACE + GS-CPR (ours)    | 0.5/0.21 | 100                              | 98.7                          |                               |
| Marepo                 | 2.1/1.04 | 95                               | 50.4                          |                               |
| Marepo + GS-CPR (ours) | 0.7/0.28 | 98.9                             | 90.9                          |                               |"
14,"| Datasets                                 | 7Scenes                                        | Cambridge                        |
| ---------------------------------------- | ---------------------------------------------- | -------------------------------- |
| Methods                                  | Avg. Acc  $ \uparrow $  [5cm, 5 $ ^{\circ} $ ] | Avg. Err  $ \downarrow $  [cm/°] |
| DFNet                                    | 43.1                                           | 6/1.93                           |
| DFNet + GS-CPR $ _{\text{rel}} $  (ours) | 80.5                                           | 2.7/0.38                         |
| DFNet + GS-CPR (ours)                    | 94.2                                           | 1.1/0.34                         |
| ACE                                      | 97.1                                           | 1.1/0.34                         |
| ACE + GS-CPR $ _{\text{rel}} $  (ours)   | 79.9                                           | 2.8/0.43                         |
| ACE + GS-CPR (ours)                      | 100                                            | 0.8/0.25                         |"
15,"| Parameters | Increasing Factor ( $ \mu $ ) | Fixed Value ( $ u_{t} $ ) | Decreasing Factor ( $ \nu $ ) |
| ---------- | ----------------------------- | ------------------------- | ----------------------------- |
| 1k         | 10k                           | 100k                      | 0.3                           |
| Low-pass   | 77.12_{0.07}                  | 77.06_{0.14}              | 76.86_{0.12}                  |
| High-pass  | 51.59_{0.78}                  | 67.55_{0.22}              | 74.72_{0.06}                  |"
15,"| Parameters     | Increasing Factor ( $ \mu $ ) | Fixed Value ( $ u_{t} $ ) | Decreasing Factor ( $ \nu $ ) |
| -------------- | ----------------------------- | ------------------------- | ----------------------------- |
| 1k             | 10k                           | 100k                      | 0.3                           |
| Low-Pass Gain  | 76.10_{0.14}                  | 80.48_{0.03}              | 78.02_{0.03}                  |
| High-Pass Gain | 75.47_{0.21}                  | 74.54_{0.16}              | 75.97_{0.27}                  |"
15,"| Dataset Model | CIFAR-10          | CIFAR-100         | Tiny-ImageNet     | ImageNet ResNet50 |
| ------------- | ----------------- | ----------------- | ----------------- | ----------------- |
| VGG16         | ResNet18          | ResNet50          | DenseNet121       | ResNet34          |
| EMA-SGDM      | 93.71 $ _{0.07} $ | 94.19 $ _{0.07} $ | 76.84 $ _{0.06} $ | 76.18 $ _{0.23} $ |
| Standard-SGDM | 94.08 $ _{0.07} $ | 95.57 $ _{0.06} $ | 79.71 $ _{0.25} $ | 80.49 $ _{0.09} $ |
| FSGDM         | 94.19 $ _{0.07} $ | 95.66 $ _{0.07} $ | 81.44 $ _{0.06} $ | 81.14 $ _{0.05} $ |"
15,"| Model         | FConv        | LightConv    | LSTM         | LSTM-W       | Transformer-tiny | Transformer  |
| ------------- | ------------ | ------------ | ------------ | ------------ | ---------------- | ------------ |
| EMA-SGDM      | 13.97_{0.01} | 10.56_{0.01} | 4.99_{0.01}  | 1.20_{0.07}  | 5.17_{0.01}      | 6.27_{0.01}  |
| Standard-SGDM | 27.41_{0.02} | 33.05_{0.04} | 28.12_{0.06} | 24.66_{0.06} | 18.16_{0.03}     | 31.50_{0.05} |
| FSGDM         | 28.30_{0.01} | 33.44_{0.02} | 29.27_{0.02} | 27.41_{0.03} | 19.94_{0.07}     | 32.40_{0.05} |"
16,"| Metric                    | $ \pi_{\theta^{1}}^{HP} $ | $ \pi_{\theta^{2}}^{HP} $ | $ \pi_{\theta^{3}}^{HP} $ | $ \pi_{\theta^{4}}^{HP} $ |
| ------------------------- | ------------------------- | ------------------------- | ------------------------- | ------------------------- |
| Number of Players         | 2                         | 2                         | 3                         | 3                         |
| Accuracy                  | 0.63                      | 0.63                      | 0.43                      | 0.44                      |
| Accuracy Difference to BC | -0.03                     | -0.08                     | -0.08                     | -0.07                     |
| Loss                      | 0.53                      | 0.54                      | 0.63                      | 0.60                      |
| Loss Difference to BC     | +0.05                     | +0.07                     | +0.08                     | +0.06                     |
| Top-10% Accuracy          | 0.82                      | 0.82                      | 0.71                      | 0.73                      |
| Top-20% Accuracy          | 0.95                      | 0.95                      | 0.87                      | 0.88                      |"
16,"| Setting                   | Source     | IPP  | Communicativeness |
| ------------------------- | ---------- | ---- | ----------------- |
| 2P                        | 2P Dataset | 0.44 | 0.47              |
| $ \pi_{\theta^{1}}^{HP} $ | 0.43       | 0.45 |                   |
| $ \pi_{\theta^{2}}^{HP} $ | 0.44       | 0.48 |                   |
| 3P                        | 3P Dataset | 0.42 | 0.49              |
| $ \pi_{\theta^{3}}^{HP} $ | 0.44       | 0.47 |                   |
| $ \pi_{\theta^{4}}^{HP} $ | 0.44       | 0.46 |                   |"
16,"| Players                              | Method                               | Mean Scores | Median Scores | Cross-Entropy |
| ------------------------------------ | ------------------------------------ | ----------- | ------------- | ------------- |
| Two Players                          | Off-Belief Learning (L4)             | 21.04       | 22            | 1.33          |
| BR-BC                                | 19.41                                | 20          | 10.82         |               |
| Fictitious Co-Play (FCP)             | 14.01                                | 16          | 3.52          |               |
| Other-Play                           | 13.91                                | 19          | 7.81          |               |
| HDR-IPPO                             | 12.76                                | 15          | 0.96          |               |
| IPPO                                 | 10.16                                | 14          | 12.60         |               |
| DeepSeek-R1 with H-Group Conventions | 9.91                                 | 0           | -             |               |
| DeepSeek-R1                          | 5.43                                 | 0           | -             |               |
| BC                                   | 2.12                                 | 0           | 0.86          |               |
| Human Proxies  $ \dagger $           | 22.76                                | 23          | 0.54          |               |
| BR-BC* $ \dagger $                   | 22.59                                | 23          | 5.00          |               |
| Three Players                        | DeepSeek-R1 with H-Group Conventions | 14.62       | 18            | -             |
| DeepSeek-R1                          | 14.38                                | 18          | -             |               |
| HDR-IPPO                             | 14.03                                | 16          | 0.80          |               |
| OP                                   | 12.87                                | 18          | 6.40          |               |
| BR-BC                                | 11.89                                | 12          | 29.89         |               |
| FCP                                  | 11.55                                | 6           | 5.97          |               |
| IPPO                                 | 6.34                                 | 0           | 8.60          |               |
| BC                                   | 3.31                                 | 0           | 0.70          |               |
| Human Proxies  $ \dagger $           | 20.86                                | 21          | 0.62          |               |
| BR-BC* $ \dagger $                   | 18.80                                | 19          | 7.53          |               |"
17,"| ΔW₁ | X   | ΔW₂ | X                |
| --- | --- | --- | ---------------- |
| 1.0 | 0.5 | 0.4 | ||ΔW₁||F = 2.1   |
| 0.2 | 0.6 | 0.8 | ||XΔW₁||F = 10.8 |
| 1.1 | 0.7 | 0.6 | 0.0              |"
17,"| Ratio         | Method   | WikiText-2 $ \downarrow $ | PTB $ \downarrow $ | C4 $ \downarrow $ | Open. | ARC_e | WinoG. | HellaS. | ARC_c | PIQA | MathQA | Average $ \uparrow $ |
| ------------- | -------- | ------------------------- | ------------------ | ----------------- | ----- | ----- | ------ | ------- | ----- | ---- | ------ | -------------------- |
| 0%            | Original | 5.68                      | 8.35               | 7.34              | 0.28  | 0.67  | 0.67   | 0.56    | 0.38  | 0.78 | 0.27   | 0.52                 |
| 20%           | SVD      | 20061                     | 20306              | 18800             | 0.14  | 0.27  | 0.51   | 0.26    | 0.21  | 0.53 | 0.21   | 0.31                 |
| FWSVD         | 1727     | 2152                      | 1511               | 0.15              | 0.31  | 0.50  | 0.26   | 0.23    | 0.56  | 0.21 | 0.32   |                      |
| ASVD          | 11.14    | 16.55                     | 15.93              | 0.25              | 0.53  | 0.64  | 0.41   | 0.27    | 0.68  | 0.24 | 0.43   |                      |
| SVD-LLM       | 7.94     | 18.05                     | 15.93              | 0.22              | 0.58  | 0.63  | 0.43   | 0.29    | 0.69  | 0.24 | 0.44   |                      |
| Basis Sharing | 7.74     | 17.35                     | 15.03              | 0.28              | 0.66  | 0.66  | 0.46   | 0.36    | 0.71  | 0.25 | 0.48   |                      |
| 30%           | SVD      | 13103                     | 17210              | 20871             | 0.13  | 0.26  | 0.51   | 0.26    | 0.21  | 0.54 | 0.22   | 0.30                 |
| FWSVD         | 20127    | 11058                     | 7240               | 0.17              | 0.26  | 0.49  | 0.26   | 0.22    | 0.51  | 0.19 | 0.30   |                      |
| ASVD          | 51       | 70                        | 41                 | 0.18              | 0.43  | 0.53  | 0.37   | 0.25    | 0.65  | 0.21 | 0.38   |                      |
| SVD-LLM       | 9.56     | 29.44                     | 25.11              | 0.20              | 0.48  | 0.59  | 0.40   | 0.26    | 0.65  | 0.22 | 0.40   |                      |
| Basis Sharing | 9.25     | 29.12                     | 22.46              | 0.27              | 0.63  | 0.63  | 0.40   | 0.30    | 0.68  | 0.24 | 0.45   |                      |
| 40%           | SVD      | 52489                     | 59977              | 47774             | 0.15  | 0.26  | 0.52   | 0.26    | 0.22  | 0.53 | 0.20   | 0.30                 |
| FWSVD         | 18156    | 20990                     | 12847              | 0.16              | 0.26  | 0.51  | 0.26   | 0.22    | 0.53  | 0.21 | 0.30   |                      |
| ASVD          | 1407     | 3292                      | 1109               | 0.13              | 0.28  | 0.48  | 0.26   | 0.22    | 0.55  | 0.19 | 0.30   |                      |
| SVD-LLM       | 13.11    | 63.75                     | 49.83              | 0.19              | 0.42  | 0.58  | 0.33   | 0.25    | 0.60  | 0.21 | 0.37   |                      |
| Basis Sharing | 12.39    | 55.78                     | 41.28              | 0.22              | 0.52  | 0.61  | 0.35   | 0.27    | 0.62  | 0.23 | 0.40   |                      |
| 50%           | SVD      | 131715                    | 87227              | 79815             | 0.16  | 0.26  | 0.50   | 0.26    | 0.23  | 0.52 | 0.19   | 0.30                 |
| FWSVD         | 24391    | 28321                     | 23104              | 0.12              | 0.26  | 0.50  | 0.26   | 0.23    | 0.53  | 0.20 | 0.30   |                      |
| ASVD          | 15358    | 47690                     | 27925              | 0.12              | 0.26  | 0.51  | 0.26   | 0.22    | 0.52  | 0.19 | 0.30   |                      |
| SVD-LLM       | 23.97    | 150.58                    | 118.57             | 0.16              | 0.33  | 0.54  | 0.29   | 0.23    | 0.56  | 0.21 | 0.33   |                      |
| Basis Sharing | 19.99    | 126.35                    | 88.44              | 0.18              | 0.42  | 0.57  | 0.31   | 0.23    | 0.58  | 0.22 | 0.36   |                      |"
18,"| Algorithm                  | Reward ( $ \uparrow $ )  | Goal        | Tracking - EMD ( $ \downarrow $ ) | Tracking - Success ( $ \uparrow $ ) | Toplines (retrained for each task) |
| -------------------------- | ------------------------ | ----------- | --------------------------------- | ----------------------------------- | ---------------------------------- |
| Proximity ( $ \uparrow $ ) | Success ( $ \uparrow $ ) | Train       | Test                              | Train                               | Test                               |
| TD3 $ ^{\dagger} $         | 249.74                   | 0.98        | 0.98                              | 1.08                                | 1.09                               |
| GOAL-GAIL $ ^{\dagger} $   |                          |             |                                   | 1.14                                | 1.14                               |
| PHC $ ^{\dagger} $         |                          |             |                                   |                                     |                                    |
| ORACLE MPPI $ ^{\dagger} $ | 178.50                   | 0.47        | 0.73                              |                                     |                                    |
| GOAL-TD3                   |                          | 0.67 (0.34) | 0.44 (0.47)                       | 1.39 (0.08)                         | 1.41 (0.09)                        |
| GOAL-GAIL                  |                          | 0.61 (0.35) | 0.35 (0.44)                       | 1.68 (0.02)                         | 1.70 (0.02)                        |
| PHC                        |                          | 0.07 (0.11) | 0.05 (0.11)                       | 1.66 (0.06)                         | 1.65 (0.07)                        |
| CALM                       |                          | 0.18 (0.27) | 0.04 (0.17)                       | 1.67 (0.02)                         | 1.70 (0.03)                        |
| ASE                        | 105.73 (3.82)            | 0.46 (0.37) | 0.22 (0.37)                       | 2.00 (0.02)                         | 1.99 (0.02)                        |
| DIFFUSER                   | 85.27 (0.99)             | 0.20 (0.03) | 0.14 (0.01)                       |                                     |                                    |
| FB-CPR                     | 151.68 (7.53)            | 0.68 (0.35) | 0.48 (0.46)                       | 1.37 (0.00)                         | 1.39 (0.01)                        |
| SCORE $ _{\text{norm}} $   | 0.61                     | 0.69        | 0.48                              | 0.80                                | 0.80                               |"
18,"| Algorithm                    | Antmaze-medium           | Antmaze-large                |
| ---------------------------- | ------------------------ | ---------------------------- |
| Proximity ( $ \downarrow $ ) | Success ( $ \uparrow $ ) | Proximity ( $ \downarrow $ ) |
| (online) FB                  | 19.71 (0.11)             | 0 (0)                        |
| (offline) FB-AWAC            | 6.70 (0.4)               | 0.67 (0.08)                  |
| (online) FB-CPR              | 3.19 (0.13)              | 0.90 (0.1)                   |"
18,"| Method    | Data               | Reward Return       | Demonstration Return | Goal Proximity         |
| --------- | ------------------ | ------------------- | -------------------- | ---------------------- |
| FB        | RND                | 0.52  $ \pm $  0.02 | 0.43  $ \pm $  0.02  | 127.38  $ \pm $  20.51 |
| FB        | RND+ $ M_{TRAIN} $ | 0.60  $ \pm $  0.03 | 0.56  $ \pm $  0.03  | 211.46  $ \pm $  17.78 |
| FB+AWAC   | $ M_{TRAIN} $      | 0.51  $ \pm $  0.02 | 0.54  $ \pm $  0.02  | 279.90  $ \pm $  44.07 |
| FB+AWAC   | RND+ $ M_{TRAIN} $ | 0.42  $ \pm $  0.03 | 0.43  $ \pm $  0.05  | 249.72  $ \pm $  23.92 |
| FB Online | None               | 0.19  $ \pm $  0.03 | 0.19  $ \pm $  0.02  | 120.51  $ \pm $  10.83 |
| FB-CPR    | $ M_{TRAIN} $      | 0.71  $ \pm $  0.02 | 0.75  $ \pm $  0.01  | 297.17  $ \pm $  52.14 |
| FB-MPR    | $ M_{TRAIN} $      | 0.77  $ \pm $  0.02 | 0.78  $ \pm $  0.01  | 258.66  $ \pm $  43.89 |"
18,"| Dataset      | Train dataset  $ M $ | Test dataset  $ M_{\text{test}} $ |
| ------------ | -------------------- | --------------------------------- |
| Count        | Steps                | Time (s)                          |
| ACCAD        | 223                  | 42146                             |
| BMLhandball  | 45                   | 13103                             |
| BMLmovi      | 1456                 | 243683                            |
| BioMotionLab | 1445                 | 504134                            |
| CMU          | 1638                 | 730307                            |
| DFaust       | 80                   | 14351                             |
| DanceDB      | 23                   | 40685                             |
| EKUT         | 124                  | 19529                             |
| Eyes         | 562                  | 484677                            |
| HumanEva     | 25                   | 13517                             |
| KIT          | 2858                 | 673239                            |
| MPI          | 264                  | 257199                            |
| SFU          | 30                   | 17081                             |
| TotalCapture | 33                   | 67124                             |
| Transitions  | 96                   | 23795                             |
| Total        | 8,902                | 3,144,570                         |"
20,"| Method                      | SQA $ ^{1} $ | GQA   | MMB    | SEED  | Average |
| --------------------------- | ------------ | ----- | ------ | ----- | ------- |
| Acc.                        | Speed        | Skip  | Acc.   | Speed | Skip    |
| LLaVA Liu et al. (2023a)    | 66.8         | 7.55  | 0.00%  | 62.0  | 6.99    |
| RoE-LLaVA $ _{10} $ %       | 68.4         | 7.65  | 10.26% | 61.4  | 7.07    |
| RoE-LLaVA $ _{20} $ %       | 68.7         | 9.15  | 20.55% | 61.3  | 7.52    |
| RoE-LLaVA $ _{30} $ %       | 68.4         | 9.67  | 23.03% | 61.4  | 7.65    |
| VILA Lin et al. (2023)      | 68.2         | 8.27  | 0.00%  | 62.3  | 8.03    |
| RoE-VILA $ _{10} $ %        | 69.5         | 8.39  | 9.19%  | 62.2  | 8.01    |
| RoE-VILA $ _{20} $ %        | 68.4         | 10.49 | 23.93% | 61.1  | 8.20    |
| RoE-VILA $ _{30} $ %        | 69.4         | 10.67 | 25.12% | 60.3  | 8.21    |
| LLaVA-HR Luo et al. (2024c) | 65.1         | 4.82  | 0.00%  | 64.2  | 4.87    |
| RoE-LLaVA-HR $ _{10} $ %    | 67.4         | 4.96  | 7.96%  | 62.5  | 5.01    |
| RoE-LLaVA-HR $ _{20} $ %    | 56.1         | 4.97  | 12.77% | 60.8  | 5.09    |"
20,"| Method                             | LLM           | Param. | Res. | POPE Acc. | MME Score | MMB Acc. | SEED Acc. | MM-Vet Score | Speed     |
| ---------------------------------- | ------------- | ------ | ---- | --------- | --------- | -------- | --------- | ------------ | --------- |
| Dense MLLMs                        |               |        |      |           |           |          |           |              |           |
| Qwen-VL Bai et al. (2023b)         | Qwen-7B       | 9.6B   | 448  | -         | -         | -        | 38.2      | 7.40         | 56.3 2.42 |
| Qwen-VL-Chat Bai et al. (2023b)    | Qwen-7B       | 9.6B   | 448  | -         | -         | 1487.5   | 3.96      | 60.6 7.55    | 58.2 2.59 |
| LLaVA Liu et al. (2023b)           | Vicuna-7B     | 7.2B   | 336  | 85.9      | 8.90      | 1510.7   | 8.61      | 64.3 8.37    | 58.6 8.33 |
| VILA Lin et al. (2023)             | Vicuna-7B     | 7.2B   | 336  | 85.5      | 9.21      | 1533.0   | 8.64      | 68.9 8.51    | 61.1 8.36 |
| LLaVA-HR Luo et al. (2024c)        | Vicuna-7B     | 7.4B   | 1024 | 85.9      | 4.70      | 1554.9   | 4.77      | 64.9 4.48    | 64.2 3.46 |
| Sparse MLLMs                       |               |        |      |           |           |          |           |              |           |
| MoE-LLaVA-1.6B×4 Lin et al. (2024) | StableLM-1.6B | 2.9B   | 336  | 85.7      | 7.65      | 1318.2   | 8.06      | 60.2 9.90    | -         |
| MoE-LLaVA-2.7B×4 Lin et al. (2024) | Phi-2.7B      | 5.3B   | 336  | 86.3      | 5.95      | 1423.0   | 5.83      | 65.2 5.27    | -         |
| RoE-LLaVA                          | Vicuna-7B     | 7.3B   | 336  | 86.1      | 9.38      | 1522.7   | 9.03      | 64.3 9.62    | 58.2 8.41 |
| RoE-VILA                           | Vicuna-7B     | 7.3B   | 336  | 86.8      | 9.25      | 1446.0   | 8.95      | 67.6 8.63    | 61.3 8.50 |
| RoE-LLaVA-HR                       | Vicuna-7B     | 7.5B   | 1024 | 88.1      | 4.75      | 1558.2   | 4.82      | 64.6 4.82    | 62.2 3.86 |"
20,"| Method                             | LLM           | Param. | Res. | VQA $ ^{2} $  Acc. Speed | GQA Acc. Speed | VizWiz Acc. Speed | SQA $ ^{1} $  Acc. Speed | VQA $ ^{T} $  Acc. Speed | Average Acc. Speed |      |
| ---------------------------------- | ------------- | ------ | ---- | ------------------------ | -------------- | ----------------- | ------------------------ | ------------------------ | ------------------ | ---- |
| Dense MLLMs                        |               |        |      |                          |                |                   |                          |                          |                    |      |
| Qwen-VL Bai et al. (2023b)         | Qwen-7B       | 9.6B   | 448  | 78.8                     | 5.23           | 59.3              | 3.48                     | 35.2                     | 67.1               | 6.97 |
| Qwen-VL-Chat Bai et al. (2023b)    | Qwen-7B       | 9.6B   | 448  | 78.2                     | 5.30           | 57.5              | 3.63                     | 38.9                     | 3.22s              | 68.2 |
| LLaVA Liu et al. (2023b)           | Vicuna-7B     | 7.2B   | 336  | 78.5                     | 6.97           | 62.0              | 6.99                     | 50.0                     | 6.44               | 66.8 |
| VILA Lin et al. (2023)             | Vicuna-7B     | 7.2B   | 336  | 79.9                     | 8.01           | 62.3              | 8.03                     | 57.8                     | 5.75               | 68.2 |
| LLaVA-HR Luo et al. (2024c)        | Vicuna-7B     | 7.4B   | 1024 | 81.9                     | 4.42           | 64.2              | 4.55                     | 48.7                     | 4.06               | 65.1 |
| Sparse MLLMs                       |               |        |      |                          |                |                   |                          |                          |                    |      |
| MoE-LLaVA-1.6B×4 Lin et al. (2024) | StableLM-1.6B | 2.9B   | 336  | 76.7                     | 7.79           | 60.3              | 7.43                     | 36.2                     | 6.27               | 62.6 |
| MoE-LLaVA-2.7B×4 Lin et al. (2024) | Phi-2.7B      | 5.3B   | 336  | 77.6                     | 6.01           | 61.4              | 5.23                     | 43.9                     | 3.95               | 68.5 |
| RoE-LLaVA                          | Vicuna-7B     | 7.3B   | 336  | 80.3                     | 7.02           | 61.4              | 7.07                     | 52.5                     | 6.52               | 68.4 |
| RoE-VILA                           | Vicuna-7B     | 7.3B   | 336  | 78.8                     | 8.25           | 62.2              | 8.01                     | 53.7                     | 6.28               | 69.5 |
| RoE-LLaVA-HR                       | Vicuna-7B     | 7.5B   | 1024 | 80.9                     | 4.79           | 62.5              | 5.01                     | 47.6                     | 4.12               | 67.4 |"
22,"| Methods                       | In-Domain | Out-Of-Domain |
| ----------------------------- | --------- | ------------- |
| Alpaca Eval 2.0               | MT-Bench  | MMLU          |
| LC-WR                         | WR        | Score         |
| 8B Setting: Student=Llama3-8B |           |               |
| No fine-tuning                | 2.09%     | 3.39%         |
| Self-Instruct                 | 50%       | 50%           |
| Self-Instruct $ ^{*} $        | 54.95%    | 56.39%        |
| Self-Reward $ ^{*} $          | 51.87%    | 55.38%        |
| Iteration 1                   | 53.49%    | 57.32%        |
| LLM2LLM                       | 51.49%    | 53.12%        |
| Iteration 1                   | 52.63%    | 55.02%        |
| Montessori-Instruct           | 54.92%    | 58.59%        |
| Iteration 1                   | 56.37%    | 60.15%        |"
22,"| LLM-as-a-Judge              | 53.42% | 54.93% | 6.731 |
| --------------------------- | ------ | ------ | ----- |
| Training loss               | 52.34% | 54.99% | 6.656 |
| Local data influence (Ours) | 54.92% | 58.59% | 6.903 |"
22,"| Bootstrap                       | 50.59% | 48.14% | 6.618 |
| ------------------------------- | ------ | ------ | ----- |
| Response optimization           | 51.59% | 54.22% | 6.556 |
| Instruction optimization (Ours) | 54.92% | 58.59% | 6.903 |"
24,"| performance    | algorithmic desiderata   | theory             |
| -------------- | ------------------------ | ------------------ |
| task accuracy  | faithfulness             | learning theory    |
| generalization | algorithmic complexity   | algorithmic theory |
| error analysis | computational efficiency | complexity         |"
24,"| model development | data benchmarks | theoretical frameworks | algorithm discovery |
| ----------------- | --------------- | ---------------------- | ------------------- |"
25,"|                  | 1024 | 4096 | 32768 | 65536 |
| ---------------- | ---- | ---- | ----- | ----- |
| W-MCTS-0S, p=1   | 50.9 | 51.0 | 52.2  | 54.6  |
| W-MCTS-TS, p=100 | 67.4 | 75.6 | 77.7  | 77.7  |
| D2NG             | 71.6 | 75.4 | 76.9  | 72.2  |
| UCT              | 23.4 | 23.6 | 24.9  | 28.5  |"
25,"| Tag       | Laser | RS(15) | Pocman |
| --------- | ----- | ------ | ------ |
| W-MCTS-OS | -6.05 | -18.17 | 19.76  |
| W-MCTS-TS | -5.90 | -8.75  | 20.29  |
| POMCP     | -7.14 | -19.58 | 12.23  |
| AB-DESPOT | -6.57 | -11.13 | 18.18  |
| AR-DESPOT | -6.26 | -9.34  | 18.57  |"
26,"|          | Methods | Images | Videos |
| -------- | ------- | ------ | ------ |
| LPIPS ↓  | FID ↓   | CSIM ↑ | PSNR ↑ |
| LRS3     | SMIRK   | 0.109  | 25.39  |
| 3DDFA-V3 | 0.181   | 56.18  | 0.604  |
| Ours     | 0.077   | 19.41  | 0.804  |
| HDF      | SMIRK   | 0.114  | 35.27  |
| 3DDFA-V3 | 0.399   | 104.90 | 0.597  |
| Ours     | 0.081   | 30.19  | 0.826  |"
26,"| Method                                     | Stirling benchmark   |
| ------------------------------------------ | -------------------- |
| Median  $ \downarrow $                     | Mean  $ \downarrow $ |
| Deep3DFace* $ ^{*} $ (Deng et al. (2019b)) | 0.99                 |
| 3DDFA-V2 (Guo et al. (2020))               | 1.20                 |
| DECA (Feng et al. (2021))                  | 1.03                 |
| SMIRK (Retsinas et al. (2024))             | 1.01                 |
| TEASER (Ours)                              | 1.00                 |"
28,"| Model Name                  | CLIP score  $ \uparrow $ | Accuracy, %  $ \uparrow $ | Aesthetic score  $ \uparrow $ |
| --------------------------- | ------------------------ | ------------------------- | ----------------------------- |
| GLIDE                       | 25.09 ± 0.01             | 43.08 ± 0.30              | 4.476 ± 0.002                 |
| BLD                         | 25.64 ± 0.05             | 55.64 ± 0.59              | 4.822 ± 0.006                 |
| SDXL Inpainting             | 24.80 ± 0.02             | 52.98 ± 0.91              | 4.682 ± 0.024                 |
| DreamShaper-ControlNet Inp. | 25.73 ± 0.01             | 58.74 ± 0.27              | 4.946 ± 0.005                 |
| SmartBrush reprod.          | 25.86 ± 0.03             | 66.88 ± 0.48              | 4.856 ± 0.004                 |
| Stable 1.5 Inpainting       | 25.10 ± 0.02             | 55.25 ± 0.46              | 4.881 ± 0.006                 |
| Stable 2.0 Inpainting       | 25.07 ± 0.03             | 51.74 ± 0.54              | 4.885 ± 0.006                 |
| DreamShaper Inpainting      | 25.61 ± 0.02             | 58.93 ± 0.18              | 4.965 ± 0.004                 |
| Stable 1.5 + HD-Painter     | 25.83 ± 0.05             | 59.57 ± 0.58              | 4.864 ± 0.006                 |
| Stable 2.0 + HD-Painter     | 26.48 ± 0.03             | 59.74 ± 0.56              | 4.846 ± 0.011                 |
| Dreamshaper 8 + HD-Painter  | 26.32 ± 0.03             | 68.05 ± 0.48              | 4.980 ± 0.003                 |"
28,"| Model Name              | CLIP score  $ \uparrow $ | Accuracy  $ \uparrow $ | Aesthetic score  $ \uparrow $ |
| ----------------------- | ------------------------ | ---------------------- | ----------------------------- |
| base (DreamShaper Inp.) | 25.61 ± 0.02             | 58.93 ± 0.18           | 4.965 ± 0.004                 |
| only PAIntA             | 26.07 ± 0.03             | 63.95 ± 0.50           | 4.985 ± 0.003                 |
| only RASG               | 25.94 ± 0.02             | 63.75 ± 0.48           | 4.965 ± 0.003                 |
| RASG &amp; PAIntA       | 26.32 ± 0.03             | 68.05 ± 0.48           | 4.980 ± 0.003                 |"
29,"| Method        | $ \mathcal{D}_{r} $  free | $ \mathcal{D}_{f} $  free | CIFAR10 | CIFAR100      | PinsFace Recognition |
| ------------- | ------------------------- | ------------------------- | ------- | ------------- | -------------------- |
| $ A_{r}(\%) $ | $ A_{f}(\%) $             | MIA (%)                   | ET (s)  | $ A_{r}(\%) $ | $ A_{f}(\%) $        |
| Original      | -                         | -                         | 79.80   | 88.83         | 81.40                |
| Retrained     | -                         | -                         | 77.90   | 0.00          | 11.46                |
| SSD           | x                         | x                         | 54.44   | 0.00          | 19.72                |
| UNSIR         | x                         | ✓                         | 75.74   | 0.00          | 70.48                |
| ADV+IMP       | ✓                         | x                         | 49.30   | 0.00          | 40.68                |
| LAU           | ✓                         | x                         | 78.37   | 0.20          | 33.78                |
| SCAR          | ✓                         | x                         | 76.13   | 1.69          | 12.50                |
| GKT           | ✓                         | ✓                         | 44.23   | 2.19          | 24.98                |
| ISPF          | ✓                         | ✓                         | 66.46   | 0.00          | 31.70                |
| DSDA (ours)   | ✓                         | ✓                         | 77.91   | 0.00          | 11.80                |"
214,"| Distractions | Shuffled Premises | Easy  | Medium | Hard  |
| ------------ | ----------------- | ----- | ------ | ----- |
| ✓            | ✓                 | 94.20 | 79.40  | 50.00 |
| ✗            | ✓                 | 94.40 | 82.20  | 57.40 |
| ✗            | ✗                 | 95.60 | 87.40  | 62.40 |"
30,"| Datasets                    | Knowledge Base       | Methods         | Large Vision Language Models |
| --------------------------- | -------------------- | --------------- | ---------------------------- |
| Qwen2-VL (2B)               | Qwen2-VL (7B)        | InternVL-2 (8B) |                              |
| ScienceQA_{test}            | ScienceQA_{trainval} | Zero-Shot       | 67.18                        |
| ICL (random retrieval)      | 70.10                | 81.63           | 93.14                        |
| Vanilla-RAG (top retrieval) | 71.94                | 86.68           | 92.78                        |
| RCTS (ours)                 | 78.99                | 91.44           | 94.20                        |
| MMMU-Dev                    | MMMU-Val             | Zero-Shot       | 44.00                        |
| ICL (random retrieval)      | 41.33                | 47.33           | 47.33                        |
| Vanilla-RAG (top retrieval) | 42.67                | 50.00           | 46.67                        |
| RCTS (ours)                 | 44.00                | 53.33           | 51.33                        |
| MathV_{testmini}            | MathV_{test}         | Zero-Shot       | 18.75                        |
| ICL (random retrieval)      | 17.76                | 23.35           | 21.05                        |
| Vanilla-RAG (top retrieval) | 20.39                | 24.67           | 18.42                        |
| RCTS (ours)                 | 22.04                | 28.95           | 24.01                        |
| Datasets                    | Knowledge Base       | Methods         | Qwen2-VL (7B)                |
| VizWiz_{val}                | VizWiz_{train}       | Zero-Shot       | 66.49                        |
| ICL                         | 69.01                | ✗               | ✗                            |
| Vanilla-RAG                 | 69.89                | ✓               | ✓                            |
| RCTS (ours)                 | 71.50                | ✓               | ✓                            |
| VSR-MC_{test}               | VSR-MC_{trainval}    | Zero-Shot       | 51.22                        |
| ICL                         | 52.32                | Qwen2-VL (2B)   | 90.56                        |
| Vanilla-RAG                 | 52.92                | Qwen2-VL (7B)   | 100.0                        |
| RCTS (ours)                 | 55.97                | InternVL-2 (8B) | 97.37                        |"
31,"| Methods               | Combinatorial Methods               | Continuous Meothds        |
| --------------------- | ----------------------------------- | ------------------------- |
| $ k $                 | Local search &amp; Greedy           | via Multilinear extension |
| $ k=1 $  (submodular) | suboptimal except some simple cases | optimal                   |
| $ k\geq2 $            | suboptimal except some simple cases | ?                         |"
31,"| Problem type of  $ k $ -submod. max.                | Prior results                                               | Our results                                                 |
| --------------------------------------------------- | ----------------------------------------------------------- | ----------------------------------------------------------- |
| $ d $  knapsacks ( $ d=O(1) $ )                     | Monotone                                                    | $ \frac{1}{3} $  (Ha et al., 2024) ( $ d=1 $ )              |
| $ \frac{1}{2+2d}-\varepsilon $  (Gong et al., 2024) |                                                             |                                                             |
| Non-monotone ( $ k\geq2 $ )                         | $ \frac{1-e^{-4}}{4} $  (Yu et al., 2023) ( $ d=1 $ )       | $ \frac{1}{3}-\varepsilon $                                 |
| $ \frac{1}{3+2d}-\varepsilon $  (Gong et al., 2024) |                                                             |                                                             |
| single matroid                                      | Monotone                                                    | $ \frac{1}{2} $  (Sakaue, 2017) $ \spadesuit $              |
| Non-monotone ( $ k\geq2 $ )                         | $ \frac{1-e^{-4}}{4} $  (Yu et al., 2023)                   | $ \frac{1}{3}-\varepsilon $                                 |
| $ b $  matroids +  $ d $  knapsacks ( $ d=O(1) $ )  | Monotone                                                    | $ \frac{1-e^{-(b+2)}}{b+2} $  (Yu et al., 2023) ( $ d=1 $ ) |
| Non-monotone ( $ k\geq2 $ )                         | $ \frac{1-e^{-(b+3)}}{b+3} $  (Yu et al., 2023) ( $ d=1 $ ) | $ \frac{0.2}{b}-\varepsilon $                               |"
32,"|                                 | HF    | LC    | ECD   | LI    | HE     | PID   | FH    | ST    | CO    | AN    |
| ------------------------------- | ----- | ----- | ----- | ----- | ------ | ----- | ----- | ----- | ----- | ----- |
| Random guessing                 | 37.22 | 40.18 | 46.25 | 50.28 | 62.73  | 63.24 | 50.39 | 41.76 | 71.55 | 51.28 |
| FSSM $ ^{*} $ (supervised FT-T) | 88.19 | 86.61 | 99.60 | 78.94 | 100.00 | 84.72 | 66.25 | 82.98 | 99.91 | 99.92 |
| 0-shot (GPT-3.5)                | 71.88 | 78.87 | 85.71 | 76.81 | 68.51  | 73.12 | 60.32 | 63.01 | 82.60 | 90.43 |
| 8-shot $ ^{*} $  (GPT-3.5)      | 73.65 | 78.87 | 87.68 | 76.81 | 68.51  | 73.12 | 58.27 | 60.85 | 77.63 | 87.19 |
| CoT (GPT-3.5)                   | 71.88 | 78.87 | 82.36 | 76.81 | 68.51  | 70.83 | 60.32 | 63.01 | 82.60 | 90.43 |
| TabLIM (GPT-3.5)                | 76.37 | 78.87 | 87.06 | 78.24 | 74.39  | 75.69 | 61.78 | 68.48 | 85.78 | 89.11 |
| LIFT (GPT-3.5)                  | 78.23 | 80.69 | 83.92 | 73.60 | 72.57  | 73.12 | 60.32 | 70.92 | 87.93 | 90.43 |
| SERSAL (GPT-3.5)                | 91.39 | 85.42 | 86.40 | 79.39 | 85.14  | 78.97 | 63.97 | 76.36 | 96.85 | 98.37 |
| TabLLM+SERSAL (GPT-3.5)         | 93.82 | 85.42 | 88.39 | 80.71 | 89.27  | 82.54 | 65.02 | 81.74 | 97.51 | 98.16 |
| SERSAL (GPT-4)                  | 94.18 | 86.93 | 92.68 | 82.51 | 92.76  | 82.39 | 67.14 | 81.23 | 97.96 | 98.82 |"
32,"|                 | HF    | LC    | ECD   | LI    | HE    | PID   | FH    | ST    | CO    | AN    |
| --------------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| SERSAL          | 91.39 | 85.42 | 86.40 | 79.39 | 85.14 | 78.97 | 63.97 | 76.36 | 96.85 | 98.37 |
| w/o soft pseudo | 84.58 | 76.58 | 87.24 | 78.25 | 75.79 | 75.93 | 62.58 | 75.05 | 93.97 | 97.53 |
| w/o ES          | 84.03 | 74.11 | 75.92 | 59.39 | 47.41 | 68.43 | 57.08 | 74.70 | 90.57 | 97.57 |"
32,"| # Loop | FCD        | LI     |
| ------ | ---------- | ------ |
| SERSAL | LLM 0-shot | SERSAL |
| 1      | 86.40      | 85.71  |
| 2      | 87.00      | 86.42  |
| 3      | 89.00      | 87.81  |"
32,"|                        | N00041119 | N00174655 | N00312208 | N00079274 | N00694382 |
| ---------------------- | --------- | --------- | --------- | --------- | --------- |
| FSSM*(supervised FT-T) | 62.38     | 89.20     | 77.83     | 71.78     | 73.89     |
| 0-shot (GPT-3.5)       | 56.79     | 73.08     | 63.49     | 59.85     | 62.70     |
| CoT (GPT-3.5)          | 56.79     | 73.08     | 60.73     | 59.85     | 62.70     |
| SERSAL (GPT-3.5)       | 58.31     | 82.64     | 71.92     | 64.17     | 66.31     |
| SERSAL (GPT-4)         | 65.08     | 88.62     | 78.39     | 67.94     | 71.47     |"
33,"| Model                    | CIFAR-10            | FFHQ                     | ImageNet             |
| ------------------------ | ------------------- | ------------------------ | -------------------- |
| 32  $ \times $  32       | 64  $ \times $  64  | 64  $ \times $  64 Cond. |                      |
| ECT-IC (Short)           | 7.37  $ \pm $  0.05 | 13.29  $ \pm $  0.10     | 10.82  $ \pm $  0.18 |
| ECT-GC ( $ \mu $  = 0.3) | 6.41  $ \pm $  0.05 | 11.73  $ \pm $  0.09     | 10.31  $ \pm $  0.22 |
| ECT-IC (Long)            | 4.11  $ \pm $  0.03 | 9.68  $ \pm $  0.06      | 5.84  $ \pm $  0.21  |
| ECT-GC ( $ \mu $  = 0.3) | 3.74  $ \pm $  0.04 | 8.51  $ \pm $  0.09      | 6.39  $ \pm $  0.20  |"
34,"| Latent feature                                        | Struct token type                | Reconstruction |
| ----------------------------------------------------- | -------------------------------- | -------------- |
| RMSD $ \downarrow $                                   | TMscore $ \uparrow $             |                |
| $ z_{\text{cont}} $                                   | (pre-quantized) continuous token | 1.3127         |
| $ z_{\text{index}} \Leftrightarrow z_{\text{quant}} $ | (quantized) discrete token       | 1.9806         |"
34,"| Tokenizer            | Reconstruction        | Generation          |
| -------------------- | --------------------- | ------------------- |
| rRMSD $ \downarrow $ | rTMscore $ \uparrow $ | RMSD $ \downarrow $ |
| DPLM-2               | 1.9806                | 0.9385              |
| ESM3                 | 0.7248                | 0.9912              |"
34,"|            | $ z_{cont} $            | $ z_{quant} $    | $ z_{index} $ |
| ---------- | ----------------------- | ---------------- | ------------- |
| Original   | (0.1, -1.5, -3.2, 0.7)  | (+1, -1, -1, +1) | 9             |
| Flip a bit | (-0.2, -1.5, -3.2, 0.7) | (-1, -1, -1, +1) | 1             |"
34,"| Model              | Testset    | Struct Token Acc $ \uparrow $ | Struct Eval Metric   |
| ------------------ | ---------- | ----------------------------- | -------------------- |
| index              | bit        | RMSD $ \downarrow $           | TMscore $ \uparrow $ |
| DPLM-2 index-based | CAMEO 2022 | 0.0864                        | 0.7720               |
| PDB date split     | 0.1188     | 0.7932                        | 5.3071               |
| DPLM-2 Bit-based   | CAMEO 2022 | 0.1258                        | 0.7958               |
| PDB date split     | 0.2641     | 0.8648                        | 3.2213               |"
34,"| Models                             | CAMEO 2022           | PDB date split      |
| ---------------------------------- | -------------------- | ------------------- |
| RMSD $ \downarrow $                | TMscore $ \uparrow $ | RMSD $ \downarrow $ |
| ESMFold (3B) (Lin et al., 2022)    | 3.9900               | 0.8500              |
| MultiFlow (Campbell et al., 2024a) | 17.8400              | 0.5000              |
| ESM3 (1.4B) (Hayes et al., 2024)   | 6.3300               | 0.8400              |
| DPLM-2 (650M)                      | 7.7025               | 0.7936              |
| DPLM-2 + RESDIFF                   | 7.2881               | 0.8087              |
| DPLM-2 (Bit-based)                 | 6.4028               | 0.8380              |
| DPLM-2 (Bit-based) + RESDIFF       | 6.1781               | 0.8428              |
| DPLM-2 (Bit-based) + FM            | 6.1825               | 0.8414              |
| DPLM-2 (Bit-based) + FM + RESDIFF  | 6.0765               | 0.8456              |
| w/ folding SFT                     | 5.8472               | 0.8442              |
| DPLM-2 (3B) w/ folding SFT         | 5.9832               | 0.8443              |"
34,"|                      | CAMEO 2022       |
| -------------------- | ---------------- |
|                      | AAR $ \uparrow $ |
| DPLM2 650M           | 0.4962           |
| DPLM2 3B             | 0.5236           |
| DPLM2 BITWISE        | 0.5586           |
| GEO + BITWISE        | 0.5665           |
| GEO + BITWISE + REPA | 0.5681           |"
34,"| Methods             | PDB date split       | CAMEO 2022          |
| ------------------- | -------------------- | ------------------- |
| RMSD $ \downarrow $ | TMscore $ \uparrow $ | RMSD $ \downarrow $ |
| DPLM-2              | 5.521                | 0.8287              |
| w REPA              | 4.919                | 0.8508              |
| GeoDPLM             | 4.823                | 0.8521              |
| w REPA              | 4.340                | 0.8671              |"
34,"| Models              | PDB date split       | CAMEO 2022          | Uncond. Gen.         |
| ------------------- | -------------------- | ------------------- | -------------------- |
| RMSD $ \downarrow $ | TMscore $ \uparrow $ | RMSD $ \downarrow $ | TMscore $ \uparrow $ |
| DPLM-2 (650M)       | 5.307                | .8306               | 7.703                |
| Bit                 | 3.221                | .9043               | 6.403                |
| Bit + FM            | 2.870                | .9099               | 6.183                |
| Bit + FM + ResDiff  | 2.788                | .9146               | 6.077                |
| w/ SFT              | 2.370                | .9270               | 5.847                |
| Geo + Bit           | 2.551                | .9254               | 5.955                |
| Geo + Bit + FM      | 2.443                | .9261               | 6.172                |
| Geo + Bit + REPA    | 2.507                | .9264               | 6.192                |
| w/ SFT              | 2.404                | .9322               | 5.754                |
| All $ ^{*} $        | 2.379                | .9297               | 6.200                |"
34,"| Training Data | SFT       | PDB-Multimer        | CAMEO 2022           |
| ------------- | --------- | ------------------- | -------------------- |
| PDB -Multimer | Swissprot | RMSD $ \downarrow $ | TMscore $ \uparrow $ |
|               | ✓         |                     | 17.966               |
|               | ✓         | ✓                   | 19.615               |
| ✓             |           | ✓                   | 16.146               |
| ✓             | ✓         | ✓                   | 16.674               |"
35,"| Method                           | # para | MME     | Text-VQA | VSR   | SNLI-VE | CIFAR-10 | CIFAR-100 | MNIST | POPE  | MMAvg |
| -------------------------------- | ------ | ------- | -------- | ----- | ------- | -------- | --------- | ----- | ----- | ----- |
| LLaVA_{Align} (Liu et al., 2024) | -      | 1110.82 | 32.62    | 50.16 | 34.51   | 80.00    | 58.04     | 52.79 | 59.10 | 52.46 |
| LLaVA_{FT} (Liu et al., 2024)    | 100%   | 1587.26 | 37.26    | 53.76 | 43.35   | 92.97    | 63.73     | 94.27 | 80.82 | 66.59 |
| LoRA (Hu et al., 2022)           | 0.63%  | 1393.67 | 39.20    | 52.95 | 44.56   | 90.10    | 45.90     | 83.42 | 72.33 | 61.21 |
| APrompt (Wang et al., 2023a)     | 0.23%  | 1406.63 | 35.26    | 53.12 | 45.58   | 85.74    | 50.27     | 84.63 | 76.16 | 61.52 |
| PTUM (Yang et al., 2023)         | 0.12%  | 1354.62 | 34.28    | 53.75 | 30.86   | 82.88    | 57.63     | 94.29 | 80.31 | 62.00 |
| VPT (Han et al., 2024b)          | 0.06%  | 1398.74 | 33.68    | 53.93 | 32.62   | 76.49    | 52.31     | 94.73 | 79.60 | 60.48 |
| ReFT (Wu et al., 2024b)          | 0.03%  | 1473.25 | 36.34    | 49.75 | 39.66   | 90.43    | 57.53     | 88.21 | 78.35 | 62.90 |
| M^{2}PT (Wang et al., 2024)      | 1.96%  | 1503.98 | 34.48    | 53.19 | 32.89   | 89.29    | 59.14     | 95.54 | 81.26 | 63.68 |
| MixLoRA (Shen et al., 2024)      | 0.85%  | 1509.61 | 40.42    | 49.18 | 36.69   | 91.40    | 59.27     | 87.68 | 78.48 | 63.30 |
| MRT                              | 0.03%  | 1580.40 | 40.62    | 51.47 | 33.34   | 96.96    | 57.20     | 95.63 | 79.30 | 64.93 |"
35,"| Class  $ e $        | Misclassification           | Misalignment |
| ------------------- | --------------------------- | ------------ |
| $ (LLaVA_{Align}) $ | Misclassification on  $ e $ | Others       |
| (a) cat             | 18.8%                       | 100%         |
| (b) dog             | 17.3%                       | 100%         |
| (c) ship            | 21.8%                       | 100%         |
| (d) frog            | 22.5%                       | 100%         |
| (e) truck           | 21.4%                       | 100%         |"
39,"| End-to-end Real-time Object Detectors |
| ------------------------------------- |
| YOLOv10-S                             |
| YOLOv10-M                             |
| RT-DETR-R18                           |
| RT-DETR-R34                           |
| RT-DETRv2-S                           |
| RT-DETRv2-M                           |
| RT-DETRv3-R18                         |
| RT-DETRv3-R34                         |
| LW-DETR-S                             |
| LW-DETR-M                             |
| D-FINE-S (Ours)                       |
| D-FINE-M (Ours)                       |"
48,"| Algorithm 1: Identity assignment from association matrix. This greedy algorithm can be implemented efficiently on accelerators, allowing us to train end-to-end with it. |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Input                                                                                                                                                                    |
| Hyper parameters: Association score threshold  $ \theta $                                                                                                                |
| Output                                                                                                                                                                   |
| $ M \leftarrow T \times K $                                                                                                                                              |
| $ A \leftarrow $  preprocess(A)                                                                                                                                          |
| $ \hat{A} \leftarrow (A \geq \theta) $ .astype(bool)                                                                                                                     |
| $ \delta \leftarrow $  zeros(M)                                                                                                                                          |
| id_count  $ \leftarrow 0 $                                                                                                                                               |
| while  $ \hat{A}.any() &gt; 0 $  do                                                                                                                                      |
| track_len  $ \leftarrow \hat{A}.sum(axis=1) $  // Compute the number of objects in each merge.                                                                           |
| id_count  $ \leftarrow $  id_count + 1                                                                                                                                   |
| $ \delta \leftarrow \delta + id\_count * \hat{A}_i $                                                                                                                     |
| $ \hat{A} \leftarrow \hat{A} - \hat{A}_i.|\hat{A}_i $                                                                                                                    |
| end                                                                                                                                                                      |
| return  $ \delta $                                                                                                                                                       |"
37,"| Dataset       | General Medical Knowledge | Disease-Specific Knowledge | Future Prediction | Data Source                    | Clinical Workflow Related |
| ------------- | ------------------------- | -------------------------- | ----------------- | ------------------------------ | ------------------------- |
| VQA-Med       | ✓                         | Weak                       | ✗                 | Medical Database               | Low                       |
| OmniMedVQA    | ✓                         | Weak                       | ✗                 | Medical Database               | Low                       |
| VQA-RAD       | ✓                         | Weak                       | ✗                 | Medical Students &amp; Fellows | Medium                    |
| PathVQA       | ✓                         | Weak                       | ✗                 | Textbooks                      | Low                       |
| SLAKE         | ✓                         | Weak                       | ✗                 | Medical Database               | Low                       |
| HIE-Reasoning | ✓                         | Strong                     | ✓                 | Clinical Report                | High                      |"
37,"| Model                 | Lesion Grading         | Lesion Anatomy            | Lesion in Rare Locations  | MRI Injury Score     | Neurocognitive Outcome           | Interpretation Summary   |
| --------------------- | ---------------------- | ------------------------- | ------------------------- | -------------------- | -------------------------------- | ------------------------ |
| Acc ( $ \uparrow $ )  | MAE ( $ \downarrow $ ) | F1 Score ( $ \uparrow $ ) | F1 Score ( $ \uparrow $ ) | Acc ( $ \uparrow $ ) | Inter Class Acc ( $ \uparrow $ ) | ROUGE-L ( $ \uparrow $ ) |
| GPT4o-Series          | 33.83%                 | 0.1085                    | 14.81%                    | 11.47%               | 30.08%                           | 52.16%                   |
| Gemini-1.5-Flash      | 24.81%                 | 0.1977                    | 30.17%                    | 22.53%               | 30.83%                           | 56.60%                   |
| MiniGPT4-Med-7B       | 12.03%                 | 0.3785                    | 8.00%                     | 4.62%                | 34.59%                           | 53.95%                   |
| LLava-Med-7B          | 30.83%                 | 0.1468                    | 24.28%                    | 22.52%               | 11.28%                           | 49.50%                   |
| Med-Flamingo-7B       | 18.05%                 | 0.4651                    | -                         | -                    | 21.05%                           | 50.00%                   |
| CGoT-GPT4o-Series     | 56.25%                 | 0.0715                    | 34.25%                    | 33.04%               | 51.13%                           | 61.11%                   |
| CGoT-Gemini-1.5-Flash | 62.41%                 | 0.0703                    | 43.57%                    | 41.47%               | 49.62%                           | 71.73%                   |"
37,"| Clinical Knowledge | Graph of Thoughts | Neurocognitive Outcome Prediction | Lesion Grading | Lesion Anatomy | MRI Injury Score | Neurocognitive Outcome Prediction |
| ------------------ | ----------------- | --------------------------------- | -------------- | -------------- | ---------------- | --------------------------------- |
| ✓                  | ✓                 | 71.73%                            | ✓              | ✓              | ✓                | 71.73%                            |
| ✓                  | ✗                 | 52.43% ( $ \downarrow $  19.30%)  | ✓              | ✓              | ✗                | 50.94% ( $ \downarrow $  20.79%)  |
| ✗                  | ✓                 | 51.89% ( $ \downarrow $  19.84%)  | ✓              | ✗              | ✓                | 70.12% ( $ \downarrow $  1.61%)   |
| ✗                  | ✗                 | 54.44% ( $ \downarrow $  17.29%)  | ✗              | ✓              | ✓                | 61.11% ( $ \downarrow $  10.62%)  |"
38,"| Type              | Model          | Score                                  | Reasoning / Completion Tokens |
| ----------------- | -------------- | -------------------------------------- | ----------------------------- |
| Knowledge         | GPT-4o         | 76.95                                  | - / 8.67                      |
| OpenAI-o1-mini    | 77.60 (+0.65)  | 251.0 / 269.06 ( $ \times $  31.03)    |                               |
| OpenAI-o1-preview | 83.61 (+6.66)  | 518.49 / 545.37 ( $ \times $  62.9)    |                               |
| Reasoning         | GPT-4o         | 64.15                                  | - / 56.43                     |
| OpenAI-o1-mini    | 76.12 (+11.97) | 500.1 / 546.78 ( $ \times $  9.69)     |                               |
| OpenAI-o1-preview | 80.98 (+16.83) | 1106.43 / 1198.78 ( $ \times $  21.24) |                               |
| Overall           | GPT-4o         | 72.29                                  | - / 26.08                     |
| OpenAI-o1-mini    | 77.06 (+4.77)  | 341.8 / 370.29 ( $ \times $  14.2)     |                               |
| OpenAI-o1-preview | 82.65 (+10.36) | 732.79 / 783.54 ( $ \times $  30.04)   |                               |"
38,"| Type      | Example                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| --------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Knowledge | Question:The three fundamental elements of data structure include ().A: Logical structure, storage structure, operations on data.B: Logical structure, algorithm design, program implementation.C: Data types, data storage, data manipulation.D: Data Definition, Data Implementation, Data Manipulation.Answer:AAnalysis:None                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Reasoning | Question:Assuming the main memory address is 32 bits, byte-addressable, and the mapping between main memory and Cache is fully associative, with a main memory block size of one word, each word being 32 bits, using write-back policy and random replacement strategy, then a Cache that can store 32K words of data should have a total capacity of at least (). bits.A: 1536K.B: 1568K.C: 2016K.D: 2048K.Answer:CAnalysis:The total cache capacity must include data, tags, valid bits, and dirty bits. With a 32-bit main memory address and a block size of 1 word (32 bits), each cache line requires a 30-bit tag (32 bits - 2 bits offset), 1 valid bit, and 1 dirty bit. The cache stores 32K words of data (32K × 32 bits). Including overhead, the total capacity is 32K × (32 + 30 + 2) bits = 2048K bits. |"
39,"| Model                                     | #Params. | GFLOPs | Latency (ms) | AP^{val} |
| ----------------------------------------- | -------- | ------ | ------------ | -------- |
| Non-end-to-end Real-time Object Detectors |          |        |              |          |
| YOLOv6-S                                  | 7M       | 17     | 3.62         | 45.0     |
| YOLOv6-M                                  | 35M      | 86     | 5.48         | 50.0     |
| YOLOv8-S                                  | 11M      | 29     | 6.96         | 44.9     |
| YOLOv8-M                                  | 26M      | 79     | 9.66         | 50.2     |
| YOLOv9-S                                  | 7M       | 26     | 8.02         | 44.9     |
| YOLOv9-M                                  | 20M      | 76     | 10.15        | 50.2     |
| Gold-YOLO-S                               | 22M      | 46     | 2.01         | 46.4     |
| Gold-YOLO-M                               | 41M      | 88     | 3.21         | 51.1     |
| RTMDet-S                                  | 9M       | 15     | 7.77         | 44.6     |
| RTMDet-M                                  | 25M      | 39     | 10.62        | 49.4     |
| YOLO11-S                                  | 9M       | 22     | 6.81         | 46.6     |
| YOLO11-M                                  | 20M      | 68     | 8.79         | 51.2     |
| YOLO11-S*                                 | 9M       | 22     | 2.86         | 47.0     |
| YOLO11-M*                                 | 20M      | 68     | 4.95         | 51.5     |"
39,"| Model                                     | #Params. | GFLOPs | Latency (ms) | AP^{val} |
| ----------------------------------------- | -------- | ------ | ------------ | -------- |
| Non-end-to-end Real-time Object Detectors |          |        |              |          |
| YOLOv6-L                                  | 59M      | 150    | 9.04         | 52.8     |
| YOLOv7-L                                  | 36M      | 104    | 16.81        | 51.2     |
| YOLOv7-X                                  | 71M      | 189    | 21.57        | 52.9     |
| YOLOv8-L                                  | 43M      | 165    | 12.31        | 52.9     |
| YOLOv8-X                                  | 68M      | 257    | 16.59        | 53.9     |
| YOLOv9-C                                  | 25M      | 102    | 10.66        | 53.0     |
| YOLOv9-E                                  | 57M      | 189    | 20.53        | 55.6     |
| Gold-YOLO-L                               | 75M      | 152    | 9.21         | 53.3     |
| RTMDet-L                                  | 52M      | 80     | 14.23        | 51.3     |
| RTMDet-X                                  | 95M      | 142    | 21.59        | 52.8     |
| YOLO11-L                                  | 25M      | 87     | 10.28        | 53.4     |
| YOLO11-X                                  | 57M      | 195    | 14.39        | 54.7     |
| YOLO11-L*                                 | 25M      | 87     | 6.31         | 52.9     |
| YOLO11-X*                                 | 57M      | 195    | 10.52        | 54.1     |"
39,"| Endo-end Real-time Object Detectors |
| ----------------------------------- |
| YOLOv10-L                           |
| YOLOv10-X                           |
| RT-DETR-R50                         |
| RT-DETR-R101                        |
| RT-DETR-HG-L                        |
| RT-DETR-HG-X                        |
| RT-DETRv2-L                         |
| RT-DETRv2-X                         |
| RT-DETRv3-L                         |
| RT-DETRv3-X                         |
| LW-DETR-L                           |
| LW-DETR-X                           |
| D-FINE-L (Ours)                     |
| D-FINE-X (Ours)                     |"
39,"| End-to-end Real-time Object Detectors (Pretrained on Objects365) |
| ---------------------------------------------------------------- |
| RT-DETR-R18                                                      |
| LW-DETR-S                                                        |
| LW-DETR-M                                                        |
| D-FINE-S (Ours)                                                  |
| D-FINE-M (Ours)                                                  |"
39,"| Model              | #Params. | #Epochs | AP^{val}    | AP^{val}_{50} | AP^{val}_{75} | AP^{val}_{S} | AP^{val}_{M} | AP^{val}_{L} |
| ------------------ | -------- | ------- | ----------- | ------------- | ------------- | ------------ | ------------ | ------------ |
| Deformable-DETR    | 40M      | 12      | 43.7        | 62.2          | 46.9          | 26.4         | 46.4         | 57.9         |
| + FDR &amp; GO-LSD | 40M      | 12      | 47.1 (+3.4) | 64.7          | 50.8          | 29.0         | 50.3         | 62.8         |
| DAB-DETR           | 48M      | 12      | 44.2        | 62.5          | 47.3          | 27.5         | 47.1         | 58.6         |
| + FDR &amp; GO-LSD | 48M      | 12      | 49.5 (+5.3) | 67.2          | 54.1          | 31.8         | 53.2         | 63.3         |
| DN-DETR            | 48M      | 12      | 46.0        | 64.8          | 49.9          | 27.7         | 49.1         | 62.3         |
| + FDR &amp; GO-LSD | 48M      | 12      | 49.7 (+3.7) | 67.5          | 54.4          | 31.8         | 53.4         | 63.8         |
| DINO               | 47M      | 12      | 49.0        | 66.6          | 53.5          | 32.0         | 52.3         | 63.0         |
| + FDR &amp; GO-LSD | 47M      | 12      | 51.6 (+2.6) | 68.6          | 56.3          | 33.8         | 55.6         | 65.3         |
| DINO               | 47M      | 24      | 50.4        | 68.3          | 54.8          | 33.3         | 53.7         | 64.8         |
| + FDR &amp; GO-LSD | 47M      | 24      | 52.4 (+2.0) | 69.5          | 56.9          | 34.6         | 55.7         | 66.2         |"
39,"| End-to-end Real-time Object Detectors (Pretrained on Objects365) |
| ---------------------------------------------------------------- |
| YOLOv10-L                                                        |
| YOLOv10-X                                                        |
| RT-DETR-R50                                                      |
| RT-DETR-R101                                                     |
| LW-DETR-L                                                        |
| LW-DETR-X                                                        |
| D-FINE-L (Ours)                                                  |
| D-FINE-X (Ours)                                                  |"
40,"| Model       | Params | v1 Acc (%)          |
| ----------- | ------ | ------------------- |
| CLIP-H/14   | 986M   | 59.23               |
| ImageReward | 447M   | 61.10               |
| HPS         | 986M   | 66.70               |
| PickScore   | 986M   | 71.85               |
| PAL-A-Tiny  | 8.4M   | 69.29  $ \pm $  0.6 |
| PAL-B-Tiny  | 6.3M   | 71.13  $ \pm $  0.3 |"
40,"| Model                   | Train Dataset | Test Accuracy on Pick-a-Pic v2 (%) |
| ----------------------- | ------------- | ---------------------------------- |
| No-leakage              | Leakage       |                                    |
| CLIP-H/14               | -             | 62.57                              |
| PickScore               | pickapic v1   | 68.04                              |
| PAL-B-Tiny on CLIP-H    | pickapic v1   | 70.02  $ \pm $  0.39               |
| PAL-B-Tiny on CLIP-H    | pickapic v2   | 70.51  $ \pm $  0.22               |
| PAL-B-Tiny on PickScore | pickapic v2   | 70.16  $ \pm $  0.19               |"
40,"| Model                      | Seen Accuracy (%)    | Unseen Accuracy (%)  |
| -------------------------- | -------------------- | -------------------- |
| P-DPO individual           | 91.04                | 55.34                |
| P-DPO K=5                  | 91.12                | 54.55                |
| Vanilla DPO                | 58.91                | 55.37                |
| PAL-B-Tiny K = 2, N = 10   | 79.54  $ \pm $  0.54 | 74.88  $ \pm $  0.79 |
| PAL-B-Large K = 2, N = 10  | 91.63  $ \pm $  1.43 |                      |
| PAL-B-Large K = 2, N = 20  | 92.82  $ \pm $  0.95 | 91.72  $ \pm $  1.40 |
| PAL-B-Large K = 2, N = 50  |                      | 92.02  $ \pm $  1.10 |
| PAL-B-Large K = 2, N = 100 |                      | 91.97  $ \pm $  1.91 |"
48,"| Dataset                     | Annotation type         | Train set size ( $ 10^{3} $ ) | $ L_{object} $ | $ L_{assoc} $ | $ L_{caption} $ |
| --------------------------- | ----------------------- | ----------------------------- | -------------- | ------------- | --------------- |
| COCO (Lin et al., 2014)     | Image detection         | 118                           | ✓              |               |                 |
| VG (Krishna et al., 2017b)  | Image object captioning | 70                            | ✓              |               | ✓               |
| SMiT (Monfort et al., 2021) | Video captioning        | 480                           |                |               | ✓               |
| Aug-COCO (Lin et al., 2014) | Video object tracking   | 118                           | ✓              | ✓             |                 |"
42,"| Task Type | *Len   | TDT   | SFT    | Reflexion | S-GPT4 | D-GPT4 | S-Llama3 | D-Llama3 |
| --------- | ------ | ----- | ------ | --------- | ------ | ------ | -------- | -------- |
| 1-1(L)    | 107.70 | 0.71  | 15.00  | 4.22      | 97.04  | 100.00 | 40.33    | 82.67    |
| 1-2(L)    | 78.60  | 0.44  | 24.40  | 10.61     | 87.04  | 92.75  | 79.00    | 91.50    |
| 1-3(L)    | 88.90  | 3.88  | 32.20  | 7.78      | 72.78  | 74.00  | 59.33    | 82.67    |
| 1-4(L)    | 75.20  | 0.55  | 57.45  | 0.92      | 100.00 | 100.00 | 84.00    | 90.66    |
| 2-1(M)    | 21.40  | 6.16  | 9.45   | 5.92      | 99.17  | 100.00 | 76.00    | 78.67    |
| 2-2(M)    | 35.20  | 6.43  | 6.75   | 28.59     | 88.17  | 80.17  | 58.00    | 46.67    |
| 2-3(L)    | 65.00  | 19.87 | 5.75   | 22.37     | 95.73  | 88.33  | 76.00    | 76.00    |
| 3-1(S)    | 13.60  | 40.55 | 70.00  | 100.00    | 88.67  | 91.50  | 76.00    | 78.67    |
| 3-2(M)    | 20.80  | 14.26 | 48.33  | 17.45     | 55.33  | 58.00  | 100.00   | 100.00   |
| 3-3(M)    | 25.60  | 10.16 | 59.50  | 72.54     | 71.90  | 78.57  | 100.00   | 100.00   |
| 3-4(M)    | 29.00  | 21.65 | 69.00  | 70.22     | 77.86  | 88.14  | 100.00   | 100.00   |
| 4-1(S)    | 14.60  | 41.93 | 100.00 | 64.93     | 100.00 | 100.00 | 100.00   | 100.00   |
| 4-2(S)    | 8.80   | 55.76 | 100.00 | 87.27     | 100.00 | 100.00 | 100.00   | 100.00   |
| 4-3(S)    | 12.60  | 27.82 | 94.45  | 16.42     | 91.67  | 100.00 | 72.33    | 76.29    |
| 4-4(S)    | 14.60  | 47.15 | 100.00 | 100.00    | 100.00 | 100.00 | 100.00   | 100.00   |
| 5-1(L)    | 69.50  | 6.89  | 13.45  | 7.33      | 74.59  | 73.14  | 58.00    | 78.00    |
| 5-2(L)    | 79.60  | 11.86 | 44.67  | 13.00     | 93.93  | 90.57  | 35.67    | 57.33    |
| 6-1(M)    | 33.60  | 15.10 | 26.25  | 70.35     | 49.40  | 57.40  | 100.00   | 78.67    |
| 6-2(S)    | 15.10  | 15.70 | 53.33  | 70.67     | 100.00 | 100.00 | 100.00   | 100.00   |
| 6-3(M)    | 23.00  | 5.25  | 8.00   | 15.77     | 91.48  | 92.43  | 84.67    | 68.00    |
| 7-1(S)    | 7.00   | 30.00 | 11.19  | 100.00    | 95.00  | 100.00 | 100.00   | 85.71    |
| 7-2(S)    | 7.00   | 8.43  | 83.33  | 67.50     | 85.00  | 85.71  | 84.67    | 92.00    |
| 7-3(S)    | 8.00   | 8.34  | 100.00 | 50.00     | 93.33  | 92.71  | 80.00    | 100.00   |
| 8-1(M)    | 40.00  | 3.86  | 77.87  | 2.58      | 89.00  | 100.00 | 52.00    | 100.00   |
| 8-2(S)    | 16.30  | 8.00  | 33.00  | 8.00      | 68.50  | 38.50  | 61.67    | 45.00    |
| 9-1(L)    | 97.00  | 2.53  | 8.00   | 50.63     | 75.00  | 75.00  | 50.00    | 57.14    |
| 9-2(L)    | 84.90  | 14.66 | 73.33  | 100.00    | 70.00  | 83.33  | 66.67    | 100.00   |
| 9-3(L)    | 123.10 | 9.12  | 73.33  | 70.62     | 60.00  | 71.43  | 77.67    | 88.67    |
| 10-1(L)   | 130.10 | 1.51  | 53.33  | 50.90     | 92.30  | 87.71  | 43.00    | 53.00    |
| 10-2(L)   | 132.10 | 1.29  | 17.00  | 23.69     | 77.60  | 78.00  | 78.00    | 84.00    |"
42,"|                   | In-Distribution      | NovelScenes          | NovelTasks           |
| ----------------- | -------------------- | -------------------- | -------------------- |
| EXEC.             | SR.                  | EXEC.                | SR.                  |
| ProgPrompt        | 87.33  $ \pm $  2.02 | 82.33  $ \pm $  1.76 | 38.67  $ \pm $  1.45 |
| Inner Monologue   | 79.67  $ \pm $  3.38 | 79.33  $ \pm $  3.18 | 54.33  $ \pm $  1.76 |
| Tree Planner      | -                    | -                    | 89.33  $ \pm $  0.17 |
| DGAP-Llama3       | 90.67  $ \pm $  0.86 | 84.33  $ \pm $  2.12 | 63.00  $ \pm $  1.68 |
| DGAP-GPT4         | 93.33  $ \pm $  1.76 | 88.00  $ \pm $  2.45 | 71.67  $ \pm $  1.15 |
| DGAP-InternVL2-8B | 84.33  $ \pm $  1.15 | 68.67  $ \pm $  2.30 | 57.06  $ \pm $  2.11 |"
43,"|                                           | Base                | PCA                   | AE                  | NPCA                | Ours               |
| ----------------------------------------- | ------------------- | --------------------- | ------------------- | ------------------- | ------------------ |
| HV( $ \times 10^{7} $ ,  $ \uparrow $ )   | 3.1  $ \pm $  4.7   | 3.2  $ \pm $  4.2     | 0                   | 1.7  $ \pm $  3.1   | 25.6  $ \pm $  6.9 |
| SP( $ \times 10^{2} $ ,  $ \downarrow $ ) | 31.2  $ \pm $  25.3 | 188.6  $ \pm $  180.7 | 31.3  $ \pm $  30.6 | 53.0  $ \pm $  47.7 | 1.1  $ \pm $  1.2  |"
43,"|                            | Base        | PCA         | AE            | NPCA        | Ours         |
| -------------------------- | ----------- | ----------- | ------------- | ----------- | ------------ |
| HV( $ \times 10^{61} $ ,↑) | 4.4 ± 6.8   | 0           | 0.007 ± 0.018 | 19.4 ± 15.3 | 166.9 ± 48.1 |
| SP( $ \times 10^{5} $ ,↓)  | 1842 ± 1290 | 3837 ± 2164 | 7834 ± 3323   | 34.2 ± 52.3 | 2.3 ± 1.0    |"
43,"|                                           | NPCA                | NPCA-ortho           | Ours                 |
| ----------------------------------------- | ------------------- | -------------------- | -------------------- |
| HV( $ \times 10^{61} $ ,  $ \uparrow $ )  | 19.4  $ \pm $  15.3 | 0.3  $ \pm $  0.5    | 166.9  $ \pm $  48.1 |
| SP( $ \times 10^{5} $ ,  $ \downarrow $ ) | 34.2  $ \pm $  52.3 | 203.7  $ \pm $  24.1 | 2.3  $ \pm $  1.0    |
| Rank                                      | 1                   | 4                    | 4                    |"
43,"|                                           | Ours  | +bias | -rowst | -positivity | -rowst, -positivity |
| ----------------------------------------- | ----- | ----- | ------ | ----------- | ------------------- |
| HV( $ \times 10^{61} $ ,  $ \uparrow $ )  | 166.9 | 132.9 | 46.8   | 0           | 0                   |
| SP( $ \times 10^{5} $ ,  $ \downarrow $ ) | 2.3   | 2.7   | 38.8   | 4066.6      | 5310.7              |"
44,"| Method                  | Law school             | Adult                  |
| ----------------------- | ---------------------- | ---------------------- |
| RMSE ( $ \downarrow $ ) | MAE ( $ \downarrow $ ) | MMD ( $ \downarrow $ ) |
| Constant                | 0.940±0.005            | 0.762±0.004            |
| Full                    | 0.883±0.004            | 0.701±0.005            |
| Unaware                 | 0.917±0.005            | 0.731±0.007            |
| Fair-K                  | 0.904±0.005            | 0.723±0.005            |
| CLAIRE                  | 0.910±0.003            | 0.735±0.003            |
| EXOC                    | 0.902±0.005            | 0.720±0.004            |"
45,"|     | MMLU  | GSM8k | HumanEval |
| --- | ----- | ----- | --------- |
| CoT | 80.73 | 93.60 | 78.05     |
| SC  | 82.13 | 95.67 | -         |
| MAD | 74.73 | 94.93 | 68.09     |"
45,"|           | CCC   | CCL   | CGG   | CLL   | GCC   | GGG   | GGL   | GLC   | GLL   | LLL   |
| --------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| GSM8k     | 87.80 | 90.28 | 94.40 | 90.49 | 91.60 | 94.80 | 95.00 | 94.34 | 94.40 | 84.00 |
| HumanEval | 64.02 | 65.85 | 74.39 | 64.58 | 72.56 | 68.29 | 77.44 | 77.44 | 76.83 | 69.51 |
| MMLU      | 55.13 | 67.20 | 70.02 | 70.00 | 67.40 | 75.00 | 79.00 | 71.92 | 88.20 | 84.20 |"
46,"| $ \theta_{1} $     | $ \theta_{2} $     | Indep.? | $ \phi_{\rm JSD} $  (log) | $ \phi_{\ell_{2}} $ | p-values        |
| ------------------ | ------------------ | ------- | ------------------------- | ------------------- | --------------- |
| $ \phi_{\rm CSW} $ | $ \phi_{\rm CSH} $ |         |                           |                     |                 |
| Llama-2-7b-hf      | llama-7b-hf        | ✓       | -11.10                    | 0.98                | 0.60            |
| Llama-2-7b-hf      | vicuna-7b-v1.1     | ✓       | -10.40                    | 0.63                | 0.16            |
| Llama-2-7b-hf      | Amber              | ✓       | -10.69                    | 0.75                | 0.36            |
| Llama-2-7b-hf      | open-llama-7b      | ✓       | -8.38                     | 0.26                | 0.36            |
| Llama-2-7b-hf      | vicuna-7b-v1.5     | ✗       | -10.87                    | 0.01                | $ \varepsilon $ |
| Llama-2-7b-hf      | CodeLlama-7b-hf    | ✗       | -10.62                    | 0.01                | $ \varepsilon $ |
| Llama-2-7b-hf      | llemma-7b          | ✗       | -10.24                    | 0.01                | $ \varepsilon $ |
| Llama-2-7b-hf      | Orca-2-7b          | ✗       | -10.34                    | 0.01                | $ \varepsilon $ |"
46,"| $ \theta_{1} $ | $ \theta_{2} $      | $ \phi_{\text{CSW}} $ |
| -------------- | ------------------- | --------------------- |
| Llama-2-70b-hf | miqu-1-70b-pytorch  | $ \varepsilon $       |
| Llama-2-70b-hf | Llama-3.1-70B       | 0.571                 |
| Llama-2-70b-hf | Palmyra-Fin-70B-32K | 0.539                 |"
69,"| Model         | Gen    | LAMB  | Wik2  | PTB             | Wik3  | 1BW   |
| ------------- | ------ | ----- | ----- | --------------- | ----- | ----- |
| CEDD $ ^{*} $ | 101.83 | 52.70 | 41.57 | 115.99 $ ^{*} $ | 41.31 | 77.96 |
| CEDDT         | 108.88 | 53.20 | 42.24 | 121.05          | 42.07 | 78.10 |"
48,"| #                                                     | CHOTA | DetA | AssA | CapA | Consistent captions |
| ----------------------------------------------------- | ----- | ---- | ---- | ---- | ------------------- |
| 1 Per-frame cap. w. IOU tracker                       | 49.9  | 64.4 | 52.2 | 37.1 | ✗                   |
| 2 Per-frame cap. w. FairMOT Zhang et al. (2021b)      | 51.2  | 63.4 | 57.2 | 37.0 | ✗                   |
| 3 Per-frame cap. w. ByteTrack Zhang et al. (2022b)    | 52.3  | 64.2 | 60.2 | 37.1 | ✗                   |
| 4 Middle-frame cap. w. ByteTrack Zhang et al. (2022b) | 50.7  | 64.2 | 60.2 | 33.8 | ✓                   |
| 5 Ours, soft aggregation                              | 54.6  | 64.4 | 65.9 | 38.4 | ✓                   |
| 6 Ours, hard aggregation                              | 54.9  | 64.2 | 65.9 | 39.1 | ✓                   |"
48,"| #COCO VG SMiT | Aug        | VidSTG (zero-shot) | VLN (zero-shot) | VidSTG (finetuned) | VLN (finetuned) |
| ------------- | ---------- | ------------------ | --------------- | ------------------ | --------------- |
| COCO          | CHOTA DetA | AssA CapA          | CHOTA DetA      | AssA CapA          | CHOTA DetA      |
| 0             |            |                    | -               | -                  | -               |
| 1             | ✓          |                    | -               | 48.9               | -               |
| 2             | ✓          |                    | -               | 17.8               | -               |
| 3             |            | ✓                  | -               | -                  | -               |
| 4             |            | ✓                  | -               | 19.1               | -               |
| 5             | ✓          | ✓                  | -               | 49.9               | -               |
| 6             | ✓          |                    | -               | 50.4               | -               |
| 7             | ✓          | ✓                  | -               | 51.3               | -               |
| 8             | ✓          | ✓                  | 31.1            | 51.4               | 59.6            |"
48,"|                   | HOTA  | DetA  | AssA  | CIDEr |
| ----------------- | ----- | ----- | ----- | ----- |
| Li et al. (2024b) | 71.98 | 80.79 | 73.71 | 0.087 |
| Ours              | 90.19 | 90.79 | 89.59 | 0.254 |
| - Pretrain        | 88.56 | 89.38 | 87.74 | 0.150 |
| - Backbone        | 86.55 | 84.33 | 89.19 | 0.129 |"
48,"|           | CHOTA | DetA | AssA | CapA |
| --------- | ----- | ---- | ---- | ---- |
| OW-VisCap | 53.0  | 60.1 | 54.0 | 43.9 |
| Ours      | 56.9  | 65.8 | 70.4 | 39.7 |"
48,"|             | Backbone | Rec. &amp; Prec. &gt; 0.5 |
| ----------- | -------- | ------------------------- |
| ReferFormer | ResNet50 | 48.3                      |
| GRiT        | ViT-B    | 62.1                      |
| Ours        | ViT-B    | 65.1                      |"
48,"|          | Finetuned | Zero-shot |
| -------- | --------- | --------- |
| STVGBert | 47.3      | -         |
| TubeDETR | 59.0      | -         |
| STCAT    | 61.7      | -         |
| Ours     | 61.9      | 54.1      |"
49,"| Max New Tokens: 512 | Max New Tokens: 64 |
| ------------------- | ------------------ |
| Methods             | LLaVA-1.5          |
| $ C_{S} $           | $ C_{I} $          |
| Sampling            | 51.3               |
| +TAME               | 47.7               |
| Greedy              | 49.6               |
| +TAME               | 47.3               |
| Beam                | 48.0               |
| +TAME               | 45.2               |
| VCD                 | 51.7               |
| +TAME               | 43.8               |
| ICD                 | 47.4               |
| +TAME               | 44.5               |
| OPERA               | 46.4               |
| +TAME               | 38.2               |
| SID                 | 44.2               |
| +TAME               | 32.2               |"
50,"| Method   | SW-K               | PF-K               | Perslay            | Persformer         |
| -------- | ------------------ | ------------------ | ------------------ | ------------------ |
| Accuracy | 83.6  $ \pm $  0.9 | 85.9  $ \pm $  0.8 | 87.7  $ \pm $  1.0 | 91.2  $ \pm $  0.8 |
| Method   | ECC + XGB          | HT1 + XGB          | ECS + XGB          | HT2 + XGB          |
| Accuracy | 83.8  $ \pm $  0.5 | 82.8  $ \pm $  1.4 | 91.8  $ \pm $  0.4 | 89.9  $ \pm $  0.5 |"
50,"| Dataset                                                                                             | MUTAG     | DHFR      | PROTEINS  | NCI1      |
| --------------------------------------------------------------------------------------------------- | --------- | --------- | --------- | --------- |
| ECC 1D                                                                                              | 87.2(0.7) | 79.4(0.5) | 74.7(0.4) | 74.4(0.2) |
| HT 1D                                                                                               | 87.4(0.8) | 77.9(0.4) | 73.3(0.4) | 73.9(0.2) |
| ECP                                                                                                 | 90.0(0.8) | 82.0(0.4) | 75.0(0.3) | 76.3(0.1) |
| HT nD                                                                                               | 89.4(0.7) | 83.1(0.5) | 75.4(0.4) | 76.4(0.2) |
| Improvement by considering multiple filtrations (up to 5)! Extract various information on the graph |           |           |           |           |"
51,"| Suffice   | Reduced | In    | In    | In    | In    |
| --------- | ------- | ----- | ----- | ----- | ----- |
| 30        | 31      | 32    | 33    | 34    | 35    |
| 1,000,000 | 1,000   | 1,000 | 1,000 | 1,000 | 1,000 |
| 1,000,000 | 1,000   | 1,000 | 1,000 | 1,000 | 1,000 |
| 1,000,000 | 1,000   | 1,    |       |       |       |"
51,"| Method             | EX   |
| ------------------ | ---- |
| Tsf                | + 3F |
| (L.i.o/m)1-6B-hand | + CC |
|                    | + SC |"
51,"| Substance        | Virtual | ROI | ROI | ROI | ROI |
| ---------------- | ------- | --- | --- | --- | --- |
| 100              | 80      | 80  | 10  | 100 | 10  |
| E-2017/78 Rental | Gbpss   | 100 | 10  | 10  | 100 |
| 2017/78, 2018    | 100     | 100 | 10  | 10  | 100 |
| 2017/78, 2019    | 100     | 100 | 10  | 10  | 100 |
| 2017/78, 2019    | 100     | 100 | 10  | 10  | 100 |
| 2017/78, 2019    | 100     | 100 | 10  | 10  | 100 |
| 2017/78, 2019    | 100     | 100 | 10  | 10  | 100 |
| 201              |         |     |     |     |     |"
69,"| Model                    | LAM   | Wik2  | PTB    | Wik3  | 1BW   |
| ------------------------ | ----- | ----- | ------ | ----- | ----- |
| CEDD* a  $ \exp(J_{1}) $ | 64.60 | 65.04 | 192.99 | 64.69 | 79.81 |
| CEDD* a  $ \exp(J_{2}) $ | 64.11 | 64.54 | 191.38 | 64.30 | 79.17 |
| CEDD* r  $ \exp(J_{1}) $ | 67.84 | 70.54 | 216.91 | 70.18 | 86.76 |
| CEDD* r  $ \exp(J_{2}) $ | 67.27 | 69.61 | 213.90 | 69.45 | 85.64 |
| CEDD u  $ \exp(J_{1}) $  | 80.27 | 87.91 | 279.65 | 87.46 | 99.34 |
| CEDD u  $ \exp(J_{2}) $  | 79.46 | 86.82 | 276.61 | 86.52 | 98.44 |"
215,"| Dataset  | Group | Number | No. ROI |
| -------- | ----- | ------ | ------- |
| ADNI     | NC    | 190    | 90      |
| MCI      | 170   | 90     |         |
| AD       | 47    | 90     |         |
| ADHD-200 | NC    | 230    | 190     |
| ADHD     | 229   | 190    |         |
| ABIDE    | NC    | 493    | 200     |
| Autism   | 516   | 200    |         |"
54,"| Model          | LC-Lung | LC-Colon | CRC100K | SkinCancer | Pcam | BACH | Osteo | WSSSLUAD | SICAPv2 | Average |
| -------------- | ------- | -------- | ------- | ---------- | ---- | ---- | ----- | -------- | ------- | ------- |
| OpenAI-CLIP    | 33.1    | 75.7     | 26.2    | 9.6        | 53.9 | 21.7 | 46.9  | 64.6     | 32.8    | 40.6    |
| OpenAI-CLIP-L  | 70.4    | 81.1     | 40.3    | 19.4       | 55.5 | 34.3 | 53.9  | 81.2     | 25.4    | 51.3    |
| PLIP           | 87.9    | 90.2     | 52.8    | 42.5       | 51.8 | 34.3 | 52.9  | 73.1     | 42.5    | 58.6    |
| PubmedCLIP     | 33.3    | 80.5     | 31.5    | 11.3       | 65.4 | 34.8 | 30.0  | 65.4     | 7.0     | 39.8    |
| PMC-CLIP       | 33.3    | 51.9     | 8.7     | 11.4       | 53.8 | 21.3 | 29.2  | 65.2     | 31.5    | 34.0    |
| QuiltNet       | 80.0    | 91.0     | 49.5    | 46.4       | 58.7 | 43.8 | 53.8  | 70.5     | 37.3    | 58.9    |
| PathCLIP       | 88.9    | 94.3     | 55.3    | 35.1       | 72.5 | 46.8 | 69.2  | 85.1     | 48.3    | 66.2    |
| CONCH          | 74.7    | 97.9     | 59.4    | 63.2       | 78.7 | 58.3 | 73.5  | 79.8     | 33.0    | 68.7    |
| BiomedCLIP     | 48.8    | 94.3     | 29.9    | 31.7       | 84.0 | 39.8 | 36.7  | 73.7     | 32.2    | 52.9    |
| PathGen-CLIP   | 90.0    | 97.5     | 63.3    | 65.6       | 89.2 | 59.5 | 73.5  | 82.9     | 50.3    | 74.3    |
| PathGen-CLIP-L | 89.8    | 99.3     | 78.0    | 70.6       | 88.2 | 71.5 | 74.6  | 82.2     | 63.5    | 79.7    |"
54,"| Method                | Performance | CAMELYON-17 | CAMELYON-16 | BRACS    | Average  |
| --------------------- | ----------- | ----------- | ----------- | -------- | -------- |
| F1-score              | AUC         | F1-score    | AUC         | F1-score | AUC      |
| ABMIL                 | OpenAI-CLIP | 23.5±4.6    | 60.7±3.3    | 62.8±3.5 | 61.4±2.7 |
| OpenAI-CLIP-L         | 37.7±2.9    | 76.7±2.1    | 75.8±0.8    | 69.7±1.6 | 51.6±3.9 |
| PLIP                  | 45.6±5.0    | 82.8±1.1    | 86.6±1.4    | 90.0±2.7 | 51.7±2.0 |
| Quilt-Net             | 44.3±2.0    | 84.2±1.0    | 82.9±1.6    | 87.1±2.2 | 54.7±3.0 |
| BiomedCLIP            | 55.5±2.5    | 84.1±1.2    | 82.6±1.4    | 83.6±3.4 | 57.4±4.8 |
| PathCLIP              | 45.2±3.3    | 82.6±1.9    | 80.2±1.5    | 85.0±1.5 | 56.4±3.2 |
| CONCH                 | 55.0±2.0    | 86.4±0.7    | 93.9±1.1    | 95.2±0.7 | 62.0±1.4 |
| PathGen-CLIP (ours)   | 58.2±3.3    | 87.5±1.1    | 93.5±2.2    | 96.9±1.5 | 62.6±1.9 |
| PathGen-CLIP-L (ours) | 58.6±6.7    | 87.9±1.2    | 94.3±1.8    | 95.8±1.4 | 66.6±6.4 |
| GigaPath-G            | 54.4±3.9    | 88.3±1.2    | 98.0±0.4    | 98.3±0.2 | 62.3±4.9 |
| ACMIL                 | OpenAI-CLIP | 25.4±4.1    | 59.4±3.7    | 63.7±4.3 | 67.3±4.3 |
| OpenAI-CLIP-L         | 34.9±4.5    | 78.5±3.0    | 78.7±3.7    | 73.4±3.4 | 55.5±3.6 |
| PLIP                  | 46.0±1.5    | 86.1±1.0    | 90.4±2.2    | 94.8±1.3 | 57.0±3.3 |
| Quilt-Net             | 44.4±1.0    | 86.0±0.9    | 84.8±3.5    | 90.5±3.4 | 60.8±3.5 |
| BiomedCLIP            | 53.6±4.0    | 83.6±1.3    | 82.9±2.0    | 84.8±3.3 | 63.2±2.0 |
| PathCLIP              | 44.2±0.6    | 82.9±1.3    | 84.4±1.6    | 87.3±1.6 | 58.0±7.6 |
| CONCH                 | 56.3±3.6    | 87.5±0.9    | 94.4±1.0    | 97.2±0.5 | 66.1±1.6 |
| PathGen-CLIP (ours)   | 53.3±4.6    | 89.4±1.2    | 92.6±1.6    | 97.2±0.9 | 66.9±3.0 |
| PathGen-CLIP-L (ours) | 58.4±5.2    | 92.0±0.7    | 94.5±1.0    | 97.4±1.9 | 66.9±5.0 |
| GigaPath-G            | 55.9±3.4    | 89.8±1.4    | 95.7±1.2    | 99.2±0.4 | 61.0±4.5 |"
54,"|                                            | Test Overall | PubMed     | SocialPath | EduContent | Atlas      | PathCLS    |
| ------------------------------------------ | ------------ | ---------- | ---------- | ---------- | ---------- | ---------- |
| Tiny (1156)                                | ALL (9677)   | Tiny (281) | ALL (3068) | Tiny (235) | All (1855) | Tiny (255) |
| Expert performance                         | 71.8         | -          | 72.9       | -          | 71.5       | -          |
| General Large Multimodal Models            |              |            |            |            |            |            |
| BLIP-2 FLAN-T5-XXL                         | 33.3         | 33.5       | 37.0       | 37.4       | 35.7       | 34.6       |
| InstructBLIP-FLAN-T5-XXL                   | 34.3         | 33.9       | 39.1       | 37.2       | 33.6       | 34.3       |
| LLaVA-1.5-13B                              | 38.8         | 37.6       | 44.5       | 41.0       | 40.4       | 40.4       |
| Qwen-VL-MAX                                | 49.2         | 45.9       | 53.0       | 50.9       | 53.6       | 49.3       |
| Gemini Pro Vision                          | 42.8         | 42.7       | 43.8       | 44.9       | 42.4       | 42.0       |
| GPT-4V-1106                                | 53.9         | 49.8       | 59.4       | 53.5       | 58.7       | 53.9       |
| Pathology-specific Large Multimodal Models |              |            |            |            |            |            |
| LLaVA-Med                                  | 25.3         | 26.2       | 28.5       | 27.7       | 28.9       | 27.3       |
| Quilt-LLaVA                                | 45.6         | 41.5       | 47.3       | 42.6       | 46.4       | 46.6       |
| PathGen-LLaVA                              | 60.1         | 58.4       | 60.1       | 60.1       | 60.9       | 58.8       |"
55,"| Method             | LSUN Bedroom        | LSUN Church                  |
| ------------------ | ------------------- | ---------------------------- |
| L2  $ \downarrow $ | KID  $ \downarrow $ | User Pref. (%)  $ \uparrow $ |
| SDEdit-SD1.5       | 86.72               | 0.029                        |
| SDEdit-Flux        | 94.89               | 0.032                        |
| DDIM Inv.          | 87.95               | 0.113                        |
| NTI                | 82.77               | 0.095                        |
| NTI+P2P            | 46.46               | 0.234                        |
| Ours               | 82.65               | 0.025                        |"
56,"| Task Instruction                                                                                                                                                                   | Subtasks                          | Success Criteria                                                                           |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- | ------------------------------------------------------------------------------------------ |
| Train a multitask model on the Clintox dataset to predict a drug&#x27;s toxicity and FDA approval status.                                                                          | Feature Engineering Deep Learning | The trained model gets  $ \geq $  0.77 ROC-AUC score on the test set.                      |
| Develop a drug-target interaction model with the DAVIS dataset to repurpose the antiviral drugs for COVID.                                                                         | Feature Engineering Deep Learning | The top-5 repurposed drugs match the gold top-5 drugs.                                     |
| Analyze the inertial measurement unit (IMU) data collected during sleep and compute sleep endpoints: time of falling asleep, time of awakening, and total duration spent sleeping. | Computational Analysis            | Each computed endpoint is close (math.isclose in Python) to the corresponding gold answer. |
| Analyze Toronto fire stations and their service coverage. Visualize the results to identify coverage gaps.                                                                         | Map Visualization                 | The resulting figure gets  $ \geq $  60 score by the GPT-4o Judge.                         |"
58,"| Metric                      | SD v2.1 | SD + NEG | SLD    | S-Clip | ESD-u  | ESD-x  | SPM    | GIE (*Ours) |
| --------------------------- | ------- | -------- | ------ | ------ | ------ | ------ | ------ | ----------- |
| FID ( $ \downarrow $ )      | 17.132  | 20.764   | 19.871 | 16.710 | 18.545 | 19.688 | 13.566 | 15.452      |
| CLIP score ( $ \uparrow $ ) | 26.332  | 25.571   | 25.788 | 26.002 | 24.232 | 25.493 | 26.221 | 26.433      |"
83,"|            | DTW   | FCN   | TS2Vec | InterpGN |
| ---------- | ----- | ----- | ------ | -------- |
| Avg. Accu. | 0.650 | 0.746 | 0.704  | 0.760    |
| Avg. Rank  | 9.138 | 5.000 | 7.533  | 3.500    |
| Num. Top-1 | 1     | 4     | 0      | 8        |"
59,"| Method                    | Q-Align (IQ)  $ \uparrow $ | Q-Align (IA)  $ \uparrow $ | Q-Align (VQ)  $ \uparrow $ | Quality (UC)  $ \uparrow $ | Amplitude (UC)  $ \uparrow $ | Naturalness (UC)  $ \uparrow $ |
| ------------------------- | -------------------------- | -------------------------- | -------------------------- | -------------------------- | ---------------------------- | ------------------------------ |
| 3D-Cinemagraphy (zoom-in) | 0.47                       | 0.38                       | 0.57                       | 7%                         | 29.4%                        | 19.7%                          |
| 3D-Cinemagraphy (circle)  | 0.48                       | 0.40                       | 0.58                       | 12%                        | 32.0%                        | 21.1%                          |
| Ours (holistic pipeline)  | 0.66                       | 0.44                       | 0.62                       | 81%                        | 38.6%                        | 59.2%                          |"
60,"| Method              | $ \uparrow $  Segmentation | $ \downarrow $  Depth | $ \downarrow $  Normals | $ \uparrow $  Fine-grained retrieval (UnED) | $ \uparrow $  ImageNet classif. |
| ------------------- | -------------------------- | --------------------- | ----------------------- | ------------------------------------------- | ------------------------------- |
| PASCAL              | ADE20k                     | NYUv2                 | NAVI                    | NYUv2                                       | NAVI                            |
| DINO-B              | 66.4                       | 31.8                  | 0.555                   | -                                           | 28.4                            |
| MAE-H/14            | 67.6                       | 33.3                  | 0.517                   | -                                           | -                               |
| iBOT-L/16           | 82.3                       | 44.6                  | 0.417                   | -                                           | 24.5                            |
| JFT-3B ViT-g/14     | 70.7                       | 37.5                  | 0.605                   | 0.096                                       | 24.4                            |
| DINOv2-g/14         | 83.0                       | 49.0                  | 0.344                   | 0.054                                       | 20.5                            |
| CLIP-L              | 74.5                       | 39.0                  | 0.553                   | 0.073                                       | 24.3                            |
| SigLIP-SO/14        | 67.8                       | 35.8                  | 0.580                   | 0.074                                       | 25.6                            |
| OpenCLIP-G/14       | 71.4                       | 39.3                  | 0.541                   | -                                           | -                               |
| TIPS-g/14 LR (ours) | 82.8                       | 47.4                  | 0.377                   | 0.061                                       | 23.0                            |
| TIPS-g/14 HR (ours) | 83.6                       | 49.9                  | 0.353                   | 0.058                                       | 21.9                            |"
60,"| Method              | $ \uparrow $  I $ \rightarrow $ T retrieval | $ \uparrow $  T $ \rightarrow $ I retrieval | $ \uparrow $  ImageNet 0-shot |
| ------------------- | ------------------------------------------- | ------------------------------------------- | ----------------------------- |
| COCO                | Flickr                                      | DOCCI                                       | COCO                          |
| CLIP-L/14           | 56.3                                        | 85.2                                        | 44.4                          |
| OpenCLIP-G/14       | 67.3                                        | 92.9                                        | -                             |
| EVA-CLIP-g/14       | 68.2                                        | 91.6                                        | -                             |
| SigLIP-SO/14        | 70.2                                        | 91.0                                        | 27.5                          |
| SILC-G/16           | 73.2                                        | -                                           | -                             |
| TIPS-g/14 LR (ours) | 73.7                                        | 93.0                                        | 56.4                          |
| TIPS-g/14 HR (ours) | 74.0                                        | 93.0                                        | 57.2                          |"
62,"| Methods   | CoreFull | OGB-Arxiv  | Reddit   | OGB-Products |
| --------- | -------- | ---------- | -------- | ------------ |
| AA/%↑     | AF/%↑    | AA/%↑      | AF/%↑    | AA/%↑        |
| Fine-tune | 3.5±0.5  | -95.2±0.5  | 4.9±0.0  | -89.7±0.4    |
| EWC       | 52.6±8.2 | -38.5±12.1 | 8.5±1.0  | -69.5±8.0    |
| MAS       | 12.3±3.8 | -83.7±4.1  | 4.9±0.0  | -86.8±0.6    |
| GEM       | 8.4±1.1  | -88.4±1.4  | 4.9±0.0  | -89.8±0.3    |
| TWP       | 62.6±2.2 | -30.6±4.3  | 6.7±1.5  | -50.6±13.2   |
| LwF       | 33.4±1.6 | -59.6±2.2  | 9.9±12.1 | -43.6±11.9   |
| ER-GNN    | 34.5±4.4 | -61.6±4.3  | 30.3±1.5 | -54.0±1.3    |
| SSM       | 75.4±0.1 | -9.7±0.0   | 48.3±0.5 | -10.7±0.3    |
| SEM       | 77.7±0.8 | -10.0±1.2  | 49.9±0.6 | -8.4±1.3     |
| Joint     | 81.2±0.4 | -3.3±0.8   | 51.3±0.5 | -6.7±0.5     |
| DMSG      | 77.8±0.3 | -0.5±0.5   | 50.7±0.4 | -1.9±1.0     |"
63,"| Framework Type          | Framework Name     | General            | Web Shopping       |
| ----------------------- | ------------------ | ------------------ | ------------------ |
| Training                | Test               | Training           | Test               |
| Prompting               | AppAgent + GPT-4v  | 41.4               | 43.0               |
| AppAgent + Gemini       | 39.1               | 45.3               | 30.5               |
| Learning                | AutoUI             | 38.3               | 40.6               |
| DigiRL (single, online) | 64.6  $ \pm $  1.5 | 59.9  $ \pm $  2.1 | 63.3  $ \pm $  1.5 |
| DigiRL (multi)          | 67.7  $ \pm $  1.3 | 61.2  $ \pm $  2.4 | 64.5  $ \pm $  1.1 |
| DistRL (Ours)           | 75.5  $ \pm $  0.2 | 73.2  $ \pm $  1.1 | 69.8  $ \pm $  0.5 |"
65,"| Application                                              | Baseline                                       |
| -------------------------------------------------------- | ---------------------------------------------- |
| Fisher Merging [5]                                       | Parameter-Averaging                            |
| Model Merging by Uncertainty-Based Gradient Matching [2] | Parameter-Averaging                            |
| Fisher Pruning [7]                                       | Random Pruning                                 |
| FISH Mask [6]                                            | Random Masking                                 |
| Task Embedding [1]                                       | Using datasize to predict task-transferability |
| Elastic Weight Consolidation [3] (Continual Learning)    | Incremental training without regularization    |"
65,"| Setting        | Time (s)              |
| -------------- | --------------------- |
| Fisher Merging | $ 2.93 \cdot 10^{4} $ |
| UBGM           | $ 4.36 \cdot 10^{2} $ |
| Fisher Pruning | 2.52                  |
| FISH Mask      | $ 6.98 \cdot 10^{2} $ |
| Task Embedding | $ 5.19 \cdot 10^{4} $ |
| EWC            | $ 1.25 \cdot 10^{3} $ |"
69,"| Dataset     | SEDD   | SEDDs  | CEDD   | CEDD $ ^{*} $ |
| ----------- | ------ | ------ | ------ | ------------- |
| GenPerp     | 172.35 | 166.35 | 148.21 | 143.86        |
| LAMBADA     | 70.07  | 67.05  | 65.18  | 64.60         |
| WikiText2   | 75.20  | 69.37  | 65.66  | 65.04         |
| PTB         | 240.43 | 208.69 | 199.69 | 192.99        |
| WikiText103 | 74.79  | 69.17  | 65.62  | 64.69         |
| 1BW         | 88.99  | 83.87  | 79.83  | 79.81         |"
69,"| Dataset     | SEDD   | SEDDs  | CEDD   | CEDD $ ^{*} $ |
| ----------- | ------ | ------ | ------ | ------------- |
| GenPerp     | 178.94 | 172.93 | 167.67 | 158.56        |
| LAMBADA     | 72.07  | 69.10  | 69.77  | 67.84         |
| WikiText2   | 80.13  | 74.38  | 72.91  | 70.54         |
| PTB         | 230.74 | 209.12 | 227.16 | 216.91        |
| WikiText103 | 79.68  | 74.16  | 72.49  | 70.18         |
| 1BW         | 93.45  | 88.02  | 86.55  | 86.76         |"
69,"| Dataset     | SEDD   | SEDDs  | CEDD   | CEDD $ ^{*} $ |
| ----------- | ------ | ------ | ------ | ------------- |
| GenPerp     | 169.66 | 163.88 | 161.84 | 175.42        |
| LAMBADA     | 80.74  | 81.13  | 80.27  | 82.54         |
| WikiText2   | 91.79  | 89.21  | 87.91  | 89.68         |
| PTB         | 252.81 | 228.37 | 279.65 | 289.09        |
| WikiText103 | 91.40  | 88.56  | 87.46  | 88.90         |
| 1BW         | 102.75 | 100.80 | 99.34  | 106.32        |"
70,"|                              | KRLS     | Log-Det  | BlockGreedy | REDOR     |
| ---------------------------- | -------- | -------- | ----------- | --------- |
| Hopper-medium-v0             | 69.4±2.5 | 58.4±3.6 | 83.7±2.2    | 94.3±4.6  |
| Hopper-expert-v0             | 91.0±1.1 | 90.7±1.3 | 98.7±0.5    | 110.0±0.5 |
| Hopper-medium-replay-v0      | 28.5±3.2 | 29.4±1.2 | 30.5±2.4    | 35.3±3.2  |
| Walker2d-medium-v0           | 49.1±2.8 | 47.5±3.4 | 53.3±3.6    | 80.5±2.9  |
| Walker2d-expert-v0           | 68.4±3.2 | 67.5±5.6 | 74.8±3.4    | 104.6±2.5 |
| Walker2d-medium-replay-v0    | 14.3±1.2 | 15.2±2.2 | 16.7±1.3    | 21.1±1.8  |
| Halfcheetah-medium-v0        | 23.4±0.5 | 21.9±0.9 | 27.5±0.7    | 41.0±0.2  |
| Halfcheetah-expert-v0        | 73.9±1.4 | 72.1±2.2 | 79.2±1.8    | 88.5±2.4  |
| Halfcheetah-medium-replay-v0 | 39.5±0.3 | 39.9±0.5 | 40.5±1.0    | 41.1±0.4  |"
71,"|                             | Methods                    | RealEstate10K | Structured3D | CAD-estate |
| --------------------------- | -------------------------- | ------------- | ------------ | ---------- |
| mAA@30                      | RRA@15                     | RTA@15        | mAA@30       | RRA@15     |
| (a)                         | DUSt3R (Wang et al., 2024) | 61.2          | 89.44        | 85.00      |
| MASt3R (Leroy et al., 2024) | 76.4                       | 92.94         | 89.77        | 85.34      |
| (b)                         | Plane-DUSt3R (metric)      | -             | 98.21        | 96.66      |
| Plane-DUSt3R (aligned)      | -                          | 97.95         | 96.59        | 91.80      |"
71,"| Method              | re-IoU(%)↑ | re-PE(%)↓ | re-EE↓ | re-RMSE↓ | 3D-precision(%)↑ | 3D-recall(%)↑ |
| ------------------- | ---------- | --------- | ------ | -------- | ---------------- | ------------- |
| Noncuboid + MASt3R  | 74.51      | 8.57      | 12.72  | 0.4831   | 37.00            | 43.39         |
| Noncuboid + GT pose | 75.93      | 7.97      | 11.37  | 0.4457   | 46.96            | 50.59         |
| Ours (metric)       | 75.34      | 8.60      | 10.83  | 0.4388   | 48.98            | 45.35         |
| Ours (aligned)      | 76.84      | 7.82      | 9.53   | 0.4099   | 52.63            | 48.37         |"
73,"| Methods                            | R Precision↑ | FID↓       | LaMP-BertScore↑ | MultiModal Dist↓ | Diversity → |
| ---------------------------------- | ------------ | ---------- | --------------- | ---------------- | ----------- |
| Top 1                              | Top 2        | Top 3      |                 |                  |             |
| Ground Truth                       | 0.511±.003   | 0.703±.003 | 0.797±.002      | 0.002±.00        | 100.00      |
| TM2T (Guo et al., 2022b)           | 0.424±.003   | 0.618±.003 | 0.729±.002      | 1.501±.017       | -           |
| T2M (Guo et al., 2022a)            | 0.455±.003   | 0.636±.003 | 0.736±.002      | 1.087±.021       | -           |
| MDM (Tevet et al., 2023)           | -            | -          | 0.611±.007      | 0.544±.044       | -           |
| MLD (Chen et al., 2023)            | 0.481±.003   | 0.673±.003 | 0.772±.002      | 0.473±.013       | 52.08       |
| MotionDiffuse (Zhang et al., 2022) | 0.491±.001   | 0.681±.001 | 0.782±.001      | 0.630±.001       | -           |
| T2M-GPT (Zhang et al., 2023a)      | 0.492±.003   | 0.679±.002 | 0.775±.002      | 0.141±.005       | 56.21       |
| PhysDiff (Yuan et al., 2023)       | -            | -          | 0.631           | 0.433            | -           |
| MotionGPT (Zhang et al., 2024)     | -            | -          | -               | 0.567            | -           |
| M2DM (Kong et al., 2023)           | 0.497±.003   | 0.682±.002 | 0.763±.003      | 0.352±.005       | -           |
| Fg-T2M (Wang et al., 2023)         | 0.492±.002   | 0.683±.003 | 0.783±.002      | 0.243±.019       | -           |
| AttT2M (Zhong et al., 2023)        | 0.499±.003   | 0.690±.002 | 0.786±.002      | 0.112±.006       | -           |
| DiverseMotion (Lou et al., 2023)   | 0.496±.004   | 0.687±.004 | 0.783±.003      | 0.070±.004       | -           |
| ParCo (Zou et al., 2024)           | 0.515±.003   | 0.706±.003 | 0.801±.002      | 0.109±.005       | -           |
| MMM (Pinyoanuntapong et al., 2024) | 0.504±.003   | 0.696±.003 | 0.794±.002      | 0.080±.003       | -           |
| ReMoDiffuse (Zhang et al., 2023b)  | 0.510±.005   | 0.698±.006 | 0.795±.004      | 0.103±.004       | -           |
| MoMask (Guo et al., 2023)          | 0.521±.002   | 0.713±.002 | 0.807±.002      | 0.045±.002       | 60.40       |
| Ours                               | 0.557±.003   | 0.751±.002 | 0.843±.001      | 0.032±.002       | 60.81       |"
73,"| Methods                        | Text-Motion Retrieval↑ | Motion-Text Retrieval↑ |
| ------------------------------ | ---------------------- | ---------------------- |
| R@1 ↑                          | R@2 ↑                  | R@3 ↑                  |
| TEMOS (Petrovich et al., 2022) | 0.424                  | 53.52                  |
| T2M (Guo et al., 2022a)        | 52.48                  | 71.05                  |
| TMR (Petrovich et al., 2023)   | 67.16                  | 81.32                  |
| Ours                           | 67.18±0.5              | 81.9±0.4               |"
73,"| Method                         | R Precision $ \uparrow $ | MultiModal Dist $ \downarrow $ | LaMP-BertScore $ \uparrow $ | Bleu@1  $ \uparrow $ | Bleu@4  $ \uparrow $ | Rouge $ \uparrow $ | Cider $ \uparrow $ |
| ------------------------------ | ------------------------ | ------------------------------ | --------------------------- | -------------------- | -------------------- | ------------------ | ------------------ |
| T2MT (Guo et al., 2022a)       | 0.516                    | 0.823                          | 2.935                       | 32.2                 | 48.9                 | 7.00               | 38.1               |
| Motiongpt (Jiang et al., 2023) | 0.543                    | 0.827                          | 2.821                       | 32.4                 | 48.2                 | 12.47              | 37.4               |
| LaMP-M2T (Ours)                | 0.547                    | 0.831                          | 2.808                       | 32.7                 | 47.8                 | 13.04              | 37.1               |"
74,"| Method                           | Metrics                   | Noisy MNIST      | CIFAR-10         |
| -------------------------------- | ------------------------- | ---------------- | ---------------- |
| 2  $ \times $  2                 | 3  $ \times $  3          | 4  $ \times $  4 | 5  $ \times $  5 |
| Gumbel-Sinkhorn                  | Kendall-Tau  $ \uparrow $ | 0.9984           | 0.6908           |
| Network (Mena et al., 2018)      | Accuracy (%)              | 99.81            | 44.65            |
| Correct (%)                      | 99.91                     | 80.20            | 49.51            |
| DiffSort (Petersen et al., 2022) | Kendall-Tau  $ \uparrow $ | 0.9931           | 0.3054           |
| Accuracy (%)                     | 99.02                     | 5.56             | 0.00             |
| Correct (%)                      | 99.50                     | 42.25            | 10.77            |
| Error-free                       | Kendall-Tau  $ \uparrow $ | 0.9899           | 0.2014           |
| DiffSort (Kim et al., 2024)      | Accuracy (%)              | 98.62            | 0.82             |
| Correct (%)                      | 99.28                     | 32.65            | 7.40             |
| Symmetric                        | Kendall-Tau  $ \uparrow $ | 0.9992           | 0.8126           |
| Diffusers                        | Accuracy (%)              | 99.88            | 57.38            |
| (Ours)                           | Correct (%)               | 99.94            | 86.16            |"
74,"| Method                      | Metrics                   | Sequence Length |
| --------------------------- | ------------------------- | --------------- |
| 3                           | 5                         | 7               |
| DiffSort                    | Kendall-Tau  $ \uparrow $ | 0.930           |
| (Petersen et al., 2022)     | Accuracy (%)              | 93.8            |
| Correct (%)                 | 95.8                      | 92.9            |
| Error-free                  | Kendall-Tau  $ \uparrow $ | 0.974           |
| DiffSort (Kim et al., 2024) | Accuracy (%)              | 97.7            |
| Correct (%)                 | 98.4                      | 97.7            |
| Symmetric                   | Kendall-Tau  $ \uparrow $ | 0.976           |
| Diffusers                   | Accuracy (%)              | 98.0            |
| (Ours)                      | Correct (%)               | 98.5            |"
87,"| Model       | Source    | Style Reference | Timbre Reference |
| ----------- | --------- | --------------- | ---------------- |
| Vevo-Timbre | $ U_{i} $ | /               | $ U_{r} $        |
| Vevo-Style  | $ U_{i} $ | $ U_{r} $       | $ U_{i} $        |
| Vevo-Voice  | $ U_{i} $ | $ U_{r} $       | $ U_{r} $        |
| Vevo-TTS    | $ T_{i} $ | $ U_{r} $       | $ U_{r} $        |"
74,"| Method                      | TSP-20                             | TSP-50                      |
| --------------------------- | ---------------------------------- | --------------------------- |
| Tour Length  $ \downarrow $ | Optimality Gap (%)  $ \downarrow $ | Tour Length  $ \downarrow $ |
| OR Solvers                  | Concorde                           | 3.84                        |
| LKH-3                       | 3.84                               | 0.00                        |
| 2-Opt                       | 4.02                               | 4.64                        |
| Learning-Based Models       | GCN                                | 3.85 $ ^{*} $               |
| DIFUSCO                     | 3.88 $ ^{*} $                      | 1.07 $ ^{*} $               |
| Ours                        | 3.85                               | 0.18                        |"
75,"| Models             | Datasets | ETTh1  | ETTh2  | ETTm1  | ETTm2  | ECL    | EXG    | ILI    | Solar  | Traffic |
| ------------------ | -------- | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------- |
| TimeGrad (2021)    | CRPS     | 0.606  | 1.212  | 0.647  | 0.775  | 0.397  | 0.826  | 1.140  | 0.293  | 0.407   |
| QICE               | 6.731    | 9.488  | 6.693  | 6.962  | 7.118  | 9.464  | 6.519  | 7.378  | 4.581  |         |
| CSDI (2022)        | CRPS     | 0.492  | 0.647  | 0.524  | 0.817  | 0.577  | 0.855  | 1.244  | 0.432  | 1.418   |
| QICE               | 3.107    | 5.331  | 2.828  | 8.106  | 7.506  | 7.864  | 7.693  | 9.957  | 13.613 |         |
| TimeDiff (2023)    | CRPS     | 0.465  | 0.471  | 0.464  | 0.316  | 0.750  | 0.433  | 1.153  | 0.700  | 0.771   |
| QICE               | 14.931   | 14.813 | 14.795 | 13.385 | 15.466 | 14.556 | 14.942 | 14.914 | 15.439 |         |
| DiffusionTS (2024) | CRPS     | 0.603  | 1.168  | 0.574  | 1.035  | 0.633  | 1.251  | 1.612  | 0.470  | 0.668   |
| QICE               | 6.423    | 9.577  | 5.605  | 9.959  | 8.205  | 10.411 | 10.090 | 6.627  | 5.958  |         |
| TMDM (2024)        | CRPS     | 0.452  | 0.383  | 0.375  | 0.289  | 0.461  | 0.336  | 0.967  | 0.350  | 0.557   |
| QICE               | 2.821    | 4.471  | 2.567  | 2.610  | 10.562 | 6.393  | 6.217  | 9.342  | 10.676 |         |
| NsDiff (ours)      | CRPS     | 0.392  | 0.358  | 0.346  | 0.256  | 0.290  | 0.324  | 0.806  | 0.300  | 0.378   |
| QICE               | 1.470    | 2.074  | 2.041  | 2.030  | 6.685  | 5.930  | 5.598  | 6.820  | 3.601  |         |"
75,"| Variance    | Linear | Quadratic |
| ----------- | ------ | --------- |
| Models      | CRPS   | QICE      |
| TimeGrad    | 1.129  | 3.669     |
| CSDI        | 1.100  | 3.332     |
| TimeDiff    | 1.274  | 10.314    |
| DiffusionTS | 1.454  | 9.290     |
| TMDM        | 1.111  | 4.542     |
| NsDiff      | 1.057  | 0.987     |"
75,"| Metrics  | Forward Noise      | QICE        | CRPS        |
| -------- | ------------------ | ----------- | ----------- |
| w/o LSNM | βtI                | 2.821±0.718 | 0.452±0.027 |
| w/o UANS | βtgψ(X)            | 3.184±0.787 | 0.413±0.015 |
| NsDiff   | βt2gψ(X) + βtαtσY0 | 1.470±0.207 | 0.392±0.009 |"
76,"| CIFAR-10                    | ADP   | GAL   | DVERGE | TRS  | FDT-random           | FDT-target           | FDT-hybrid           |
| --------------------------- | ----- | ----- | ------ | ---- | -------------------- | -------------------- | -------------------- |
| Clean accuracy              | 91.84 | 91.81 | 91.37  | -    | 89.88  $ \pm $  0.02 | 90.16  $ \pm $  0.04 | 90.20  $ \pm $  0.03 |
| FGSM ( $ \epsilon=0.01 $ )  | 59.48 | 44.97 | 70.05  | -    | 66.96  $ \pm $  0.12 | 72.88  $ \pm $  0.12 | 72.24  $ \pm $  0.12 |
| FGSM ( $ \epsilon=0.02 $ )  | 53.38 | 30.58 | 56.33  | 44.2 | 46.28  $ \pm $  0.10 | 55.54  $ \pm $  0.09 | 58.04  $ \pm $  0.13 |
| PGD ( $ \epsilon=0.01 $ )   | 14.45 | 1.35  | 40.55  | 50.5 | 45.42  $ \pm $  0.09 | 46.58  $ \pm $  0.07 | 48.48  $ \pm $  0.09 |
| PGD ( $ \epsilon=0.02 $ )   | 2.95  | 0.34  | 11.49  | 15.1 | 12.24  $ \pm $  0.03 | 15.08  $ \pm $  0.05 | 20.01  $ \pm $  0.04 |
| BIM ( $ \epsilon=0.01 $ )   | 14.15 | 1.37  | 40.51  | 50.6 | 45.24  $ \pm $  0.03 | 46.86  $ \pm $  0.04 | 48.57  $ \pm $  0.05 |
| BIM ( $ \epsilon=0.02 $ )   | 3.01  | 0.27  | 10.65  | 15.8 | 11.68  $ \pm $  0.03 | 14.86  $ \pm $  0.03 | 16.63  $ \pm $  0.02 |
| MIM ( $ \epsilon=0.01 $ )   | 20.38 | 2.05  | 44.74  | 51.5 | 47.73  $ \pm $  0.05 | 49.97  $ \pm $  0.06 | 51.50  $ \pm $  0.07 |
| MIM ( $ \epsilon=0.02 $ )   | 5.11  | 0.69  | 14.76  | 17.2 | 15.14  $ \pm $  0.04 | 18.27  $ \pm $  0.02 | 20.09  $ \pm $  0.03 |
| AA ( $ \epsilon=0.01 $ )    | 1.80  | 0.00  | 43.34  | -    | 46.09  $ \pm $  0.09 | 48.83  $ \pm $  0.08 | 51.56  $ \pm $  0.08 |
| AA ( $ \epsilon=0.02 $ )    | 0.00  | 0.00  | 13.72  | -    | 9.38  $ \pm $  0.05  | 15.70  $ \pm $  0.05 | 19.42  $ \pm $  0.04 |
| C&amp;W ( $ \lambda=0.1 $ ) | 20.96 | 31.57 | 52.35  | 58.1 | 45.01  $ \pm $  0.10 | 55.48  $ \pm $  0.10 | 56.08  $ \pm $  0.11 |"
77,"| Selection Method     | Proxy Model (M)                 | Proxy Function (F_M)                   | Domain Independent |
| -------------------- | ------------------------------- | -------------------------------------- | ------------------ |
| Heuristic Cls.       | Trained binary classifier       | Probability to target domain           | ✗                  |
| DSIR (NeurIPS’23)    | Hashed n-gram extractor         | Similarity to target distribution      | ✗                  |
| QuRating (ICML’24)   | Trained judge model via GPT-3.5 | Judgement score of target ability      | ✗                  |
| INGENIOUS (Emnlp’23) | Pre-trained LLM with warming-up | Facility Location on similarity matrix | ✓                  |
| D4 (NeurIPS’23)      | Existing text feature extractor | Distance in feature space              | ✓                  |
| DiSF (Ours)          | Existing text feature extractor | Decorrelation of feature dimensions    | ✓                  |"
77,"| Model Size  | Method | Pre-training TinyLlama from scratch with 10B training budget on 1.5% selected files |
| ----------- | ------ | ----------------------------------------------------------------------------------- |
| #Metric     | ARC-e  | ARC-c                                                                               |
| 120M        | Random | 37.1                                                                                |
| DSIR        | 36.6   | 18.2                                                                                |
| D4          | 38.0   | 18.1                                                                                |
| QuRating-W  | 37.7   | 18.6                                                                                |
| QuRating-A  | 39.1   | 18.3                                                                                |
| Doremi      | 36.8   | 18.2                                                                                |
| INGENIOUS   | 39.5   | 18.6                                                                                |
| DiSF (Ours) | 39.9   | 17.8                                                                                |"
77,"| Method              | Random | DSIR | QuRating-W | QuRating-A | DiSF (Ours) |
| ------------------- | ------ | ---- | ---------- | ---------- | ----------- |
| Commonsense ability | 43.1   | 41.9 | 42.6       | 43.6       | 44.4        |"
78,"| Model            | # Params | Medical LVLM | VQA-RAD  $ \uparrow $  close | SLAKE  $ \uparrow $  close | PathVQA  $ \uparrow $  close | MMMU-Med | OMVQA $ \uparrow $ | Avg.  $ \uparrow $ |
| ---------------- | -------- | ------------ | ---------------------------- | -------------------------- | ---------------------------- | -------- | ------------------ | ------------------ |
| Med-Flamingo     | 8.3B     | ✓            | 58.6                         | 43.0                       | 47.0                         | 25.5     | 61.9               | 31.3               |
| LLaVA-Med        | 7B       | ✓            | 60.2                         | 48.1                       | 58.4                         | 44.8     | 62.3               | 35.7               |
| HuatuoGPT-Vision | 7B       | ✓            | 66.9                         | 53.0                       | 59.8                         | 49.1     | 52.9               | 32.0               |
| BLIP-2           | 6.7B     | ✗            | 43.4                         | 36.8                       | 41.6                         | 35.3     | 48.5               | 28.8               |
| LLaVA-v1.5       | 7B       | ✗            | 51.8                         | 42.8                       | 37.1                         | 37.7     | 53.5               | 31.4               |
| InstructBLIP     | 7B       | ✗            | 61.0                         | 44.8                       | 66.8                         | 43.3     | 56.0               | 32.3               |
| Yi-VL            | 6B       | ✗            | 52.6                         | 42.1                       | 52.4                         | 38.4     | 54.9               | 30.9               |
| InternVL2        | 8B       | ✗            | 64.9                         | 49.0                       | 66.6                         | 50.1     | 60.0               | 31.9               |
| Llama-3.2        | 11B      | ✗            | 68.9                         | 45.5                       | 72.4                         | 52.1     | 62.8               | 33.6               |
| Show-o           | 1.3B     | ✗            | 50.6                         | 33.9                       | 31.5                         | 17.9     | 52.9               | 28.2               |
| Unified-IO 2     | 7B       | ✗            | 46.2                         | 32.6                       | 35.9                         | 21.9     | 52.5               | 27.0               |
| Janus            | 1.3B     | ✗            | 70.9                         | 52.8                       | 34.7                         | 26.9     | 51.9               | 27.9               |
| Janus-Pro        | 7B       | ✗            | 62.9                         | 49.2                       | 51.3                         | 38.7     | 48.9               | 27.2               |
| Emu3             | 8B       | ✗            | 72.1                         | 55.2                       | 57.3                         | 39.8     | 54.1               | 30.6               |
| HealthGPT-M3     | 3.8B     | ✓            | 73.7                         | 55.9                       | 74.6                         | 56.4     | 78.7               | 39.7               |
| HealthGPT-L14    | 14B      | ✓            | 77.7                         | 58.3                       | 76.4                         | 64.5     | 85.9               | 44.4               |
| HealthGPT-XL32   | 32B      | ✓            | 78.1                         | 60.5                       | 83.7                         | 68.2     | 92.4               | 50.9               |"
78,"| Model              | CT to MRI (Brain)  | CT to MRI (Pelvis)  | MRI to CT (Brain)  | MRI to CT (Pelvis) |
| ------------------ | ------------------ | ------------------- | ------------------ | ------------------ |
| SSIM  $ \uparrow $ | PSNR  $ \uparrow $ | MSE  $ \downarrow $ | SSIM  $ \uparrow $ | PSNR  $ \uparrow $ |
| pix2pix            | 71.09              | 32.65               | 36.85              | 59.17              |
| CycleGAN           | 54.76              | 32.23               | 40.56              | 54.54              |
| BBDM               | 71.69              | 32.91               | 34.44              | 57.37              |
| Vmanba             | 69.54              | 32.67               | 36.42              | 63.01              |
| DiffMa             | 71.47              | 32.74               | 35.77              | 62.56              |
| HealthGPT-M3       | 79.38              | 33.03               | 33.48              | 71.81              |
| HealthGPT-L14      | 79.73              | 33.10               | 32.96              | 71.92              |"
79,"| Model    | $ \Delta C\downarrow $ | $ \Delta W\downarrow $ | $ \Delta T\downarrow $ | $ \Delta D\downarrow $ |
| -------- | ---------------------- | ---------------------- | ---------------------- | ---------------------- |
| Gemma2   | 24.1±0.1               | 22.8±1.1               | 9.5±0.3                | 38.6±0.2               |
| Llama3   | 4.5±0.1                | 2.2±0.2                | 25.5±0.1               | 44.7±0.2               |
| Qwen2    | 16.1±0.1               | 17.5±0.1               | 16.2±0.1               | 15.9±0.2               |
| GPT-4o   | 13.2±3.7               | 14.9±1.2               | 22.6±0.9               | 13.0±4.2               |
| Llama3.1 | 1.0±0.1                | 2.5±0.2                | 2.5±0.5                | 30.2±0.2               |"
79,"| Model         | $ CR^{C}\downarrow $ | $ CR^{W}\downarrow $ | $ CR^{T}\downarrow $ | $ CR^{D}\downarrow $ |
| ------------- | -------------------- | -------------------- | -------------------- | -------------------- |
| Qwen2-7B      | 98.7±0.1             | 95.3±0.1             | 79.4±0.1             | 27.5±0.1             |
| Qwen2-72B     | 56.1±0.3             | 28.9±0.4             | 30.5±0.6             | 30.0±0.2             |
| Llama3-8B     | 70.9±0.1             | 61.1±0.4             | 54.6±0.3             | 70.7±0.1             |
| Llama3-70B    | 35.9±0.8             | 14.7±0.7             | 44.4±0.1             | 69.9±0.1             |
| Llama3.1-8B   | 32.3±0.1             | 35.3±0.3             | 31.5±0.5             | 91.2±0.1             |
| Llama3.1-70B  | 12.0±0.5             | 9.2±0.1              | 15.6±0.1             | 73.5±0.2             |
| Llama3.1-405B | 29.0±0.5             | 10.0±0.1             | 15.3±0.4             | 43.2±0.1             |"
79,"| Model      | # A&amp;C | # A&amp;S | # D&amp;C | # D&amp;S |
| ---------- | --------- | --------- | --------- | --------- |
| Llama3-70B | 129       | 31        | 22        | 72        |
| Qwen2-72B  | 1         | 6         | 31        | 222       |"
81,"| Method            | Obj.       | $ |V|=200 $  Gap (%) | Time (s) | Obj.       | $ |V|=500 $  Gap (%) | Time (s) | Obj.       | $ |V|=1000 $  Gap (%) | Time (s) |
| ----------------- | ---------- | -------------------- | -------- | ---------- | -------------------- | -------- | ---------- | --------------------- | -------- |
| LKH-3(100)        | 28.833135  | -                    | 1.65     | 66.902511  | -                    | 5.86     | 131.795858 | -                     | 19.30    |
| LKH-3(1000)       | 28.278438  | -1.92                | 10.72    | 64.387969  | -3.76                | 23.55    | 124.575469 | -5.48                 | 66.57    |
| LKH-3(10000)      | 28.041563  | -2.75                | 59.81    | 63.320078  | -5.35                | 233.72   | 120.531406 | -8.55                 | 433.90   |
| POMO( $ ^{*} $ 8) | 29.178707  | 1.20                 | 0.33     | 79.785673  | 19.26                | 0.88     | 192.78563  | 46.28                 | 3.20     |
| POMO              | 29.424647  | 2.05                 | 0.26     | 83.079016  | 24.19                | 0.62     | 233.093524 | 76.86                 | 1.62     |
| NeuOpt            | 38.478607  | 33.45                | 17.34    | 187.812195 | 180.73               | 39.41    | -          | -                     | -        |
| GANCO             | 29.978834  | 3.97                 | 0.50     | 71.258026  | 6.51                 | 1.31     | 145.40277  | 10.32                 | 4.88     |
| AGFN-100          | 31.260145  | 8.41                 | 0.17     | 71.051109  | 6.20                 | 0.45     | 133.96624  | 1.65                  | 0.72     |
| ACO               | 71.5753186 | 143.24               | 3.50     | 187.616745 | 179.88               | 11.36    | 383.960999 | 191.11                | 25.08    |
| GFACS             | 45.357657  | 57.31                | 4.82     | 76.771554  | 14.75                | 13.27    | 158.971658 | 20.26                 | 28.52    |
| AGFN-200          | 30.35164   | 5.27                 | 0.17     | 69.599289  | 4.03                 | 0.46     | 132.477417 | 0.52                  | 0.72     |
| AGFN-500          | 31.826736  | 10.38                | 0.17     | 69.375366  | 3.70                 | 0.46     | 129.017487 | -2.11                 | 0.72     |
| AGFN-1000         | 32.235001  | 11.80                | 0.17     | 69.873100  | 4.44                 | 0.46     | 129.624237 | -1.65                 | 0.73     |"
82,"| Benchmark    | Accuracy | GPT-4o | LLaMa3.1 | DeepSeekCoder |
| ------------ | -------- | ------ | -------- | ------------- |
| Raw          | Prompt   | Raw    | SAFE     | SAFE+         |
| VerusBench   | @1       | 11.51  | 25.90    | 3.60          |
| @2           | 14.39    | 30.93  | 5.76     | 48.20         |
| @10          | 24.46    | 41.01  | 11.51    | 53.96         |
| @100         | 43.88    | 46.76  | 28.78    | 55.40         |
| CodeNet-Test | @1       | 0.28   | 2.86     | 0.00          |
| @2           | 0.70     | 3.41   | 0.03     | 45.34         |"
82,"| Self-Evolving | Metric     | Proof Generation | Self-Debugging ( $ K + K * K $ ) |
| ------------- | ---------- | ---------------- | -------------------------------- |
| SV            | MBPP       | Tutorial         | Total                            |
| GPT-4o        | Accuracy@1 | 39.47            | 19.23                            |
| Accuracy@10   | 60.53      | 33.33            | 34.78                            |
| Round 1       | Accuracy@1 | 55.26            | 29.49                            |
| Accuracy@10   | 81.58      | 30.77            | 13.04                            |
| Round 2       | Accuracy@1 | 73.68            | 29.49                            |
| Accuracy@10   | 89.47      | 30.77            | 47.83                            |
| Round 3       | Accuracy@1 | 78.95            | 30.77                            |
| Accuracy@10   | 92.11      | 35.90            | 52.17                            |"
82,"| Model (Spec-Quality) | Metric     | Proof Generation | Self-Debugging ( $ K + K * K $ ) |
| -------------------- | ---------- | ---------------- | -------------------------------- |
| SV                   | MBPP       | Tutorial         | Total                            |
| SAFE                 | Accuracy@1 | 78.95            | 30.77                            |
| Accuracy@10          | 92.11      | 35.90            | 52.17                            |
| Low-Quality          | Accuracy@1 | 0.00             | 29.49                            |
| Accuracy@10          | 5.26       | 32.05            | 21.74                            |
| Mix-Quality          | Accuracy@1 | 36.84            | 30.77                            |
| Accuracy@10          | 65.79      | 32.05            | 30.43                            |"
84,"| Title Generation            | 2000 | Instructional Rewriting      | 2826 | Chitchat               | 5500 |
| --------------------------- | ---- | ---------------------------- | ---- | ---------------------- | ---- |
| Language Polish             | 2000 | Value Judgement              | 4318 | Planning               | 5500 |
| Recommendation              | 5500 | Roleplay                     | 5111 | Fact Verification      | 3177 |
| General Explanation         | 5500 | Category Identification      | 4157 | Creative Writing       | 5500 |
| Question Generation         | 3852 | Text Summarization           | 4033 | Reading Comprehension  | 2000 |
| General Analysis            | 5500 | Functional Writing           | 5500 | Code Writing           | 3674 |
| Information Extraction      | 3249 | Reasoning                    | 4453 | Topic Modeling         | 2000 |
| Seeking Advice              | 5500 | Solving General Exam Problem | 2689 | Open Question          | 5500 |
| Solving Math Exam Problem   | 2000 | Text-To-Text Translation     | 5500 | Ranking                | 2000 |
| Data Analysis               | 3863 | Writing Social Media Post    | 2000 | Brainstorming          | 5500 |
| Keywords Extraction         | 2000 | Literary Appreciation        | 2000 | Seeking Medical Advice | 2000 |
| Safe Experimental Practices | 2000 | Writing Legal Document       | 2000 | Other                  | 5500 |"
84,"| Label Distribution (Label, # of Samples)           |
| -------------------------------------------------- |
| Model A Win                                        |
| Source Dataset Distribution (Source, # of Samples) |
| Synthetic GPT-J                                    |
| Lmsys Chat                                         |
| PKU-SafeRLHF                                       |"
84,"| Evaluator         | Vicuna Bench | FLASK Eval  | Feedback Bench |
| ----------------- | ------------ | ----------- | -------------- |
| Pearson           | Spearman     | Kendall-Tau | Pearson        |
| GPT-4o-mini       | 0.456        | 0.323       | 0.286          |
| Claude-3.5-Sonnet | 0.489        | 0.326       | 0.286          |
| GPT-4o            | 0.434        | 0.306       | 0.263          |
| Llama-2-7B-Chat   | 0.021        | 0.042       | 0.037          |
| Llama-2-13B-Chat  | 0.072        | 0.014       | 0.013          |
| Llama-3-8B-Inst   | -0.007       | -0.008      | -0.007         |
| Llama-3.1-8B-Inst | 0.298        | 0.188       | 0.167          |
| Mistral-7B-Inst   | 0.190        | 0.117       | 0.104          |
| AutoJ-13B         | 0.360        | 0.364       | 0.317          |
| Prometheus-7B     | 0.413        | 0.416       | 0.354          |
| Prometheus-13B    | 0.268        | 0.272       | 0.237          |
| Prometheus2-7B    | 0.267        | 0.254       | 0.219          |
| ArmoRM-8B         | 0.446        | 0.396       | 0.300          |
| SaMer-8B          | 0.476        | 0.458       | 0.354          |"
84,"| Evaluator         | HHH Alignment | AutoJ Eval | Preference Bench | AlpacaEval |
| ----------------- | ------------- | ---------- | ---------------- | ---------- |
| Help.             | Harm.         | Hon.       | Other            | Total Avg. |
| GPT-4o-mini       | 89.83         | 91.38      | 75.41            | 93.02      |
| Claude-3.5-Sonnet | 91.53         | 94.83      | 88.52            | 93.02      |
| GPT-4o            | 89.83         | 94.83      | 88.52            | 93.02      |
| Llama-2-7B-Chat   | 59.32         | 71.43      | 45.90            | 53.66      |
| Llama-2-13B-Chat  | 71.43         | 76.47      | 61.22            | 71.43      |
| Llama-3-8B-Inst   | 79.66         | 80.70      | 73.77            | 88.37      |
| Llama-3.1-8B-Inst | 83.05         | 84.62      | 78.69            | 88.10      |
| Mistral-7B-Inst   | 71.19         | 81.03      | 67.21            | 74.42      |
| AutoJ-13B         | 70.49         | 82.76      | 77.97            | 72.09      |
| Prometheus-7B     | 50.85         | 43.10      | 54.10            | 48.84      |
| Prometheus-13B    | 66.10         | 48.28      | 34.43            | 65.12      |
| Prometheus2-7B    | 83.05         | 75.86      | 63.93            | 76.74      |
| ArmoRM-8B         | 89.83         | 93.10      | 78.69            | 93.02      |
| SaMer-8B          | 88.14         | 89.66      | 78.69            | 95.35      |"
84,"| Evaluator         | GPTInst | GPTOut | Manual | Neighbor | Natural |
| ----------------- | ------- | ------ | ------ | -------- | ------- |
| GPT-4o-mini       | 83.70   | 65.96  | 63.04  | 67.16    | 91.00   |
| Claude-3.5-Sonnet | 88.04   | 61.70  | 78.26  | 85.07    | 92.00   |
| GPT-4o            | 88.04   | 76.60  | 78.26  | 77.61    | 99.00   |
| Llama-2-7B-Chat   | 48.35   | 46.81  | 41.30  | 43.61    | 58.00   |
| Llama-2-13B-Chat  | 33.77   | 47.83  | 31.82  | 29.13    | 70.10   |
| Llama-3-8B-Inst   | 39.13   | 55.32  | 41.30  | 21.64    | 78.00   |
| Llama-3.1-8B-Inst | 43.48   | 55.32  | 43.48  | 33.08    | 83.00   |
| Mistral-7B-Inst   | 51.09   | 46.81  | 45.65  | 45.52    | 76.00   |
| AutoJ-13B         | 23.91   | 50.00  | 26.67  | 23.48    | 71.13   |
| Prometheus-7B     | 15.22   | 36.17  | 34.78  | 17.16    | 48.00   |
| Prometheus-13B    | 14.13   | 46.81  | 28.26  | 15.67    | 59.00   |
| Prometheus2-7B    | 29.35   | 58.70  | 37.78  | 22.39    | 77.00   |
| ArmoRM-8B         | 77.17   | 63.83  | 69.57  | 67.16    | 93.00   |
| SaMer-8B          | 54.35   | 65.96  | 69.57  | 86.57    | 84.00   |"
84,"| Evaluator         | Dim Acc. | Overall Acc. |
| ----------------- | -------- | ------------ |
| GPT-4o-mini       | 72.99    | 78.00        |
| Claude-3.5-Sonnet | 61.63    | 74.15        |
| GPT-4o            | -        | -            |
| Llama-2-7B-Chat   | 53.13    | 53.58        |
| Llama-2-13B-Chat  | 48.47    | 53.47        |
| Llama-3-8B-Inst   | 64.96    | 66.67        |
| Llama-3.1-8B-Inst | 73.13    | 71.91        |
| Mistral-7B-Inst   | 55.70    | 62.80        |
| AutoJ-13B         | 53.58    | 61.12        |
| Prometheus-7B     | 60.22    | 38.33        |
| Prometheus-13B    | 64.96    | 43.67        |
| Prometheus-2-7B   | 67.11    | 71.24        |
| ArmoRM-8B         | -        | 79.33        |
| SaMer-8B          | 75.67    | 82.33        |"
84,"| Evaluator        | ID (%) | OOD (%)  |
| ---------------- | ------ | -------- |
| Precision        | Recall | Win Rate |
| GPT-4o           | 63.42  | 38.10    |
| GPT-4o-mini      | 57.61  | 37.89    |
| Mistral-7B-Inst  | 42.73  | 23.82    |
| Llama2-7B-Chat   | 27.78  | 37.36    |
| Llama3-8B-Inst   | 39.81  | 36.37    |
| Llama3.1-8B-Inst | 34.96  | 60.55    |
| SaMer-8B         | 74.84  | 72.33    |"
87,"| Task                       | Source               | Reference                             | Target               | Related Areas    |
| -------------------------- | -------------------- | ------------------------------------- | -------------------- | ---------------- |
| Zero-Shot Timbre Imitation | Content Style Timbre | Content Style Timbre                  | Content Style Timbre | Voice Conversion |
| Zero-Shot Style Imitation  | Content Style Timbre | Accent Conversion, Emotion Conversion |                      |                  |
| Zero-Shot Voice Imitation  | Content Style Timbre | Voice Conversion                      |                      |                  |
| Text Content               | Text to Speech       |                                       |                      |                  |"
87,"|                                         | Representations      | #Vocab               | WER ( $ \downarrow $ ) | S-SIM (to ref) ( $ \uparrow $ ) | S-SIM (to src) ( $ \downarrow $ ) | FPC (to src) ( $ \uparrow $ )           | Analysis                                 |
| --------------------------------------- | -------------------- | -------------------- | ---------------------- | ------------------------------- | --------------------------------- | --------------------------------------- | ---------------------------------------- |
| Ground Truth                            | -                    | 5.526                | 0.762                  | 0.087                           | 1.000                             | -                                       |                                          |
| Starting point of information filtering | 24th layer features  | -                    | 5.706                  | 0.266                           | 0.400                             | 0.768                                   | Pros: Intelligibility, Style consistency |
| 18th layer features                     | -                    | 5.324                | 0.250                  | 0.505  $ \uparrow $             | 0.824                             | Cons: Timbre imitation                  |                                          |
| 12th layer features                     | -                    | 5.348                | 0.200                  | 0.626  $ \uparrow $             | 0.805                             |                                         |                                          |
| PPG features                            | -                    | 6.143                | 0.449                  | 0.157                           | 0.741                             | Pros: Intelligibility, Timbre imitation |                                          |
| ASR tokens                              | 29                   | 7.836                | 0.463                  | 0.125                           | 0.698                             | Cons: Style consistency                 |                                          |
| K-means tokens                          | 1024                 | 11.493               | 0.398                  | 0.150                           | 0.734                             | Worse than VQ-VAE tokens (1024)         |                                          |
| Content-style Tokens                    | 16384                | 6.807                | 0.398                  | 0.306                           | 0.826                             | As the vocabulary size decreases,       |                                          |
| 4096                                    | 6.908  $ \uparrow $  | 0.403                | 0.236  $ \downarrow $  | 0.797  $ \downarrow $           | Pros:                             |                                         |                                          |
| 1024                                    | 6.967  $ \uparrow $  | 0.418                | 0.249                  | 0.764  $ \downarrow $           | Timbre imitation  $ \uparrow $    |                                         |                                          |
| 32                                      | 9.731  $ \uparrow $  | 0.426                | 0.161  $ \downarrow $  | 0.706  $ \downarrow $           | Cons:                             |                                         |                                          |
| Content Tokens                          | 16                   | 13.169  $ \uparrow $ | 0.441                  | 0.146  $ \downarrow $           | 0.672  $ \downarrow $             | Intelligibility  $ \downarrow $         |                                          |
| 8                                       | 21.813  $ \uparrow $ | 0.392                | 0.109  $ \downarrow $  | 0.675                           | Style consistency  $ \downarrow $ |                                         |                                          |"
87,"| Model        | AR? | Training Data (ContRep / Model) | Natural ness | Prosody Similarity | Speaker Similarity | Accent Similarity | Emotion Similarity |
| ------------ | --- | ------------------------------- | ------------ | ------------------ | ------------------ | ----------------- | ------------------ |
| HierSpeech++ | ✗   | 500K / 2.8K                     | 3.04         | 3.08               | 3.15               | 3.13              | 2.55               |
| LM-VC        | ✓   | 1K / 60K                        | 2.40         | 2.16               | 2.56               | 3.02              | 2.46               |
| UniAudio     | ✓   | 1K / 100K                       | 2.95         | 2.51               | 2.39               | 2.42              | 2.41               |
| FACodec      | ✗   | 60K / 60K                       | 2.36         | 3.10               | 3.19               | 3.01              | 2.30               |
| Vevo-Timbre  | ✗   | 60K / 60K                       | 3.43         | 3.45               | 3.46               | 3.55              | 2.66               |
| Vevo-Voice   | ✓   | 60K / 60K                       | 3.24         | 2.60               | 3.70               | 3.90              | 3.20               |"
87,"| Model           | Zero-Shot    | Supervision | CMOS        |
| --------------- | ------------ | ----------- | ----------- |
| Parallel Corpus | Style Labels | Text        | Naturalness |
| VoiceShop       | ✗            | ✓           | ✓           |
| Vevo-Style      | ✓            | ✗           | ✗           |
| Emovox          | ✗            | ✗           | ✓           |
| Vevo-Style      | ✓            | ✗           | ✗           |"
87,"| Model      | AR? | Training Data | Natural ness | Speaker Similarity | Accent Similarity | Emotion Similarity |
| ---------- | --- | ------------- | ------------ | ------------------ | ----------------- | ------------------ |
| CosyVoice  | ✓   | 171K          | -0.18        | 4.11               | 3.99              | 3.66               |
| MaskGCT    | ✗   | 100K          | -0.04        | 4.16               | 4.38              | 3.76               |
| VALL-E     | ✓   | 45K           | -1.24        | 2.82               | 2.77              | 2.63               |
| Voicebox   | ✗   | 60K           | -0.35        | 3.87               | 3.49              | 3.61               |
| VoiceCraft | ✓   | 9K            | -0.50        | 3.47               | 3.29              | 3.52               |
| Vevo-TTS   | ✓   | 60K           | -0.14        | 4.05               | 4.12              | 4.03               |"
88,"| Model Family     | # F   | TPF  | Avg. VDCscore | Detailed | Camera | Short | Background | Object |
| ---------------- | ----- | ---- | ------------- | -------- | ------ | ----- | ---------- | ------ |
| Acc.             | Score | Acc. | Score         | Acc.     | Score  | Acc.  | Score      | Acc.   |
| Aria-3.5B×8      | 64    | 128  | 42.5          | 2.2      | 47.3   | 2.4   | 42.4       | 2.2    |
| Gemini-1.5 Pro   | unk.  | unk. | 41.7          | 2.1      | 43.1   | 2.2   | 38.7       | 2.0    |
| LLaVA-Video-7B   | 64    | 676  | 39.0          | 2.0      | 35.0   | 1.8   | 46.1       | 2.3    |
| AuroraCap-7B     | 10    | 224  | 38.2          | 2.0      | 41.3   | 2.1   | 43.5       | 2.3    |
| InternVL-2-8B    | 10    | 256  | 37.7          | 2.0      | 34.9   | 1.8   | 39.1       | 2.1    |
| LLaVA-OV-7B      | 10    | 729  | 37.5          | 1.9      | 41.2   | 2.1   | 37.8       | 2.0    |
| LLaVA-1.6-7B     | 10    | 576  | 36.5          | 1.9      | 36.5   | 1.9   | 36.5       | 1.9    |
| ShareGPT4Vide... | 10    | 576  | 36.2          | 1.9      | 35.6   | 1.8   | 33.3       | 1.8    |
| LLaVA-1.6-13B    | 10    | 576  | 35.9          | 1.9      | 36.2   | 1.9   | 35.6       | 1.9    |
| LLaVA-NeXT-V7B   | 8     | 576  | 35.5          | 1.9      | 33.8   | 1.8   | 39.7       | 2.1    |
| LLaVA-1.5-13B    | 10    | 576  | 34.8          | 1.8      | 33.0   | 1.7   | 39.0       | 2.1    |
| LongVA-7B        | 10    | 144  | 34.5          | 1.8      | 27.9   | 1.5   | 35.3       | 1.9    |
| LLaVA-1.5-7B     | 10    | 576  | 34.0          | 1.8      | 33.4   | 1.7   | 38.4       | 2.0    |"
88,"| Video Source                   | # Sample | Proportion | Duration (sec.) | Ave. Length (sec.) | Ave. # Keyframe |
| ------------------------------ | -------- | ---------- | --------------- | ------------------ | --------------- |
| Panda-70M (Chen et al., 2024e) | 229      | 22.25%     | 5,714           | 24.95              | 7.18            |
| Ego4D (Grauman et al., 2022)   | 196      | 19.05%     | 10,935          | 55.79              | 19.46           |
| Mixkit (Mixkit, 2024)          | 197      | 19.14%     | 3,261           | 16.55              | 6.58            |
| Pixabay (Pixabay, 2024)        | 199      | 19.34%     | 4,748           | 23.86              | 8.99            |
| Pexels (Pexels, 2024)          | 208      | 20.21%     | 4,343           | 20.88              | 7.99            |
| Total                          | 1,027    | -          | 29,001          | 28.18              | 10.43           |"
89,"| Algorithm     | Setting       | Lower Level     | Upper Level     | Het. Data |
| ------------- | ------------- | --------------- | --------------- | --------- |
| FOPM          | Centralized   | Convex          | Nonconvex       | NA        |
| CG-BiO        | Centralized   | Convex          | Nonconvex       | NA        |
| F $ ^{2} $ BA | Centralized   | Nonconvex, PL   | Nonconvex       | NA        |
| SBCGF         | Centralized   | Convex          | Nonconvex       | NA        |
| LV-HBA        | Centralized   | Convex          | Nonconvex       | NA        |
| sl-BAMM       | Centralized   | Convex          | Strongly Convex | NA        |
| INTERACT      | Decentralized | Strongly Convex | Nonconvex       | i.i.d     |
| DIAMOND       | Decentralized | Strongly Convex | Nonconvex       | i.i.d     |
| Prometheus    | Decentralized | Strongly Convex | Nonconvex       | i.i.d     |
| SLDBO         | Decentralized | Strongly Convex | Nonconvex       | non-i.i.d |
| LoPA          | Decentralized | Strongly Convex | Nonconvex       | non-i.i.d |
| DUET (Ours)   | Decentralized | Convex          | Nonconvex       | non-i.i.d |"
140,"|                             | CIFAR10 | ImageNet-1K |
| --------------------------- | ------- | ----------- |
| Pruning Rate                | 30%     | 50%         |
| ELFS (DINO)                 | 95.5    | 95.2        |
| Random                      | 94.3    | 93.4        |
| FreeSel (DINO)              | 94.5    | 93.8        |
| D2 (SwAV)                   | 94.3    | 93.8        |
| ELFS (Self-Trained Encoder) | 94.8    | 94.6        |"
89,"| Bilevel                                                                                               | Equivalent                                   | Single Level w/ constraint |
| ----------------------------------------------------------------------------------------------------- | -------------------------------------------- | -------------------------- |
| $ \mathcal{S}(\mathbf{x}_i) := \arg \min_{y_i \in \mathbb{R}^{p_2}} g_i(\mathbf{x}_i, \mathbf{y}_i) $ | $ \nabla_y g_i(\mathbf{x}_i, \mathbf{y}_i) $ |                            |"
91,"| Method           | Sketch-level       | Extrusion-level    |                    |                     |
| ---------------- | ------------------ | ------------------ | ------------------ | ------------------- |
| COV $ \uparrow $ | MMD $ \downarrow $ | JSD $ \downarrow $ | Novel $ \uparrow $ | Unique $ \uparrow $ |
| GPT-4o           | 58.2%              | 1.34               | 1.43               | 69.7%               |
| SkexGen          | 60.6%              | 1.27               | 1.51               | 90.7%*              |
| Hnc-cad          | 62.4%*             | 1.21*              | 1.07*              | 87.6%               |
| Ours             | 65.6%              | 1.19               | 0.82               | 92.1%               |"
92,"| Method                       | Reference Model |
| ---------------------------- | --------------- |
| ViT-B/32                     | ViT-B/16        |
| Reference                    | 63.32           |
| OpenCLIP                     | 66.94           |
| FastCLIP                     | 67.37           |
| JEST                         | 56.40           |
| JEST (Top-k)                 | 57.75           |
| DRRho-CLIP                   | 68.84           |
| MobileCLIP (w/ Distillation) | 66.94           |
| FastCLIP (w/ Distillation)   | 67.33           |
| DRRho-CLIP (w/ Distillation) | 68.84           |"
92,"| Ref. Model          | Loss Variance ( $ \times 10^{-3} $ ) | DC          |
| ------------------- | ------------------------------------ | ----------- |
| Image               | Text                                 |             |
| No Ref.             | 7.26 (0.58)                          | 7.02 (0.91) |
| ViT-B/32 (WIT-400M) | 4.49 (0.54)                          | 4.09 (0.60) |"
93,"| Datasets | Metric | ID-based | Feature + ID | Generative | Improv. |
| -------- | ------ | -------- | ------------ | ---------- | ------- |
| BERT4Rec | SASRec | FDSA     | S^{3}-Rec    | VQ-Rec     | P5-CID  |
| Sports   | R@5    | 0.0115   | 0.0233       | 0.0182     | 0.0251  |
| N@5      | 0.0075 | 0.0154   | 0.0122       | 0.0161     | 0.0132  |
| R@10     | 0.0191 | 0.0350   | 0.0288       | 0.0385     | 0.0251  |
| N@10     | 0.0099 | 0.0192   | 0.0156       | 0.0204     | 0.0154  |
| Beauty   | R@5    | 0.0203   | 0.0387       | 0.0267     | 0.0387  |
| N@5      | 0.0124 | 0.0249   | 0.0163       | 0.0244     | 0.0311  |
| R@10     | 0.0347 | 0.0605   | 0.0407       | 0.0647     | 0.0741  |
| N@10     | 0.0170 | 0.0318   | 0.0208       | 0.0327     | 0.0372  |
| CDs      | R@5    | 0.0326   | 0.0351       | 0.0226     | 0.0213  |
| N@5      | 0.0201 | 0.0177   | 0.0137       | 0.0130     | 0.0209  |
| R@10     | 0.0547 | 0.0619   | 0.0378       | 0.0375     | 0.0485  |
| N@10     | 0.0271 | 0.0263   | 0.0186       | 0.0182     | 0.0264  |"
95,"| Dataset                        | Type  | Trajectory | Highest Resolution | Frames | Ave count | Total count |
| ------------------------------ | ----- | ---------- | ------------------ | ------ | --------- | ----------- |
| Penguin (Arteta et al., 2016)  | Image | ✗          | 1536 × 2048        | 33,405 | 178.4     | 5,970,899   |
| Bird-Count (Wang et al., 2023) | Image | ✗          | 768 × 1024         | 1,372  | 131.1     | 173,458     |
| DroneBird                      | Video | ✓          | 2160 × 4096        | 21,500 | 171.5     | 3,686,409   |"
95,"| Method                        | Type                | Mall               | FDST                | VSCrowd            | DroneBird           |
| ----------------------------- | ------------------- | ------------------ | ------------------- | ------------------ | ------------------- |
| MAE $ \downarrow $            | RMSE $ \downarrow $ | MAE $ \downarrow $ | RMSE $ \downarrow $ | MAE $ \downarrow $ | RMSE $ \downarrow $ |
| MCNN (Zhang et al., 2016)     | Image               | -                  | -                   | 3.77               | 4.88                |
| CSRNet (Li et al., 2018)      | Image               | 2.46               | 4.70                | 2.56               | 3.12                |
| CAN (Liu et al., 2019)        | Image               | -                  | -                   | -                  | -                   |
| MAN (Lin et al., 2022)        | Image               | -                  | -                   | 2.79               | 4.21                |
| HMoDE (Du et al., 2023)       | Image               | 2.82               | 3.41                | 2.49               | 3.51                |
| PET (Liu et al., 2023)        | Image               | 1.89               | 2.46                | 1.73               | 2.27                |
| Gramformer (Lin et al., 2024) | Image               | 1.69               | 2.14                | 5.15               | 6.32                |
| EPF (Liu et al., 2020)        | Video               | -                  | -                   | 2.17               | 2.62                |
| PFTF (Avvenuti et al., 2022)  | Video               | 2.99               | 3.72                | 2.07               | 2.69                |
| GNANet (Li et al., 2022)      | Video               | -                  | -                   | 2.10               | 2.90                |
| FRVCC (Hou et al., 2023)      | Video               | 1.41               | 1.79                | 1.88               | 2.45                |
| STGN (Wu et al., 2023)        | Video               | 1.53               | 1.97                | 1.38               | 1.82                |
| Ours                          | Video               | 1.35               | 1.76                | 1.29               | 1.69                |"
95,"| Exp. | DEMO | SAM | TCF | MAE $ \downarrow $ | RMSE $ \downarrow $ |
| ---- | ---- | --- | --- | ------------------ | ------------------- |
| I    |      |     |     | 2.45               | 3.22                |
| II   |      |     | ✓   | 2.32               | 2.69                |
| III  |      | ✓   | ✓   | 1.57               | 1.99                |
| IV   | ✓    |     | ✓   | 1.69               | 2.20                |
| V    | ✓    | ✓   | ✓   | 1.29               | 1.69                |"
95,"| Table 9: Effect of TCF. | Table 10: Effect of E-MAC architecture. |
| ----------------------- | --------------------------------------- |
| Exp.                    | Model                                   |
| 3                       | ViT                                     |
| 4                       | ViT w/ TCF                              |
| 5                       | E-MAC                                   |
| 6                       | E-MAC TCF                               |"
97,"| Rank | Method      | Mean Rank | Ranks: Domain  $ \rightarrow $  Interaction | Reference |
| ---- | ----------- | --------- | ------------------------------------------- | --------- |
| 1    | RGCN        | 2.5       | [1, 5, 1, 1, 1, 1, 2, 2, 2, 2, 8, 3]        | paper     |
| 2    | CNN         | 3.5       | [7, 6, 2, 2, 2, 2, 3, 5, 3, 2, 4]           | paper     |
| 3    | CompGCN     | 3.9       | [5, 1, 3, 3, 4, 3, 1, 1, 7, 10, 5]          | paper     |
| 4    | GIN         | 5.1       | [2, 3, 4, 4, 10, 5, 6, 6, 6, 4, 6]          | paper     |
| 5    | MPNN        | 5.6       | [6, 7, 5, 5, 3, 4, 4, 4, 10, 3, 10]         | paper     |
| 6    | ResNet      | 6.0       | [8, 8, 7, 6, 5, 8, 8, 9, 4, 1, 2]           | paper     |
| 7    | LSTM        | 6.3       | [9, 9, 6, 7, 6, 6, 9, 10, 1, 5, 1]          | paper     |
| 8    | GAT         | 6.6       | [4, 2, 8, 9, 7, 7, 5, 3, 9, 9, 9]           | paper     |
| 9    | GCN         | 7.2       | [3, 4, 10, 8, 8, 9, 7, 7, 8, 7, 8]          | paper     |
| 10   | Transformer | 8.5       | [10, 10, 9, 10, 9, 10, 10, 8, 5, 6, 7]      | paper     |"
108,"| Model                         | Method           | HighwayEnv Dataset | nuScenes Dataset   |
| ----------------------------- | ---------------- | ------------------ | ------------------ |
| ASR $ \uparrow $              | Acc $ \uparrow $ | BDR                | FAR $ \downarrow $ |
| GPT-3.5                       | Original         | -                  | 68.8               |
| Benign fine-tune              | -                | 100.0              | -1.6               |
| BadChain (Xiang et al., 2024) | 12.9             | 96.8               | -                  |
| BALD-word (ours)              | 100.0            | 99.2               | -                  |
| BALD-scene (ours)             | 95.1             | 78.0               | -                  |
| GPT-3.5 + RAG                 | Original         | -                  | 77.4               |
| Benign fine-tune              | -                | 100.0              | 0.0                |
| BALD-RAG (ours)               | 100.0            | 100.0              | -                  |
| LLaMA2                        | Original         | -                  | 41.9               |
| Benign fine-tune              | -                | 100.0              | 0                  |
| BadChain (Xiang et al., 2024) | 48.4             | 79.0               | -                  |
| BALD-word (ours)              | 100.0            | 100.0              | -                  |
| BALD-scene (ours)             | 74.2             | 93.5               | -                  |
| LLaMA2 + RAG                  | Original         | -                  | 55.3               |
| Benign fine-tune              | -                | 96.8               | -1.7               |
| BALD-RAG (ours)               | 96.8             | 98.4               | -                  |
| PaLM2                         | Original         | -                  | 61.3               |
| Benign fine-tune              | -                | 99.2               | -0.8               |
| BadChain (Xiang et al., 2024) | 5.6              | 83.9               | -                  |
| BALD-word (ours)              | 100.0            | 96.8               | -                  |
| BALD-scene (ours)             | 100.0            | 80.6               | -                  |
| PaLM2 + RAG                   | Original         | -                  | 87.1               |
| Benign fine-tune              | -                | 99.2               | -0.8               |
| BALD-RAG (ours)               | 95.2             | 98.4               | -                  |"
140,"|             | Pseudo-label Accuracy |
| ----------- | --------------------- |
|             | CIFAR10               |
| TEMI (SwAV) | 60.7%                 |
| TEMI (DINO) | 92.5%                 |"
98,"| |D L|        | Methods        | Environments   |
| ------------ | -------------- | -------------- |
| Seaquest     | Boxing         | Battlezone     |
| \            | Labeler        | 1071.92±224.88 |
| Rainbow      | 707.04±98.96   | 1.99±0.71      |
| Large        | HGDAgger-L     | 790.10±73.98   |
| PVP-I        | 635.83±65.74   | 7.76±0.84      |
| PVP-IR       | 770.88±91.00   | 13.36±2.00     |
| BC-L         | 758.40±41.31   | 21.39±1.21     |
| w/o a TGT    | 1213.98±102.54 | 48.19±7.90     |
| w/o Finetune | 1234.39±113.44 | 51.83±26.67    |
| DQfD-I       | 1155.43±85.98  | 41.71±5.09     |
| ICoPro       | 1274.76±109.53 | 61.52±2.47     |
| Small        | HGDAgger-L     | 508.34±77.91   |
| PVP-I        | 439.96±68.62   | 1.09±2.05      |
| PVP-IR       | 733.73±116.70  | 14.33±3.98     |
| BC-L         | 537.20±51.86   | 9.09±4.64      |
| w/o a TGT    | 1065.65±164.01 | 40.56±10.22    |
| w/o Finetune | 1081.16±131.98 | 19.35±16.53    |
| DQfD-I       | 1003.21±128.45 | 39.27±20.85    |
| ICoPro       | 1167.19±103.02 | 61.40±23.91    |"
100,"| Model        | Dataset  | Method | ASR-L | ASR-R |
| ------------ | -------- | ------ | ----- | ----- |
| 0            | 128      | 256    | 0     | 128   |
| Llama-3.1-8B | AdvBench | MSJ    | 0.19  | 85.96 |
| PANDAS       | 94.42    | 94.62  | 97.31 | 96.15 |
| HarmBench    | MSJ      | 20.75  | 70.25 | 66.00 |
| PANDAS       | 82.25    | 76.50  | 88.75 | 83.50 |
| Qwen-2.5-7B  | AdvBench | MSJ    | 0.19  | 4.04  |
| PANDAS       | 18.65    | 19.81  | 45.38 | 47.12 |
| HarmBench    | MSJ      | 16.50  | 35.75 | 36.75 |
| PANDAS       | 50.75    | 49.75  | 67.00 | 68.75 |"
101,"| Model        | Method | RestBench-TMDB | RestBench-Spotify | ToolBench |
| ------------ | ------ | -------------- | ----------------- | --------- |
| CP%          | Win%   | CP%            | Win%              | CP%       |
| GPT-4o-mini  | ReAct  | 48.00          | 50.00             | 24.56     |
| DFSDT        | 50.00  | 68.00          | 35.08             | 61.40     |
| EasyTool     | 56.00  | 75.00          | -                 | -         |
| DRAFT (Ours) | 62.00  | 82.00          | 43.85             | 78.94     |
| Llama-3-70B  | ReAct  | 72.00          | 50.00             | 26.31     |
| DFSDT        | 74.00  | 38.00          | 63.15             | 61.40     |
| EasyTool     | 76.00  | 64.00          | -                 | -         |
| DRAFT (Ours) | 86.00  | 64.00          | 66.66             | 64.91     |
| GPT-4o       | ReAct  | 71.00          | 50.00             | 28.07     |
| DFSDT        | 74.00  | 61.00          | 64.91             | 56.14     |
| EasyTool     | 79.00  | 62.00          | -                 | -         |
| DRAFT (Ours) | 88.00  | 71.00          | 70.17             | 84.21     |"
102,"| Model                     | MKG-W | MKG-Y | DB15K | KVC16K |
| ------------------------- | ----- | ----- | ----- | ------ |
| MRR                       | Hit@1 | MRR   | Hit@1 | MRR    |
| Uni-modal KGC Methods     |       |       |       |        |
| TransE                    | 29.19 | 21.06 | 30.73 | 23.45  |
| DistMult                  | 20.99 | 15.93 | 25.04 | 19.33  |
| ComplEx                   | 24.93 | 19.09 | 28.71 | 22.26  |
| RotatE                    | 33.67 | 26.80 | 34.95 | 29.10  |
| PairRE                    | 34.40 | 28.24 | 32.01 | 25.53  |
| TuckER                    | 29.59 | 23.93 | 37.05 | 34.59  |
| Multi-modal KGC Methods   |       |       |       |        |
| IKRL                      | 32.36 | 26.11 | 33.22 | 30.37  |
| TBKGC                     | 31.48 | 25.31 | 33.99 | 30.47  |
| TransAE                   | 30.00 | 21.23 | 28.10 | 25.31  |
| MMKRL                     | 30.10 | 22.16 | 36.81 | 31.66  |
| RSME                      | 29.23 | 23.36 | 34.44 | 31.78  |
| VBKGC                     | 30.61 | 24.91 | 37.04 | 33.76  |
| OTKGE                     | 34.36 | 28.85 | 35.51 | 31.97  |
| MoSE*                     | 33.34 | 27.78 | 36.28 | 33.64  |
| IMF*                      | 34.50 | 28.77 | 35.79 | 32.95  |
| QEB                       | 32.38 | 25.47 | 34.37 | 29.49  |
| VISTA                     | 32.91 | 26.12 | 30.45 | 24.87  |
| AdaMF                     | 34.27 | 27.21 | 38.06 | 33.49  |
| Negative Sampling Methods |       |       |       |        |
| MANS                      | 30.88 | 24.89 | 29.03 | 25.25  |
| MMRNS                     | 35.03 | 28.59 | 35.93 | 30.53  |
| MoMoK                     | 35.89 | 30.38 | 37.91 | 35.09  |
| Improvements              | +2.5% | +4.2% | -     | +3.9%  |"
102,"| Modality   | Outperforming Predictions                                                                               |
| ---------- | ------------------------------------------------------------------------------------------------------- |
| Structure  | RouteJunction, PartOf, Nearest City, Publisher, PrimeMinister, ComputingPlatform, LargestCity           |
| Image      | SisterStation, League, Parent, HubAirport, Company, Owner, Capital                                      |
| Text       | SisterStation, Publisher, Head, FederalState, Parent, ComputingPlatform, CountrySeat                    |
| Joint      | GoverningBody, PartOf, Creator, Company, ComputingPlatform, RegionServed                                |
| Full Model | SisterStation, RouteJunction, GoverningBody, Publisher, PartOf, FederalState, ComputingPlatform, Parent |"
102,"| Setting                          | MKG-W                                   | DB15K |
| -------------------------------- | --------------------------------------- | ----- |
| MRR                              | Hit@1                                   | MRR   |
| Full Model                       | 35.89                                   | 30.38 |
| Modality Contribution            | (1.1). Structure Modality               | 32.82 |
| (1.2). Image Modality            | 32.75                                   | 27.78 |
| (1.3). Text Modality             | 32.62                                   | 27.66 |
| (1.4). Joint Modality            | 34.76                                   | 29.33 |
| Model Design                     | (2.1). w/o relational  $ \epsilon_{r} $ | 35.50 |
| (2.2). w/o noise  $ \delta_{m} $ | 35.31                                   | 29.69 |
| (2.3). w/o adaptive fusion       | 35.34                                   | 30.04 |
| (2.4). w/o joint training        | 32.73                                   | 27.09 |
| (2.5). w/o ExID                  | 34.99                                   | 29.49 |"
103,"| Isomorphism                                          | Mechanism Isomorphism                                                                                                 | Distributional Isomorphism                                        | Mechanistic Requirement | Structural Requirement |
| ---------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- | ----------------------- | ---------------------- |
| Counterfactual Equivalence (Peters et al., 2017)     | $ f^{(1)}(\mathbf{v}_{\mathrm{pa}^{(1)}}, u^{(1)}) = f^{(2)}(\mathbf{v}_{\mathrm{pa}^{(2)}}, u^{(1)}) $               | $ P_{\mathbf{U}}^{(2)} = P_{\mathbf{U}}^{(1)} $                   | -                       | order                  |
| BGM Equivalence (Nasr-Esfahany et al. 2023)          | $ f^{(1)}(\mathbf{v}_{\mathrm{pa}^{(1)}}, u^{(1)}) = f^{(2)}(\mathbf{v}_{\mathrm{pa}^{(2)}}, g(u^{(1)})) $            | -                                                                 | bijection               | graph                  |
| LCM Isomorphism (Brehmer et al. 2022)                | $ f^{(1)}(\mathbf{v}_{\mathrm{pa}^{(1)}}, u^{(1)}) = f^{(2)}(\mathbf{v}_{\mathrm{pa}^{(2)}}, \varphi(u^{(1)})) $      | $ P_{\mathbf{U}}^{(2)} = \varphi_{\sharp} P_{\mathbf{U}}^{(1)} $  | diffeomorphism          | graph isomorphism      |
| Domain Counterfactual Equivalence (Zhou et al. 2024) | $ f^{(1)}(\mathbf{v}_{\mathrm{pa}^{(1)}}, h_{1}(u^{(1)})) = f^{(2)}(\mathbf{v}_{\mathrm{pa}^{(2)}}, h_{2}(u^{(2)})) $ | $ P_{\mathbf{U}}^{(k)} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) $ | bijection               | domain label           |
| Exogenous Isomorphism (Ours)                         | $ f^{(1)}(\mathbf{v}_{\mathrm{pa}^{(1)}}, u^{(1)}) = f^{(2)}(\mathbf{v}_{\mathrm{pa}^{(2)}}, h(u^{(1)})) $            | $ P_{\mathbf{U}}^{(2)} = h_{\sharp} P_{\mathbf{U}}^{(1)} $        | -                       | order                  |"
103,"| Method | ER-DIAG-50  | ER-TRIL-50  |
| ------ | ----------- | ----------- |
| OBSWD  | CTFRMSE     | CTFWD       |
| DNME   | -           | 3.47±0.53   |
| w/o O  | 5.06±1.75   | 0.78±0.05   |
| w/o M  | 4.35±0.77   | 0.62±0.04   |
| TNME   | -           | 4.65±2.93   |
| w/o O  | 5.40±2.55   | 11.24±20.98 |
| w/o M  | 3.88±0.65   | 0.62±0.04   |
| CMSM   | -           | 15.70±23.20 |
| w/o O  | 41.74±45.13 | 2.64±3.72   |
| w/o M  | 8.00±9.88   | 1.69±2.60   |
| w/o T  | 3.74±1.09   | 0.64±0.05   |
| TVSM   | -           | 3.66±1.05   |
| w/o O  | 5.25±2.31   | 0.79±0.04   |
| w/o M  | 3.54±0.53   | 0.53±0.05   |
| w/o T  | 22.22±36.74 | 0.67±0.05   |"
108,"| Methods          | SR $ \uparrow $   | PSR $ \uparrow $  | ASR $ \uparrow $ |
| ---------------- | ----------------- | ----------------- | ---------------- |
| Original         | 0.37 $ \pm $ 0.06 | 0.66 $ \pm $ 0.06 | -                |
| Benign fine-tune | 0.40 $ \pm $ 0.17 | 0.70 $ \pm $ 0.05 | -                |
| BadChain         | 0.17 $ \pm $ 0.06 | 0.49 $ \pm $ 0.04 | 0.20             |
| BALD-word        | 0.47 $ \pm $ 0.06 | 0.76 $ \pm $ 0.01 | 1.00             |
| BALD-scene       | 0.67 $ \pm $ 0.08 | 0.85 $ \pm $ 0.04 | 0.85             |
| BALD-RAG         | 0.40 $ \pm $ 0.00 | 0.69 $ \pm $ 0.02 | 1.00             |"
109,"| Metrics | Method     | ARC-C      | ARC-E      | OBQA       | CSQA       | SciQ       | RACE       |
| ------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |
| Acc ↑   | MAP        | 79.74±0.27 | 92.27±0.17 | 88.60±0.87 | 81.51±0.60 | 93.37±0.12 | 88.15±0.31 |
| MCD     | 79.55±0.21 | 92.25±0.13 | 88.63±0.84 | 81.52±0.60 | 93.30±0.20 | 88.12±0.25 |            |
| Ens     | 79.50±0.10 | 92.26±0.41 | 88.53±0.59 | 81.39±0.63 | 93.27±0.15 | 88.09±0.08 |            |
| LA      | 77.79±0.39 | 92.18±0.24 | 88.34±0.62 | 81.30±0.44 | 93.37±0.15 | 88.09±0.19 |            |
| EDL     | 79.35±1.11 | 92.31±0.76 | 87.67±0.31 | 80.67±0.29 | 93.13±0.36 | 87.76±0.21 |            |
| VID     | 79.99±0.13 | 92.30±0.25 | 87.57±0.29 | 80.82±0.94 | 92.93±0.23 | 88.23±0.25 |            |
| I-EDL   | 80.37±0.48 | 92.76±0.47 | 88.52±0.50 | 81.13±0.39 | 93.47±0.06 | 85.91±0.40 |            |
| R-EDL   | 80.60±0.93 | 92.41±0.30 | 88.00±0.20 | 80.83±1.00 | 93.33±0.21 | 87.62±0.16 |            |
| IB-EDL  | 81.14±0.09 | 92.55±0.15 | 89.00±0.40 | 81.71±0.38 | 93.57±0.15 | 88.03±0.21 |            |
| ECE ↓   | MAP        | 19.68±0.43 | 7.18±0.14  | 10.52±0.87 | 17.29±0.57 | 5.74±0.08  | 7.95±0.35  |
| MCD     | 19.91±0.39 | 7.10±0.02  | 10.48±0.86 | 16.98±0.10 | 5.74±0.09  | 7.93±0.35  |            |
| Ens     | 18.20±0.17 | 3.81±1.60  | 10.08±0.90 | 16.11±1.63 | 5.72±0.24  | 7.80±0.19  |            |
| LA      | 18.49±0.44 | 3.33±0.66  | 5.26±1.30  | 6.62±0.10  | 2.47±0.08  | 3.61±0.27  |            |
| EDL     | 6.52±0.12  | 5.94±0.87  | 8.28±1.62  | 7.43±1.48  | 11.13±1.11 | 6.51±0.80  |            |
| VID     | 10.96±0.40 | 3.33±1.40  | 5.99±1.41  | 8.38±0.93  | 2.60±0.13  | 4.58±0.14  |            |
| I-EDL   | 5.08±1.94  | 9.69±0.58  | 7.57±0.52  | 8.95±0.59  | 13.07±0.24 | 14.52±1.15 |            |
| R-EDL   | 10.09±1.01 | 2.93±1.32  | 4.68±1.35  | 6.59±0.78  | 3.18±0.18  | 2.61±0.09  |            |
| IB-EDL  | 2.78±0.87  | 2.70±0.58  | 2.34±0.61  | 4.34±0.20  | 3.86±0.86  | 4.47±0.31  |            |
| NLL ↓   | MAP        | 2.25±0.08  | 0.61±0.02  | 0.78±0.05  | 1.25±0.04  | 0.36±0.00  | 0.47±0.01  |
| MCD     | 2.27±0.09  | 0.59±0.00  | 0.77±0.05  | 1.22±0.03  | 0.36±0.01  | 0.45±0.03  |            |
| Ens     | 2.02±0.07  | 0.43±0.09  | 0.74±0.05  | 1.22±0.12  | 0.35±0.01  | 0.46±0.05  |            |
| LA      | 0.80±0.01  | 0.33±0.04  | 0.42±0.01  | 0.62±0.04  | 0.22±0.00  | 0.37±0.01  |            |
| EDL     | 0.74±0.02  | 0.33±0.01  | 0.45±0.02  | 0.68±0.01  | 0.31±0.01  | 0.43±0.01  |            |
| VID     | 0.78±0.01  | 0.35±0.01  | 0.46±0.02  | 0.72±0.03  | 0.28±0.01  | 0.36±0.00  |            |
| I-EDL   | 0.72±0.02  | 0.36±0.02  | 0.43±0.00  | 0.68±0.01  | 0.32±0.01  | 0.52±0.02  |            |
| R-EDL   | 0.74±0.01  | 0.32±0.01  | 0.43±0.02  | 0.68±0.02  | 0.27±0.01  | 0.43±0.02  |            |
| IB-EDL  | 0.69±0.01  | 0.32±0.02  | 0.40±0.03  | 0.66±0.01  | 0.25±0.01  | 0.35±0.00  |            |"
109,"| Model               | Method              | OBQA  $ \rightarrow $  ARC-C | OBQA  $ \rightarrow $  ARC-E | OBQA  $ \rightarrow $  CSQA |
| ------------------- | ------------------- | ---------------------------- | ---------------------------- | --------------------------- |
| AUROC  $ \uparrow $ | AUROC  $ \uparrow $ | AUROC  $ \uparrow $          |                              |                             |
| MP                  | UM                  | MP                           | UM                           | MP                          |
| Llama3-8B           | MAP                 | 63.12±1.21                   | —                            | 58.11±1.55                  |
| MCD                 | 62.81±1.00          | —                            | 58.30±1.98                   | —                           |
| Ens                 | 62.70±1.04          | —                            | 58.19±1.72                   | —                           |
| LA                  | 62.14±0.55          | —                            | 56.11±0.84                   | —                           |
| EDL                 | 82.04±0.69          | 78.99±1.18                   | 78.80±1.47                   | 74.77±2.16                  |
| VID                 | 88.85±1.57          | 89.95±1.59                   | 87.55±0.94                   | 90.02±1.72                  |
| I-EDL               | 81.31±0.52          | 78.54±0.61                   | 78.29±1.14                   | 74.79±1.43                  |
| R-EDL               | 75.79±0.51          | 73.64±0.49                   | 71.51±0.79                   | 68.81±0.79                  |
| IB-EDL              | 88.85±0.96          | 92.58±0.37                   | 88.14±1.10                   | 94.77±0.42                  |"
109,"| Method            | Test Samples/s  $ \uparrow $  Training Samples/s  $ \uparrow $  Memory (GB) at Training $ \downarrow $ |
| ----------------- | ------------------------------------------------------------------------------------------------------ |
| MAP               | 69.55±2.86                                                                                             |
| MCD (10 forwards) | 9.79±1.21                                                                                              |
| Ens (3 models)    | 25.77±3.54                                                                                             |
| LA                | 5.95±0.49                                                                                              |
| EDL               | 68.99±1.59                                                                                             |
| VID               | 69.17±0.99                                                                                             |
| I-EDL             | 68.94±2.18                                                                                             |
| R-EDL             | 68.84±1.09                                                                                             |
| IB-EDL            | 68.08±1.75                                                                                             |"
110,"| algorithm        | regret                                                              | time complexity           | space complexity                                                                            |
| ---------------- | ------------------------------------------------------------------- | ------------------------- | ------------------------------------------------------------------------------------------- |
| Zooming          | $ \widetilde{\mathcal{O}}\left(T^{\frac{d_{z}+1}{d_{z}+2}}\right) $ | $ \mathcal{O}(T^{2}) $    | $ \mathcal{O}(T) $                                                                          |
| HOO              | $ \widetilde{\mathcal{O}}\left(T^{\frac{d_{z}+1}{d_{z}+2}}\right) $ | $ \mathcal{O}(T \log T) $ | $ \mathcal{O}(T) $                                                                          |
| A-BLiN           | $ \widetilde{\mathcal{O}}\left(T^{\frac{d_{z}+1}{d_{z}+2}}\right) $ | $ \mathcal{O}(T) $        | $ \mathcal{O}\left(T^{\frac{d_{z}+1}{d_{z}+2}} (\log T)^{-\frac{d_{z}+1}{d_{z}+2}}\right) $ |
| Log-Li(our work) | $ \widetilde{\mathcal{O}}\left(T^{\frac{d_{z}+1}{d_{z}+2}}\right) $ | $ \mathcal{O}(T) $        | $ \mathcal{O} (\log T) $                                                                    |"
111,"| Decaf (Shimada et al., 2023)                       | O | 9.65 | —    | —    | 1.03 | 83.6 | 96.6 | 89.6 | 19.59 |
| -------------------------------------------------- | - | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----- |
| Benchmark (Lugaresi et al., 2019; Li et al., 2017) | O | 17.7 | —    | —    | 19.3 | 64.2 | 73.2 | 68.4 | 16.40 |
| PIXIE (hand+face) (Feng et al., 2021a)             | O | 26.3 | —    | —    | 7.04 | 75.9 | 75.1 | 75.5 | —     |
| DICE (Ours)                                        | R | 8.32 | 9.95 | 7.27 | 0.16 | 66.6 | 79.9 | 72.7 | 0.088 |"
111,"| PIXIE (whole-body) (Feng et al., 2021a)       | R | 39.7 | -    | -    | 0.11 | 97.1 | 51.8 | 67.6 | 0.070 |
| --------------------------------------------- | - | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----- |
| PIXIE-R (Feng et al., 2021a)                  | R | 11.0 | 22.0 | 21.2 | 0.27 | 62.6 | 83.0 | 72.0 | 0.070 |
| METRO* (hand+face) (Lin et al., 2021a)        | R | 11.8 | 15.4 | 11.9 | 0.08 | 80.7 | 54.8 | 65.2 | 0.103 |
| FastMETRO* (single-target) (Cho et al., 2022) | R | 9.27 | 11.8 | 9.41 | 0.09 | 82.2 | 55.5 | 66.2 | 0.110 |
| DICE (Ours)                                   | R | 8.32 | 9.95 | 7.27 | 0.16 | 66.6 | 79.9 | 72.7 | 0.088 |"
112,"| Model      | Children   | Comp.      | Fitness    | History    | Photo      | Amazon.    | Mines.     | Questions  | Roman.     | Tolokers   |
| ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |
| GCN        | 53.88±0.37 | 83.38±1.78 | 87.56±0.79 | 84.26±0.38 | 84.48±0.65 | 51.40±0.49 | 90.01±0.53 | 76.28±1.18 | 75.51±0.65 | 84.19±0.75 |
| GCN+GFS    | 59.29±0.31 | 90.23±0.18 | 93.09±0.13 | 84.99±0.38 | 87.31±0.36 | 53.32±0.81 | 89.99±0.60 | 76.50±1.38 | 83.30±0.57 | 85.01±0.91 |
| $ \Delta $ | +5.41      | +6.85      | +5.53      | +0.73      | +2.83      | +1.92      | -0.02      | +0.22      | +7.79      | +0.82      |
| GAT        | 53.54±0.49 | 86.52±0.92 | 88.23±0.91 | 83.83±0.27 | 85.67±0.45 | 51.08±0.60 | 90.26±0.53 | 77.64±1.15 | 84.57±0.80 | 83.41±0.47 |
| GAT+GFS    | 57.74±0.35 | 90.50±0.20 | 93.20±0.11 | 84.54±0.38 | 87.58±0.28 | 53.75±0.57 | 90.22±0.64 | 77.03±1.11 | 86.17±0.56 | 84.41±0.77 |
| $ \Delta $ | +4.20      | +3.98      | +4.97      | +0.71      | +1.91      | +2.67      | -0.04      | -0.61      | +1.60      | +1.00      |
| SAGE       | 54.68±0.84 | 86.08±0.50 | 88.65±1.22 | 84.06±0.42 | 85.08±0.64 | 53.80±0.56 | 90.74±0.59 | 74.91±1.06 | 82.81±0.61 | 82.77±0.38 |
| SAGE+GFS   | 59.14±0.33 | 90.47±0.24 | 93.63±0.10 | 84.68±0.33 | 87.23±0.51 | 54.17±0.61 | 90.71±0.62 | 75.37±1.33 | 85.47±0.53 | 83.16±0.73 |
| $ \Delta $ | +4.46      | +4.39      | +4.98      | +0.62      | +2.15      | +0.37      | -0.03      | +0.46      | +2.66      | +0.39      |
| GT         | 51.20±0.38 | 85.63±1.16 | 87.37±1.38 | 83.61±0.43 | 83.65±0.59 | 51.30±0.73 | 90.11±0.57 | 77.57±1.09 | 84.95±0.54 | 83.20±0.60 |
| GT+GFS     | 56.01±0.35 | 89.97±0.27 | 92.44±0.11 | 84.09±0.32 | 87.41±0.41 | 52.47±0.54 | 89.93±0.64 | 77.70±0.73 | 86.99±0.45 | 83.46±0.67 |
| $ \Delta $ | +4.81      | +4.34      | +5.07      | +0.48      | +3.76      | +1.17      | -0.18      | +0.13      | +2.04      | +0.26      |
| SGC        | 52.69±0.52 | 82.50±0.34 | 84.23±0.31 | 83.98±0.37 | 82.79±0.45 | 49.00±0.42 | 76.43±1.01 | 71.78±0.81 | 69.89±0.51 | 78.65±1.01 |
| SGC+GFS    | 55.93±0.37 | 87.78±0.40 | 90.29±0.25 | 84.15±0.23 | 85.46±0.33 | 51.43±0.64 | 88.79±0.66 | 73.53±1.10 | 74.00±0.67 | 82.76±0.84 |
| $ \Delta $ | +3.24      | +5.28      | +6.06      | +0.17      | +2.67      | +2.43      | +12.36     | +1.75      | +4.11      | +4.11      |
| APPNP      | 50.63±0.89 | 83.67±0.90 | 86.76±1.18 | 83.37±0.26 | 82.20±1.41 | 48.73±0.61 | 81.61±0.79 | 75.29±1.06 | 71.48±0.65 | 79.82±1.17 |
| APPNP+GFS  | 56.71±0.36 | 88.51±0.34 | 91.22±0.25 | 84.75±0.33 | 86.65±0.32 | 50.76±0.62 | 83.19±0.97 | 75.66±1.03 | 72.35±0.69 | 83.66±0.65 |
| $ \Delta $ | +6.08      | +4.84      | +4.46      | +1.38      | +4.45      | +2.03      | +1.58      | +0.37      | +0.87      | +3.84      |
| ACMGCN     | 54.60±0.50 | 85.94±0.72 | 89.10±0.98 | 84.22±0.34 | 84.99±0.34 | 51.91±0.39 | 90.59±0.58 | 76.66±1.27 | 85.27±0.57 | 83.61±0.83 |
| ACMGCN+GFS | 59.04±0.37 | 89.59±0.19 | 93.44±0.07 | 84.70±0.30 | 86.61±0.42 | 52.81±0.75 | 90.45±0.59 | 75.99±1.27 | 87.03±0.57 | 83.45±1.03 |
| $ \Delta $ | +4.44      | +3.65      | +4.34      | +0.48      | +1.62      | +0.90      | -0.14      | -0.67      | +1.76      | -0.16      |
| FAGCN      | 50.43±0.86 | 79.92±0.99 | 83.10±0.45 | 82.04±0.62 | 80.67±0.77 | 46.08±0.52 | 78.22±2.86 | 58.60±2.08 | 62.02±2.98 | 73.07±0.70 |
| FAGCN+GFS  | 56.17±0.38 | 87.72±0.49 | 89.66±0.35 | 84.32±0.36 | 85.54±0.37 | 50.72±0.82 | 88.12±1.32 | 71.94±2.03 | 72.05±1.45 | 82.82±1.28 |
| $ \Delta $ | +5.74      | +7.80      | +6.56      | +2.28      | +4.87      | +4.64      | +9.90      | +13.34     | +10.03     | +9.75      |"
113,"| Llama 2 7B | FiQA  | Causal 20 | Multifin | Average |
| ---------- | ----- | --------- | -------- | ------- |
| F1         | F1    | F1        |          |         |
| Chat       | 56.40 | 90.40     | 38.74    | 61.48   |
| Chat (CPT) | 62.53 | 90.16     | 38.23    | 63.64   |
| Chat (CFT) | 67.69 | 90.17     | 46.01    | 67.96   |"
113,"| Llama 2 13B | FiQA  | Causal 20 | Multifin | Average |
| ----------- | ----- | --------- | -------- | ------- |
| F1          | F1    | F1        |          |         |
| Chat        | 61.18 | 84.77     | 45.81    | 63.92   |
| Chat (CPT)  | 66.96 | 90.06     | 45.33    | 67.45   |
| Chat (CFT)  | 70.55 | 89.87     | 50.94    | 70.45   |"
113,"| Llama 2 7B | Accuracy ( $ \uparrow $ ) |
| ---------- | ------------------------- |
| Anatomy    | Clinical Knowledge        |
| Chat       | 44.07                     |
| Chat (CPT) | 45.19                     |
| Chat (CFT) | 48.15                     |"
113,"| Llama 2 13B | Anatomy | Clinical Knowledge | College Biology | College Medicine | Medical Genetics | Professional Medicine | MedQA | Average |
| ----------- | ------- | ------------------ | --------------- | ---------------- | ---------------- | --------------------- | ----- | ------- |
| Chat        | 51.85   | 56.60              | 54.17           | 46.82            | 63.50            | 56.99                 | 45.33 | 53.61   |
| Chat (CPT)  | 50.37   | 60.00              | 55.90           | 50.58            | 62.00            | 57.35                 | 43.95 | 54.31   |
| Chat (CFT)  | 53.33   | 63.21              | 57.99           | 56.35            | 62.50            | 57.72                 | 44.85 | 56.56   |"
113,"| Llama 2 7B       | Accuracy ( $ \uparrow $ ) |
| ---------------- | ------------------------- |
| Anatomy          | Clinical Knowledge        |
| Base             | 43.52                     |
| Base (CPT)       | 47.50                     |
| Base (CFT)       | 47.87                     |
| Base (CPT + IFT) | 49.91                     |
| Base (CFT + IFT) | 51.11                     |"
113,"| Model (Method)         | Accuracy (%) |
| ---------------------- | ------------ |
| Gemini-1.5-Flash (CPT) | 37.18        |
| Gemini-1.5-Flash (CFT) | 43.89        |"
113,"| Llama 2 7B  | Accuracy ( $ \uparrow $ ) |
| ----------- | ------------------------- |
| Anatomy     | Clinical Knowledge        |
| Chat (CFT)  | 48.15                     |
| Chat (-CFT) | 41.48                     |
| Llama 2 13B | Anatomy                   |
| Chat (CFT)  | 53.33                     |
| Chat (-CFT) | 50.00                     |"
115,"| PE method          | PE processing | spd         | lpd         | wp(4,·)     | spd         | lpd         | wp(4,·)     |
| ------------------ | ------------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| Lap                | Naive         | 0.488±0.005 | 0.727±0.005 | 0.370±0.004 | 2.068±0.004 | 1.898±0.001 | 0.480±0.000 |
| SignNet            | 0.537±0.013   | 0.771±0.013 | 0.437±0.000 | 2.064±0.004 | 1.900±0.002 | 0.518±0.027 |             |
| SPE                | 0.355±0.001   | 0.655±0.002 | 0.326±0.001 | 2.066±0.005 | 1.920±0.000 | 0.452±0.001 |             |
| SVD                | Naive         | 0.649±0.002 | 0.853±0.002 | 0.721±0.000 | 2.196±0.002 | 1.982±0.004 | 0.519±0.000 |
| SignNet            | 0.673±0.003   | 0.872±0.002 | 0.443±0.001 | 2.229±0.003 | 1.996±0.005 | 0.541±0.001 |             |
| SPE                | 0.727±0.001   | 0.912±0.001 | 0.721±0.000 | 2.261±0.002 | 2.122±0.007 | 0.755±0.000 |             |
| MagLap-1q (q=0.1)  | Naive         | 0.366±0.003 | 0.593±0.003 | 0.241±0.010 | 1.826±0.005 | 1.760±0.007 | 0.311±0.013 |
| SignNet            | 0.554±0.001   | 0.699±0.002 | 0.268±0.042 | 2.048±0.004 | 1.881±0.003 | 0.413±0.001 |             |
| SPE                | 0.124±0.002   | 0.433±0.002 | 0.043±0.000 | 1.620±0.005 | 1.547±0.004 | 0.133±0.001 |             |
| MagLap-1q (best q) | SPE           | 0.124±0.002 | 0.432±0.004 | 0.040±0.001 | 1.533±0.007 | 1.493±0.003 | 0.132±0.000 |
| MagLap-Multi-q     | Naive         | 0.353±0.003 | 0.535±0.006 | 0.188±0.010 | 1.708±0.012 | 1.661±0.002 | 0.257±0.007 |
| SignNet            | 0.473±0.000   | 0.579±0.001 | 0.280±0.004 | 1.906±0.001 | 1.784±0.008 | 0.377±0.007 |             |
| SPE                | 0.016±0.000   | 0.185±0.036 | 0.002±0.000 | 0.546±0.068 | 1.100±0.007 | 0.074±0.001 |             |"
115,"| PE method          | PE processing      | Test F1             |
| ------------------ | ------------------ | ------------------- |
| Lap                | Naive              | 51.39 $ \pm $ 2.36  |
| SignNet            | 49.50 $ \pm $ 4.01 |                     |
| SPE                | 56.35 $ \pm $ 3.70 |                     |
| Maglap-1q (best q) | Naive              | 68.68 $ \pm $ 9.66  |
| SignNet            | 76.28 $ \pm $ 6.82 |                     |
| SPE                | 86.86 $ \pm $ 3.85 |                     |
| Maglap-5q          | Naive              | 75.12 $ \pm $ 12.78 |
| SignNet            | 72.97 $ \pm $ 9.38 |                     |
| SPE                | 91.27 $ \pm $ 0.71 |                     |"
138,"| Method      | Expert MPE | Good MAMuJoCo | Good SMAC    |
| ----------- | ---------- | ------------- | ------------ |
| Spread      | Tag        | World         | 2halfcheetah |
| MA-ICQ      | 79.2 ± 4.3 | 92.6 ± 15.5   | 83.5 ± 20.7  |
| MA-CQL      | 74.1 ± 5.8 | 68.3 ± 13.2   | 57.7 ± 20.5  |
| OMAR        | 82.9 ± 2.4 | 97.9 ± 16.4   | 84.8 ± 21.0  |
| MA-SiBC     | 87.5 ± 7.3 | 77.4 ± 13.9   | 97.3 ± 19.1  |
| DOM2        | 88.7 ± 6.3 | 98.2 ± 14.4   | 99.5 ± 17.1  |
| MADiff      | 82.1 ± 5.9 | 103.0 ± 12.0  | 96.4 ± 13.7  |
| MCGD        | 93.8 ± 2.7 | 109.6 ± 13.3  | 110.9 ± 11.5 |
| Abs.% Avg.↑ | 5.1(5.7)   | 6.6(6.4)      | 11.4(11.5)   |"
115,"| Base Model           | PE method   | PE processing | Gain        | BW          | PM          | DSP         | LUT         |
| -------------------- | ----------- | ------------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| Undirected-GIN       | None        | N/A           | 0.416+0.017 | 4.908+0.083 | 1.119+0.011 | 2.827+0.206 | 2.153+0.169 |
| Lap                  | SignNet     | 0.430+0.009   | 4.712+0.134 | 1.127+0.007 | 2.665+0.184 | 2.025+0.057 |             |
| SPE                  | 0.416+0.021 | 4.321+0.084   | 1.127+0.020 | 2.662+0.187 | 1.925+0.059 |             |             |
| Maglap-1q (q=0.01)   | SignNet     | 0.426+0.009   | 4.670+0.113 | 1.116+0.009 | 2.673+0.090 | 2.027+0.091 |             |
| SPE                  | 0.405+0.016 | 4.305+0.092   | 1.121+0.018 | 2.666+0.190 | 2.024+0.068 |             |             |
| Maglap-1q (best q)   | SPE         | 0.398+0.025   | 4.281+0.085 | 1.113+0.022 | 2.614+0.098 | 2.010+0.082 |             |
| Maglap-Multi-q       | SignNet     | 0.421+0.015   | 4.743+0.215 | 1.126+0.011 | 2.665+0.111 | 2.025+0.076 |             |
| SPE                  | 0.389±0.017 | 4.175+0.115   | 1.137+0.004 | 2.582±0.133 | 1.976+0.089 |             |             |
| Bidirected-GIN       | None        | N/A           | 0.386+0.008 | 4.594+0.087 | 1.123+0.010 | 2.232+0.143 | 1.939+0.068 |
| Lap                  | SignNet     | 0.382+0.008   | 4.371+0.171 | 1.127+0.021 | 2.256+0.109 | 1.806+0.096 |             |
| SPE                  | 0.391+0.007 | 4.153+0.160   | 1.135+0.035 | 2.267+0.126 | 1.786+0.072 |             |             |
| Maglap-1q (q=0.01)   | SignNet     | 0.388+0.012   | 4.351+0.132 | 1.131+0.012 | 2.304+0.143 | 1.882+0.085 |             |
| SPE                  | 0.384+0.008 | 4.152+0.056   | 1.123+0.026 | 2.344+0.134 | 1.830+0.116 |             |             |
| Maglap-1q (best q)   | SPE         | 0.383+0.002   | 4.113+0.052 | 1.099+0.020 | 2.256+0.144 | 1.768+0.090 |             |
| Maglap-Multi-q       | SignNet     | 0.381+0.008   | 4.443+0.116 | 1.119+0.016 | 2.212+0.116 | 1.791+0.091 |             |
| SPE                  | 0.371+0.008 | 4.051+0.139   | 1.116+0.012 | 2.207+0.185 | 1.735+0.096 |             |             |
| SAT (undirected-GIN) | None        | N/A           | 0.368+0.019 | 4.107+0.103 | 1.077+0.032 | 3.154+0.263 | 2.286+0.147 |
| Lap                  | SignNet     | 0.368+0.022   | 4.085+0.189 | 1.038+0.016 | 3.103+0.101 | 2.223+0.175 |             |
| SPE                  | 0.375+0.016 | 4.180+0.093   | 1.065+0.034 | 3.167+0.193 | 2.425+0.168 |             |             |
| Maglap-1q (q=0.01)   | SignNet     | 0.382+0.009   | 4.143+0.181 | 1.073+0.021 | 3.087+0.183 | 2.214+0.150 |             |
| SPE                  | 0.366+0.003 | 4.081+0.071   | 1.089+0.023 | 3.206+0.197 | 2.362+0.154 |             |             |
| Maglap-1q (best q)   | SPE         | 0.361+0.016   | 4.014+0.068 | 1.057+0.036 | 3.101+0.176 | 2.362+0.154 |             |
| Maglap-Multi-q       | SignNet     | 0.368+0.020   | 4.044+0.090 | 1.066+0.028 | 3.121+0.143 | 2.207+0.113 |             |
| SPE                  | 0.350+0.004 | 4.044+0.153   | 1.035+0.025 | 3.076+0.240 | 2.333+0.147 |             |             |
| SAT (bidirected-GIN) | None        | N/A           | 0.392+0.036 | 4.035+0.111 | 1.065+0.019 | 2.724+0.158 | 2.117+0.106 |
| Lap                  | SignNet     | 0.384+0.025   | 3.949+0.125 | 1.069+0.029 | 2.569+0.116 | 2.048+0.088 |             |
| SPE                  | 0.368+0.022 | 4.024+0.106   | 1.046+0.021 | 2.713+0.135 | 2.173+0.107 |             |             |
| Maglap-1q (q=0.01)   | SignNet     | 0.384+0.015   | 4.023+0.032 | 1.055+0.028 | 2.616+0.120 | 2.054+0.127 |             |
| SPE                  | 0.364+0.012 | 3.996+0.178   | 1.074+0.030 | 2.687+0.209 | 2.192+0.135 |             |             |
| Maglap-1q (best q)   | SPE         | 0.360+0.009   | 3.960+0.060 | 1.062+0.024 | 2.657+0.128 | 2.107+0.135 |             |
| Maglap-Multi-q       | SignNet     | 0.420+0.035   | 4.022+0.128 | 1.089+0.046 | 2.741+0.110 | 2.045+0.079 |             |
| SPE                  | 0.359+0.008 | 3.930+0.069   | 1.045+0.012 | 2.616+0.151 | 2.082+0.099 |             |             |"
117,"| Method   | MSASL100 | WLASL2000 |
| -------- | -------- | --------- |
| P-I      | P-C      | P-I       |
| MSLU     | 74.07    | 71.81     |
| NLA-SLR  | 72.56    | 69.86     |
| Uni-Sign | 78.16    | 76.97     |"
117,"| Method   | CSL-Daily |
| -------- | --------- |
| Dev      | Test      |
| SEN      | 31.1      |
| CorrNet  | 30.6      |
| Uni-Sign | 26.7      |"
117,"| Method        | CSL-Daily |
| ------------- | --------- |
| BLEU4         | ROUGE     |
| SignLLM       | 15.75     |
| C $ ^{2} $ RL | 21.61     |
| Uni-Sign      | 26.36     |"
117,"| Method        | OpenASL |
| ------------- | ------- |
| BLEU4         | ROUGE   |
| OpenASL       | 8.59    |
| C $ ^{2} $ RL | 13.21   |
| Uni-Sign      | 23.14   |"
118,"|                  | Methods | Avg. Acc | Avg. Rank | Win      | P-value  |
| ---------------- | ------- | -------- | --------- | -------- | -------- |
| DL-CNN           | FCN     | 0.8296   | 9.53      | 13       | 1.43E-12 |
| T-Loss           | 0.8325  | 11.12    | 9         | 2.95E-14 |          |
| SelfTime         | 0.8017  | 13.80    | 0         | 4.53E-25 |          |
| TS-TCC           | 0.7807  | 13.96    | 0         | 1.60E-15 |          |
| TS2Vec           | 0.8691  | 8.43     | 9         | 1.69E-15 |          |
| TimesNet         | 0.8367  | 10.13    | 7         | 4.22E-15 |          |
| InceptionTime    | 0.9181  | 4.05     | 29        | 7.39E-06 |          |
| ShapeConv        | 0.7688  | 13.91    | 5         | 3.58E-24 |          |
| ModernTCN        | 0.7938  | 11.37    | 9         | 1.78E-18 |          |
| TSLANet          | 0.9205  | 3.68     | 31        | 1.06E-03 |          |
| DL Trans         | TST     | 0.7755   | 13.54     | 1        | 2.00E-19 |
| PatchTST         | 0.8265  | 9.56     | 12        | 1.27E-15 |          |
| Medformer        | 0.8541  | 9.26     | 7         | 7.15E-16 |          |
| DL FM            | GPT4TS  | 0.8593   | 9.34      | 6        | 7.89E-16 |
| UniTS            | 0.8502  | 9.66     | 5         | 1.41E-13 |          |
| Non DL           | RDST    | 0.8897   | 6.41      | 23       | 7.54E-10 |
| MR-H             | 0.8972  | 5.51     | 29        | 3.80E-07 |          |
| SoftShape (Ours) | 0.9334  | 2.72     | 53        | -        |          |"
118,"| Methods               | Avg. Acc | Avg. Rank | Win | P-value  |
| --------------------- | -------- | --------- | --- | -------- |
| w/o Soft Sparse       | 0.9123   | 3.04      | 29  | 4.69E-06 |
| w/o Intra             | 0.9245   | 2.75      | 31  | 5.39E-04 |
| w/o Inter             | 0.9022   | 3.74      | 19  | 1.81E-09 |
| w/o Intra &amp; Inter | 0.8696   | 5.02      | 11  | 4.35E-16 |
| with Linear Shape     | 0.9164   | 3.23      | 22  | 1.80E-09 |
| SoftShape (Ours)      | 0.9334   | 2.04      | 60  | -        |"
118,"| Sparse Ratio | Avg. Acc | Avg. Rank | Win | P-value  |
| ------------ | -------- | --------- | --- | -------- |
| 0%           | 0.9461   | 2.44      | 4   | -        |
| 10%          | 0.9469   | 2.39      | 7   | 2.97E-01 |
| 30%          | 0.9448   | 2.78      | 5   | 2.66E-01 |
| 50%          | 0.9453   | 2.61      | 6   | 3.46E-01 |
| 70%          | 0.9323   | 3.89      | 5   | 9.37E-03 |
| 90%          | 0.9261   | 4.50      | 2   | 4.02E-04 |"
119,"| Training data setup                  | Clipart     | Sketch       |
| ------------------------------------ | ----------- | ------------ |
| Leave-out-domain                     | 27.4        | 30.1         |
| CG high-diversity                    | 27.6        | 28.1         |
| w/ classes  $ C_{2} $  (upper bound) | 36.6 (+9.0) | 44.3 (+16.2) |"
119,"| Natural-only  $ \rightarrow $  Leave-out-domain  | +7.1 | +4.1 |
| ------------------------------------------------ | ---- | ---- |
| Natural-only  $ \rightarrow $  CG high-diversity | +6.9 | +3.4 |"
119,"| Captions        | Classes          |
| --------------- | ---------------- |
| domain-specific | domain-invariant |
| 50%             | 50%              |
| 0%              | 100%             |"
120,"|             | COVIDx | Came17 | KVASIR-f | MVAD-1 |
| ----------- | ------ | ------ | -------- | ------ |
| Init        | 49.34  | 50.47  | 33.43    | 33.33  |
| RF          | 50.01  | 54.82  | 34.66    | 48.17  |
| GCap        | 50.86  | 55.77  | 32.66    | 27.33  |
| B           | 50.42  | 54.41  | 32.57    | 43.21  |
| LE          | 50.02  | 55.44  | 35.51    | 27.93  |
| DPImg       | 49.14  | 61.06  | 33.35    | 37.03  |
| PE          | 59.63  | 63.66  | 48.88    | 57.41  |
| PE-EM       | 57.60  | 63.34  | 43.01    | 50.06  |
| PCEvolve-GM | 56.91  | 62.63  | 43.55    | 55.56  |
| PCEvolve    | 64.04  | 69.10  | 50.95    | 59.26  |"
121,"| Algorithm          | no DPO            | OR               | LP              | RC-LWR | no DPO | OR   | LP   | RC-LWR | no DPO | OR    | LP    | RC-LWR |
| ------------------ | ----------------- | ---------------- | --------------- | ------ | ------ | ---- | ---- | ------ | ------ | ----- | ----- | ------ |
|                    | Length-Controlled | Avg Character    | Avg Performance |        |        |      |      |        |        |       |       |        |
|                    | Win Rate          | Length in Alpaca | on 8 Benchmarks |        |        |      |      |        |        |       |       |        |
| Llama-3-8B-Fsfairx | 23.91             | 41.82            | 49.98           | 51.37  | 1967   | 2404 | 1989 | 1867   | 62.69  | 60.45 | 61.75 | 61.94  |
| Llama-3-8B-GRM     | 23.91             | 41.00            | 48.41           | 50.49  | 1967   | 2438 | 1996 | 1858   | 62.69  | 60.94 | 62.11 | 61.83  |
| gemma2-9b-Fsfairx  | 46.96             | 62.79            | 67.46           | 69.54  | 1521   | 2365 | 1491 | 1798   | 63.69  | 58.88 | 60.52 | 60.79  |
| gemma2-9b-Grm      | 46.96             | 63.21            | 66.28           | 70.45  | 1521   | 2106 | 1426 | 1780   | 63.69  | 55.28 | 57.28 | 58.31  |"
139,"| CLIP-I | PSNR  | SSIM  | LPIPS | FVD    | CD-FVD |
| ------ | ----- | ----- | ----- | ------ | ------ |
| 0.9227 | 9.039 | 0.637 | 0.512 | 801.10 | 816.23 |
| 0.9535 | 23.28 | 0.904 | 0.173 | 142.34 | 459.10 |"
122,"| Method                             | MUTAG      | DD         | COX2       | ER_MD      |
| ---------------------------------- | ---------- | ---------- | ---------- | ---------- |
| AUC                                | F1-Score   | AUC        | F1-Score   | AUC        |
| SP (Borgwardt &amp; Kriegel, 2005) | 67.52±0.00 | 60.00±0.00 | 82.73±0.00 | 76.09±0.00 |
| WL (Shervashidze et al., 2011)     | 60.00±0.00 | 89.12±0.00 | 81.57±0.00 | 74.64±0.00 |
| NH (Hido &amp; Kashima, 2009)      | 79.97±0.40 | 76.00±0.00 | 81.61±0.32 | 73.91±0.65 |
| RW (Vishwanathan et al., 2010)     | 86.98±0.00 | 83.33±0.00 | OM         | OM         |
| OCGIN (Zhao &amp; Akoglu, 2023)    | 74.66±1.68 | 62.95±0.00 | 66.59±4.44 | 56.12±0.00 |
| OCGTL (Qiu et al., 2022)           | 87.04±1.74 | 80.00±0.00 | 77.52±0.43 | 71.65±0.73 |
| GLocalKD (Ma et al., 2022)         | 90.59±0.61 | 86.17±0.91 | 80.59±0.00 | 73.48±0.57 |
| iGAD (Zhang et al., 2022)          | 92.58±1.25 | 85.20±2.30 | 74.83±2.30 | 70.39±2.60 |
| SIGNET (Liu et al., 2023a)         | 87.73±2.45 | 73.07±4.11 | 59.53±3.45 | 56.76±3.47 |
| MUSE (Kim et al., 2024)            | 83.81±5.17 | 75.36±5.02 | 61.06±3.03 | 58.32±3.08 |
| DO2HSC (Zhang et al., 2024)        | 88.83±6.58 | 86.80±6.21 | 77.12±2.15 | 70.87±2.73 |
| AGDiff                             | 95.83±2.15 | 89.45±1.37 | 88.23±0.67 | 84.06±0.59 |"
123,"| Expert 1 (0.72)                       | Expert 2 (0.73)             | Expert 3 (0.75)                   | Expert 4 (0.76)                 |
| ------------------------------------- | --------------------------- | --------------------------------- | ------------------------------- |
| View - Sunset - City - Building - Sky | View - Boat - Sea           | Artist - Actor                    | Actor - Dress - Portrait        |
| Expert 5 (0.77)                       | Expert 6 (0.78)             | Expert 7 (0.79)                   | Expert 8 (0.79)                 |
| Illustration - Portrait - Photo       | Player - Ball - Game - Team | Background - Water - River - Tree | Biological Species - Dog - Cat  |
| Expert 9 (0.79)                       | Expert 10 (0.80)            | Expert 11 (0.81)                  | Expert 12 (0.81)                |
| Illustration - Vector                 | People                      | Car - City - Road                 | Person - Player - Team - Couple |
| Expert 13 (0.86)                      | Expert 14 (0.90)            | Expert 15 (0.95)                  | Expert 16 (0.98)                |
| Room - House                          | Art - Artist - Digital      | Food - Water                      | Person - Man - Woman - Text     |"
123,"| Expert 1 (0.65, Indoor Scenes and Dining)      | Expert 2 (0.77, Food and Small Groups)           |
| ---------------------------------------------- | ------------------------------------------------ |
| table - plate - kitchen - sitting              | food - pizza - sandwich                          |
| Expert 3 (0.78, People and Objects)            | Expert 4 (0.79, Sports and Activities)           |
| skateboard - surfboard - laptop - tie - phone  | tennis - baseball - racquet - skateboard - skis  |
| Expert 5 (0.79, Wildlife and Nature)           | Expert 6 (0.80, Urban Scenes and Transportation) |
| giraffe - herd - sheep - zebra - elephants     | street - train - bus - park - building           |
| Expert 7 (0.81, Outdoor Activities and Nature) | Expert 8 (0.83, Domestic Life and Pets)          |
| beach - ocean - surfboard - kite - wave        | man - woman - girl - hand - bed - cat            |"
123,"| CC3M                    |
| ----------------------- |
| Method                  |
| MACs (@768)             |
| Norm Pruning @50k       |
| Structural Pruning @30k |
| BKSDM @30k              |
| APTP(0.66) @30k         |
| APTP(0.85) @30k         |
| SD 2.1                  |"
123,"| MS-COCO                 |
| ----------------------- |
| Method                  |
| MACs (@768)             |
| Norm Prunnig @50k       |
| Structural Pruning @30k |
| BKSDM @30k              |
| APTP(0.64) @30k         |
| APTP(0.78) @30k         |
| SD 2.1                  |
| (b)                     |"
123,"| Train on MS-COCO   |
| ------------------ |
| Method             |
| MACs (@768)        |
| Norm Pruning       |
| Structural Pruning |
| BKSDM              |
| APTP (0.64)        |
| APTP (0.78)        |
| SD 2.1             |"
123,"| Method                | MACs(@768) | Latency(@768) | FID ( $ \downarrow $ ) | Clip Score ( $ \uparrow $ ) | CMMD ( $ \downarrow $ ) |
| --------------------- | ---------- | ------------- | ---------------------- | --------------------------- | ----------------------- |
| Uni-Arch Baseline     | 1088.8G    | 3.1           | 46.56                  | 29.11                       | 0.91                    |
| Contrastive Router    | 1079.5G    | 3.1           | 48.78                  | 28.90                       | 0.92                    |
| + Optimal Transport   | 1076.6G    | 3.1           | 38.56                  | 30.07                       | 0.74                    |
| + Distillation (APTP) | 1076.6G    | 3.1           | 25.57                  | 31.13                       | 0.58                    |"
126,"| Domain      | Video Tutorial      | Task Category            | Intent                                                                                                    | Intermediate Intent                                                           |
| ----------- | ------------------- | ------------------------ | --------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| OneStopShop | Buy Cheapest Item   | Skill Retention          | Buy the cheapest red blanket from Blankets &amp; Throws.                                                  | N/A                                                                           |
| reddit      |                     | Audio Perception         | Search for the company the person said they work at in the video and find the first post&#x27;s comments. | What company did the person in the video say they work for?                   |
| GitLab      |                     | Full Video Understanding | Follow all the repos visited in the video.                                                                | What are the names of all the visited repos?                                  |
| Map         |                     | Temporal Reasoning       | Find the page that shows the zipcode of the 2nd destination in the video.                                 | What was the name of the 2nd destination used in the video?                   |
| OsClass     | See Listing Ratings | Visual Reasoning         | Take me to the first red vehicle listing that appears in the video.                                       | What was the name of the first red vehicle listing that appears in the video? |"
140,"| Encoder     | Pruning Rate    | CIFAR10 | CIFAR100 | ImageNet-1K |
| ----------- | --------------- | ------- | -------- | ----------- |
| 30%         | 50%             | 70%     | 80%      | 90%         |
| -           | Best Supervised | 95.7    | 94.9     | 93.3        |
| -           | Random          | 94.3    | 93.4     | 90.9        |
| Badge (AL)  | 93.6            | 93.0    | 91.0     | 87.9        |
| SwAV        | Prototypicality | 94.7    | 92.9     | 90.1        |
| D2          | 94.3            | 93.8    | 91.6     | 85.1        |
| ELFS (Ours) | 95.0            | 94.3    | 91.8     | 89.8        |
| DINO        | FreeSel         | 94.5    | 93.8     | 91.7        |
| ELFS (Ours) | 95.5            | 95.2    | 93.2     | 90.7        |"
126,"| Model                           | Task Domain | Final Score | Intermediate Score | # Steps (Avg) |
| ------------------------------- | ----------- | ----------- | ------------------ | ------------- |
| Gemini 1.5 Pro Video Agent      | Classifieds | 6.7%        | 41.7%              | 17.1          |
| Gitlab                          | 5.7%        | 35.7%       | 18.5               |               |
| Map                             | 6.7%        | 73.3%       | 9.9                |               |
| Reddit                          | 3.4%        | 39.0%       | 18.2               |               |
| Shopping (admin)                | 8.5%        | 48.9%       | 23.7               |               |
| Shopping                        | 10.0%       | 24.7%       | 21.6               |               |
| Total                           | 7.0%        | 37.0%       | 19.4               |               |
| GPT4-o Summary Agent            | Classifieds | 10.0%       | 40.0%              | 9.7           |
| Gitlab                          | 14.2%       | 34.7%       | 13.0               |               |
| Map                             | 26.7%       | 66.7%       | 3.8                |               |
| Reddit                          | 11.5%       | 39.0%       | 13.8               |               |
| Shopping (admin)                | 8.5%        | 29.1%       | 13.7               |               |
| Shopping                        | 15.7%       | 33.8%       | 14.3               |               |
| Total                           | 13.3%       | 36.8%       | 12.8               |               |
| GPT4-o Frame Agent (30 Frames)  | Classifieds | 18.3%       | 46.6%              | 9.3           |
| Gitlab                          | 5.7%        | 50.0%       | 11.8               |               |
| Map                             | 26.7%       | 73.3%       | 4.7                |               |
| Reddit                          | 6.9%        | 42.5%       | 11.6               |               |
| Shopping (admin)                | 8.5%        | 57.4%       | 16.8               |               |
| Shopping                        | 12.4%       | 37.2%       | 19.5               |               |
| Total                           | 11.0%       | 45.8%       | 14.0               |               |
| GPT4-o Frame Agent (60 Frames)  | Classifieds | 10.0%       | 30.0%              | 9.5           |
| Gitlab                          | 5.7%        | 55.7%       | 13.4               |               |
| Map                             | 26.7%       | 60.0%       | 3.5                |               |
| Reddit                          | 2.3%        | 44.8%       | 11.2               |               |
| Shopping (admin)                | 4.3%        | 48.9%       | 13.6               |               |
| Shopping                        | 5.0%        | 38.0%       | 16.9               |               |
| Total                           | 6.0%        | 43.5%       | 13.0               |               |
| GPT4-o Frame Agent (100 Frames) | Classifieds | 13.3%       | 41.6%              | 7.64          |
| Gitlab                          | 7.1%        | 58.6%       | 14.8               |               |
| Map                             | 20.0%       | 53.3%       | 3.8                |               |
| Reddit                          | 5.7%        | 43.7%       | 11.6               |               |
| Shopping (admin)                | 8.5%        | 51%         | 14.4               |               |
| Shopping                        | 10.7%       | 38.8%       | 16.4               |               |
| Total                           | 9.5%        | 45.8%       | 13.0               |               |
| Human Performance               | Classifieds | 61.5%       | 69.2%              | 7.9           |
| Gitlab                          | 81.3%       | 81.3%       | 7.1                |               |
| Map                             | 69.2%       | 76.9%       | 4.8                |               |
| Reddit                          | 81.8%       | 86.4%       | 9.0                |               |
| Shopping (admin)                | 68.4%       | 73.7%       | 5.1                |               |
| Shopping                        | 75.0%       | 82.1%       | 5.0                |               |
| Total                           | 73.9%       | 79.3%       | 6.4                |               |"
126,"| Task Category                                      | GPT-4o Summary | GPT-4o (30 Frames) | GPT-4o (60 Frames) | GPT-4o (100 Frames) | Gemini 1.5 Pro |
| -------------------------------------------------- | -------------- | ------------------ | ------------------ | ------------------- | -------------- |
| Visual Perception Task Success Rate                | 14.1%          | 11.1%              | 6.8%               | 9.3%                | 7.7%           |
| Audio Perception Task Success Rate                 | 14.8%          | 18.1%              | 7.7%               | 12.5%               | 11.1%          |
| Full Video Understanding Task Success Rate         | 15.5%          | 10.0%              | 7.2%               | 10.5%               | 6.5%           |
| Temporal Reasoning Task Success Rate               | 13.7%          | 12.4%              | 6.2%               | 10.4%               | 8.8%           |
| Agentic Easy Task Success Rate                     | 19.5%          | 12.8%              | 9.0%               | 13.0%               | 8.3%           |
| Agentic Medium Task Success Rate                   | 14.2%          | 13.4%              | 5.7%               | 9.4%                | 7.7%           |
| Agentic Hard Task Success Rate                     | 10.8%          | 8.1%               | 6.2%               | 9.1%                | 6.9%           |
| Visual Perception Intermediate Success Rate        | 32.7           | 43.9%              | 43.0%              | 43.5%               | 34.0%          |
| Audio Perception Intermediate Success Rate         | 50.0%          | 60.2%              | 62.8%              | 62.5%               | 67.9%          |
| Full Video Understanding Intermediate Success Rate | 34.2           | 40.0%              | 40.9%              | 41.2%               | 26.2%          |
| Temporal Reasoning Intermediate Success Rate       | 35.9           | 50.5%              | 50.9%              | 50.0%               | 38.9%          |
| Video Easy Intermediate Success Rate               | 39.5%          | 52.9%              | 52.2%              | 53.2%               | 47.1%          |
| Video Medium Intermediate Success Rate             | 39.4%          | 46.2%              | 50.4%              | 48.3%               | 46.6%          |
| Video Hard Intermediate Success Rate               | 32.2%          | 42.4%              | 40.7%              | 41.0%               | 26.1%          |"
127,"| Model      | # Param. (M) | Val. Loss ( $ \downarrow $ ) | # Param. (M) | Val. Loss ( $ \downarrow $ ) |
| ---------- | ------------ | ---------------------------- | ------------ | ---------------------------- |
| $ T=1024 $ | $ T=4096 $   | $ T=1024 $                   | $ T=4096 $   |                              |
| RetNet     | 129.1        | 3.569                        | 3.492        | 373.2                        |
| GLA        | 123.8        | 3.381                        | 3.364        | 361.1                        |
| RWKV       | 124.4        | 3.291                        | 3.276        | 354.8                        |
| Mamba      | 129.2        | 3.238                        | 3.231        | 371.5                        |
| LLaMA      | 124.4        | 3.247                        | 3.273        | 357.7                        |
| Longhorn   | 128.6        | 3.225                        | 3.192        | 369.8                        |"
127,"| Model             | State Size             | PIQA              | Hella             | Wino.                  | ARC-e             | ARC-c                  | OBQA              | SIQA  | BoolQ | Avg.  |
| ----------------- | ---------------------- | ----------------- | ----------------- | ---------------------- | ----------------- | ---------------------- | ----------------- | ----- | ----- | ----- |
| acc  $ \uparrow $ | acc_norm  $ \uparrow $ | acc  $ \uparrow $ | acc  $ \uparrow $ | acc_norm  $ \uparrow $ | acc  $ \uparrow $ | acc_norm  $ \uparrow $ | acc  $ \uparrow $ |       |       |       |
| LLaMA             | 8M                     | 55.08             | 55.36             | 71.73                  | 59.26             | 32.19                  | 43.35             | 45.16 | 62.13 | 53.03 |
| GLA               | 512K                   | 55.55             | 49.10             | 71.12                  | 58.86             | 28.11                  | 41.67             | 44.91 | 59.21 | 51.07 |
| Mamba             | 64K                    | 54.21             | 53.61             | 71.67                  | 61.05             | 30.15                  | 43.94             | 44.18 | 59.22 | 52.25 |
| Longhorn          | 64K                    | 55.78             | 52.30             | 71.00                  | 60.63             | 29.53                  | 43.55             | 44.68 | 61.29 | 52.35 |"
127,"| Model            | # Param | Top-1 Accuracy |
| ---------------- | ------- | -------------- |
| ViM-Tiny         | 7M      | 76.1           |
| ViL-Tiny (ours)  | 7M      | $ 76.4 $       |
| ViM-Small        | 26M     | 80.5           |
| ViL-Small (ours) | 26M     | $ 80.7 $       |"
127,"| Method   | Online Learning Objective  $ L_{t}(s) $  (assume  $ x_{t} \in \mathbb{R} $ )                                                                                             | Online Update                                                                                                                                                                                   |
| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| LA       | $ \|S - S_{t-1}\|_{F}^{2} - 2\langle Sk_{t}, x_{t} \rangle $                                                                                                             | $ S_{t} = S_{t-1} + x_{t} \otimes k_{t} $                                                                                                                                                       |
| RetNet   | $ \gamma \left\|S - S_{t-1}\right\|^{2} + (1 - \gamma) \left\|S\right\|_{F}^{2} - 2\langle Sk_{t}, x_{t} \rangle $                                                       | $ S_{t} = \gamma S_{t-1} + x_{t} \otimes k_{t} $                                                                                                                                                |
| GLA      | $ \|S - S_{t-1}\mathrm{diag}(\alpha_{t})\|_{F}^{2} + 2\langle Sk_{t}, x_{t} \rangle $                                                                                    | $ S_{t} = S_{t-1}\mathrm{diag}(\alpha_{t}) + x_{t} \otimes k_{t} $                                                                                                                              |
| Griffin  | $ \left\|\sqrt{\alpha_{t}} \odot (s - s_{t-1})\right\|^{2} + \left\|\sqrt{1 - \alpha_{t}} \odot s\right\|^{2} - 2\sqrt{1 - \alpha_{t}} \odot s \odot i_{t} \odot x_{t} $ | $ s_{t} = \alpha_{t} \odot s_{t-1} + \sqrt{(1 - \alpha_{t})} \odot i_{t} \odot x_{t} $                                                                                                          |
| Longhorn | $ \|S - S_{t-1}\|_{F}^{2} + \left\|Sk_{t} - x_{t}\right\|_{diag(\beta_{t})}^{2} $                                                                                        | $ S_{t} = (1_{m \times n} - \varepsilon_{t} \otimes k_{t}^{\odot2}) \odot S_{t-1} + (\varepsilon_{t} \odot x_{t}) \odot k_{t}, \varepsilon_{t} = \beta_{t}/(1 + \beta_{t}k_{t}^{\odot2}k_{t}) $ |"
128,"| Target Model         | Method | Train ASR  $ \uparrow $  (%) | Test ASR  $ \uparrow $  (%) | Perplexity  $ \downarrow $ |
| -------------------- | ------ | ---------------------------- | --------------------------- | -------------------------- |
| ASR@10               | ASR@1  | ASR@10                       | ASR@1                       |                            |
| Vicuna-13b-v1.5      | ReMiss | 96.2                         | 73.1                        | 94.2                       |
| AdvPrompter          | 81.1   | 48.7                         | 67.5                        | 19.5                       |
| AutoDAN              | 85.1   | 45.3                         | 78.4                        | 23.1                       |
| GCG                  | 84.7   | 49.6                         | 81.2                        | 29.4                       |
| Vicuna-7b-v1.5       | ReMiss | 96.5                         | 77.6                        | 98.1                       |
| AdvPrompter          | 93.3   | 56.7                         | 87.5                        | 33.4                       |
| AutoDAN              | 85.3   | 53.2                         | 84.9                        | 63.2                       |
| GCG                  | 86.3   | 55.2                         | 82.7                        | 36.7                       |
| Llama2-7b-chat       | ReMiss | 14.7                         | 13.1                        | 10.6                       |
| AdvPrompter          | 17.6   | 8.0                          | 7.7                         | 1.0                        |
| AutoDAN              | 4.1    | 1.5                          | 2.1                         | 1.0                        |
| GCG                  | 0.3    | 0.3                          | 2.1                         | 1.0                        |
| Mistral-7b-instruct  | ReMiss | 99.0                         | 91.3                        | 100.0                      |
| AdvPrompter          | 97.1   | 69.6                         | 96.1                        | 54.3                       |
| AutoDAN              | 89.4   | 65.6                         | 86.5                        | 51.9                       |
| GCG                  | 98.5   | 56.6                         | 99.0                        | 46.2                       |
| Llama3.1-8b-instruct | ReMiss | 43.6                         | 31.4                        | 40.4                       |
| AdvPrompter          | 29.8   | 16.3                         | 27.9                        | 7.7                        |"
129,"| Grant        | Dataset            | Time (s)          | Loss ↓        | MAE ↓         | ROC-AUC ↑     | AP ↑ |
| ------------ | ------------------ | ----------------- | ------------- | ------------- | ------------- | ---- |
| ✓            | QM9                | 9654.81           | 2.0444        | 0.0051±0.0009 | -             | -    |
| ZINC         | 33033.82           | 3.1160            | 0.0048±0.0004 | -             | -             |      |
| ogbg-molhiv  | 2163.50            | 0.1266            | -             | 0.7572±0.0005 | -             |      |
| ogbg-molpcba | 130191.26          | 0.0577            | -             | -             | 0.3270±0.0000 |      |
| gen-reg      | 3344.78            | 0.0086            | 0.0007±0.0001 | -             | -             |      |
| gen-cls      | 11662.25           | 0.1314            | -             | 0.9150±0.0024 | -             |      |
| ✓            | QM9                | 6392.26 (-33.79%) | 2.0436        | 0.0051±0.0009 | -             | -    |
| ZINC         | 20935.24 (-36.62%) | 3.1165            | 0.0048±0.0004 | -             | -             |      |
| ogbg-molhiv  | 1457.39 (-32.64%)  | 0.1238            | -             | 0.7676±0.0036 | -             |      |
| ogbg-molpcba | 80465.06 (-38.19%) | 0.0577            | -             | -             | 0.3358±0.0001 |      |
| gen-reg      | 2308.97 (-30.97%)  | 0.0086            | 0.0007±0.0001 | -             | -             |      |
| gen-cls      | 6145.72 (-47.30%)  | 0.1314            | -             | 0.9157±0.0013 | -             |      |
| S            | QM9                | 7076.37 (-26.71%) | 2.0443        | 0.0051±0.0009 | -             | -    |
| ZINC         | 22265.83 (-32.60%) | 3.1170            | 0.0048±0.0004 | -             | -             |      |
| ogbg-molhiv  | 1597.69 (-26.15%)  | 0.1421            | -             | 0.7705±0.0027 | -             |      |
| ogbg-molpcba | 89858.65 (-30.98%) | 0.0575            | -             | -             | 0.3351±0.0025 |      |
| gen-reg      | 2337.46 (-30.12%)  | 0.0086            | 0.0007±0.0001 | -             | -             |      |
| gen-cls      | 8171.21 (-29.93%)  | 0.1313            | -             | 0.9157±0.0014 | -             |      |"
132,"| Methods   | Node prompt | Time prompt | NCN       | TCN    | Node classification | Transductive Link Prediction | Inductive Link Prediction |
| --------- | ----------- | ----------- | --------- | ------ | ------------------- | ---------------------------- | ------------------------- |
| Wikipedia | Reddit      | MOOC        | Wikipedia | Reddit | MOOC                | Wikipedia                    | Reddit                    |
| Variant 1 | ✗           | ✗           | ✗         | ✗      | 67.00               | 53.64                        | 59.27                     |
| Variant 2 | ✓           | ✗           | ✗         | ✗      | 72.59               | 61.82                        | 63.50                     |
| Variant 3 | ✗           | ✓           | ✗         | ✗      | 73.22               | 62.51                        | 62.59                     |
| Variant 4 | ✓           | ✓           | ✗         | ✗      | 72.25               | 63.11                        | 62.87                     |
| Variant 5 | ✓           | ✗           | ✓         | ✗      | 81.40               | 73.12                        | 77.15                     |
| Variant 6 | ✗           | ✓           | ✗         | ✓      | 80.34               | 72.59                        | 76.16                     |
| DYGPROMPT | ✓           | ✓           | ✓         | ✓      | 82.09               | 73.50                        | 77.78                     |"
131,"| Your task is to classify the image into one classes: {ID classes, none of these classes} and assign confidence to each class.                                                                                                                                  | Task Description |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------- |
| You can classify the image into &#x27;none of these classes&#x27;: if you cannot classify the image into ID classes, if you are not sure whether the image belongs to one of the ID classes, or if you think you need other classes other than the ID classes. | Rejection Class  |
| The following are guidelines for your response. Please respond according to these guidelines. You should provide your confidence for each class between 0.00 and 100.00. ••• Strictly follow the guidelines above.                                             | Guidelines       |
| Here is example of your response. Please respond with the following examples format: Prediction: car                                                                                                                                                           | Examples         |
| Confidence: {airplane: 6.34, car: 73.07, bird: 12.72, cat: 54.94, deer: 23.03, •••, none of these classes: 1.29} •••                                                                                                                                           | ••               |"
131,"| Models            | ID              | Near-OoD | iNaturalist                | Far-OoD        | Openimage-O    | All OoD        |
| ----------------- | --------------- | -------- | -------------------------- | -------------- | -------------- | -------------- |
| IN200             | NINCO           | SSB-Hard | Textures                   |                |                |                |
| Valid             | ACC (↑)         |          | FPR@95%TPR (↓) / AUROC (↑) |                |                |                |
| SCALE**           | -               | 86.37    | 84.84                      |                |                | 93.98          |
| fDBD**            | -               | 86.37    | 84.27                      |                |                | 93.45          |
| AugMix+ASH**      | -               | 87.01    | 55.83 / 85.74              | 71.22 / 80.00  | 19.14 / 95.81  | 21.00 / 95.67  |
| OpenCLIP          | 100.00 (23,031) | 87.41    | 62.27 / 85.31              | 71.48 / 78.36  | 42.76 / 92.49  | 47.83 / 89.62  |
| GPT-4o            | 85.49 (19,689)  | 89.78    | 22.30 / 92.08              | 38.95 / 81.41  | 2.06 / 97.58   | 7.45 / 95.85   |
| Claude 3.5 Sonnet | 80.39 (18,515)  | 86.06    | 52.92 / 72.18              | 78.41 / 58.09  | 9.23 / 94.93   | 10.17 / 94.28  |
| Gemini Pro 1.5    | 91.92 (21,170)  | 88.84    | 21.55 / 89.03              | 55.24 / 77.40  | 1.53 / 97.73   | 5.12 / 95.61   |
| LLaVA-v1.6        | 71.63 (16,496)  | 2.45     | 100.00 / 50.85             | 100.00 / 48.95 | 100.00 / 50.05 | 100.00 / 59.26 |
| GLM-4v            | 89.00 (20,498)  | 69.41    | 100.00 / 79.23             | 100.00 / 74.35 | 100.00 / 83.01 | 100.00 / 83.45 |
| InternVL2-26B     | 62.68 (14,436)  | 90.22    | 82.59 / 58.32              | 94.21 / 52.51  | 36.69 / 81.26  | 28.08 / 85.56  |
| InternVL2-76B     | 97.36 (22,424)  | 88.30    | 100.00 / 72.27             | 100.00 / 62.39 | 100.00 / 95.57 | 100.00 / 91.62 |"
131,"|               |               | ID                   | Near-OoD                                                                               | iNaturalist           | Far-OoD               | OpenImage-O           |
| ------------- | ------------- | -------------------- | -------------------------------------------------------------------------------------- | --------------------- | --------------------- | --------------------- |
| IN200         | NINCO         | SSB-Hard             | Textures                                                                               | OpenImage-O           |                       |                       |
| Models        | Valid         | ACC ( $ \uparrow $ ) | FPR@90%TPR ( $ \downarrow $ ) / FPR@95%TPR ( $ \downarrow $ ) / AUROC ( $ \uparrow $ ) |                       |                       |                       |
| InternVL2-26B | 61.01 (2,544) | 91.23                | 82.73 / 82.73 / 58.31                                                                  | 94.34 / 94.34 / 52.51 | 38.03 / 38.03 / 80.66 | 28.86 / 28.86 / 85.25 |
| + GPT-text    | 69.88 (2,914) | 89.58                | 69.44 / 69.44 / 62.17                                                                  | 85.65 / 85.73 / 53.55 | 26.82 / 28.00 / 84.72 | 29.20 / 29.20 / 83.51 |
| + ReGuide     | 86.14 (3,592) | 93.53                | 22.39 / 22.89 / 86.53                                                                  | 15.21 / 15.21 / 90.41 | 1.39 / 1.39 / 98.02   | 3.93 / 3.93 / 97.05   |
| InternVL2-76B | 97.26 (4,056) | 89.09                | 51.28 / 51.28 / 71.89                                                                  | 71.02 / 71.02 / 62.02 | 2.20 / 2.20 / 96.43   | 10.76 / 10.76 / 92.15 |
| + ReGuide     | 95.80 (3,995) | 90.93                | 8.05 / 56.36 / 91.35                                                                   | 14.58 / 66.65 / 87.65 | 0.00 / 59.75 / 95.35  | 4.08 / 60.00 / 93.38  |
| GPT-4o        | 87.58 (3,652) | 90.64                | 8.57 / 14.76 / 93.96                                                                   | 29.25 / 34.50 / 82.28 | 0.81 / 1.83 / 98.11   | 5.60 / 6.47 / 95.37   |
| + ReGuide     | 79.57 (3,318) | 91.59                | 0.49 / 18.72 / 96.76                                                                   | 7.53 / 31.17 / 92.56  | 0.00 / 17.05 / 97.08  | 1.32 / 26.43 / 95.96  |"
131,"| Models        | Baseline               | +ReGuide               |
| ------------- | ---------------------- | ---------------------- |
| InternVL2-26B | 26.88% (601 / 2,236)   | 89.13% (2,651 / 2,974) |
| InternVL2-76B | 55.54% (2,039 / 3,671) | 91.07% (3,297 / 3,620) |
| GPT-4o        | 83.92% (2,751 / 3,278) | 95.97% (2,885 / 3,006) |"
132,"| Methods        | Node Classification | Transductive Link Prediction | Inductive Link Prediction |
| -------------- | ------------------- | ---------------------------- | ------------------------- |
| Wikipedia      | Reddit              | MOOC                         | Genre                     |
| GCN-ROLAND     | 58.86±10.3          | 48.25±9.57                   | 49.93±6.74                |
| GAT-ROLAND     | 62.81±9.88          | 47.95±8.42                   | 50.01±6.34                |
| TGAT           | 67.00±5.35          | 53.64±5.50                   | 59.27±4.43                |
| TGN            | 50.61±13.6          | 49.54±6.23                   | 50.33±4.47                |
| TREND          | 69.92±9.27          | 64.85±4.71                   | 66.79±5.44                |
| GRAPHMixer     | 65.43±4.21          | 60.21±5.36                   | 63.72±4.98                |
| DDGCL          | 65.15±4.54          | 55.21±6.19                   | 62.34±5.13                |
| CPDG           | 43.56±6.41          | 65.92±6.25                   | 50.32±5.06                |
| GRAPHPrompt    | 73.78±5.62          | 60.89±6.37                   | 64.60±5.76                |
| PROG           | 60.86±7.43          | 68.60±5.64                   | 63.18±4.79                |
| TGAT-TIGPROMPT | 69.21±8.88          | 67.70±9.64                   | 73.90±6.68                |
| TGN-TIGPROMPT  | 44.80±5.45          | 63.75±5.60                   | 55.42±3.60                |
| TGAT-DYGPROMPT | 82.09±6.43          | 73.50±6.47                   | 77.78±5.08                |
| TGN-DYGPROMPT  | 74.47±3.44          | 74.00±3.10                   | 69.06±3.89                |"
135,"| Method                            | Sparsity Mean | Sparsity Std | Perplexity | PIQA  | HellaS. | WinoG. | ARC-E | ARC-C | Average |
| --------------------------------- | ------------- | ------------ | ---------- | ----- | ------- | ------ | ----- | ----- | ------- |
| Uniform Allocation                | 30%           | 0%           | 9.06       | 65.18 | 55.31   | 63.69  | 52.36 | 30.80 | 53.47   |
| Global Sparsity Allocation (Ours) | 30%           | 26.72%       | 7.51       | 71.40 | 63.26   | 67.32  | 63.26 | 38.73 | 60.79   |
| OWL Yin et al. (2023)             | 30%           | 4.46%        | 6.9        | 68.17 | 59.12   | 65.67  | 56.9  | 33.36 | 56.64   |"
140,"| Pseudo-Label ACC | CIFAR10  | CIFAR100              |
| ---------------- | -------- | --------------------- |
| CIFAR10          | CIFAR100 | Pruning Rate          |
| 92.5%            | 66.3%    | ELFS (DINO + TEMI)    |
| 93.7%            | 60.6%    | ELFS (DINO + SCAN)    |
| 88.3%            | 57.7%    | ELFS (DINO + K-Means) |
|                  | -        | Best Label-Free       |
| -                | Random   | 94.3                  |"
132,"| Pre-training Backbone | Downstream Adaptation | Node classification | Transductive link prediction | Inductive link prediction |
| --------------------- | --------------------- | ------------------- | ---------------------------- | ------------------------- |
| Wikipedia             | Reddit                | MOOC                | Wikipedia                    | Reddit                    |
| DYREP                 | -                     | 50.61               | 49.54                        | 50.33                     |
| DYGPROMPT             | 53.52                 | 50.98               | 51.62                        | 91.64                     |
| JODIE                 | -                     | 51.37               | 49.80                        | 50.53                     |
| DYGPROMPT             | 62.84                 | 60.93               | 67.84                        | 63.56                     |
| TGAT                  | -                     | 67.00               | 53.64                        | 59.27                     |
| DYGPROMPT             | 82.09                 | 73.50               | 77.78                        | 69.88                     |
| TGN                   | -                     | 50.61               | 49.54                        | 50.33                     |
| DYGPROMPT             | 74.47                 | 74.00               | 69.06                        | 94.33                     |
| TREND                 | -                     | 69.92               | 64.85                        | 66.79                     |
| DYGPROMPT             | 70.15                 | 65.24               | 67.58                        | 64.35                     |
| GraphMixer            | -                     | 65.43               | 60.21                        | 63.72                     |
| DYGPROMPT             | 66.39                 | 61.42               | 64.18                        | 60.25                     |"
133,"| Agent Index        | Agent 1                    | Agent 2                     | Agent 3                    | Agent 4                    |
| ------------------ | -------------------------- | --------------------------- | -------------------------- | -------------------------- |
| Metric             | AP@50                      | AP@50                       | MIoU                       | MIoU                       |
| Downstream Task    | Object Det                 | Object Det                  | Static Seg                 | Dynamic Seg                |
| Sensor Modality    | Lidar                      | Camera                      | Lidar                      | Lidar                      |
| Backbone           | SECOND                     | EfficientNet-b0             | PointPillar                | PointPillar                |
| Feature Resolution | 64  $ \times $  64         | 128  $ \times $  128        | 128  $ \times $  128       | 128  $ \times $  128       |
| Channel Size       | 256                        | 64                          | 64                         | 64                         |
| Fusion Method      | Window Attention           | Pyramid Fusion              | Pyramid Fusion             | Pyramid Fusion             |
| Non-Collab         | 0.941                      | 0.399                       | 0.548                      | 0.675                      |
| Collab w/o. CFA    | 0.909 $ \downarrow $ 0.032 | 0.399 $ \rightarrow $ 0.000 | 0.114 $ \downarrow $ 0.434 | 0.070 $ \downarrow $ 0.605 |
| STAMP (ours)       | 0.936 $ \downarrow $ 0.005 | 0.760 $ \uparrow $ 0.362    | 0.624 $ \uparrow $ 0.076   | 0.690 $ \uparrow $ 0.014   |"
134,"| Model                         | Agg. | AlpacaEval 2.0      | Arena-Hard          | MT-Bench            |
| ----------------------------- | ---- | ------------------- | ------------------- | ------------------- |
| LC win.                       | win. | win.                | Avg.                | 1st turn            |
| MoA w/ GPT-4o                 | 83.3 | 65.7  $ \pm $  0.7% | 78.7  $ \pm $  0.2% | 90.3  $ \pm $  0.5% |
| MoA                           | 78.3 | 65.1  $ \pm $  0.6% | 59.8  $ \pm $  0.3% | 77.4  $ \pm $  0.5% |
| GPT-4 Turbo (04/09)           | 76.7 | 55.0%               | 46.1%               | 82.0                |
| GPT-4 Omni (05/13)            | 76.2 | 57.5%               | 51.3%               | 79.2                |
| MoA-Lite                      | 74.1 | 59.3  $ \pm $  0.2% | 57.0  $ \pm $  0.7% | 71.3  $ \pm $  0.7% |
| GPT-4 Preview (11/06)         | 73.6 | 50.0%               | 50.0%               | 78.7                |
| WizardLM 8x22B $ ^{\dagger} $ | 70.1 | 51.3%               | 62.3%               | 71.3                |
| MoA-Lite-Single               | 64.7 | 47.8%               | 37.9%               | 59.5                |
| Qwen1.5 110B Chat             | 63.3 | 43.9%               | 33.8%               | 56.4                |
| Llama 3 70B Instruct          | 56.8 | 34.4%               | 33.2%               | 46.6                |
| GPT-4 (03/14)                 | 53.9 | 35.3%               | 22.1%               | 37.9                |
| Qwen1.5 72B Chat              | 52.4 | 36.6%               | 26.5%               | 36.1                |"
134,"| Setting   | Multiple-Proposer | Single-Proposer |
| --------- | ----------------- | --------------- |
| $ n = 6 $ | 61.3%             | 56.7%           |
| $ n = 3 $ | 58.0%             | 56.1%           |
| $ n = 2 $ | 58.8%             | 54.5%           |
| $ n = 1 $ | 47.8%             | 47.8%           |"
135,"| Model                                      | Compress.                          | Method | ARC-e | ARC-c | PIQA  | WinoG. | HellaS. | Average |
| ------------------------------------------ | ---------------------------------- | ------ | ----- | ----- | ----- | ------ | ------- | ------- |
| LLAMA-2 7B                                 | 0%                                 | Dense  | 74.58 | 46.25 | 79.11 | 69.06  | 75.99   | 69.00   |
| 30%                                        | ShortGPT (Men et al., 2024)        | 48.65  | 32.85 | 64.31 | 64.33 | 56.13  | 53.25   |         |
| SliceGPT (Ashkboos et al., 2024)           | 58.88                              | 33.36  | 68.55 | 58.01 | 49.86 | 53.73  |         |         |
| LLM surgeon (van der Ouderaa et al., 2023) | 63.09                              | 36.69  | 73.56 | 61.09 | 60.72 | 59.03  |         |         |
| MoDeGPT (ours)                             | 63.26                              | 38.73  | 70.40 | 67.32 | 63.26 | 60.78  |         |         |
| MoDeGPT-Alpaca (ours)                      | 65.49                              | 39.16  | 73.34 | 66.22 | 65.90 | 62.02  |         |         |
| 40%                                        | ShortGPT (Men et al., 2024)        | 41.16  | 29.94 | 60.12 | 60.46 | 43.67  | 47.07   |         |
| SliceGPT (Ashkboos et al., 2024)           | 36.49                              | 24.57  | 54.90 | 53.43 | 34.80 | 40.84  |         |         |
| LLM surgeon (van der Ouderaa et al., 2023) | 52.31                              | 30.29  | 69.26 | 54.38 | 48.04 | 50.86  |         |         |
| MoDeGPT (ours)                             | 49.45                              | 30.03  | 64.96 | 61.96 | 53.01 | 51.88  |         |         |
| MoDeGPT-Alpaca (ours)                      | 59.76                              | 34.73  | 70.35 | 64.40 | 58.63 | 57.58  |         |         |
| LLAMA-3 8B                                 | 0%                                 | Dense  | 77.69 | 53.58 | 80.63 | 72.69  | 79.16   | 72.75   |
| 25%                                        | ShortGPT-Alpaca (Men et al., 2024) | 38.13  | 31.40 | 60.94 | 54.22 | 31.52  | 43.24   |         |
| SliceGPT-Alpaca (Ashkboos et al., 2024)    | 44.44                              | 29.27  | 57.56 | 58.48 | 41.08 | 46.17  |         |         |
| MoDeGPT-Alpaca (ours)                      | 67.05                              | 41.13  | 75.52 | 69.61 | 66.49 | 63.96  |         |         |"
135,"| Module Type        | I            | II        | III          |
| ------------------ | ------------ | --------- | ------------ |
| Weight Matrices    | up,down,gate | key,query | value,output |
| Associated Decomp. | Nyström      | CR        | SVD          |
| # Nonlinearities   | 1            | 2         | 0            |
| Compression Alg.   | Alg. 1       | Alg. 2    | Alg. 3       |"
135,"| Type        | I        | II        | III    |
| ----------- | -------- | --------- | ------ |
| Parameters  | MLP      | K-Q       | V-O    |
| Method      | Nyström  | CR        | SVD    |
| Size Ratio  | 66.84%   | 16.58%    | 16.58% |
| Complexity  | O(d3int) | O(d3/H 2) | O(Hd3) |
| Effective r | 0.094    | 0.121     | 0.095  |
| Time        | 1h13m    | 0h36m     | 2h26m  |"
140,"| Pruning Rate             | 30%  | 50%  | 70%  | 80%  | 90%  |
| ------------------------ | ---- | ---- | ---- | ---- | ---- |
| CCS (Pseudo-Label)       | 75.5 | 72.9 | 67.2 | 60.9 | 51.8 |
| D2 (Pseudo-Label)        | 74.8 | 70.9 | 61.2 | 49.6 | 27.7 |
| ELFS (Pseudo-Label)      | 76.8 | 73.6 | 68.4 | 62.3 | 54.9 |
| CCS (Ground-Truth Label) | 77.1 | 74.5 | 68.9 | 64.0 | 56.0 |"
141,"| Method                           | PSNR $ \uparrow $ | 1-LPIPS $ \uparrow $ | MD $ \downarrow $ |
| -------------------------------- | ----------------- | -------------------- | ----------------- |
| VFD-Bench                        |                   |                      |                   |
| DiffEditor (Mou et al., 2024)    | 16.23             | 0.67                 | 43.35             |
| DragDiffusion (Shi et al., 2024) | 17.55             | 0.76                 | 38.42             |
| DragNoise (Liu et al., 2024)     | 16.58             | 0.71                 | 40.52             |
| FreeDrag (Ling et al., 2023)     | 17.38             | 0.72                 | 42.78             |
| GoodDrag (Zhang et al., 2024)    | 18.14             | 0.79                 | 35.31             |
| FlowDrag (Ours)                  | 18.55             | 0.82                 | 28.23             |"
142,"| Methods                                | W/A | MobileNet-v2 | ResNet-18 | MnasNet2.0 |
| -------------------------------------- | --- | ------------ | --------- | ---------- |
| Case 1.1.1 (AdaRound, last PTQ SOTAs)  | 3/2 | 0.32         | 41.65     | 1.07       |
| Case 1.1.2 (NWQ, current PTQ SOTAs)    | 3/2 | 38.92        | 60.82     | 52.17      |
| Case 1.2 (LSQ, QAT&#x27;s SOTA on PTQ) | 3/2 | 39.65        | 60.26     | 49.78      |
| Case 2.1                               | 3/2 | 38.77        | 59.90     | 48.40      |
| Case 2.2                               | 3/2 | 42.60        | 61.06     | 54.19      |
| Case 2.3                               | 3/2 | 41.42        | 60.86     | 49.33      |"
142,"| Methods                       | W/A   | Mobile-v2 | Res-18 | Reg-600 | Mnas2.0 |
| ----------------------------- | ----- | --------- | ------ | ------- | ------- |
| Full Prec.                    | 32/32 | 72.49     | 71.08  | 73.71   | 76.68   |
| AdaRound(Nagel et al., 2020)  | 4/4   | 64.33     | 69.36  | -       | -       |
| AdaQuant(Hubara et al., 2021) | 4/4   | 47.16     | 69.60  | -       | -       |
| BRECQ(Li et al., 2021)        | 4/4   | 66.57     | 69.60  | 68.33   | 73.56   |
| QDROP(Wei et al., 2022b)      | 4/4   | 68.84     | 69.62  | 71.18   | 73.71   |
| PD-Quant (Liu et al., 2023a)  | 4/4   | 68.33     | 69.30  | 71.04   | 73.30   |
| MRECG (Ma et al., 2023)       | 4/4   | 68.84     | 69.46  | 71.22   | -       |
| NWQ (Wang et al., 2022a)      | 4/4   | 69.14     | 69.85  | 71.92   | 74.60   |
| AdaQTransformNWQ(ours)        | 4/4   | 70.01     | 69.88  | 71.97   | 74.80   |
| BRECQ(Li et al., 2021)        | 3/3   | 23.41     | 65.87  | 55.16   | 49.78   |
| QDROP(Wei et al., 2022b)      | 3/3   | 57.98     | 66.75  | 65.54   | 66.81   |
| PD-Quant (Liu et al., 2023a)  | 3/3   | 57.64     | 66.12  | 65.09   | 64.88   |
| MRECG (Ma et al., 2023)       | 3/3   | 58.40     | 66.30  | 66.08   | -       |
| NWQ (Wang et al., 2022a)      | 3/3   | 61.24     | 67.58  | 67.38   | 68.85   |
| AdaQTransformNWQ(ours)        | 3/3   | 63.44     | 67.73  | 67.81   | 69.52   |
| BRECQ(Li et al., 2021)        | 2/2   | 0.24      | 42.54  | 3.58    | 0.61    |
| QDROP(Wei et al., 2022b)      | 2/2   | 13.05     | 54.72  | 41.47   | 28.77   |
| PD-Quant (Liu et al., 2023a)  | 2/2   | 13.67     | 53.14  | 40.92   | 28.03   |
| MRECG (Ma et al., 2023)       | 2/2   | 14.44     | 54.46  | 43.67   | -       |
| NWQ (Wang et al., 2022a)      | 2/2   | 26.42     | 59.14  | 48.49   | 41.17   |
| AdaQTransformNWQ(ours)        | 2/2   | 32.19     | 60.12  | 51.20   | 44.54   |"
142,"| Methods                              | W/A   | ViT-S | ViT-B | DeiT-S | DeiT-B |
| ------------------------------------ | ----- | ----- | ----- | ------ | ------ |
| FP32                                 | 32/32 | 81.39 | 84.54 | 79.80  | 81.80  |
| RepQ-ViT (Li et al., 2023)           | 6/6   | 80.43 | 83.62 | 78.90  | 81.27  |
| AdaQTransform $ _{\text{RepQ-ViT}} $ | 6/6   | 80.59 | 83.89 | 79.12  | 81.53  |
| PTQ4ViT (Yuan et al., 2022)          | 4/4   | 42.57 | 30.69 | 34.08  | 64.39  |
| APQ-ViT (Ding et al., 2022)          | 4/4   | 47.95 | 41.41 | 43.55  | 67.48  |
| NWQ (Wang et al., 2022a)             | 4/4   | 57.79 | 56.87 | 65.76  | 76.06  |
| AdaQTransform $ _{\text{NWQ}} $      | 4/4   | 58.12 | 57.24 | 66.34  | 76.20  |
| RepQ-ViT (Li et al., 2023)           | 4/4   | 65.05 | 68.48 | 69.03  | 75.61  |
| AdaQTransform $ _{\text{RepQ-ViT}} $ | 4/4   | 70.40 | 76.47 | 73.50  | 78.93  |"
144,"| MODEL     | SEMEVAL 2021 | SEMEVAL 2020 | OOGIRI-GO |
| --------- | ------------ | ------------ | --------- |
| 2T1       | 2T1(HARD)    | 3T1          | 4T1       |
| GPT       | 40           | 85.09        | 60.77     |
| LLAMA3    | 8B           | 43.85        | 54.23     |
| 70B       | 93.60        | 58.08        | 39.81     |
| QWEN1.5   | 7B           | 62.04        | 52.02     |
| 14B       | 82.05        | 51.04        | 30.92     |
| 32B       | 68.01        | 52.57        | 35.38     |
| QWEN2     | 7B           | 56.55        | 50.63     |
| 57B       | 83.30        | 52.02        | 37.08     |
| QWEN2.5   | 32B          | 94.00        | 55.22     |
| BAICHUAN2 | 13B          | 51.70        | 52.71     |
| CLOT      | 7B           | 52.49        | 51.74     |
| QWQ       | 32B          | 80.05        | 53.06     |
| OURS      | 32B          | 96.58        | 57.45     |"
144,"| Model     | Chinese Benchmark |
| --------- | ----------------- |
| 2T1       | 2T1(hard)         |
| GPT       | 4o                |
| LLAMA3    | 8B                |
| 70B       | 59.48             |
| QWEN1.5   | 7B                |
| 14B       | 53.45             |
| 32B       | 52.71             |
| QWEN2     | 7B                |
| 57B       | 65.91             |
| QWEN2.5   | 32B               |
| Baichuan2 | 13B               |
| CLoT      | 7B                |
| QwQ       | 32B               |
| QwQ+LoL   | 32B               |
| OURS      | 32B               |"
146,"| Target distribution  $ Q_0 $                                                                          | Method                                                     | Num of steps                                                                    | Results                                                          |
| ----------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| $ \nabla \log q_t, s_t L $ -Lips.  $ \forall t $                                                      | ODE-based                                                  | $ \mathcal{O}\left(\frac{\sqrt{dL}^2}{\varepsilon}\right) $                     | (Chen et al., 2023c, Thm 3)                                      |
| $ \nabla \log q_t L $ -Lips.  $ \forall t $                                                           | DDPM accl.                                                 | $ \mathcal{O}\left(\frac{\sqrt{dL}^2}{\varepsilon}\right) $                     | (Huang et al., 2024b, Thm 4.4) $ ^\dagger $                      |
| $ |\partial_a^k s_t(x)| \leq L \forall x, t, a $  and  $ \forall k \leq p + 1, Q_0 $  Bounded Support | ODE                                                        | $ \mathcal{O}\left(\frac{d^{\frac{p+1}{p}}}{\varepsilon^{\frac{1}{p}}}\right) $ | (Huang et al., 2024a, Thm 3.10) $ ^\dagger $                     |
| $ \nabla^2 \log q_0 M $ -Lips.                                                                        | DDPM accl.                                                 | $ \mathcal{O}\left(\frac{d^{1.5} \log^{1.5} M}{\varepsilon}\right) $            | (This paper, Thm 4)                                              |
| $ Q_0 $  Gaussian Mixture                                                                             | DDPM accl.                                                 | $ \mathcal{O}\left(\frac{d^{1.5} N^{1.5}}{\varepsilon}\right) $                 | (This paper, Thm 2)                                              |
| $ Q_0 $  Bounded Support                                                                              | DDPM accl.                                                 | $ \mathcal{O}\left(\frac{d^3}{\varepsilon}\right) $                             | (Li et al., 2024c, Thm 4) (Li et al., 2024a, Thm 2) $ ^\dagger $ |
| ODE                                                                                                   | $ \mathcal{O}\left(\frac{d^3}{\sqrt{\varepsilon}}\right) $ | (Li et al., 2024c, Thm 2) (Li et al., 2024a, Thm 1) $ ^\dagger $                |                                                                  |
| ODE                                                                                                   | $ \mathcal{O}\left(\frac{d^2}{\varepsilon}\right) $        | (Li et al., 2024c, Thm 1)                                                       |                                                                  |
| $ Q_0 $  Finite Variance                                                                              | DDPM accl.                                                 | $ \mathcal{O}\left(\frac{d^{1.5}}{\varepsilon}\right) $                         | (This paper, Thm 3)                                              |"
150,"| Method      | COCO-Stuff10k | CityScapes | Param.  $ * $  (M) |
| ----------- | ------------- | ---------- | ------------------ |
| mIoU        | mAcc          | mIoU       | mAcc               |
| Full ft.    | 39.64         | 51.64      | 80.87              |
| Freeze      | 28.94         | 39.58      | 62.22              |
| AdaptFormer | 34.42         | 45.60      | 70.99              |
| ProPETL     | 37.03         | 49.19      | 77.75              |"
150,"| Method      | CityScapes | Param.  $ * $  (M) |
| ----------- | ---------- | ------------------ |
| mIoU        | mAcc       |                    |
| Full ft.    | 82.38      | 89.96              |
| Freeze      | 67.04      | 79.31              |
| AdaptFormer | 79.02      | 88.05              |
| ProPETL     | 80.24      | 89.10              |"
151,"| Notation        |
| --------------- |
| I               |
| r               |
| s               |
| D               |
| K               |
| [R|t]           |
| $ \mathcal{P} $ |"
147,"| N           | Method        | Params      | FLOPs         | Time    | DIV2K  | ImageNet |
| ----------- | ------------- | ----------- | ------------- | ------- | ------ | -------- |
| Cover-Stego | Secret-Reveal | Cover-Stego | Secret-Reveal |         |        |          |
| PSNR↑       | SSIM↑         | LPIPS↓      | PSNR↑         | SSIM↑   | LPIPS↓ | PSNR↑    |
| 2           | ISN           | 3.17M       | 414.1G        | 46.0MS  | 34.661 | 0.845    |
| DeepMIH     | 12.42M        | 426.5G      | 103.4MS       | 37.460  | 0.871  | 0.209    |
| IIS         | 22.30M        | 718.0G      | 180.4MS       | 34.619  | 0.845  | 0.592    |
| AIS(ours)   | 5.57M         | 186.0G      | 69.8MS        | 42.141  | 0.913  | 0.130    |
| 3           | ISN           | 3.34M       | 436.4G        | 49.41MS | 31.233 | 0.850    |
| DeepMIH     | 19.44M        | 676.4G      | 157.1MS       | 31.286  | 0.768  | 1.089    |
| IIS         | 33.40M        | 1076.4G     | 264.6MS       | 30.395  | 0.799  | 1.030    |
| AIS(ours)   | 5.76M         | 192.1G      | 87.5MS        | 34.721  | 0.904  | 0.396    |
| 4           | ISN           | 3.51M       | 458.6G        | 50.4MS  | 29.488 | 0.712    |
| DeepMIH     | 26.46M        | 926.3G      | 218.3MS       | 28.978  | 0.682  | 1.438    |
| IIS         | 44.51M        | 1434.7G     | 344.8MS       | 27.121  | 0.746  | 4.107    |
| AIS(ours)   | 5.95M         | 198.2G      | 102.3MS       | 34.947  | 0.887  | 0.608    |
| 5           | ISN           | 3.68M       | 480.9G        | 51.7MS  | 26.735 | 0.650    |
| DeepMIH     | 33.48M        | 1176.1G     | 269.1MS       | 29.477  | 0.692  | 1.594    |
| IIS         | 55.62M        | 1793.0G     | 439.6MS       | 26.676  | 0.741  | 3.194    |
| AIS(ours)   | 6.15M         | 204.3G      | 121.1MS       | 36.149  | 0.887  | 0.334    |"
148,"| Metric        | Number of Gaussians | Peak GPU memory [MB] | Training time [minutes] | Render speed [FPS] |
| ------------- | ------------------- | -------------------- | ----------------------- | ------------------ |
| Setting       | clean               | poisoned             | clean                   | poisoned           |
| NS-hotdog     | 0.185 M             | 4.272 M (23.09x↑)    | 3336                    | 47859* (14.35x↑)   |
| NS-lego       | 0.341 M             | 4.159 M (12.20x↑)    | 3532                    | 78852* (22.33x↑)   |
| NS-mic        | 0.205 M             | 3.940 M (19.22x↑)    | 3499                    | 61835* (17.67x↑)   |
| NS-ship       | 0.272 M             | 4.317 M (15.87x↑)    | 3692                    | 80956* (21.93x↑)   |
| TT-Courthouse | 0.604 M             | 3.388 M (5.61x↑)     | 11402                   | 29856* (2.62x↑)    |
| TT-Courthroom | 2.890 M             | 13.196 M (4.57x↑)    | 9896                    | 33871* (3.42x↑)    |
| TT-Museum     | 4.439 M             | 16.501 M (3.72x↑)    | 12704                   | 43317* (3.41x↑)    |
| TT-Playground | 2.309 M             | 10.306 M (4.46x↑)    | 8717                    | 27304* (3.13x↑)    |
| MIP-bicycle   | 5.793 M             | 25.268 M (4.36x↑)    | 17748                   | 63236* (3.56x↑)    |
| MIP-counter   | 1.195 M             | 11.167 M (9.34x↑)    | 10750                   | 80732* (7.51x↑)    |
| MIP-room      | 1.513 M             | 16.019 M (10.59x↑)   | 12316                   | 57540* (4.67x↑)    |
| MIP-stump     | 4.671 M             | 13.550 M (2.90x↑)    | 14135                   | 36181* (2.56x↑)    |"
148,"| Constrained Poison-splat attack with  $ \epsilon=16/255 $ |
| --------------------------------------------------------- |
| Metric                                                    |
| Setting Scene                                             |
| NS-hotdog                                                 |
| NS-lego                                                   |
| NS-materials                                              |
| NS-ship                                                   |
| TT-Courthouse                                             |
| TT-Courthouse                                             |
| TT-Museum                                                 |
| TT-Playground                                             |
| MIP-bicycle                                               |
| MIP-counter                                               |
| MIP-room                                                  |
| MIP-stump                                                 |"
148,"| Metric                                                                                   | Number of Gaussians | Peak GPU memory [MB]           | Training time [minutes] |
| ---------------------------------------------------------------------------------------- | ------------------- | ------------------------------ | ----------------------- |
|                                                                                          | clean               | poisoned                       | clean                   |
| Constrained Black-box Poison-splat attack with  $ \epsilon=16/255 $  against Scaffold-GS |                     |                                |                         |
| NS-lego                                                                                  | 0.414 M             | 2.074 M (5.01x $ \uparrow $ )  | 3003                    |
| NS-ship                                                                                  | 1.000 M             | 3.291 M (3.29x $ \uparrow $ )  | 3492                    |
| MIP-bonsai                                                                               | 4.368 M             | 10.608 M (2.43x $ \uparrow $ ) | 10080                   |
| MIP-stump                                                                                | 6.798 M             | 14.544 M (2.14x $ \uparrow $ ) | 7322                    |"
148,"| NS-lego    | 0.414 M | 3.973 M (9.60x $ \uparrow $ )  | 3003  | 6242 (2.08x $ \uparrow $ )  | 9.77  | 26.11 (2.67x $ \uparrow $ ) |
| ---------- | ------- | ------------------------------ | ----- | --------------------------- | ----- | --------------------------- |
| NS-ship    | 1.000 M | 4.717 M (4.72x $ \uparrow $ )  | 3492  | 6802 (1.95x $ \uparrow $ )  | 11.68 | 28.22 (2.42x $ \uparrow $ ) |
| MIP-bonsai | 4.368 M | 28.042 M (6.42x $ \uparrow $ ) | 10080 | 22115 (2.19x $ \uparrow $ ) | 35.33 | 78.36 (2.22x $ \uparrow $ ) |
| MIP-stump  | 6.798 M | 34.027 M (5.01x $ \uparrow $ ) | 7322  | 20797 (2.84x $ \uparrow $ ) | 33.53 | 79.64 (2.38x $ \uparrow $ ) |"
150,"| Method           | VOC2012 | ADE20k | COCO-Stuff10k | CityScapes | Mean  | Learnable Param.*(M) |
| ---------------- | ------- | ------ | ------------- | ---------- | ----- | -------------------- |
| mIoU             | mAcc    | mIoU   | mAcc          | mIoU       | mAcc  | mIoU                 |
| Full fine-tuning | 84.38   | 89.82  | 51.71         | 63.13      | 46.30 | 58.98                |
| Freeze           | 83.32   | 89.16  | 47.51         | 59.74      | 42.36 | 54.39                |
| BitFit           | 84.25   | 90.25  | 48.00         | 60.81      | 43.75 | 56.37                |
| LoRand           | 84.09   | 90.32  | 48.08         | 59.31      | 43.96 | 56.41                |
| RLRR             | 84.77   | 90.72  | 48.72         | 60.53      | 44.51 | 57.03                |
| VPT              | 85.69   | 91.12  | 50.35         | 61.37      | 45.04 | 57.58                |
| AdaptFormer      | 85.41   | 90.93  | 50.38         | 62.55      | 45.45 | 57.83                |
| LoRA             | 85.36   | 91.34  | 50.45         | 62.91      | 45.03 | 57.81                |
| ProPETL          | 86.11   | 92.08  | 51.05         | 63.61      | 46.48 | 58.69                |"
151,"| Dataset                     | Method         | Abs Rel ↓ | Sq Rel ↓ | RMSE ↓ | δ&lt;1.25 ↑ | δ&lt;1.25² ↑ | δ&lt;1.25³ ↑ |
| --------------------------- | -------------- | --------- | -------- | ------ | ----------- | ------------ | ------------ |
| NYUv2                       | Monodepth2 [3] | 0.171     | 0.144    | 0.622  | 0.746       | 0.941        | 0.985        |
| Ours                        | 0.166          | 0.139     | 0.616    | 0.759  | 0.943       | 0.985        |              |
| Ours (Distillation)         | 0.155          | 0.121     | 0.573    | 0.782  | 0.951       | 0.988        |              |
| 7-Scenes                    | HRDepth [4]    | 0.193     | 0.115    | 0.421  | 0.682       | 0.921        | 0.982        |
| Ours                        | 0.195          | 0.109     | 0.419    | 0.674  | 0.921       | 0.984        |              |
| Ours (Distillation)         | 0.183          | 0.096     | 0.389    | 0.706  | 0.931       | 0.986        |              |
| Booster                     | MonoViT [5]    | 0.418     | 0.327    | 0.504  | 0.425       | 0.679        | 0.888        |
| Ours                        | 0.408          | 0.302     | 0.482    | 0.414  | 0.677       | 0.916        |              |
| Ours (Distillation)         | 0.375          | 0.249     | 0.440    | 0.422  | 0.734       | 0.944        |              |
| Scannet-Reflection Test set | Monodepth2 [3] | 0.181     | 0.160    | 0.521  | 0.758       | 0.932        | 0.976        |
| Ours                        | 0.157          | 0.096     | 0.468    | 0.762  | 0.949       | 0.988        |              |
| Self-Teaching [6]           | 0.179          | 0.146     | 0.502    | 0.750  | 0.938       | 0.980        |              |
| 3D Distillation [1]         | 0.156          | 0.096     | 0.459    | 0.766  | 0.945       | 0.988        |              |
| Ours (Distillation)         | 0.150          | 0.087     | 0.446    | 0.777  | 0.955       | 0.990        |              |"
155,"| Natural Language Task Description                                                                                                                                                                                                                                                                | Background Information                                                                                                                            | Query                                                                |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- |
| Coffee company sources beans from three suppliers with fixed capacity, roasts them at two facilities into either dark or light coffee, and ships the roasted coffee to three retail locations...The company aims to minimize the total cost while fulfilling the demand at each retail location. | supplier_capacity =... coffee_needed_for_cafe =... shipping_cost_supplier_to_roastery =... roasting_cost =... shipping_cost_roastery_to_cafe =... | What would be the outcome if cafe1 experienced a 23% rise in demand? |"
155,"| Method                              | Coffee | Workforce | Facility | Task Allocation | Warehouse | Average |
| ----------------------------------- | ------ | --------- | -------- | --------------- | --------- | ------- |
| Direct $ _{GPT-4o} $                | 0.8    | 2.6       | 0.0      | 0.0             | 0.0       | 0.7     |
| Direct $ _{O1-PREVIEW} $            | 25.9   | 47.6      | 4.8      | 4.0             | 66.0      | 29.7    |
| CoT $ _{GPT-4o} $                   | 0.0    | 5.6       | 0.0      | 0.0             | 16.0      | 4.3     |
| Code $ _{GPT-4o} $                  | 17.7   | 75.8      | 53.9     | 0.0             | 8.0       | 31.1    |
| Code $ _{SMT_{GPT-4o}} $            | 0.0    | 10.8      | 0.6      | 0.0             | 2.0       | 2.7     |
| LLMFP $ _{GPT-4o} $                 | 64.7   | 92.2      | 70.7     | 96.0            | 72.0      | 79.1    |
| Direct $ _{CLAUDE 3.5 SONNET} $     | 0.0    | 0.0       | 0.0      | 0.0             | 0.0       | 0.0     |
| CoT $ _{CLAUDE 3.5 SONNET} $        | 7.1    | 0.0       | 0.0      | 0.0             | 14.0      | 4.2     |
| Code $ _{CLAUDE 3.5 SONNET} $       | 59.8   | 71.9      | 47.3     | 0.0             | 42.0      | 44.2    |
| Code $ _{SMT_{CLAUDE 3.5 SONNET}} $ | 75.6   | 36.8      | 49.7     | 86.0            | 64.0      | 62.4    |
| LLMFP $ _{CLAUDE 3.5 SONNET} $      | 80.5   | 88.7      | 48.2     | 96.0            | 90.0      | 80.7    |"
155,"| Method                    | Blocksworld | Mystery Blocksworld | Movie | Gripper | Average |
| ------------------------- | ----------- | ------------------- | ----- | ------- | ------- |
| DirectGPT-4o              | 41.5        | 0.8                 | 85.7  | 0.0     | 32.0    |
| DirectO1-PREVIEW          | 88.4        | 31.9                | 100.0 | 52.0    | 68.1    |
| CoTGPT-4o                 | 39.9        | 2.7                 | 81.0  | 0.0     | 30.9    |
| CodeGPT-4o                | 0.0         | 0.3                 | 0.0   | 0.0     | 0.1     |
| Code_SMTGPT-4o            | 0.0         | 0.0                 | 0.0   | 4.0     | 1.0     |
| LLMFP GPT-4o              | 96.2        | 77.7                | 100.0 | 76.0    | 87.5    |
| DirectCLAUDE 3.5 SONNET   | 43.2        | 0.5                 | 100.0 | 12.0    | 38.9    |
| CoTCLAUDE 3.5 SONNET      | 52.8        | 2.8                 | 100.0 | 28.0    | 45.9    |
| CodeCLAUDE 3.5 SONNET     | 0.0         | 0.0                 | 0.0   | 0.0     | 0.0     |
| Code_SMTCLAUDE 3.5 SONNET | 0.0         | 0.0                 | 0.0   | 0.0     | 0.0     |
| LLMFP CLAUDE 3.5 SONNET   | 93.0        | 98.0                | 100.0 | 76.0    | 91.8    |"
155,"| Domain              | No Definer | No Formulator | No Self Assess &amp; Modification | LLMFP |
| ------------------- | ---------- | ------------- | --------------------------------- | ----- |
| Coffee              | 8.6        | 56.4          | 55.3                              | 64.7  |
| Workforce           | 84.4       | 80.5          | 27.3                              | 92.2  |
| Facility            | 61.6       | 53.7          | 53.7                              | 70.7  |
| Task Allocation     | 74.0       | 92.0          | 96.0                              | 96.0  |
| Warehouse           | 90         | 2.0           | 54.0                              | 72.0  |
| Average             | 63.7       | 56.9          | 57.2                              | 79.1  |
| Blocksworld         | N/A        | 0.2           | 95.3                              | 96.2  |
| Mystery Blocksworld | N/A        | 0.0           | 74.4                              | 77.7  |
| Movie               | N/A        | 0.0           | 66.7                              | 100.0 |
| Gripper             | N/A        | 0.0           | 64.0                              | 76.0  |
| Average             | N/A        | 0.1           | 75.1                              | 87.5  |"
155,"| Method                            | Set 1 | Set 2 | Set 3 | Set 4 | Set 5 | Set 6 | Set 7 | Average |
| --------------------------------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ------- |
| LLMFP                             | 58.3  | 70.9  | 11.8  | 42.4  | 80.0  | 83.3  | 81.5  | 61.2    |
| LLMFP $ _{\text{TASK-SPECIFIC}} $ | 78.3  | 72.7  | 70.6  | 84.8  | 91.4  | 100.0 | 100.0 | 85.4    |"
157,"| Method              | Comprehensive Benchmark | VQA   | COCO Benchmark |
| ------------------- | ----------------------- | ----- | -------------- |
| MME $ ^{\text{P}} $ | MME $ ^{\text{C}} $     | SEED  | MM-Vet         |
| LLaVA-1.5           | 1510.7                  | 348.2 | 58.6           |
| + Vlfeedback        | 1432.7                  | 321.8 | 59.3           |
| + Human-Prefer      | 1490.6                  | 335.0 | 58.1           |
| + POVID             | 1452.8                  | 325.3 | 60.2           |
| + FiSAO             | 1522.6                  | 349.0 | 60.6           |
| InstructBlip        | 1237.5                  | 292.1 | 38.5           |
| + Vlfeedback        | 1241.3                  | 298.9 | 40.4           |
| + Human-Prefer      | 1250.9                  | 304.2 | 39.3           |
| + POVID             | 1255.1                  | 301.8 | 38.3           |
| + FiSAO             | 1305.3                  | 308.9 | 40.0           |"
157,"| Method            | MME $ ^{\text{P}} $ | MME $ ^{\text{C}} $ | SEED | MMB  | MM-Vet | SQA $ ^{\text{I}} $ | GQA  |
| ----------------- | ------------------- | ------------------- | ---- | ---- | ------ | ------------------- | ---- |
| BLIP-2            | 1293.8              | 290.0               | 46.4 | 38.1 | 22.4   | 61.0                | 41.0 |
| InstructBlip      | 1237.5              | 292.1               | 38.5 | 36.0 | 26.0   | 43.5                | 48.0 |
| Qwen-VL-Chat      | 1487.6              | 360.7               | 58.2 | 60.6 | 47.3   | 68.2                | 57.5 |
| mPLUG-Owl2        | 1450.2              | 313.2               | 57.8 | 64.5 | 36.2   | 68.7                | 56.1 |
| LLaVA-1.5         | 1510.7              | 348.2               | 58.6 | 64.3 | 30.5   | 66.8                | 62.0 |
| FiSAO (LLaVA-1.5) | 1522.6              | 349.0               | 60.6 | 64.8 | 30.5   | 69.3                | 62.0 |"
157,"|              |     | Comprehensive Benchmark | VQA                 | COCO-cap Benchmark |
| ------------ | --- | ----------------------- | ------------------- | ------------------ |
| Fine-grained | PPO | MME $ ^{\text{P}} $     | MME $ ^{\text{C}} $ | SEED               |
| ✗            | ✗   | 1431.9                  | 340.0               | 59.6               |
| ✗            | ✓   | 1509.3                  | 350.4               | 59.5               |
| ✓            | ✓   | 1522.6                  | 349.0               | 60.6               |"
158,"|                                   | Extensive Data Coverage | Accurate Low-Level Dynamics |
| --------------------------------- | ----------------------- | --------------------------- |
| Simulation-learned value function | ✓                       | ✗                           |
| Real-learned dynamics model       | ✗                       | ✓                           |
| SGFT                              | ✓                       | ✓                           |"
159,"| Stage1: Pre-Alignment            |
| -------------------------------- |
| Vision Expert A Text Recognition |
| left attention                   |
| self attention                   |
| left attention                   |"
159,"| Vision Encoders  | Fusion          | #Token(V) | #Tokens/s | #Params | Avg.  |
| ---------------- | --------------- | --------- | --------- | ------- | ----- |
| CLIP + ConvNeXt  | Seq. Append     | 2048      | 46.1      | 1200M   | 690.5 |
| Channel Concat.  | 1024            | 47.3      | 1184M     | 681.5   |       |
| LLaVA-HR         | 1024            | 47.0      | 1219M     | 678.7   |       |
| Mini-Gemini      | 1024            | 45.3      | 1201M     | 672.5   |       |
| Deformable Attn. | 1024            | 47.3      | 1201M     | 674.3   |       |
| CLIP + ConvNeXt  | Seq. Append     | 3072      | 40.3      | 1529M   | 686.2 |
| + SAM            | Channel Concat. | 1024      | 46.3      | 1495M   | 690.4 |"
159,"|                         | Model     | MME  | MMB  | SEED | MathVista | MMMU | POPE | SQA $ ^{1} $ | GQA   | VizWiz | VQAv2 | OCR  | TextVQA | ChartQA |
| ----------------------- | --------- | ---- | ---- | ---- | --------- | ---- | ---- | ------------ | ----- | ------ | ----- | ---- | ------- | ------- |
| Vicuna-7B &amp; Qwen-7B | LLaVA-1.5 | 1510 | 64.3 | 58.6 | -         | -    | 85.9 | 66.8         | 62.0* | 50.0   | 78.5* | 297  | 58.2    | -       |
| LLaVA-NeXt              | 1519      | 67.4 | 70.2 | 34.6 | 35.8      | 86.5 | 70.1 | 64.2*        | 57.6  | 80.0*  | 490   | 64.9 | -       |         |
| InternVL                | 1525      | -    | 65.4 | -    | -         | 86.4 | -    | 62.9*        | 52.5  | 79.3*  | -     | 57.0 | -       |         |
| LLaVA-HR                | 1554      | -    | 64.2 | -    | -         | 87.6 | 65.1 | 64.2*        | 48.7  | 81.9*  | -     | 67.1 | -       |         |
| Monkey                  | -         | -    | -    | -    | -         | -    | -    | 60.7*        | 61.2* | 80.3*  | 514   | 67.6 | 65.1    |         |
| Mini-Gemini             | 1523      | 65.8 | -    | 32.2 | 36.8      | -    | 71.1 | 64.5*        | -     | -      | 477   | 65.2 | -       |         |
| Eagle-X5                | 1528      | 68.4 | 73.9 | 37.0 | 36.3      | 88.8 | 70.0 | 64.9*        | 54.4  | 83.4*  | 529   | 71.2 | 67.7    |         |
| Eagle-X5 (+Pre-Align)   | 1582      | 69.7 | 73.7 | 38.2 | 38.0      | 88.7 | 71.9 | 64.6*        | 58.7  | 83.6*  | 566   | 71.9 | 69.3    |         |
| Vicuna-13B              | LLaVA-1.5 | 1531 | 67.7 | 61.6 | -         | 36.4 | 85.9 | 71.6         | 63.3* | 53.6   | 80.0* | 331  | 61.3    | -       |
| LLaVA-NeXt              | 1575      | 70.0 | 71.9 | 35.3 | 36.2      | 86.2 | 73.5 | 65.4*        | 60.5  | 82.8*  | 514   | 67.1 | 62.2    |         |
| InternVL                | 1546      | -    | -    | -    | -         | 87.1 | -    | 63.9*        | 54.6  | 80.2*  | 517   | 58.7 | -       |         |
| LLaVA-UHD               | 1535      | 68.0 | -    | -    | -         | 89.1 | 72.0 | 65.2*        | 56.1  | 81.7*  | -     | 67.7 | -       |         |
| LLaVA-HR                | 1540      | -    | 64.5 | -    | -         | 87.8 | 68.1 | 64.8*        | 57.9  | 82.6*  | -     | 68.1 | -       |         |
| Mini-Gemini             | 1565      | 68.6 | 70.6 | 37.0 | 37.3      | -    | 71.9 | 65.8*        | -     | -      | 466   | 65.9 | 56.6    |         |
| Eagle-X5                | 1609      | 69.2 | 74.1 | 38.8 | 36.6      | 87.8 | 72.7 | 66.2*        | 59.3  | 83.8*  | 574   | 74.2 | 69.9    |         |
| Eagle-X5 (+Pre-Align)   | 1605      | 71.6 | 74.9 | 42.7 | 38.5      | 89.2 | 75.5 | 64.6*        | 60.9  | 84.5*  | 598   | 73.3 | 72.1    |         |"
159,"| Config Summary   | Pre-align            | Pre-train            | Fine-tune | Avg.  |
| ---------------- | -------------------- | -------------------- | --------- | ----- |
| 1 epoch          | ✗                    | LLaVA-595K           | Eagle1.8M | 697.1 |
| 2 epoch          | ✗                    | LLaVA-595K           | Eagle1.8M | 698.3 |
| 1 epoch, unlock* | ✗                    | LLaVA-595K           | Eagle1.8M | 698.0 |
| 1 epoch, unlock* | ✗                    | LLaVA-595K+Eagle1.8M | Eagle1.8M | 699.5 |
| 1 epoch          | Eagle1.8M            | LLaVA-595K           | Eagle1.8M | 706.6 |
| 1 epoch, unlock* | Eagle1.8M            | LLaVA-595K           | Eagle1.8M | 707.1 |
| 1 epoch, unlock* | LLaVA-595K+Eagle1.8M | LLaVA-595K           | Eagle1.8M | 707.8 |
| 1 epoch, unlock* | LLaVA-595K+Eagle1.8M | LLaVA-595K+Eagle1.8M | Eagle1.8M | 708.9 |"
159,"| Model      | Knowledge | General      | OCR and Chart | Vision-Centric |
| ---------- | --------- | ------------ | ------------- | -------------- |
|            | Avg       | SQA $ ^{1} $ | MMMU          | MathVista      |
| Llama3-8B  |           |              |               |                |
| MGM-HD     | 55.7      | 75.1         | 37.3          | 37.0           |
| Cambrian-1 | 61.3      | 80.4         | 42.7          | 49.0           |
| Eagle-X5   | 65.2      | 84.1         | 43.5          | 56.9           |
| Vicuna-13B |           |              |               |                |
| MGM-HD     | 54.1      | 71.9         | 37.3          | 37.0           |
| Cambrian-1 | 60.2      | 79.3         | 40.0          | 48.0           |
| Eagle-X5   | 63.8      | 82.6         | 42.2          | 54.6           |
| Yi-34B     |           |              |               |                |
| MGM-HD     | 62.4      | 77.7         | 48.0          | 43.4           |
| Cambrian-1 | 67.0      | 85.6         | 49.7          | 53.2           |
| Eagle-X5   | 68.6      | 85.5         | 53.2          | 57.9           |"
166,"| Weight Matrix Architecture | Performance | # of Parameters |
| -------------------------- | ----------- | --------------- |
| Full-rank                  | 😢           | 😢               |
| Low-rank                   | 😢           | 😢               |
| Sine-Low-rank              | 😢           | 😢               |"
166,"| Method                | Params | BoolQ | PIQA  | SIQA  | HS    | WG    | ARC-e | ARC-c | OBQA  | Avg.  | $ \Delta $         |
| --------------------- | ------ | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ------------------ |
| LoRA $ _{k=4} $       | 7.1M   | 73.58 | 86.29 | 79.99 | 94.92 | 79.95 | 63.91 | 78.70 | 83.00 | 80.04 | 3.57  $ \uparrow $ |
| Sine LoRA $ _{k=4} $  | 72.69  | 87.38 | 79.32 | 94.39 | 85.32 | 75.01 | 88.64 | 86.20 | 83.61 |       |                    |
| LoRA $ _{k=8} $       | 14.2M  | 72.97 | 87.43 | 78.81 | 72.18 | 85.80 | 77.47 | 88.38 | 83.20 | 80.79 | 2.87  $ \uparrow $ |
| Sine LoRA $ _{k=8} $  | 73.42  | 86.51 | 80.30 | 94.16 | 85.87 | 76.36 | 88.05 | 84.60 | 83.66 |       |                    |
| LoRA $ _{k=16} $      | 28.3M  | 73.57 | 85.58 | 79.27 | 93.97 | 85.71 | 75.42 | 86.44 | 83.20 | 82.90 | 2.45  $ \uparrow $ |
| Sine LoRA $ _{k=16} $ | 73.70  | 87.65 | 80.76 | 94.93 | 84.45 | 79.10 | 89.77 | 84.40 | 84.35 |       |                    |
| LoRA $ _{k=32} $      | 56.6M  | 70.64 | 86.13 | 78.25 | 91.48 | 83.19 | 69.71 | 85.73 | 81.40 | 80.82 | 2.74  $ \uparrow $ |
| Sine LoRA $ _{k=32} $ | 72.42  | 86.51 | 79.78 | 93.96 | 85.16 | 78.07 | 87.58 | 85.00 | 83.56 |       |                    |"
161,"| Methods | BackBone  | PACS  | VLCS  | OfficeHome | DomainNet | Avg.  |
| ------- | --------- | ----- | ----- | ---------- | --------- | ----- |
| ERM     |           | 79.37 | 75.14 | 62.43      | 35.76     | 63.18 |
| BN      |           | 83.08 | 68.79 | 62.29      | 34.95     | 62.28 |
| TENT    |           | 83.23 | 69.28 | 62.51      | 35.37     | 62.60 |
| PL      |           | 85.66 | 74.68 | 62.71      | 35.24     | 64.57 |
| SHOT-IM |           | 83.02 | 70.80 | 63.91      | 35.92     | 63.41 |
| T3A     | ResNet-18 | 81.70 | 75.83 | 63.90      | 36.31     | 64.44 |
| TAST    | 84.31     | 71.69 | 63.96 | 35.71      | 63.92     |       |
| TAST-BN | 86.35     | 75.17 | 62.43 | 35.82      | 64.94     |       |
| TSD     | 87.88     | 75.47 | 63.42 | 35.86      | 65.66     |       |
| PROGRAM | 83.57     | 71.64 | 63.35 | 35.97      | 63.63     |       |
| DEYO    | 86.26     | 74.91 | 63.30 | 35.37      | 64.96     |       |
| PASLE   | 88.16     | 77.91 | 63.99 | 36.89      | 66.74     |       |
| ERM     |           | 85.84 | 76.06 | 67.84      | 43.16     | 68.23 |
| BN      | 86.00     | 67.76 | 66.82 | 41.50      | 65.52     |       |
| TENT    | 86.51     | 68.41 | 67.27 | 42.38      | 66.14     |       |
| PL      | 85.66     | 73.80 | 67.31 | 42.38      | 67.29     |       |
| SHOT-IM | 85.27     | 68.49 | 67.89 | 43.41      | 66.27     |       |
| T3A     | ResNet-50 | 86.54 | 76.59 | 68.85      | 44.00     | 69.00 |
| TAST    | 86.94     | 67.32 | 68.70 | 42.84      | 66.45     |       |
| TAST-BN | 89.47     | 75.59 | 67.97 | 43.03      | 69.02     |       |
| TSD     | 91.13     | 74.77 | 68.97 | 42.44      | 69.33     |       |
| PROGRAM | 86.16     | 68.85 | 68.03 | 43.34      | 66.60     |       |
| DEYO    | 88.23     | 71.59 | 68.08 | 42.47      | 67.59     |       |
| PASLE   | 91.36     | 78.70 | 69.37 | 44.91      | 71.09     |       |"
161,"| Methods | CIFAR-10-C | CIFAR-100-C |
| ------- | ---------- | ----------- |
| ERM     | 20.66      | 5.84        |
| BN      | 75.33      | 43.88       |
| TENT    | 75.41      | 43.93       |
| PL      | 75.70      | 44.24       |
| SHOT-IM | 75.85      | 44.36       |
| T3A     | 23.52      | 6.74        |
| TAST    | 74.13      | 39.21       |
| TAST-BN | 74.56      | 31.84       |
| TSD     | 75.14      | 44.19       |
| PROGRAM | 75.00      | 44.06       |
| DEYO    | 75.74      | 44.28       |
| PASLE   | 76.67      | 45.32       |"
162,"| Method                                | Tox21 (12)  $ \uparrow $ | SIDER (27)  $ \uparrow $ | MUV (17)  $ \uparrow $ | ToxCast (617)  $ \uparrow $ |
| ------------------------------------- | ------------------------ | ------------------------ | ---------------------- | --------------------------- |
| CHEF (Adler et al., 2020)             | 61.97 ± 0.65             | 57.34 ± 0.82             | 53.17 ± 4.21           | 56.52 ± 1.24                |
| MixHop (Abu-El-Haija et al., 2019)    | 78.14 ± 0.33             | 72.01 ± 0.87             | 78.04 ± 3.01           | 77.19 ± 0.93                |
| Siamese (Koch et al., 2015)           | 80.40 ± 0.35             | 71.10 ± 4.32             | 59.59 ± 5.13           | -                           |
| ProtoNet (Snell et al., 2017)         | 74.98 ± 0.32             | 64.54 ± 0.89             | 65.88 ± 4.11           | 63.70 ± 1.26                |
| MAML (Finn et al., 2017)              | 80.21 ± 0.24             | 70.43 ± 0.76             | 63.90 ± 2.28           | 66.79 ± 0.85                |
| TPN (Liu et al., 2018)                | 76.05 ± 0.24             | 67.84 ± 0.95             | 65.22 ± 5.82           | 62.74 ± 1.45                |
| EGNN (Kim et al., 2019)               | 81.21 ± 0.16             | 72.87 ± 0.73             | 65.20 ± 2.08           | 63.65 ± 1.57                |
| IterRefLSTM (Altae-Tran et al., 2017) | 81.10 ± 0.17             | 69.63 ± 0.31             | 45.56 ± 5.12           | -                           |
| PAR (Wang et al., 2021)               | 82.06 ± 0.12             | 74.68 ± 0.31             | 66.48 ± 2.12           | 69.72 ± 1.63                |
| ADKF-IFT (Chen et al., 2023)          | 82.43 ± 0.60             | 67.72 ± 1.21             | 98.18 ± 3.05           | 72.07 ± 0.81                |
| MHNFs (Schimunek et al., 2023)        | 80.23 ± 0.84             | 65.89 ± 1.17             | 73.81 ± 2.53           | 74.91 ± 0.73                |
| UniMatch (Ours)                       | 82.62 ± 0.43             | 68.13 ± 1.54             | 79.40 ± 3.14           | 77.74 ± 0.75                |
| Pre-GNN (Hu et al., 2020)             | 82.14 ± 0.08             | 73.96 ± 0.08             | 67.14 ± 1.58           | 73.68 ± 0.74                |
| GNN-MAML (Guo et al., 2021)           | 82.97 ± 0.10             | 75.43 ± 0.21             | 68.99 ± 1.84           | -                           |
| Pre-PAR (Wang et al., 2021)           | 84.93 ± 0.11             | 78.08 ± 0.16             | 69.96 ± 1.37           | 75.12 ± 0.84                |
| Pre-ADKF-IFT (Chen et al., 2023)      | 86.06 ± 0.35             | 70.95 ± 0.60             | 95.74 ± 0.37           | 76.22 ± 0.13                |
| Pre-UniMatch (Ours)                   | 86.35 ± 0.13             | 80.34 ± 0.45             | 86.35 ± 0.76           | 81.63 ± 0.73                |"
164,"| Model                                           | #Params | Time             | QNLI | MRPC | SST-2 | CoLA | STS-B | MNLI-m | RTE  | QQP  | AVG  |
| ----------------------------------------------- | ------- | ---------------- | ---- | ---- | ----- | ---- | ----- | ------ | ---- | ---- | ---- |
| NAS-BERT-10 (Xu et al. 2021)                    | 10.0M   | 96 d             | 86.3 | 79.1 | 88.6  | 34.0 | 84.8  | 76.4   | 66.6 | 88.5 | 75.5 |
| NAS-BERT-30 (Xu et al. 2021)                    | 30.0M   | 96 d             | 88.4 | 84.6 | 90.5  | 48.7 | 87.6  | 81.0   | 71.8 | 90.2 | 80.3 |
| EfficientBERT-TINY (Dong et al. 2021)           | 9.4M    | 58 d             | 89.3 | 90.1 | 90.1  | 39.1 | 79.9  | 81.7   | 63.2 | 86.7 | 77.5 |
| EfficientBERT (Dong et al. 2021)                | 15.7M   | 58 d             | 90.4 | 91.5 | 91.3  | 50.2 | 82.5  | 83.1   | 66.8 | 87.3 | 80.4 |
| AutoBERT-Zero-small (Gao et al. 2022)           | 13.0M   | $ \sim $ 1,000 d | -    | -    | -     | -    | -     | -      | -    | -    | 80.5 |
| Synaptic Diversity (Zhou et al. 2022)           | 15.6M   | 0.7 d            | 88.9 | 87.6 | 91.4  | 32.0 | 84.1  | 81.0   | 73.4 | 88.2 | 78.3 |
| Head Confidence (Serianni &amp; Kalita 2023)    | 15.6M   | 0.5 d            | 90.1 | 89.7 | 92.4  | 37.5 | 84.1  | 82.5   | 75.9 | 89.1 | 80.2 |
| Softmax Confidence (Serianni &amp; Kalita 2023) | 15.6M   | 0.5 d            | 89.4 | 88.3 | 92.0  | 32.6 | 84.7  | 81.6   | 73.9 | 88.9 | 78.9 |
| W-PCA-Tiny                                      | 9.6M    | 0.4 d            | 89.2 | 89.2 | 92.0  | 33.2 | 84.0  | 80.5   | 71.1 | 88.0 | 78.4 |
| W-PCA-Small                                     | 15.6M   | 0.5 d            | 90.8 | 90.5 | 92.8  | 44.0 | 85.3  | 82.9   | 76.1 | 88.8 | 81.4 |"
165,"| [1]                          | BV            | ✗           | USAM |
| ---------------------------- | ------------- | ----------- | ---- |
| [7]                          | Interpolation | ✗           | USAM |
| [2]                          | Deterministic | ✗           | USAM |
| This work ER                 | ✓             | Unified SAM |      |
| General Non-convex functions |               |             |      |
| [1]                          | BV            | ✗           | USAM |
| [5]                          | BV            | ✗           | SAM  |
| [8]                          | BV            | ✗           | SAM  |
| This work ER                 | ✓             | Unified SAM |      |"
165,"| Unified SAM  | $ \lambda=0.0 $    | $ \lambda=0.5 $    | $ \lambda=1.0 $    | $ \lambda=1/t $    | $ \lambda=1-1/t $  |
| ------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |
| $ \rho=0.1 $ | 95.7 $ \pm $ 0.01  | 95.68 $ \pm $ 0.11 | 95.9 $ \pm $ 0.08  | 95.84 $ \pm $ 0.07 | 95.81 $ \pm $ 0.03 |
| $ \rho=0.2 $ | 95.8 $ \pm $ 0.05  | 95.77 $ \pm $ 0.09 | 95.93 $ \pm $ 0.07 | 95.71 $ \pm $ 0.13 | 95.98 $ \pm $ 0.1  |
| $ \rho=0.3 $ | 95.35 $ \pm $ 0.3  | 95.88 $ \pm $ 0.1  | 95.95 $ \pm $ 0.09 | 95.68 $ \pm $ 0.02 | 95.99 $ \pm $ 0.06 |
| $ \rho=0.4 $ | 95.46 $ \pm $ 0.02 | 95.76 $ \pm $ 0.1  | 95.62 $ \pm $ 0.05 | 95.46 $ \pm $ 0.27 | 95.79 $ \pm $ 0.07 |
| SGD          | 95.35 $ \pm $ 0.06 |                    |                    |                    |                    |"
165,"| Unified SAM  | $ \lambda=0.0 $    | $ \lambda=0.5 $    | $ \lambda=1.0 $    | $ \lambda=1/t $    | $ \lambda=1-1/t $  |
| ------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |
| $ \rho=0.1 $ | 95.29 $ \pm $ 0.09 | 95.32 $ \pm $ 0.02 | 95.59 $ \pm $ 0.09 | 95.24 $ \pm $ 0.03 | 95.53 $ \pm $ 0.11 |
| $ \rho=0.2 $ | 95.25 $ \pm $ 0.11 | 95.48 $ \pm $ 0.05 | 95.5 $ \pm $ 0.02  | 95.38 $ \pm $ 0.11 | 95.58 $ \pm $ 0.07 |
| $ \rho=0.3 $ | 95.25 $ \pm $ 0.11 | 95.24 $ \pm $ 0.02 | 95.18 $ \pm $ 0.04 | 95.12 $ \pm $ 0.19 | 95.26 $ \pm $ 0.1  |
| $ \rho=0.4 $ | 94.76 $ \pm $ 0.09 | 94.98 $ \pm $ 0.07 | 94.7 $ \pm $ 0.02  | 94.64 $ \pm $ 0.03 | 94.61 $ \pm $ 0.09 |
| SGD          |                    |                    | 94.82 $ \pm $ 0.02 |                    |                    |"
166,"| Method                     | Params | BoolQ | PIQA | SIQA | HS   | WG   | ARC-e | ARC-c | OBQA | Avg. | $ \Delta $        |
| -------------------------- | ------ | ----- | ---- | ---- | ---- | ---- | ----- | ----- | ---- | ---- | ----------------- |
| DoRA $ _{k=8} $            | 14.9M  | 73.2  | 87.7 | 79.9 | 94.7 | 84.5 | 89.3  | 78.0  | 83.2 | 83.8 | 1.4  $ \uparrow $ |
| Sine DoRA $ _{k=8} $       | 73.9   | 89.0  | 81.0 | 95.3 | 86.1 | 90.1 | 79.0  | 87.0  | 85.2 |      |                   |
| DoRA $ ^{*} $  $ _{k=16} $ | 29.1M  | 74.5  | 88.8 | 80.3 | 95.5 | 84.7 | 90.1  | 79.1  | 87.2 | 85.0 | 0.3  $ \uparrow $ |
| Sine DoRA $ _{k=16} $      | 75.1   | 89.0  | 81.0 | 95.3 | 86.1 | 90.0 | 79.3  | 86.2  | 85.3 |      |                   |
| DoRA $ ^{*} $  $ _{k=32} $ | 57.4M  | 74.6  | 89.3 | 79.9 | 95.5 | 85.6 | 90.5  | 80.4  | 85.8 | 85.2 | 0.1  $ \uparrow $ |
| Sine DoRA $ _{k=32} $      | 75.8   | 89.3  | 80.3 | 95.9 | 86.1 | 90.2 | 79.4  | 85.4  | 85.3 |      |                   |"
167,"|                         | Best Observed Subset Last Iterate Function Value |
| ----------------------- | ------------------------------------------------ |
| Mean                    | Std                                              |
| CWR4 - EuclidSquared    | 0.145                                            |
| CWR4 - Negative Entropy | 0.160                                            |
| CWR - Minus Ln          | 0.218                                            |
| Uniform Sampling        | 0.244                                            |
| Stochastic 1-Flip       | 0.208                                            |"
167,"|                         | Best Observed Subset Last Iterate Function Value |
| ----------------------- | ------------------------------------------------ |
| Mean                    | Std                                              |
| CWR4 - EuclidSquared    | 0.198                                            |
| CWR4 - Negative Entropy | 0.228                                            |
| CWR - Minus Ln          | 0.247                                            |
| Uniform Sampling        | 0.203                                            |
| Stochastic 1-Flip       | 0.380                                            |"
167,"|                            | Best Observed Subset Last Iterate Function Value |
| -------------------------- | ------------------------------------------------ |
| Mean                       | Std                                              |
| CWR4 - EuclidSquared       | 1.056                                            |
| CWR4 - Negative Entropy    | 1.026                                            |
| CWR - Minus Ln             | 1.222                                            |
| Uniform Sampling           | 1.331                                            |
| Stochastic 1-Flip          | 1.067                                            |
| Projected Gradient Descent | 3.690                                            |"
168,"|           | Unif    | Mix-R          | Mix-P          |
| --------- | ------- | -------------- | -------------- |
| Exact     | 0.588T  | 0.52T-1        | 0.6112T-1      |
| Empirical | unknown | linear to O(σ) | linear to O(σ) |"
168,"| Algorithm     | Iters             | Reward (train)    | Win-rate (train)  | Reward (test)      | Win-rate (test) |
| ------------- | ----------------- | ----------------- | ----------------- | ------------------ | --------------- |
| Reference     | -                 | -2.621            | -                 | -2.320             | -               |
| Vanilla DPO   | 2                 | -1.584(\pm 0.018) | 73.8(\pm 0.4)%    | -1.532(\pm 0.028)  | 70.4(\pm 0.9)%  |
| 3             | -1.395(\pm 0.013) | 77.1(\pm 0.3)%    | -1.372(\pm 0.001) | 73.3(\pm 0.5)%     |                 |
| On-policy DPO | 2                 | -1.537(\pm 0.022) | 74.7(\pm 0.2)%    | -1.490(\pm 0.023)  | 71.0(\pm 0.3)%  |
| 3             | -0.969(\pm 0.031) | 80.9(\pm 0.3)%    | -0.989(\pm 0.014) | 78.3(\pm 0.2)%     |                 |
| Hybrid GSHF   | 2                 | -1.656(\pm 0.013) | 73.3(\pm 0.3)%    | -1.580 (\pm 0.010) | 70.2(\pm 0.4)%  |
| 3             | -1.112(\pm 0.030) | 80.2(\pm 0.3)%    | -1.039(\pm 0.111) | 78.7(\pm 0.2)%     |                 |
| Ours          | 2                 | -1.579(\pm 0.018) | 74.7(\pm 0.2)%    | -1.490(\pm 0.022)  | 71.9(\pm 0.7)%  |
| 3             | -0.942(\pm 0.043) | 81.4(\pm 0.6)%    | -0.903(\pm 0.017) | 80.7(\pm 0.8)%     |                 |"
168,"| Algorithm     | Iters            | Reward (train)   | Win-rate (train) | Reward (test)    | Win-rate (test) |
| ------------- | ---------------- | ---------------- | ---------------- | ---------------- | --------------- |
| Reference     | -                | -0.665           | -                | -0.639           | -               |
| Vanilla DPO   | 2                | 1.372(\pm 0.096) | 71.0(\pm 1.0)\%  | 1.459(\pm 0.065) | 71.2(\pm 0.9)\% |
| 3             | 2.009(\pm 0.049) | 78.2(\pm 0.5)\%  | 2.101(\pm 0.053) | 79.0(\pm 0.9)\%  |                 |
| On-policy DPO | 2                | 1.997(\pm 0.013) | 78.1(\pm 0.2)\%  | 2.042(\pm 0.013) | 77.5(\pm 0.4)\% |
| 3             | 3.040(\pm 0.298) | 84.0(\pm 0.8)\%  | 3.132(\pm 0.315) | 83.5(\pm 0.9)\%  |                 |
| Hybrid GSHF   | 2                | 2.150(\pm 0.020) | 80.3(\pm 0.1)\%  | 2.189(\pm 0.051) | 80.3(\pm 0.7)\% |
| 3             | 2.384(\pm 0.129) | 81.1(\pm 0.5)\%  | 2.490(\pm 0.160) | 82.1(\pm 1.3)\%  |                 |
| Ours          | 2                | 2.001(\pm 0.066) | 77.9(\pm 0.8)\%  | 2.047(\pm 0.017) | 77.7(\pm 0.4)\% |
| 3             | 3.248(\pm 0.320) | 84.8(\pm 1.2)\%  | 3.321(\pm 0.319) | 84.4(\pm 1.7)\%  |                 |"
169,"| Pre-Training Strategies | Prompts    | Cora  | Citeseer | CoraML |
| ----------------------- | ---------- | ----- | -------- | ------ |
| Clean                   | M-0.25     | D-0.5 | R-0.5    | H-0.5  |
| GraphPrompt             | 57.68      | 34.06 | 36.01    | 42.68  |
| MultiGPrompt            | 48.34      | 39.37 | 32.97    | 43.71  |
| GraphCL                 | All-in-one | 50.93 | 31.02    | 17.91  |
| GPF                     | 54.06      | 33.83 | 27.62    | 29.43  |
| GPF-plus                | 66.39      | 36.83 | 30.52    | 42.22  |
| GPPT                    | 45.24      | 20.86 | 24.04    | 19.50  |
| $ ^{*} $ MD-PT          | 49.39      | 48.06 | 40.15    | 44.94  |
| $ ^{*} $ IA-PT          | 57.82      | 58.82 | 51.07    | 53.65  |
| GraphMAE                | All-in-one | 44.67 | 29.75    | 30.25  |
| GPF                     | 66.71      | 38.82 | 33.15    | 48.53  |
| GPF-plus                | 63.40      | 39.46 | 33.38    | 49.84  |
| GPPT                    | 67.79      | 43.17 | 47.71    | 42.40  |
| $ ^{*} $ MD-PT          | 62.49      | 62.49 | 52.61    | 48.03  |
| $ ^{*} $ IA-PT          | 68.93      | 68.30 | 61.90    | 67.30  |"
182,"| Category  | Model      | Token Accuracy |
| --------- | ---------- | -------------- |
| $ T_{a} $ | $ T_{pv} $ | $ T_{pi} $     |
| Absolute  | SDXL       | 0.709          |
| Location  | +sft-unet  | 0.718          |"
182,"| Category  | Model      | Token Accuracy |
| --------- | ---------- | -------------- |
| $ T_{a} $ | $ T_{pv} $ | $ T_{pi} $     |
| Height    | SDXL       | 0.881          |
| +sft-unet | 0.886      | 0.662          |"
537,"| Zoom In | Zoom Out | Urban100       |
| ------- | -------- | -------------- |
| ✘       | ☑        | PSNR / SSIM    |
| ✗       | ✗        | 24.87 / 0.7431 |
| ✓       | ✗        | 25.18 / 0.7551 |
| ✗       | ✓        | 25.18 / 0.7552 |
| ✓       | ✓        | 25.20 / 0.7558 |"
169,"| Pre-Training Strategies | Prompts | CoraML   | Citeseer   |
| ----------------------- | ------- | -------- | ---------- |
| GCNSVD                  | GRAND   | GNNGuard | GCNJaccard |
| GraphPrompt             | ProG    | 37.32    | 31.02      |
| GPF                     | 30.88   | 23.08    | 27.39      |
| GPF-plus                | 52.29   | 28.93    | 41.41      |
| GraphCL                 | GPPT    | 35.69    | 26.53      |
| *MD-PT                  | 47.85   | 45.54    | 49.67      |
| *IA-PT                  | 61.27   | 58.73    | 53.83      |
| GraphMAE                | ProG    | 17.05    | 23.22      |
| GPF                     | 48.56   | 25.12    | 50.84      |
| GPF-plus                | 47.21   | 30.75    | 48.48      |
| GPPT                    | 29.52   | 20.32    | 32.06      |
| *MD-PT                  | 49.17   | 45.60    | 54.60      |
| *IA-PT                  | 66.17   | 63.54    | 63.17      |"
170,"| Settings                                                                                                   | PKU-SafeRLHF |
| ---------------------------------------------------------------------------------------------------------- | ------------ |
| Llama-Guard  $ \uparrow $  Harm.  $ \downarrow $  Help.  $ \uparrow $  MT  $ \uparrow $  Win  $ \uparrow $ |              |
| LLaMA2-7B                                                                                                  |              |
| w. DPO                                                                                                     | 74.4%        |
| w. PPO                                                                                                     | 78.7%        |
| w. IPO                                                                                                     | 74.8%        |
| w. TDPO                                                                                                    | 75.9%        |
| w. KTO                                                                                                     | 79.8%        |
| w. TIS-DPO(P)                                                                                              | 75.9%        |
| w. TIS-DPO(S)                                                                                              | 89.6%        |
| w. TIS-DPO(D)                                                                                              | 96.7%        |"
172,"| Dataset               | Model          | MAE (eV)                                            |
| --------------------- | -------------- | --------------------------------------------------- |
| MP-PBE                | invDeepDFT     | 1.293  $ \pm $  0.03 Jørgensen &amp; Bhowmik (2022) |
| MP-PBE                | DeepDFT        | 1.212  $ \pm $  0.02 Jørgensen &amp; Bhowmik (2022) |
| MP-PBE                | ChargeE3Net    | 0.523  $ \pm $  0.01 Koker et al. (2024)            |
| ECD-PBE_MP-id         | ChargeE3Net    | 0.520  $ \pm $  0.01                                |
| ECD-PBE-id            | ChargeE3Net    | 0.685  $ \pm $  0.03                                |
| ECD-PBE-id            | invChargeE3Net | 0.732  $ \pm $  0.02                                |
| ECD-HSE-id            | ChargeE3Net    | 1.534  $ \pm $  0.07                                |
| ECD-PBE_HSE-id        | ChargeE3Net    | 2.156  $ \pm $  0.08                                |
| ECD-PBE_HSE_tuning-id | ChargeE3Net    | 1.085  $ \pm $  0.03                                |"
172,"| Training Dataset               | Metric                | Ratio                 |
| ------------------------------ | --------------------- | --------------------- |
| ECD-PBE-id                     | Optimal ratio         | 0.061  $ \pm $  0.016 |
| Achieved ratio  $ \downarrow $ | 0.757  $ \pm $  0.073 |                       |
| ECD-PBE_HSE_tuning-id          | Optimal ratio         | 0.071  $ \pm $  0.033 |
| Achieved ratio  $ \downarrow $ | 0.681  $ \pm $  0.113 |                       |
| Model/Dataset                  | MAE (eV)              | Time per structure    |
| ML Models                      | -                     | (s)                   |
| CGCNN                          | 1.45                  | 1.5                   |
| MEGNet                         | 1.36                  | 1.34                  |
| CrystalNet                     | 1.19                  | 1.67                  |
| CrystalNet-TL                  | 0.70                  | 1.58                  |
| ECD-Based Models               | -                     | (min)                 |
| ECD-PBE                        | 1.17                  | 14.4                  |
| ECD-PBE_HSE                    | 0.65                  | 14.4                  |
| PBE-Based Datasets             | -                     | (min)                 |
| MP                             | 1.38                  | -                     |
| Matgen                         | 1.21                  | 24.5                  |
| AFLOW                          | 1.20                  | -                     |
| OQMD                           | 1.09                  | -                     |
| HSE-Based Dataset              | -                     | (min)                 |
| HSE                            | 0.41                  | 228.1                 |"
173,"| Method                | Alfworld | BabyAI   | SciWorld | PDDL     | Jericho |
| --------------------- | -------- | -------- | -------- | -------- | ------- |
|                       | Success  | Progress | Success  | Progress | Success |
| AgentRefine           | 48.5     | 61.5     | 37.1     | 51.7     | 7.7     |
| - w/o refinement loss | 40.3     | 58.8     | 34.8     | 45.6     | 4.4     |
| - w/o refinement data | 49.3     | 65.2     | 30.4     | 43.1     | 5.5     |
| - w erroneous loss    | 29.9     | 43.9     | 23.2     | 31.6     | 3.3     |"
173,"| Model              | Alfworld | Perturbation 1 | Perturbation 2 | Perturbation 3 | Perturbation 4 | Perturbation 5 | Average  | Std     |
| ------------------ | -------- | -------------- | -------------- | -------------- | -------------- | -------------- | -------- | ------- |
| Success            | Progress | Success        | Progress       | Success        | Progress       | Success        | Progress | Success |
| LLaMA3-8B-Instruct | 22.4     | 46.1           | 23.1           | 45.6           | 24.6           | 45.0           | 17.9     | 45.1    |
| AgentGym           | 61.9     | 76.9           | 29.1           | 59.2           | 49.2           | 65.3           | 32.8     | 53.9    |
| Agent-FLAN         | 67.2     | 79.7           | 21.6           | 58.8           | 51.4           | 71.3           | 27.6     | 53.5    |
| AgentRefine        | 44.8     | 63.8           | 50.0           | 66.5           | 51.5           | 66.7           | 54.5     | 70.0    |"
203,"| LVLMs             | TicTacToe | Reversi | Sudoku | Minesweeper | Gomoku | Chess |
| ----------------- | --------- | ------- | ------ | ----------- | ------ | ----- |
| GPT-4o            | 0.709     | 0.195   | 0.122  | 0.148       | 0.013  | 0.271 |
| Gemini1.5-pro     | 0.994     | 0.882   | 0.723  | 0.580       | 0.583  | 0.473 |
| Claude-3.5-sonnet | 0.985     | 0.992   | 0.912  | 0.741       | 0.742  | 0.554 |
| Qwen2-vl-7b       | 0.613     | 0.359   | 0.567  | 0.203       | 0.589  | 0.271 |
| Deepseek-vl-7b    | 0.369     | 0.463   | 0.282  | 0.202       | 0.010  | 0.276 |
| Phi3-vl           | 0.535     | 0.463   | 0.287  | 0.202       | 0.593  | 0.271 |
| LLaVA-1.6-7b      | 0.302     | 0.477   | 0      | 0.188       | 0      | 0.209 |
| InternVL2-8b      | 0.489     | 0.178   | 0.405  | 0.343       | 0.416  | 0.242 |
| Random            | 0.332     | 0.333   | 0.103  | 0.093       | 0.332  | 0.079 |"
173,"| Method                   | Alfworld | BabyAI  | SciWorld | PDDL    | Jericho  |
| ------------------------ | -------- | ------- | -------- | ------- | -------- |
| Success                  | Progress | Success | Progress | Success | Progress |
| GPT Series               |          |         |          |         |          |
| GPT-4o                   | 66.4     | 79.9    | 48.2     | 64.1    | 40       |
| GPT-4o-mini              | 37.3     | 65.0    | 36.6     | 51.9    | 23.3     |
| LLaMA-3-8B Series        |          |         |          |         |          |
| LLaMA-3-8B-Instruct      | 22.4     | 46.1    | 45.5     | 56.5    | 7.8      |
| AgentGen                 | 29.1     | 47.6    | 20.5     | 35.0    | -        |
| AgentGym                 | 61.9     | 76.9    | 47.3     | 61.4    | 18.9     |
| Agent-FLAN               | 67.2     | 79.7    | 25.0     | 35.3    | 1.1      |
| AgentRefine              | 44.8     | 63.8    | 37.5     | 50.4    | 14.4     |
| Mistral Series           |          |         |          |         |          |
| Mistral-7B-Instruct-v0.3 | 12.4     | 35.9    | 36.6     | 45.8    | 6.7      |
| AgentGym                 | 76.9     | 86.7    | 40.2     | 56.3    | 15.6     |
| Agent-FLAN               | 77.6     | 87.6    | 15.2     | 21.0    | 0        |
| AgentRefine              | 51.4     | 68.8    | 25.9     | 42.4    | 4.4      |
| LLaMA-3-70B Series       |          |         |          |         |          |
| LLaMA-3-70B-Instruct     | 67.2     | 75.2    | 48.2     | 61.8    | 42.2     |
| Agent-FLAN               | 80.5     | 86.8    | 32.1     | 41.2    | 5.5      |
| AgentRefine              | 67.2     | 72.1    | 44.6     | 59.7    | 17.7     |"
176,"|                          | Learning principle            | Generation complexity | Training dynamics | Requires pretrained model? |
| ------------------------ | ----------------------------- | --------------------- | ----------------- | -------------------------- |
| Diffusion model          | Denoising score matching      | T &gt; 1              | Stable            | No                         |
| Diffusion distillation   | Reverse KLD minimization      | T ≥ 1                 | Stable            | Yes                        |
| GAN                      | Minimize JSD w/ discriminator | T = 1                 | Unstable          | No                         |
| Consistency training     | Emulate                       | T ≥ 1                 | Unstable          | No                         |
| Consistency distillation | PF ODE paths                  | Stable                | Yes               |                            |
| SMT (from scratch)       | Minimize α-JSD                | T = 1                 | Stable            | No                         |
| SMD (distillation)       | w/ score of mixture           | Yes                   |                   |                            |"
176,"| Method                                  | ImageNet 64x64 | CIFAR-10 32x32     |
| --------------------------------------- | -------------- | ------------------ |
| # params                                | NFE            | FID $ \downarrow $ |
| Training from scratch: Diffusion models |                |                    |
| DDPM (Ho et al., 2020)                  | -              | -                  |
| ADM (Dhariwal &amp; Nichol, 2021)       | 296M           | 250                |
| EDM (Karras et al., 2022b)              | 296M           | 512                |
| Training from scratch: One-step models  |                |                    |
| CT (Song et al., 2023)                  | 296M           | 1                  |
| iCT (Song &amp; Dhariwal, 2024a)        | 296M           | 1                  |
| iCT-deep (Song &amp; Dhariwal, 2024a)   | 592M           | 1                  |
| ECT (Geng et al., 2024)                 | 280M           | 1                  |
| SMT (ours)                              | 296M           | 1                  |
| Diffusion distillation                  |                |                    |
| PD (Salimans &amp; Ho, 2022)            | 296M           | 1                  |
| TRACT (Berthelot et al., 2023)          | 296M           | 1                  |
| CD (LPIPS) (Song et al., 2023)          | 296M           | 1                  |
| Diff-Instruct (Luo et al., 2024a)       | 296M           | 1                  |
| MultiStep-CD (Heek et al., 2024)        | 1200M          | 1                  |
| DMD w/o reg (Yin et al., 2024b)         | 296M           | 1                  |
| DMD2 w/ GAN (Yin et al., 2024a)         | 296M           | 1                  |
| MMD (Salimans et al., 2024)             | 400M           | 1                  |
| SiD (Zhou et al., 2024)                 | 296M           | 1                  |
| SiM (Luo et al., 2024b)                 | -              | -                  |
| SMD (ours)                              | 296M           | 1                  |
| w/ expensive regularizer or finetuning  |                |                    |
| CTM (Kim et al., 2024)                  | 296M           | 1                  |
| DMD w/ reg (Yin et al., 2024b)          | 296M           | 1                  |
| DMD2 (finetuned) (Yin et al., 2024a)    | 296M           | 1                  |"
179,"| Tasks                | Metric               | CQL-Sauté   | BCQ-Lag     | CPQ        | COptiDICE   | VOCE       | CDT       | TREBI     | FISOR     | CCAC(ours) |
| -------------------- | -------------------- | ----------- | ----------- | ---------- | ----------- | ---------- | --------- | --------- | --------- | ---------- |
| Run                  | Reward  $ \uparrow $ | 0.55±0.44   | 1.51±0.91   | 0.95±0.03  | 1.38±0.5    | 1.65±1.07  | 0.99±0.02 | 0.82±0.19 | 0.74±0.08 | 0.96±0.03  |
| Cost  $ \downarrow $ | 5.23±6.26            | 19.85±12.76 | 1.92±2.66   | 6.27±6.29  | 9.35±8.95   | 1.34±0.74  | 2.12±2.36 | 0.54±1.90 | 0.23±0.27 |            |
| Circle               | Reward  $ \uparrow $ | 0.4±0.33    | 1.11±0.31   | 0.64±0.42  | 0.63±0.25   | 0.06±0.11  | 0.91±0.17 | 0.43±0.25 | 0.27±0.18 | 0.79±0.24  |
| Cost  $ \downarrow $ | 6.88±9.15            | 15.61±6.1   | 3.73±6.43   | 9.82±10.57 | 10.05±15.53 | 2.58±2.49  | 3.53±8.26 | 0.17±0.76 | 0.17±0.79 |            |
| Velocity             | Reward  $ \uparrow $ | 0.44±0.43   | 0.78±0.37   | 0.41±1.15  | 0.65±0.36   | -0.41±0.44 | 0.88±0.24 | 0.45±0.24 | 0.47±0.26 | 0.86±0.2   |
| Cost  $ \downarrow $ | 2.08±3.23            | 27.26±29.75 | 35.18±44.66 | 6.94±7.22  | 0.0±0.0     | 2.91±2.87  | 1.07±2.52 | 0.08±0.27 | 0.38±0.2  |            |
| Average              | Reward  $ \uparrow $ | 0.45±0.4    | 1.09±0.58   | 0.63±0.75  | 0.8±0.47    | 0.26±0.96  | 0.91±0.19 | 0.51±0.28 | 0.44±0.27 | 0.85±0.21  |
| Cost  $ \downarrow $ | 4.91±7.33            | 20.44±19.33 | 13.81±30.22 | 8.07±8.85  | 6.54±12.1   | 2.41±2.45  | 2.41±6.0  | 0.22±1.05 | 0.25±0.56 |            |"
181,"| UNLEARNABLE METHOD | CIFAR-10 | CIFAR-100 | IMAGENET-100 |
| ------------------ | -------- | --------- | ------------ |
| Test Acc           | #LP      | UD        | Test Acc     |
| Vanilla            | 94.11    | 3.32      | \            |
| EM                 | 26.52    | 0.62      | 0.187        |
| REM                | 30.26    | 1.54      | 0.464        |
| DC                 | 18.51    | 1.00      | 0.301        |
| TAP                | 29.85    | 5.44      | 1.639        |
| LSP                | 10.23    | 1.14      | 0.343        |
| OPS                | 11.98    | 0.52      | 0.157        |"
182,"| Models                | Gemini 1.5 Pro           | Claude 3.5 Sonnet           | GPT-4o               | GPT-4 Turbo           |
| --------------------- | ------------------------ | --------------------------- | -------------------- | --------------------- |
| $ \bar{S}(\uparrow) $ | $ \gamma_{w}(\uparrow) $ | $ \gamma_{wo}(\downarrow) $ | $ \kappa(\uparrow) $ | $ \bar{S}(\uparrow) $ |
| Open-source Models    |                          |                             |                      |                       |
| SD 1.5                | 0.55                     | 0.43                        | 0.46                 | -0.03                 |
| SD 2.1                | 0.58                     | 0.45                        | 0.46                 | -0.01                 |
| SD XL 1.0             | 0.62                     | 0.39                        | 0.39                 | -0.00                 |
| SD CA                 | 0.59                     | 0.42                        | 0.41                 | 0.01                  |
| DeepFloyd             | 0.64                     | 0.44                        | 0.44                 | 0.00                  |
| PixArt                | 0.60                     | 0.35                        | 0.32                 | 0.02                  |
| Kolors                | 0.60                     | 0.41                        | 0.42                 | -0.01                 |
| SD 3                  | 0.67                     | 0.45                        | 0.40                 | 0.05                  |
| FLUX.1                | 0.72                     | 0.43                        | 0.35                 | 0.08                  |
| API-based Models      |                          |                             |                      |                       |
| MidJ V6               | 0.68                     | 0.46                        | 0.39                 | 0.07                  |
| DALL-E 3              | 0.75                     | 0.46                        | 0.33                 | 0.14                  |
| CogV3-Plus            | 0.79                     | 0.52                        | 0.35                 | 0.17                  |
| Ideogram 2            | 0.80                     | 0.47                        | 0.29                 | 0.18                  |"
182,"| Category              | Models                   | GPT-4o                      |
| --------------------- | ------------------------ | --------------------------- |
| $ \bar{S}(\uparrow) $ | $ \gamma_{w}(\uparrow) $ | $ \gamma_{wo}(\downarrow) $ |
| Absolute              | SD XL                    | 0.64                        |
| Location              | + sft-unet               | 0.65(\uparrow)              |
| + sft-text            | 0.64( $ \downarrow $ )   | 0.31(\uparrow)              |
| + dpo-unet            | 0.60( $ \downarrow $ )   | 0.29( $ \uparrow $ )        |
| + dpo-text            | 0.57( $ \downarrow $ )   | 0.33( $ \uparrow $ )        |
| Category              | Models                   | GPT-4o                      |
| $ \bar{S}(\uparrow) $ | $ \gamma_{w}(\uparrow) $ | $ \gamma_{wo}(\downarrow) $ |
| Height                | SD XL                    | 0.77                        |
| + sft-unet            | 0.77( $ - $ )            | 0.33( $ \downarrow $ )      |
| + sft-text            | 0.73( $ \downarrow $ )   | 0.39( $ \uparrow $ )        |
| + dpo-unet            | 0.71( $ \downarrow $ )   | 0.34( $ - $ )               |
| + dpo-text            | 0.66( $ \downarrow $ )   | 0.40( $ \uparrow $ )        |"
182,"| Class                     | Reasons                   | SD XL                     | FT SD XL (trained on cat $ \leftrightarrow $ dog) |
| ------------------------- | ------------------------- | ------------------------- | ------------------------------------------------- |
| mouse $ \rightarrow $ cat | cat $ \rightarrow $ mouse | mouse $ \rightarrow $ cat | cat $ \rightarrow $ mouse                         |
| Wrong                     | Missing Objects           | 12                        | 14                                                |
| No Interaction            | 4                         | 5                         | 2                                                 |
| Wrong Interaction         | 3                         | 1                         | 4                                                 |
| Wrong Direction           | 7                         | 8                         | 4                                                 |
| Reversed Role             | 2                         | 0                         | 5                                                 |
| Right                     | Partial/Full Match        | 0                         | 2                                                 |"
183,"| Model                  | Contamination |
| ---------------------- | ------------- |
| 0%                     | 10%           |
| FedAvg                 | 85.72±0.52    |
| FedPA                  | 88.08±0.30    |
| $ \beta $ -PredBayes   | 87.58±0.13    |
| PVI                    | 86.21±0.21    |
| FedGVI  $ \delta=0.0 $ | 87.12±0.12    |
| FedGVI  $ \delta=0.4 $ | 88.73±0.21    |
| FedGVI  $ \delta=0.5 $ | 89.02±0.18    |
| FedGVI  $ \delta=0.8 $ | 88.59±0.03    |
| FedGVI  $ \delta=1.0 $ | 88.09±0.08    |"
185,"| Datasets                | O2BR   | YinQiWenYuan_detection |
| ----------------------- | ------ | ---------------------- |
| LMM (variant)           | What↑  | Yes-or-No↑             |
| HUMAN (public)          | 0.9364 | 100%                   |
| Proprietary LMMs:       |        |                        |
| GEMINI 1.5 PRO          | 0.5726 | 98.75%                 |
| GEMINI 1.5 FLASH        | 0.4123 | 96.50%                 |
| GPT-4V                  | 0.5408 | 99.30%                 |
| GPT-4O (ver. 0806)      | 0.6114 | 99.95%                 |
| QWEN-VL-MAX (ver. 0809) | 0.6071 | 99.63%                 |
| GLM-4V                  | 0.5319 | 39.17%                 |"
185,"| LMM (variant)           | Yes-or-No↑ | How↑   |
| ----------------------- | ---------- | ------ |
| Acc@1                   | Acc@5      | Acc@10 |
| GEMINI 1.5 Pro          | 28.53%     | 24.88% |
| GEMINI 1.5 FLASH        | 22.17%     | 19.33% |
| GPT-4v                  | 27.63%     | 26.86% |
| GPT-4o (ver. 0806)      | 32.21%     | 29.13% |
| QWEN-VL-MAX (ver. 0809) | 21.77%     | 13.14% |
| GLM-4V                  | 5.33%      | 4.58%  |"
185,"| Datasets                | HUST-OBS | EVOBC  | OBI Component 20 | Average    |
| ----------------------- | -------- | ------ | ---------------- | ---------- |
| LMM (variant)           | Tier-1   | Tier-2 | Tier-3           | pictograph |
| HUMAN (public)          | 0.4507   | 0.3884 | 0.3437           | 0.4966     |
| GEMINI 1.5 PRO          | 0.3766   | 0.3834 | 0.3589           | 0.4226     |
| GEMINI 1.5 FLASH        | 0.3545   | 0.3829 | 0.3567           | 0.3661     |
| GPT-4V                  | 0.3764   | 0.3808 | 0.3596           | 0.4424     |
| GPT-4o (ver. 0806)      | 0.3891   | 0.3510 | 0.3660           | 0.4535     |
| QWEN-VL-MAX (ver. 0809) | 0.2345   | 0.2273 | 0.2322           | 0.2565     |
| GLM-4V                  | 0.2180   | 0.1638 | 0.2122           | 0.2353     |"
186,"| Scenarios                               | EST     | PEFT    | HEFT   | GPHH   | ERL-DWS | Ours-Offline | Ours-Online |
| --------------------------------------- | ------- | ------- | ------ | ------ | ------- | ------------ | ----------- |
| Obj.                                    | Gap     | Obj.    | Gap    | Obj.   | Gap     | Obj.         | Gap         |
| $ \langle 6 \times 4, 5.4, 5k\rangle $  | 1076.01 | 277.05% | 439.28 | 53.93% | 391.63  | 37.23%       | 303.70      |
| $ \langle 6 \times 4, 5.4, 10k\rangle $ | 1077.09 | 279.13% | 439.64 | 54.75% | 390.26  | 37.37%       | 305.31      |
| $ \langle 6 \times 4, 5.4, 20k\rangle $ | 1072.90 | 276.97% | 439.88 | 54.55% | 391.18  | 37.44%       | 309.12      |
| $ \langle 6 \times 4, 9, 5k\rangle $    | 994.00  | 233.40% | 387.84 | 30.09% | 355.51  | 19.24%       | 303.57      |
| $ \langle 6 \times 4, 9, 10k\rangle $   | 993.97  | 238.09% | 387.64 | 31.85% | 355.21  | 20.82%       | 307.27      |
| $ \langle 6 \times 4, 9, 20k\rangle $   | 997.53  | 231.28% | 388.79 | 29.12% | 356.39  | 18.36%       | 312.56      |
|                                         | 6       | 5       | 4      | 3      | 7       | 1.83         | 1.17        |"
187,"| Statistic                         | Number       |
| --------------------------------- | ------------ |
| Total oversensitivity samples     | 300          |
| - Exaggerated Risk                | 100 (33.33%) |
| - Negated Harm                    | 100 (33.33%) |
| - Counterintuitive Interpretation | 100 (33.33%) |
| - synthesized images              | 61 (20.33%)  |
| - natural images                  | 239 (79.67%) |
| - images with human               | 178 (59.33%) |
| - images w/o human                | 122 (40.67%) |
| - images with OCR                 | 89 (29.67%)  |
| - images w/o OCR                  | 211 (70.33%) |"
187,"| Model                           | Refusal rate (%)            |
| ------------------------------- | --------------------------- |
| Exaggerated Risk                | GPT_evaluation Negated Harm |
| Proprietary MLLMs - Web version |                             |
| GPT-4o [34]                     | 6                           |
| Gemini Advanced [20]            | 41                          |
| Claude 3 opus [1]               | 41                          |
| Proprietary MLLMs - API         |                             |
| GPT-4V [32]                     | 3                           |
| GPT-4o [34]                     | 6                           |
| Gemini-Pro Vision [43]          | 20                          |
| Gemini-Pro 1.5 [44]             | 25                          |
| Claude 3 opus [1]               | 11                          |
| Claude 3 sonnet [1]             | 39                          |
| Claude 3 haiku [1]              | 27                          |
| Reka [45]                       | 11                          |
| Open-source MLLMs               |                             |
| IDEFICS-9b-Instruct [24]        | 17                          |
| Qwen-VL-Chat [5]                | 16                          |
| InternLM-XComposer2-7b [17]     | 14                          |
| InstructBLIP-Vicuna-7b [15]     | 21                          |
| MiniCPM-V 2.0 [21]              | 16                          |
| MiniCPM-Llama3-V 2.5 [21]       | 8                           |
| LLaVA-1.5-7b [27]               | 18                          |
| LLaVA-1.5-13b [27]              | 9                           |
| mPLUG-Owl2 [52]                 | 11                          |"
234,"| Method           | QueryDiff | C    | B    | M    | Avg. |
| ---------------- | --------- | ---- | ---- | ---- | ---- |
| DAFormer (2022a) | ✗         | 52.7 | 47.9 | 54.7 | 51.7 |
| ✓                | 56.3      | 50.1 | 58.8 | 55.1 |      |
| HRDA (2022b)     | ✗         | 57.4 | 49.1 | 61.2 | 55.9 |
| ✓                | 60.1      | 52.4 | 63.7 | 58.7 |      |
| DGInStyle (2023) | ✗         | 58.6 | 52.3 | 62.5 | 57.8 |
| ✓                | 61.7      | 56.3 | 63.9 | 60.6 |      |"
188,"|        | CLIP                                                                                                   | a pole vaul ter picks up the pole and starts running and then lifting the pole to jump                 |
| ------ | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------ |
| CLAVER | a pole vaul ter picks up the pole and starts running and then lifting the pole to jump                 |                                                                                                        |
|        | CLIP                                                                                                   | a man dri bbles under his hips on the basketball court to showcase his superb dri b bling skills       |
| CLAVER | a man dri bbles under his hips on the basketball court to showcase his superb dri b bling skills       |                                                                                                        |
|        | CLIP                                                                                                   | a man is ab se iling with ropes and ice axes along a steep ice surface                                 |
| CLAVER | a man is ab se iling with ropes and ice axes along a steep ice surface                                 |                                                                                                        |
|        | CLIP                                                                                                   | cross - country skiing involves gli ding forward, turning, stopping, and maneu vering around obstacles |
| CLAVER | cross - country skiing involves gli ding forward, turning, stopping, and maneu vering around obstacles |                                                                                                        |"
190,"| Dataset           | Method         | clear           | $ \delta_{k} $  | $ \delta_{u} $ |
| ----------------- | -------------- | --------------- | --------------- | -------------- |
| $ \epsilon=4 $    | $ \epsilon=8 $ | $ \epsilon=16 $ | $ \epsilon=32 $ | $ \epsilon=4 $ |
| CUB               | CBM            | 96.3(2.2)       | 89.2(3.5)       | 77.4(2.8)      |
| Hard AR           | 85.6(0.9)      | 77.3(1.2)       | 66.4(3.2)       | 50.8(1.8)      |
| ICBM              | 95.4(1.3)      | 86.4(0.8)       | 77.3(0.4)       | 56.4(1.6)      |
| PCBM              | 95.7(1.6)      | 85.6(1.0)       | 76.3(1.5)       | 58.7(2.1)      |
| ProbCBM           | 96.8(0.4)      | 85.5(0.2)       | 77.6(1.1)       | 59.7(1.5)      |
| Label-free CBM    | 96.4(1.3)      | 84.3(1.7)       | 76.9(1.4)       | 58.5(2.1)      |
| ProtoCBM          | 97.3(0.2)      | 92.3(1.3)       | 84.5(1.6)       | 64.5(3.2)      |
| ECBMs             | 96.4(1.3)      | 92.1(1.5)       | 87.5(3.7)       | 70.4(2.8)      |
| CBM-AT            | 93.4(1.4)      | 92.9(1.3)       | 85.6(0.7)       | 75.6(1.7)      |
| Hard AR-AT        | 82.5(0.3)      | 78.6(0.5)       | 76.6(1.6)       | 69.9(1.3)      |
| ICBM-AT           | 91.5(1.3)      | 91.6(2.4)       | 86.3(1.9)       | 79.3(1.6)      |
| PCBM-AT           | 93.6(0.6)      | 92.5(1.6)       | 84.4(0.9)       | 76.5(1.3)      |
| ProbCBM-AT        | 93.5(0.9)      | 90.3(1.4)       | 83.3(1.3)       | 78.2(1.6)      |
| Label-free CBM-AT | 93.4(1.3)      | 91.4(0.8)       | 86.2(1.4)       | 80.9(1.6)      |
| ProtoCBM-AT       | 94.4(0.7)      | 92.7(1.1)       | 87.2(1.9)       | 70.3(4.2)      |
| ECBMs-AT          | 93.6(1.0)      | 91.9(2.5)       | 88.1(2.1)       | 83.1(3.8)      |
| LEN               | 96.4(0.8)      | 89.1(3.4)       | 77.8(1.6)       | 56.7(1.2)      |
| DKAA              | 96.2(1.1)      | 91.2(1.5)       | 85.6(1.6)       | 76.9(1.3)      |
| MORDAA            | 96.5(0.1)      | 91.7(1.5)       | 86.1(1.8)       | 80.6(2.1)      |
| DeepProblog       | 96.4(0.2)      | 89.2(3.5)       | 77.4(2.8)       | 53.7(1.3)      |
| MBM               | 96.2(0.2)      | 93.5(3.1)       | 90.3(2.4)       | 88.7(3.2)      |
| C-HMCNN           | 96.5(0.4)      | 93.6(7.2)       | 89.7(1.2)       | 87.6(2.5)      |
| AGAIN             | 96.3(0.5)      | 92.4(1.2)       | 93.1(2.3)       | 93.8(1.9)      |"
191,"|                     | Stage 1 | Stage 2 |
| ------------------- | ------- | ------- |
| FID  $ \downarrow $ | 18.32   | 15.58   |"
191,"| Method                               | NFE | FID  | # param. (M) |
| ------------------------------------ | --- | ---- | ------------ |
| Consistency models                   |     |      |              |
| 1-step                               |     |      |              |
| iCT (Song &amp; Dhariwal, 2023)      | 1   | 2.83 | 56.4         |
| iCT-deep (Song &amp; Dhariwal, 2023) | 1   | 2.51 | 112          |
| CTM (Kim et al., 2023) (w/o GAN)     | 1   | 5.19 | 55.7         |
| ECM (Geng et al., 2024)              | 1   | 3.60 | 55.7         |
| TCM (ours)                           | 1   | 2.46 | 55.7         |"
191,"| Method                               | NFE | FID  | # param. (M) |
| ------------------------------------ | --- | ---- | ------------ |
| Consistency models                   |     |      |              |
| 1-step                               |     |      |              |
| iCT (Song &amp; Dhariwal, 2023)      | 1   | 4.02 | 296          |
| iCT-deep (Song &amp; Dhariwal, 2023) | 1   | 3.25 | 592          |
| ECM (Geng et al., 2024) (EDM2-S)     | 1   | 4.05 | 280          |
| TCM (ours; EDM2-S)                   | 1   | 2.88 | 280          |"
193,"| Design               | # PI                             | # PO                 | # Gates                          | DeepGate2 | MGVGA (Ours) |
| -------------------- | -------------------------------- | -------------------- | -------------------------------- | --------- | ------------ |
| NDCG@k  $ \uparrow $ | Top-k% Commonality  $ \uparrow $ | NDCG@k  $ \uparrow $ | Top-k% Commonality  $ \uparrow $ |           |              |
| k=3                  | k=5                              | k=3                  | k=5                              | k=10      | k=3          |
| bc0                  | 21                               | 11                   | 2784                             | 0.331     | 0.395        |
| apex1                | 45                               | 45                   | 2661                             | 0.645     | 0.643        |
| div                  | 128                              | 128                  | 101698                           | -0.063    | 0.029        |
| k2                   | 45                               | 45                   | 4075                             | -0.060    | 0.040        |
| i10                  | 257                              | 224                  | 3618                             | -0.133    | -0.080       |
| mainpla              | 26                               | 49                   | 9441                             | 0.674     | 0.629        |
| or1200_cpu           | 2343                             | 2072                 | 56570                            | 0.498     | 0.485        |
| picorv32             | 1631                             | 1601                 | 25143                            | 0.563     | 0.406        |
| Rocket               | 4413                             | 4187                 | 96507                            | 0.578     | 0.543        |
| sqrt                 | 128                              | 64                   | 40920                            | 0.304     | 0.153        |
| Average              |                                  |                      |                                  | 0.334     | 0.324        |"
193,"| Design     | DeepGate2 | MGVGA (Ours) |
| ---------- | --------- | ------------ |
| Precision  | Recall    | F1-Score     |
| bc0        | 0.199     | 0.930        |
| apex1      | 0.133     | 0.680        |
| div        | 0.203     | 0.980        |
| k2         | 0.171     | 0.720        |
| i10        | 0.414     | 0.940        |
| mainpla    | 0.178     | 0.790        |
| or1200_cpu | 0.451     | 0.790        |
| picorv32   | 0.448     | 0.870        |
| Rocket     | 0.346     | 0.930        |
| sqrt       | 0.189     | 0.740        |
| Average    | 0.295     | 0.841        |"
193,"| Masking Ratio | QoR Prediction | Logic Equivalence Identification |
| ------------- | -------------- | -------------------------------- |
| MGM           | VGA            | NDCG@k  $ \uparrow $             |
| k=3           | k=5            | k=3                              |
| 0.3           | 0.3            | 0.517                            |
| 0.3           | 0.5            | 0.540                            |
| 0.3           | 0.7            | 0.498                            |
| 0.5           | 0.3            | 0.439                            |
| 0.5           | 0.5            | 0.438                            |
| 0.5           | 0.7            | 0.385                            |
| 0.7           | 0.3            | 0.347                            |
| 0.7           | 0.5            | 0.400                            |
| 0.7           | 0.7            | 0.366                            |"
193,"| GNNs              | QoR Prediction | Logic Equivalence Identification |           |
| ----------------- | -------------- | -------------------------------- | --------- |
| NDCG@k ↑          | Top-k%         | Commonality ↑                    | Precision |
| k=3               | k=5            | k=3                              | k=5       |
| DeepGate2         | 0.334          | 0.324                            | 0.116     |
| GraphSAGE         | 0.469          | 0.479                            | 0.153     |
| Graph Transformer | 0.452          | 0.470                            | 0.154     |
| DeepGCN (Ours)    | 0.540          | 0.537                            | 0.193     |"
206,"| method                  | belief inference | goal inference | all  |
| ----------------------- | ---------------- | -------------- | ---- |
| 1.1                     | 1.2              | 1.3            | avg. |
| Human                   | 95.8             | 96.7           | 100  |
| Video-Llama2-13B        | 36.0             | 38.0           | 52.0 |
| LLaVA-7B                | 46.0             | 14.0           | 69.0 |
| GPT-4V                  | 94.0             | 13.0           | 59.0 |
| BIPALM w/ GPT-J-6B      | 90.0             | 69.0           | 86.0 |
| BIPALM w/ Llama2-7B     | 88.0             | 68.0           | 85.0 |
| Ours (w/ Llama3.1-405B) | 92.1             | 76.0           | 93.0 |"
194,"| Method        | Pred-Activity (median)  $ \uparrow $ | ATAC-Acc  $ \uparrow $  (%) | 3-mer Corr  $ \uparrow $ | JASPAR Corr  $ \uparrow $ | Log-Lik (median)  $ \uparrow $ |
| ------------- | ------------------------------------ | --------------------------- | ------------------------ | ------------------------- | ------------------------------ |
| Pretrained    | 0.17(0.04)                           | 1.5(0.2)                    | -0.061(0.034)            | 0.249(0.015)              | -261(0.6)                      |
| CG            | 3.30(0.00)                           | 0.0(0.0)                    | -0.065(0.001)            | 0.212(0.035)              | -266(0.6)                      |
| SMC           | 4.15(0.33)                           | 39.9(8.7)                   | 0.840(0.045)             | 0.756(0.068)              | -259(2.5)                      |
| TDS           | 4.64(0.21)                           | 45.3(16.4)                  | 0.848(0.008)             | 0.846(0.044)              | -257(1.5)                      |
| CFG           | 5.04(0.06)                           | 92.1(0.9)                   | 0.746(0.001)             | 0.864(0.011)              | -265(0.6)                      |
| DRAKES w/o KL | 6.44(0.04)                           | 82.5(2.8)                   | 0.307(0.001)             | 0.557(0.015)              | -281(0.6)                      |
| DRAKES        | 5.61(0.07)                           | 92.5(0.6)                   | 0.887(0.002)             | 0.911(0.002)              | -264(0.6)                      |"
194,"| Method        | Pred-ddG (median)  $ \uparrow $ | $ \% $ (ddG &gt; 0) (\%)  $ \uparrow $ | scRMSD (median)  $ \downarrow $ | $ \% $ (scRMSD &lt; 2) (\%)  $ \uparrow $ | Success Rate (\%)  $ \uparrow $ | Sequence Entropy  $ \uparrow $ |
| ------------- | ------------------------------- | -------------------------------------- | ------------------------------- | ----------------------------------------- | ------------------------------- | ------------------------------ |
| Pretrained    | -0.544(0.037)                   | 36.6(1.0)                              | 0.849(0.013)                    | 90.9(0.6)                                 | 34.4(0.5)                       | 34.7(0.2)                      |
| CG            | -0.561(0.045)                   | 36.9(1.1)                              | 0.839(0.012)                    | 90.9(0.6)                                 | 34.7(0.9)                       | 34.6(0.1)                      |
| SMC           | 0.659(0.044)                    | 68.5(3.1)                              | 0.841(0.006)                    | 93.8(0.4)                                 | 63.6(4.0)                       | 24.9(1.2)                      |
| TDS           | 0.674(0.086)                    | 68.2(2.4)                              | 0.834(0.001)                    | 94.4(1.2)                                 | 62.9(2.8)                       | 24.9(0.5)                      |
| CFG           | -1.186(0.035)                   | 11.0(0.4)                              | 3.146(0.062)                    | 29.4(1.0)                                 | 1.3(0.4)                        | 8.4(0.1)                       |
| DRAKES w/o KL | 1.108(0.004)                    | 100.0(0.0)                             | 7.307(0.054)                    | 34.1(0.2)                                 | 34.1(0.2)                       | 25.7(0.1)                      |
| DRAKES        | 1.095(0.026)                    | 86.4(0.2)                              | 0.918(0.006)                    | 91.8(0.5)                                 | 78.6(0.7)                       | 33.3(0.2)                      |"
194,"| Method     | Pred-Score (mean)  $ \uparrow $ |
| ---------- | ------------------------------- |
| Pretrained | 6.996                           |
| CG         | 8.488                           |
| SMC        | 9.286                           |
| TDS        | 9.817                           |
| DRAKES     | 10.044                          |"
195,"| Method   | FAD $ \times $ 100 | SI-SDR | fwSSNR |
| -------- | ------------------ | ------ | ------ |
| NFE = 6  |                    |        |        |
| FlowDec  | 1.62               | 7.55   | 15.46  |
| ScoreDec | 145.30             | -27.23 | 3.15   |
| NFE = 50 |                    |        |        |
| FlowDec  | 1.34               | 7.41   | 15.65  |
| ScoreDec | 5.73               | 7.50   | 14.45  |"
196,"| Method                                  | Acc↑ | Aes↑ | Rec↑ |
| --------------------------------------- | ---- | ---- | ---- |
| Stable Diffusion 2.1                    | 37.5 | 5.19 | 76.7 |
| + Sampling with top seeds (Ours)        | 43.0 | 5.23 | 73.9 |
| + Fine-tuning (Ours)                    | 51.3 | 5.13 | 71.3 |
| + LMD (Lian et al., 2023)               | 35.8 | 4.65 | 49.4 |
| + MultiDiffusion (Bar-Tal et al., 2023) | 29.2 | 4.40 | 36.2 |
| + Ranni (Feng et al., 2024b)            | 50.7 | 4.43 | 46.6 |"
199,"|               | Training-Free | Adaptable | Performance |
| ------------- | ------------- | --------- | ----------- |
| Quantization  | ✓             | ✗         | ✓           |
| Distillation  | ✗             | ✗         | ✓           |
| Pruning       | ✗             | ✗         | ✓           |
| Decomposition | ✓             | ✓         | ✗           |
| BitStack      | ✓             | ✓         | ✓           |"
201,"| Data | Split | Examples per tier | Tier 1 | Tier 2 | Tier 3 |
| ---- | ----- | ----------------- | ------ | ------ | ------ |
| Med. | Max   | Vocab.            | Med.   | Max    | Vocab. |
| CLD  | Train | 19195             | 27     | 49     | 6528   |
| CLD  | Val   | 5225              | 27     | 46     | 3743   |
| CLD  | Test  | 5225              | 27     | 28     | 5225   |
| ACD  | Train | 48660             | 29     | 47     | 3287   |
| ACD  | Val   | 2456              | 28     | 46     | 2350   |
| ACD  | Test  | 4680              | 29     | 47     | 3287   |"
201,"|            |          | Tier 1   | Tier 2 | Tier 3 |
| ---------- | -------- | -------- | ------ | ------ |
| Task       | Models   | BLEU_{4} | METEOR | SPIDEr |
| ACD        | Baseline | 0.118    | 0.210  | 0.220  |
| QwenAC (L) | 0.132    | 0.214    | 0.235  | 0.166  |
| QwenAC (F) | 0.110    | 0.183    | 0.258  | 0.163  |
| ADIFF      | 0.135    | 0.221    | 0.303  | 0.180  |
| CLD        | Baseline | 0.128    | 0.237  | 0.212  |
| QwenAC (L) | 0.140    | 0.285    | 0.230  | 0.232  |
| QwenAC (F) | 0.126    | 0.232    | 0.204  | 0.273  |
| ADIFF      | 0.203    | 0.302    | 0.652  | 0.213  |"
201,"|            | Studio | FSD50K | GTZAN | Average |
| ---------- | ------ | ------ | ----- | ------- |
| Model      | LLM    | COR    | GRA   | RDB     |
| QwenAC (Z) | 7B     | 2.73   | 2.64  | 3.09    |
| Baseline   | 128M   | 2.99   | 3.26  | 3.21    |
| QwenAC (L) | 7B     | 3.05   | 3.41  | 3.25    |
| QwenAC (F) | 7B     | 3.09   | 3.50  | 3.37    |
| ADIFF      | 128M   | 3.12   | 3.73  | 3.34    |"
201,"| Audio 1 prefix                                        | Audio 2 prefix                                   | Text prefix                                                   |
| ----------------------------------------------------- | ------------------------------------------------ | ------------------------------------------------------------- |
| explosion, crack, hit                                 | helicop, loud                                    | mid-high, frequency, pitch, dynamic, range                    |
| bee, robots, voices, repetition, downstairs, speakers | motor, rustic, radio, pursuit, vintage, throttle | development, plus, nature, catch, dynamic range, loud, single |
| baby, cry, speaker, female                            | speaker, cheering, male                          | clean, predict, loud, greater                                 |
| clap, man, speaker, after                             | drum, bang, tik                                  | dur, loud, intelli, soft, surprise                            |"
203,"| Ability    | Tic Tac Toe | Reversi | Sudoku | Minesweeper | Gomoku | Chess |
| ---------- | ----------- | ------- | ------ | ----------- | ------ | ----- |
| Perception | ☆           | ★★☆     | ★★★☆   | ★★★★        | ★★★★★  | ★★★☆  |
| Reasoning  | ☆           | ★★☆     | ★★     | ★★★★★       | ★★★★   | ★★★   |
| Decision   | ☆           | ★★☆     | ★★     | ★★★         | ★★★☆   | ★★★★★ |
| Adversary  | ☆           | ★★☆     | N/A    | N/A         | ★★★★   | ★★★★★ |"
362,"| Model         | Grid Size                           | Log-Lik.  $ \uparrow $ | FPT  $ \downarrow $ |
| ------------- | ----------------------------------- | ---------------------- | ------------------- |
| ViTNP (PT)    | 24  $ \times $  60  $ \rightarrow $ | 7.288                  | 392                 |
| Swin-TNP (PT) | 24  $ \times $  60                  | 8.603                  | 375                 |"
203,"| LVLMs             | TicTacToe | Reversi | Sudoku | Minesweeper | Gomoku | Chess |
| ----------------- | --------- | ------- | ------ | ----------- | ------ | ----- |
| GPT-4o            | 0.812     | 0.440   | 0.430  | 0.436       | 0.351  | 0.485 |
| Gemini1.5-pro     | 0.843     | 0.524   | 0.471  | 0.420       | 0.361  | 0.414 |
| Claude-3.5-sonnet | 0.821     | 0.694   | 0.694  | 0.778       | 0.467  | 0.656 |
| Qwen2-vl-7b       | 0.553     | 0.409   | 0.407  | 0.546       | 0.296  | 0.441 |
| Deepseek-vl-7b    | 0.326     | 0.319   | 0.260  | 0.357       | 0.265  | 0.396 |
| Phi3-vl           | 0.477     | 0.354   | 0.313  | 0.470       | 0.278  | 0.281 |
| LLaVA-1.6-7b      | 0.269     | 0.244   | 0.281  | 0.248       | 0.249  | 0.272 |
| InternVL2-8b      | 0.600     | 0.334   | 0.365  | 0.457       | 0.280  | 0.393 |
| Random            | 0.256     | 0.267   | 0.268  | 0.281       | 0.277  | 0.264 |"
203,"| LVLMs             | TicTacToe | Reversi | Sudoku | Minesweeper | Gomoku | Chess |
| ----------------- | --------- | ------- | ------ | ----------- | ------ | ----- |
| GPT-4o            | 0.895     | 0.070   | 0.462  | 0.333       | 0.413  | 0.509 |
| Gemini1.5-pro     | 1.000     | 0.155   | 0.508  | 0.697       | 0.625  | 0.313 |
| Claude-3.5-sonnet | 0.874     | 0.255   | 0.721  | 0.308       | 0.506  | 0.133 |
| Qwen2-vl-7b       | 0.690     | 0.174   | 0.238  | 0.570       | 0.517  | 0.440 |
| Deepseek-vl-7b    | 0.521     | 0.091   | 0.212  | 0.587       | 0.509  | 0.419 |
| Phi3-vl           | 0.461     | 0.145   | 0.235  | 0.567       | 0.483  | 0.370 |
| LLaVA-1.6-7b      | 0.639     | 0.102   | 0.193  | 0.584       | 0.454  | 0.003 |
| InternVL2-8b      | 0.706     | 0.170   | 0.206  | 0.583       | 0.510  | 0.378 |
| Random            | 0.342     | 0.127   | 0.214  | 0.422       | 0.508  | 0.014 |"
203,"| LVLMs             | TicTacToe | Reversi | Sudoku | Minesweeper | Gomoku | Chess |
| ----------------- | --------- | ------- | ------ | ----------- | ------ | ----- |
| GPT-4o            | 18.8      | 94.9    | 9.2    | 82.8        | 55.5   | 70.8  |
| Gemini1.5-pro     | 30.1      | 100.7   | 8.3    | 68.8        | 61.1   | 12.7  |
| Claude-3.5-sonnet | 31.0      | 70.0    | 22.5   | 68.7        | 66.7   | 32.6  |
| Qwen2-vl-7b       | 20.0      | 40.0    | 1.1    | 30.2        | 10.0   | 27.9  |
| Deepseek-vl-7b    | 14.9      | 40.0    | 1.5    | 36.3        | 10.0   | 10.0  |
| Phi3-vl           | 20.0      | 40.0    | 1.9    | 49.8        | 30.0   | 10.0  |
| LLaVA-1.6-7b      | 10.0      | 40.0    | 1.2    | 0.0         | 10.0   | 0.0   |
| InternVL2-8b      | 19.2      | 40.0    | 0.9    | 38.8        | 20.0   | 10.0  |"
204,"| Model           | CLIP $ _{\text{dir}} $ | CLIP $ _{\text{img}} $ | DINO   | SSIM   | Reward Score |
| --------------- | ---------------------- | ---------------------- | ------ | ------ | ------------ |
| Raw             | Sigmoid                |                        |        |        |              |
| InstructPix2Pix | 0.0444                 | 0.7353                 | 0.7252 | 0.1673 | -0.54        |
| HIVE            | 0.0970                 | 0.8633                 | 0.8851 | 0.4646 | -0.09        |
| MagicBrush      | 0.1109                 | 0.8173                 | 0.8246 | 0.2735 | -0.05        |
| HQ-Edit (Ours)  | 0.1351                 | 0.9246                 | 0.9692 | 0.6561 | -0.03        |"
204,"| Model           | CLIP $ _{\text{dir}} $ | CLIP $ _{\text{img}} $ | DINO   | SSIM   | Reward Score |
| --------------- | ---------------------- | ---------------------- | ------ | ------ | ------------ |
| Raw             | Sigmoid                |                        |        |        |              |
| InstructPix2Pix | 0.0775                 | 0.8396                 | 0.7879 | 0.2223 | -2.05        |
| HIVE            | 0.0527                 | 0.8567                 | 0.7855 | 0.1978 | -2.07        |
| MagicBrush      | 0.1011                 | 0.8526                 | 0.8278 | 0.2516 | -2.13        |
| HQ-Edit (Ours)  | 0.1067                 | 0.8588                 | 0.8139 | 0.2231 | -1.97        |"
205,"| Dataset | Cases | #Unique RC | Telemetry Data |
| ------- | ----- | ---------- | -------------- |
| Count   | C.    | R.         | Size           |
| Telecom | 51    | 15         | 5              |
| Bank    | 136   | 14         | 8              |
| Market  | 148   | 44         | 15             |
| Total   | 335   | 73         | 28             |"
205,"|                    | Balanced          | Oracle  | RCA-agent |
| ------------------ | ----------------- | ------- | --------- |
| Model              | Correct           | Partial | Correct   |
| Closed             | Claude 3.5 Sonnet | 3.88    | 18.81     |
| GPT-4o             | 3.28              | 14.33   | 6.27      |
| Gemini 1.5 Pro     | 6.27              | 24.18   | 7.16      |
| Open               | Mistral Large 2*  | 3.58    | 6.40      |
| Command R+         | 4.18              | 8.96    | 4.78      |
| Llama 3.1 Instruct | 2.99              | 14.63   | 3.88      |"
205,"| Model     | Method | Category |
| --------- | ------ | -------- |
| Easy      | Mid    | Hard     |
| Claude    | Oracle | 8.72     |
| RCA-agent | 16.78  | 9.09     |
| GPT       | Oracle | 9.40     |
| RCA-agent | 13.42  | 6.99     |
| Llama     | Oracle | 7.38     |
| RCA-agent | 6.71   | 0.70     |"
205,"| Type   | Category | Absolute | Delta |
| ------ | -------- | -------- | ----- |
|        |          | R.       | C.    |
| Oracle | Claude   | 12.56    | 13.54 |
| GPT-4o | 13.39    | 12.56    | 12.14 |
| Llama  | 11.30    | 11.72    | 6.70  |
| Agent  | Claude   | 19.67    | 18.00 |
| GPT-4o | 18.00    | 14.65    | 13.39 |
| Llama  | 7.53     | 3.35     | 2.51  |
| Random | -        | 11.06    | 4.92  |"
205,"| Model   | RCA-agent           |
| ------- | ------------------- |
| Correct | Drop $ \downarrow $ |
| Claude  | 9.31                |
| GPT     | 7.56                |
| Gemini  | 0.85                |
| Llama   | 3.18                |"
206,"| LM               | config       | belief inference | goal inference | all   |
| ---------------- | ------------ | ---------------- | -------------- | ----- |
| 1.1              | 1.2          | 1.3              | avg.           | 2.1   |
| Llama3.1         | 8B-zero-shot | 88.00            | 72.00          | 91.00 |
| 8B-post-trained  | 90.00        | 71.00            | 93.00          | 84.67 |
| 70B-zero-shot    | 85.00        | 63.00            | 93.00          | 80.33 |
| 70B-post-trained | 91.00        | 69.00            | 95.00          | 85.00 |
| 405B-zero-shot   | 86.00        | 70.00            | 90.00          | 82.00 |
| 70B-ours         | 90.00        | 74.00            | 93.00          | 85.67 |
| 405B-ours        | 92.10        | 76.00            | 93.00          | 87.10 |"
206,"| LM            | Method          | Belief Avg. | Goal Avg. | All Avg. |
| ------------- | --------------- | ----------- | --------- | -------- |
| Llama3.1 70B  | Naive Logit Add | 82.50       | 66.40     | 74.45    |
| Llama3.1 405B | Naive Logit Add | 83.67       | 65.67     | 74.67    |"
234,"| $ t_{w} $ | $ t_{s} $ | C    | B    | M    | Avg. |
| --------- | --------- | ---- | ---- | ---- | ---- |
| 0         | 50        | 68.2 | 62.0 | 68.4 | 66.2 |
| 50        | 100       | 68.7 | 62.1 | 68.6 | 66.5 |
| 0         | 100       | 69.1 | 62.3 | 68.6 | 66.7 |
| 75        | 100       | 68.3 | 62.0 | 68.2 | 66.1 |
| 100       | 200       | 68.5 | 62.1 | 67.1 | 65.9 |
| 0         | 200       | 67.6 | 61.9 | 67.4 | 65.6 |"
207,"| Model                   | Objective Metrics   |
| ----------------------- | ------------------- |
| WER  $ \downarrow $     | CER  $ \downarrow $ |
| Ground Truth            | 2.15                |
| YourTTS                 | 7.57                |
| VALL-E                  | 3.8*                |
| Voicebox                | 2.0*                |
| CLaM-TTS                | 2.36*               |
| Simple-TTS              | 3.86                |
| DiTTo-en-S              | 2.01                |
| DiTTo-en-B              | 1.87                |
| DiTTo-en-L              | 1.85                |
| DiTTo-en-XL             | 1.78                |
| DiTTo-en-XL $ \dagger $ | 1.80                |"
207,"| Model      | WER  $ \downarrow $ | SIM-r  $ \uparrow $ | Inference Time  $ \downarrow $ |
| ---------- | ------------------- | ------------------- | ------------------------------ |
| U-Net      | 3.70                | 0.3890              | 1.328s                         |
| Flat-U-Net | 2.97                | 0.5471              | 1.310s                         |
| DiTTo-mls  | 2.93                | 0.5877              | 0.903s                         |"
207,"| Model             | WER  $ \downarrow $ | SIM-r  $ \uparrow $ | Inference Time  $ \downarrow $ |
| ----------------- | ------------------- | ------------------- | ------------------------------ |
| fixed-length-full | 8.89                | 0.4078              | 1.254s                         |
| fixed-length      | 6.81                | 0.4385              | 1.265s                         |
| SLP-CE            | 5.58                | 0.4961              | 0.948s                         |
| SLP-Regression    | 5.36                | 0.4636              | 0.930s                         |"
207,"| Text Encoder  | Neural Audio Codec | WER  $ \downarrow $ | CER  $ \downarrow $ | SIM-o  $ \uparrow $ | SIM-r  $ \uparrow $ |
| ------------- | ------------------ | ------------------- | ------------------- | ------------------- | ------------------- |
| U (ByT5-base) | U (Mel-VAE)        | 6.22                | 3.82                | 0.5482              | 0.5945              |
| J (SpeechT5)  | U (Mel-VAE)        | 3.07                | 1.15                | 0.5423              | 0.5858              |
| U (ByT5-base) | J (Mel-VAE++)      | 3.11                | 1.17                | 0.5323              | 0.5965              |
| J (SpeechT5)  | J (Mel-VAE++)      | 2.99                | 1.06                | 0.5364              | 0.5982              |"
207,"| Model                    | WER  $ \downarrow $ | SIM-o  $ \uparrow $ | SIM-r  $ \uparrow $ | Inference Time  $ \downarrow $ | Codec PESQ  $ \uparrow $ | Codec ViSQOL  $ \uparrow $ |
| ------------------------ | ------------------- | ------------------- | ------------------- | ------------------------------ | ------------------------ | -------------------------- |
| DiTTo-local-adaln        | 3.38                | 0.5263              | 0.5673              | 0.937s                         |                          |                            |
| DiTTo-uvit-skip          | 3.17                | 0.5456              | 0.5848              | 0.940s                         |                          |                            |
| DiTTo-no-skip            | 3.30                | 0.5304              | 0.5727              | 0.905s                         | 2.95                     | 4.66                       |
| DiTTo-no-pooled-text     | 3.00                | 0.5410              | 0.5791              | 0.912s                         |                          |                            |
| DiTTo-no-rvq-decoding    | 2.97                | 0.5468              | 0.5883              | 0.894s                         |                          |                            |
| DiTTo-mls (from Table 4) | 2.93                | 0.5467              | 0.5877              | 0.903s                         |                          |                            |
| DiTTo-encodec            | 4.19                | 0.5105              | 0.5460              | n/a                            | 2.59                     | 4.26                       |
| DiTTo-dac-24k            | 7.21                | 0.5478              | 0.5545              | n/a                            | 4.37                     | 4.91                       |
| DiTTo-dac-44k            | 14.58               | 0.5391              | 0.5597              | n/a                            | 3.74                     | 4.85                       |"
208,"|                               | DomainNet: Subset-50 | DomainNet: Subset-100 | DomainNet: Subset-150 | PACS    |
| ----------------------------- | -------------------- | --------------------- | --------------------- | ------- |
| ood acc                       | ind acc              | rounds                | ood acc               | ind acc |
| local                         | 0.7361               | 0.8609                | 0                     | 0.6554  |
| FedAvg (McMahan et al. 2017a) | 0.7902               | 0.7345                | 24                    | 0.7628  |
| FedProx (Li et al. 2020)      | 0.7752               | 0.7178                | 10                    | 0.7499  |
| Ditto (Li et al. 2021b)       | 0.7811               | 0.7624                | 20                    | 0.7511  |
| MOON (Li et al. 2021a)        | 0.7902               | 0.7344                | 28                    | 0.7623  |
| FedProto (Tan et al. 2022b)   | 0.7296               | 0.7696                | 5                     | 0.6732  |
| DBE (Zhang et al. 2024)       | 0.7421               | 0.7622                | 22                    | 0.7179  |
| MPFT (Average)                | 0.8077               | 0.7813                | 1                     | 0.7674  |
| MPFT (Cluster, rate=0.1)      | 0.7951               | 0.7957                | 1                     | 0.7641  |
| MPFT (Cluster, rate=0.3)      | 0.8204               | 0.8294                | 1                     | 0.7766  |
| MPFT (Random, rate=0.1)       | 0.7953               | 0.7899                | 1                     | 0.7566  |
| MPFT (Random, rate=0.3)       | 0.8236               | 0.8294                | 1                     | 0.7803  |"
208,"|                          | Computation Cost (total training time) | Communication Cost (total parameter transmission) |
| ------------------------ | -------------------------------------- | ------------------------------------------------- |
| Subset-50                | Subset-100                             | Subset-150                                        |
| local                    | 240.8s                                 | 834.8s                                            |
| FedAvg                   | 1933.5s                                | 5812.4s                                           |
| FedProx                  | 803.9s                                 | 1253.5s                                           |
| Ditto                    | 3095.7s                                | 8365.8s                                           |
| MOON                     | 2164.2s                                | 2237.4s                                           |
| FedProto                 | 393.1s                                 | 1153.4s                                           |
| DBE                      | 1693.6s                                | 837.6s                                            |
| MPFT (Average)           | 1.9s                                   | 7.3s                                              |
| MPFT (Cluster, rate=0.1) | 44.7s                                  | 302.4s                                            |
| MPFT (Cluster, rate=0.3) | 525.9s                                 | 624.1s                                            |
| MPFT (Random, rate=0.1)  | 33.1s                                  | 99.9s                                             |
| MPFT (Random, rate=0.3)  | 454.5s                                 | 478.9s                                            |"
208,"|          | q = 0.1 | s = 0.05 |
| -------- | ------- | -------- |
| original | s = 0.1 | s = 0.5  |
| ood acc  | 0.8077  | 0.8064   |
| ind acc  | 0.7813  | 0.7806   |"
212,"| Estimator                                                                    | Concentration                                                                                                                            | Convergence Rate                      | Heavy-tailed | Regret Bound | Noisy Reward | Differentiability | Subgaussian Like Tail |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- | ------------ | ------------ | ------------ | ----------------- | --------------------- |
| SN-IPS (Swaminathan Joachims, 2015b)                                         | $ R_{\text{max}}(B^{\text{SN}} + \sqrt{V^{\text{ES}} \log \frac{1}{\delta}}) $                                                           | -                                     | ✗            | ✗            | ✗            | ✓                 | ✗                     |
| IPS-TR (M &gt; 0) (Ionides, 2008a)                                           | $ R_{\text{max}} \sqrt{\frac{P_2(\pi_\theta \parallel \pi_0) \log \frac{1}{\delta}}{n}} $                                                | $ O(n^{-1/2}) $                       | ✗            | ✓            | ✗            | ✗                 | ✓                     |
| IX ( $ \eta &gt; 0 $ ) (Gabbianelli et al., 2023)                            | $ R_{\text{max}}(2\eta C_\eta(\pi_\theta) + \frac{\log(2/\delta)}{\eta n}) $                                                             | $ O(n^{-1/2}) $                       | ✗            | ✓            | ✗            | ✓                 | ✓                     |
| PM ( $ \hat{\lambda} \in [0, 1] $ ) (Metelli et al., 2021)                   | $ R_{\text{max}} \sqrt{\frac{P_2(\pi_\theta \parallel \pi_0) \log \frac{1}{\delta}}{n}} $                                                | $ O(n^{-1/2}) $                       | ✗            | ✗            | ✗            | ✓                 | ✓                     |
| ES ( $ \alpha \in [0, 1] $ ) (Aouali et al., 2023)                           | $ R_{\text{max}} \sqrt{\frac{D_{\text{KL}}(\pi_\theta \parallel \pi_0) + \log(4\sqrt{n}/\delta)}{n}} + T^{\text{ES}} $                   | $ O\left((\log(n)/n)^{1/2}\right) $   | ✗            | ✓            | ✗            | ✓                 | ✗                     |
| OS ( $ \tau &gt; 0 $ ) (Su et al., 2020)                                     | $ \max_{\beta \in \{2, 3\}} \sqrt{\frac{P_\beta(\pi_\theta \parallel \pi_0)\left(\log \frac{1}{\delta}\right)^{\beta-1}}{n^{\beta-1}}} $ | $ O\left(n^{(1-\beta)/\beta}\right) $ | ✗            | ✗            | ✗            | ✓                 | ✗                     |
| LS ( $ \hat{\lambda} \geq 0 $ ) (Sakhi et al., 2024)                         | $ \tilde{\lambda} S_\lambda(\pi_\theta) + \frac{\log(2/\delta)}{\hat{\lambda} n} $                                                       | $ O(n^{-1/2}) $                       | ✗            | ✓            | ✗            | ✓                 | ✓                     |
| LSE (0 &gt;  $ \lambda &gt; -\infty $  and  $ \epsilon \in [0, 1] $ ) (ours) | $ C\left(\frac{2 \log(2|\Pi_\theta|/\delta)}{n}\right)^{\epsilon/(1+\epsilon)} $                                                         | $ O(n^{-\epsilon/(1+\epsilon)}) $     | ✓            | ✓            | ✓            | ✓                 | ✓                     |"
209,"| Methods   | Cora     | Citeseer | Pubmed   | Texas    | Wisconsin | Cornell   | OGB-arXiv |
| --------- | -------- | -------- | -------- | -------- | --------- | --------- | --------- |
| MLP       | 55.1±1.3 | 59.1±0.5 | 71.4±0.4 | 92.3±0.7 | 91.8±3.1  | 91.3±0.7  | 55.0±0.3  |
| GCN       | 81.5±0.5 | 70.9±0.5 | 79.0±0.3 | 75.7±1.0 | 66.7±1.4  | 66.5±13.8 | 72.7±0.3  |
| GAT       | 83.0±0.7 | 72.0±0.7 | 78.5±0.3 | 78.8±0.9 | 71.0±4.6  | 76.0±1.0  | 72.0±0.5  |
| GPRGNN    | 83.8±0.9 | 75.9±1.2 | 79.8±0.8 | 75.9±6.2 | 89.9±3.0  | 85.0±5.2  | 70.4±1.5  |
| APPNP     | 83.5±0.7 | 75.9±0.6 | 79.0±0.3 | 83.9±0.7 | 90.1±3.5  | 89.8±0.6  | 70.3±2.5  |
| H2GCN     | 83.4±0.5 | 73.1±0.4 | 79.2±0.3 | 85.9±4.6 | 87.9±4.2  | 85.1±6.4  | 72.8±2.4  |
| SAGE      | 74.5±0.8 | 67.2±1.0 | 76.8±0.6 | 79.3±1.2 | 64.8±5.2  | 71.4±1.2  | 70.6±1.6  |
| GRAND++   | 82.9±1.4 | 70.8±1.1 | 79.2±1.5 | 81.4±3.5 | 88.6±2.1  | 75.6±3.2  | 74.1±2.3  |
| Framelets | 83.3±0.5 | 71.0±0.6 | 79.4±0.4 | 82.3±2.5 | 88.9±3.2  | 72.6±0.3  | 71.9±0.2  |
| DMD-GCN   | 82.6±0.7 | 71.4±2.6 | 79.3±0.9 | 84.2±2.6 | 85.2±2.1  | 83.6±3.1  | 73.9±2.8  |
| DMD-SGC   | 84.1±0.4 | 72.6±0.8 | 79.4±1.4 | 81.2±2.5 | 83.0±1.8  | 80.0±1.9  | 74.1±0.9  |
| DMD++     | 82.3±0.3 | 73.2±0.4 | 79.9±0.7 | 92.6±3.4 | 91.9±2.6  | 91.4±1.7  | 74.4±1.5  |
| DMD-ACMP  | 82.9±0.9 | 73.0±2.1 | 81.2±1.5 | 89.4±3.8 | 89.4±2.2  | 88.1±2.4  | 75.5±0.8  |"
209,"| Methods | COCO-SP     | PascalVOC-SP |
| ------- | ----------- | ------------ |
| MLP     | 0.031±0.016 | 0.114±0.023  |
| GCN     | 0.079±0.025 | 0.238±0.016  |
| GPRGNN  | 0.044±0.015 | 0.152±0.024  |
| SGC     | 0.056±0.011 | 0.216±0.039  |
| DMD-GCN | 0.081±0.015 | 0.243±0.014  |
| DMD-SGC | 0.083±0.012 | 0.217±0.036  |
| DMD++   | 0.091±0.009 | 0.241±0.017  |"
209,"| Methods | Cora     | Citeseer | Chameleon | Squirrel |
| ------- | -------- | -------- | --------- | -------- |
| MLP     | 75.0±1.1 | 78.3±0.8 | 76.9±0.4  | 73.2±0.2 |
| GCN     | 76.9±0.8 | 77.9±0.9 | 78.7±0.5  | 74.7±0.1 |
| SAGE    | 75.1±0.5 | 76.3±1.2 | 82.1±0.4  | 75.3±0.3 |
| APPNP   | 76.0±0.9 | 75.9±1.1 | 78.9±0.8  | 73.6±0.2 |
| DMD-GCN | 75.7±0.4 | 78.7±0.6 | 76.1±0.4  | 78.8±0.4 |
| DMD-SGC | 77.4±0.2 | 77.2±0.7 | 78.3±0.7  | 74.3±1.2 |
| DMD++   | 75.3±0.6 | 76.6±0.6 | 77.5±0.3  | 78.1±0.5 |"
209,"| Dataset    | MLP           | GCN           | GAT           | GAT-KNN       | GCN-KNN       | ChebNet       | Grand++       | Framelet      | DMD-GCN       | DMD-SGC       | DMD-ACMP      | DMD++         |
| ---------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |
| Chickenpox | 0.924(±0.001) | 0.923(±0.001) | 0.924(±0.002) | 0.926(±0.004) | 0.926(±0.004) | 0.916(±0.014) | 0.991(±0.009) | 0.923(±0.03)  | 0.978(±0.093) | 0.935(±0.014) | 0.924(±0.001) | 0.910(±0.002) |
| Covid      | 0.956(±0.034) | 0.956(±0.029) | 1.052(±0.081) | 0.861(±0.045) | 1.475(±0.028) | 0.991(±0.078) | 1.271(±0.073) | 0.938(±0.081) | 0.760(±0.025) | 0.805(±0.067) | 0.809(±0.011) | 0.784(±0.013) |
| WikiMath   | 1.073(±0.042) | 1.292(±0.125) | 1.339(±0.073) | 0.826(±0.070) | 1.023(±0.058) | 1.589(±0.048) | 1.044(±0.328) | 0.988(±0.069) | 0.848(±0.031) | 0.901(±0.028) | 0.934(±0.015) | 0.805(±0.042) |"
210,"| Dataset             | Model   | Bandwidth  $ \downarrow $  Nq  $ \downarrow $  token/s  $ \downarrow $ | UTMOS  $ \uparrow $  PESQ  $ \uparrow $  STOI  $ \uparrow $ | V/UV F1  $ \uparrow $ |
| ------------------- | ------- | ---------------------------------------------------------------------- | ----------------------------------------------------------- | --------------------- |
| LibriTTS test-clean | GT      | -                                                                      | -                                                           | -                     |
| DAC                 | 9.0kbps | 9                                                                      | 900                                                         | 3.9097                |
| Encodec             | 6.0kbps | 8                                                                      | 600                                                         | 3.0399                |
| Vocos               | 6.0kbps | 8                                                                      | 600                                                         | 3.6954                |
| SpeechTokenizer     | 6.0kbps | 8                                                                      | 600                                                         | 3.8794                |
| DAC                 | 4.0kbps | 4                                                                      | 400                                                         | 3.4329                |
| HiFi-Codec          | 3.0kbps | 4                                                                      | 400                                                         | 3.7529                |
| HiFi-Codec          | 4.0kbps | 4                                                                      | 300                                                         | 3.9035                |
| Encodec             | 3.0kbps | 4                                                                      | 300                                                         | 2.3070                |
| Vocos               | 3.0kbps | 4                                                                      | 300                                                         | 3.5390                |
| SpeechTokenizer     | 3.0kbps | 4                                                                      | 300                                                         | 3.5632                |
| DAC                 | 1.0kbps | 1                                                                      | 100                                                         | 1.4940                |
| WavTokenizer        | 0.5kbps | 1                                                                      | 40                                                          | 3.6016                |
| WavTokenizer        | 0.9kbps | 1                                                                      | 75                                                          | 4.0486                |
| LibriTTS test-other | GT      | -                                                                      | -                                                           | -                     |
| DAC                 | 9.0kbps | 9                                                                      | 900                                                         | 3.3566                |
| Encodec             | 6.0kbps | 8                                                                      | 600                                                         | 2.6568                |
| Vocos               | 6.0kbps | 8                                                                      | 600                                                         | 3.1956                |
| SpeechTokenizer     | 6.0kbps | 8                                                                      | 600                                                         | 3.2851                |
| DAC                 | 4.0kbps | 4                                                                      | 400                                                         | 2.9448                |
| HiFi-Codec          | 4.0kbps | 4                                                                      | 400                                                         | 3.0750                |
| HiFi-Codec          | 3.0kbps | 4                                                                      | 300                                                         | 3.3034                |
| Encodec             | 3.0kbps | 4                                                                      | 300                                                         | 2.0883                |
| Vocos               | 3.0kbps | 4                                                                      | 300                                                         | 3.0558                |
| SpeechTokenizer     | 3.0kbps | 4                                                                      | 300                                                         | 3.0183                |
| DAC                 | 1.0kbps | 1                                                                      | 100                                                         | 1.4986                |
| WavTokenizer        | 0.5kbps | 1                                                                      | 40                                                          | 3.0545                |
| WavTokenizer        | 0.9kbps | 1                                                                      | 75                                                          | 3.4312                |
| LJSpeech            | GT      | -                                                                      | -                                                           | -                     |
| DAC                 | 9.0kbps | 9                                                                      | 900                                                         | 4.3007                |
| Encodec             | 6.0kbps | 8                                                                      | 600                                                         | 3.2286                |
| Vocos               | 6.0kbps | 8                                                                      | 600                                                         | 4.0332                |
| SpeechTokenizer     | 6.0kbps | 8                                                                      | 600                                                         | 4.2373                |
| DAC                 | 4.0kbps | 4                                                                      | 400                                                         | 3.8109                |
| HiFi-Codec          | 4.0kbps | 4                                                                      | 400                                                         | 4.1656                |
| HiFi-Codec          | 3.0kbps | 4                                                                      | 300                                                         | 4.2692                |
| Encodec             | 3.0kbps | 4                                                                      | 300                                                         | 2.3905                |
| Vocos               | 3.0kbps | 4                                                                      | 300                                                         | 3.7880                |
| SpeechTokenizer     | 3.0kbps | 4                                                                      | 300                                                         | 3.9908                |
| DAC                 | 1.0kbps | 1                                                                      | 100                                                         | 1.4438                |
| WavTokenizer        | 0.5kbps | 1                                                                      | 40                                                          | 4.0186                |
| WavTokenizer        | 0.9kbps | 1                                                                      | 75                                                          | 4.2580                |"
211,"| Degradations | Method | PSNR   | SSIM   | LPIPS $ \downarrow $ | MANIQA | CLIP-IQA | MUSIQ |
| ------------ | ------ | ------ | ------ | -------------------- | ------ | -------- | ----- |
| Group A      | AirNet | 19.13  | 0.6019 | 0.4283               | 0.2581 | 0.3930   | 42.46 |
| PromptIR     | 20.06  | 0.6088 | 0.4127 | 0.2633               | 0.4013 | 42.62    |       |
| MiOIR        | 20.84  | 0.6558 | 0.3715 | 0.2451               | 0.3933 | 47.82    |       |
| DA-CLIP      | 19.58  | 0.6032 | 0.4266 | 0.2418               | 0.4139 | 42.51    |       |
| InstructIR   | 18.03  | 0.5751 | 0.4429 | 0.2660               | 0.3528 | 45.77    |       |
| AutoDIR      | 19.64  | 0.6286 | 0.3967 | 0.2500               | 0.3767 | 47.01    |       |
| AgenticIR    | 21.04  | 0.6818 | 0.3148 | 0.3071               | 0.4474 | 56.88    |       |
| Group B      | AirNet | 19.31  | 0.6567 | 0.3670               | 0.2882 | 0.4274   | 47.88 |
| PromptIR     | 20.47  | 0.6704 | 0.3370 | 0.2893               | 0.4289 | 48.10    |       |
| MiOIR        | 20.56  | 0.6905 | 0.3243 | 0.2638               | 0.4330 | 51.87    |       |
| DA-CLIP      | 18.56  | 0.5946 | 0.4405 | 0.2435               | 0.4154 | 43.70    |       |
| InstructIR   | 18.34  | 0.6235 | 0.4072 | 0.3022               | 0.3790 | 50.94    |       |
| AutoDIR      | 19.90  | 0.6643 | 0.3542 | 0.2534               | 0.3986 | 49.64    |       |
| AgenticIR    | 20.55  | 0.7009 | 0.3072 | 0.3204               | 0.4648 | 57.57    |       |
| Group C      | AirNet | 17.95  | 0.5145 | 0.5782               | 0.1854 | 0.3113   | 30.12 |
| PromptIR     | 18.51  | 0.5166 | 0.5756 | 0.1906               | 0.3104 | 29.71    |       |
| MiOIR        | 15.63  | 0.4896 | 0.5376 | 0.1717               | 0.2891 | 37.95    |       |
| DA-CLIP      | 18.53  | 0.5320 | 0.5335 | 0.1916               | 0.3476 | 33.87    |       |
| InstructIR   | 17.09  | 0.5135 | 0.5582 | 0.1732               | 0.2537 | 33.69    |       |
| AutoDIR      | 18.61  | 0.5443 | 0.5019 | 0.2045               | 0.2939 | 37.86    |       |
| AgenticIR    | 18.82  | 0.5474 | 0.4493 | 0.2698               | 0.3948 | 48.68    |       |"
211,"| Degradations | As planned | PSNR   | SSIM   | LPIPS $ \downarrow $ | MANIQA | CLIP-IQA | MUSIQ |
| ------------ | ---------- | ------ | ------ | -------------------- | ------ | -------- | ----- |
| Group A      | ✓          | 21.14  | 0.6836 | 0.2753               | 0.3469 | 0.5091   | 60.77 |
| ✗            | 20.79      | 0.6652 | 0.3060 | 0.3385               | 0.4819 | 59.85    |       |
| Group B      | ✓          | 21.14  | 0.7088 | 0.2683               | 0.3588 | 0.5275   | 61.92 |
| ✗            | 20.32      | 0.6811 | 0.2976 | 0.3623               | 0.5257 | 60.15    |       |
| Group C      | ✓          | 18.78  | 0.5352 | 0.4239               | 0.3118 | 0.4876   | 51.08 |
| ✗            | 18.49      | 0.5277 | 0.4345 | 0.3058               | 0.4719 | 51.32    |       |"
211,"| Degradations | Method Ref. Rb. | PSNR  | SSIM   | LPIPS $ \downarrow $ | MANIQA | CLIP-IQA | MUSIQ  |
| ------------ | --------------- | ----- | ------ | -------------------- | ------ | -------- | ------ |
| Group A      | ✓               | ✗     | 21.12  | 0.6809               | 0.3079 | 0.3179   | 0.4617 |
| ✗            | ✗               | 20.47 | 0.6659 | 0.3282               | 0.2906 | 0.4387   |        |
| Group B      | ✓               | ✗     | 20.74  | 0.6986               | 0.3084 | 0.3126   | 0.4567 |
| ✗            | ✗               | 20.46 | 0.6798 | 0.3412               | 0.2966 | 0.4359   |        |
| Group C      | ✓               | ✗     | 18.85  | 0.5510               | 0.4559 | 0.2557   | 0.3771 |
| ✗            | ✗               | 18.93 | 0.5447 | 0.4764               | 0.2349 | 0.3595   |        |"
211,"| Degradations | Method | PSNR  | SSIM   | LPIPS $ \downarrow $ | MANIQA | CLIP-IQA | MUSIQ  |
| ------------ | ------ | ----- | ------ | -------------------- | ------ | -------- | ------ |
| Group A      | ✓      | ✓     | 20.23  | 0.6626               | 0.3249 | 0.3197   | 0.4158 |
| ✓            | ✗      | 19.77 | 0.6725 | 0.3067               | 0.3042 | 0.4484   |        |
| Group B      | ✓      | ✓     | 18.76  | 0.6642               | 0.3348 | 0.3251   | 0.4525 |
| ✓            | ✗      | 18.30 | 0.6348 | 0.3591               | 0.3082 | 0.4528   |        |
| Group C      | ✓      | ✓     | 18.99  | 0.5461               | 0.4604 | 0.2643   | 0.3974 |
| ✓            | ✗      | 18.64 | 0.5446 | 0.4634               | 0.2348 | 0.3669   |        |"
213,"| Datasets                                   | # Vid. | # Clips. | Avg. Clip Len. | # Classes    | Evt. Len. | # Evt. / sec |
| ------------------------------------------ | ------ | -------- | -------------- | ------------ | --------- | ------------ |
| (a) Fine-grained                           |        |          |                | Fine-grained | Fast      | Frequent     |
| FineAction [41]                            | -      | 16,732   | 149.5s         | 101          | 6.9s      | 0.3          |
| ActivityNet [4]                            | -      | 19,994   | 116.7s         | 200          | 49.2s     | 0.01         |
| FineGym [58]                               | 303    | 32,697   | 50.3s          | 530          | 1.7s      | 0.3          |
| (b) Fast                                   |        |          |                |              |           |              |
| CCTV-Pipe [42]                             | 575    | 575      | 549.3s         | 16           | &lt; 0.1s | 0.02         |
| SoccerNetV2 [11]                           | 9      | 9        | 99.6min        | 12           | &lt; 0.1s | 0.3          |
| (c) Frequent                               |        |          |                |              |           |              |
| FineDiving [69]                            | 135    | 3,000    | 4.2s           | 29           | 1.1s      | $ \sim $ 1   |
| (d) Fast &amp; Frequent                    |        |          |                |              |           |              |
| ShuttleSet [66]                            | 44     | 3,685    | 10.9s          | 18           | &lt; 0.1s | $ \sim $ 1   |
| P $ ^{2} $ ANet [3]                        | 200    | 2,721    | 360.0s         | 14           | &lt; 0.1s | $ \sim $ 2   |
| (d) Fast &amp; Frequent &amp; Fine-grained |        |          |                |              |           |              |
| F $ ^{3} $ Set                             | 114    | 11,584   | 8.4s           | 1,108        | &lt; 0.1s | $ \sim $ 1   |"
213,"| Video encoder     | Head arch.    | F3Set (G_{high}) | F3Set (G_{mid}) | F3Set (G_{low}) |
| ----------------- | ------------- | ---------------- | --------------- | --------------- |
| F1_{evt}          | F1_{elm}      | Edit             | F1_{evt}        | F1_{elm}        |
| TSN [64]          | MS-TCN [19]   | 15.9             | 59.8            | 53.5            |
| ASformer [71]     | 11.9          | 54.3             | 49.8            | 17.3            |
| G-TAD [70]        | 6.0           | 47.5             | 24.7            | 14.1            |
| ActionFormer [72] | 18.4          | 60.6             | 55.2            | 24.8            |
| E2E-Spot [24]     | 24.7          | 65.3             | 60.1            | 31.5            |
| SlowFast [20]     | MS-TCN [19]   | 17.2             | 63.1            | 56.2            |
| ASformer [71]     | 14.1          | 60.8             | 55.3            | 20.3            |
| G-TAD [70]        | 23.0          | 66.1             | 64.0            | 29.6            |
| ActionFormer [72] | 28.7          | 70.0             | 67.6            | 35.5            |
| E2E-Spot [24]     | 25.9          | 69.4             | 65.7            | 33.8            |
| I3D [5]           | E2E-Spot [24] | 22.7             | 59.7            | 68.7            |
| VTN [52]          | E2E-Spot [24] | 14.8             | 58.3            | 56.7            |
| TSM [35]          | MS-TCN [19]   | 21.7             | 67.3            | 58.6            |
| ASformer [71]     | 17.6          | 61.9             | 57.5            | 25.5            |
| G-TAD [70]        | 16.9          | 62.5             | 55.2            | 29.8            |
| ActionFormer [72] | 22.4          | 65.7             | 60.3            | 31.0            |
| E2E-Spot [24]     | 31.4          | 71.4             | 68.7            | 39.5            |
| TSM[35]           | F3ED          | 40.3             | 75.2            | 74.0            |"
213,"| Experiment                          | F3Set (G_{high}) | F3Set (G_{mid}) | F3Set (G_{low}) |
| ----------------------------------- | ---------------- | --------------- | --------------- |
| F1_{evt}                            | F1_{elm}         | Edit            | F1_{evt}        |
| TSM + E2E-Spot                      | 31.4             | 71.4            | 68.7            |
| (a) Feature extractor               |                  |                 |                 |
| I3D [5] (clip-wise)                 | 22.7             | 59.7            | 68.7            |
| VTN [52] (video transformer)        | 14.8             | 58.3            | 56.7            |
| ST-GCN++ [17] (skeleton-based)      | 25.4             | 62.1            | 56.1            |
| PoseConv3D [18] ( (skeleton-based)) | 20.1             | 54.5            | 53.2            |
| (b) Stride size = 4                 | 25.9             | 69.2            | 62.7            |
| Stride size = 8                     | 14.0             | 56.7            | 44.3            |
| (c) without GRU                     | 27.6             | 69.0            | 60.6            |
| (d) Clip Length = 32                | 26.3             | 67.4            | 54.5            |
| Clip Length = 64                    | 30.7             | 71.2            | 67.4            |
| Clip Length = 192                   | 29.3             | 70.3            | 65.7            |
| (e) Multi-label                     | 37.9             | 74.3            | 71.7            |
| (f) Multi-label + CTX (Transformer) | 39.0             | 74.3            | 72.8            |
| Multi-label + CTX (BiGRU)           | 40.3             | 75.2            | 74.0            |"
213,"| Head arch.        | ShuttleSet [66] | FineDiving [69] | FineGym [58] | SoccerNetV2 [11] | CCTV-Pipe [42] |
| ----------------- | --------------- | --------------- | ------------ | ---------------- | -------------- |
| F1_{evt}          | Edit            | F1_{evt}        | Edit         | F1_{evt}         | Edit           |
| MS-TCN [19]       | 70.3            | 74.4            | 65.7         | 92.2             | 57.6           |
| ASformer [71]     | 55.9            | 70.6            | 49.9         | 87.6             | 53.6           |
| G-TAD [70]        | 48.2            | 61.1            | 52.1         | 82.6             | 45.8           |
| ActionFormer [72] | 62.1            | 67.5            | 68.3         | 92.4             | 54.0           |
| E2E-Spot [24]     | 70.2            | 75.0            | 75.8         | 93.7             | 62.1           |
| F3ED              | 70.7            | 77.1            | 77.6         | 95.1             | 70.9           |"
214,"| Dataset      | Creation Method | Scalability | Natural &amp; Diverse Language | Symbolic Representations | Faithful Reasoning Chains |
| ------------ | --------------- | ----------- | ------------------------------ | ------------------------ | ------------------------- |
| RuleTaker    | Synthetic       | ✓           | ✗                              | ✗                        | ✓                         |
| ProofWriter  | Synthetic       | ✓           | ✗                              | ✗                        | ✓                         |
| ProntoQA     | Synthetic       | ✓           | ✗                              | ✓                        | ✓                         |
| ProntoQA-OOD | Synthetic       | ✓           | ✗                              | ✓                        | ✓                         |
| LogicNLI     | Synthetic       | ✓           | ✗                              | ✗                        | ✗                         |
| FOLIO        | Manual          | ✗           | ✓                              | ✓                        | ✗                         |
| ProverQA     | Synthetic       | ✓           | ✓                              | ✓                        | ✓                         |"
214,"| FOL Training Set               | ProverQA | ProntoQA | ProofWriter | FOLIO | OOD Avg  $ \Delta^{\dagger} $ |
| ------------------------------ | -------- | -------- | ----------- | ----- | ----------------------------- |
| Easy                           | Medium   | Hard     |             |       |                               |
| Llama3.1-8B-Instruct           |          |          |             |       |                               |
| -                              | 75.60    | 46.60    | 33.60       | 79.60 | 56.83                         |
| Finetuned Llama3.1-8B-Instruct |          |          |             |       |                               |
| ProofWriter                    | 44.60    | 55.00    | 47.20       | 92.00 | 71.67                         |
| FOLIO                          | 53.20    | 44.80    | 31.00       | 63.40 | 42.83                         |
| ProverQA                       | 97.00    | 90.60    | 68.20       | 88.40 | 65.67                         |"
215,"| Method                   | ADHD-200          | ABIDE       | ADNI       |
| ------------------------ | ----------------- | ----------- | ---------- |
| ACC                      | AUC               | ACC         | AUC        |
| ML Methods               | SVM Random Forest | 53.56±2.73  | 54.66±3.40 |
| 58.96±2.77               | 59.49±2.38        | 51.14±3.08  | 51.41±3.23 |
| Graph Transformer Models | SAN               | 51.09±2.00  | 51.22±2.21 |
| Graph Trans.             | 50.76±2.07        | 51.49±1.15  | 50.20±0.50 |
| Graphormer               | 61.60±0.90        | 58.64±1.50  | 58.40±0.68 |
| SAT-PE                   | 60.00±2.73        | 59.68±2.60  | 60.60±3.11 |
| SAT+PE                   | 64.44±3.45        | 64.21±3.40  | 58.76±4.88 |
| BRAINNETTF               | 70.80±2.70        | 79.36±3.43  | 68.24±2.24 |
| Polynormer               | 64.78±2.34        | 63.61±2.43  | 57.03±0.96 |
| Gradformer               | 68.94±3.18        | 67.83±4.66  | 61.56±4.13 |
| GTSP                     | 61.70±3.81        | 61.41±2.90  | 61.37±3.59 |
| Graph Neural Networks    | GAT               | 55.38±3.18  | 54.97±3.28 |
| BrainGNN                 | 55.76±1.20        | 58.00±0.49  | 51.34±1.17 |
| BrainGB                  | 68.20±7.81        | 74.64±10.10 | 65.12±3.90 |
| MCST-GCN                 | 59.06±2.69        | 59.05±3.89  | 54.22±2.40 |
| GroupBNA                 | 69.87±3.02        | 71.16±4.53  | 63.14±2.65 |
| Our Model                | BioBGT            | 71.06±0.08  | 71.64±1.14 |"
215,"|        | ABIDE      | ADNI       | ADHD-200   |
| ------ | ---------- | ---------- | ---------- |
| F1     | ACC        | AUC        | F1         |
| +PE    | 54.00±2.97 | 60.60±2.25 | 60.91±2.05 |
| +DC    | 59.73±4.23 | 61.20±1.88 | 61.28±1.80 |
| +PE+DC | 56.64±2.40 | 63.00±1.63 | 63.32±1.55 |
| +BC    | 52.77±1.30 | 70.00±6.12 | 65.62±7.29 |
| +CC    | 62.43±1.53 | 73.75±6.50 | 70.84±7.65 |
| +EC    | 53.13±1.62 | 71.25±8.20 | 66.67±9.88 |
| BioBGT | 68.41±2.19 | 74.00±2.01 | 73.33±2.37 |"
216,"| Method                                | Architecture         | Bits  | Spike -driven | Time Step | Param (M) | Power (mJ) | Acc. (%) |
| ------------------------------------- | -------------------- | ----- | ------------- | --------- | --------- | ---------- | -------- |
| Transformer (Yu et al., 2023)         | CAformer*            | 32-32 | ✗             | N/A       | 15.1      | 40.3       | 79.9     |
| QCFS (Bu et al., 2021)                | ResNet-34            | 32-1  | ✓             | 256       | 21.8      | -          | 73.4     |
| MST (Wang et al., 2023)               | Swin-T               | 32-1  | ✓             | 128       | 28.5      | -          | 77.9     |
| SEW-ResNet (Fang et al., 2021)        | SEW-ResNet-34        | 32-1  | ✗             | 4         | 25.6      | 4.9        | 67.8     |
| SEW-ResNet-152                        | 32-1                 | ✗     | 4             | 60.2      | 12.9      | 69.2       |          |
| MS-ResNet (Hu et al., 2024b)          | MS-ResNet-34         | 32-1  | ✓             | 4         | 21.8      | 5.1        | 69.4     |
| MS-ResNet-104                         | 32-1                 | ✓     | 4             | 77.3      | 10.2      | 75.3       |          |
| Spikformer (Zhou et al., 2023b)       | Spikformer-8-512     | 32-1  | ✗             | 4         | 29.7      | 11.6       | 73.4     |
| Spikformer-8-768                      | 32-1                 | ✗     | 4             | 66.3      | 21.5      | 74.8       |          |
| SD-Transformer (Yao et al., 2023b)    | SD-Transformer-8-512 | 32-1  | ✓             | 4         | 29.7      | 4.5        | 74.6     |
| SD-Transformer-8-768                  | 32-1                 | ✓     | 4             | 66.3      | 6.1       | 76.3       |          |
| SpikingResformer (Shi et al., 2024)   | SpikingResformer-T   | 32-1  | ✓             | 4         | 11.1      | 4.2        | 74.3     |
| SpikingResformer-L                    | 32-1                 | ✓     | 4             | 60.4      | 9.7       | 78.7       |          |
| SD-Transformer v2 (Yao et al., 2023a) | SD-Transformer v2-T  | 32-1  | ✓             | 4         | 15.1      | 16.7       | 74.1     |
| SD-Transformer v2-M                   | 32-1                 | ✓     | 4             | 31.3      | 32.8      | 77.2       |          |
| SD-Transformer v2-L                   | 32-1                 | ✓     | 4             | 55.4      | 52.4      | 79.7       |          |
| QSD-Transformer                       | SD-Transformer v2-T  | 4-1   | ✓             | 4         | 1.8       | 2.5        | 77.5     |
| SD-Transformer v2-M                   | 4-1                  | ✓     | 4             | 3.9       | 5.7       | 78.9       |          |
| SD-Transformer v2-L                   | 4-1                  | ✓     | 4             | 6.8       | 8.7       | 80.3       |          |"
216,"| Method                                | Architecture        | Bits  | Spike -driven | Time Step | Param (M) | Power (mJ) | mAP@0.5 (%) |
| ------------------------------------- | ------------------- | ----- | ------------- | --------- | --------- | ---------- | ----------- |
| Transformer (Yu et al., 2023)         | CAformer            | 32-32 | ✗             | N/A       | 31.2      | 890.6      | 54.0        |
| Transformer (Zhu et al., 2020)        | DETR                | 32-32 | ✗             | N/A       | 41.0      | 860.2      | 57.0        |
| Spiking-Yolo (Kim et al., 2020)       | ResNet-18           | 32-1  | ✓             | 3500      | 10.2      | -          | 25.7        |
| Spike Calibration (Li et al., 2022)   | ResNet-18           | 32-1  | ✓             | 512       | 17.1      | -          | 45.3        |
| EMS-SNN (Su et al., 2023)             | EMS-ResNet-18       | 32-1  | ✓             | 4         | 26.9      | -          | 50.1        |
| SD-Transformer v2 (Yao et al., 2023a) | SD-Transformer v2-M | 32-1  | ✓             | 1         | 75.0      | 140.8      | 51.2        |
| QSD-Transformer                       | SD-Transformer v2-T | 4-1   | ✓             | 4         | 16.9      | 45.1       | 48.1        |
| SD-Transformer v2-M                   | 4-1                 | ✓     | 4             | 34.9      | 117.2     | 57.0       |             |"
216,"| Method                                | Architecture        | Bits  | Spike-driven | Time Step | Param (M) | Power (mJ) | MIoU (%) |
| ------------------------------------- | ------------------- | ----- | ------------ | --------- | --------- | ---------- | -------- |
| Segformer (Xie et al., 2021)          | Segformer           | 32-32 | ✗            | N/A       | 3.8       | 38.9       | 37.4     |
| DeepLab-V3 (Zhang et al., 2022a)      | DeepLab-V3          | 32-32 | ✗            | N/A       | 68.1      | 1240.6     | 42.7     |
| SD-Transformer v2 (Yao et al., 2023a) | SD-Transformer v2-M | 32-1  | ✓            | 4         | 59.8      | 183.6      | 35.3     |
| QSD-Transformer                       | SD-Transformer v2-T | 4-1   | ✓            | 4         | 3.3       | 17.5       | 39.0     |
| SD-Transformer v2-M                   | 4-1                 | ✓     | 4            | 9.6       | 37.9      | 40.5       |          |"
216,"| Method                              | Param (M) | CIFAR10            | CIFAR100           | CIFAR10-DVS        |
| ----------------------------------- | --------- | ------------------ | ------------------ | ------------------ |
| T                                   | Acc. (%)  | T                  | Acc. (%)           | T                  |
| Spikformer (Zhou et al., 2023b)     | 29.1      | 4                  | 97.0               | 4                  |
| SpikingResformer (Shi et al., 2024) | 17.3      | 4                  | 97.4               | 4                  |
| QSD-Transformer                     | 1.8       | 4                  | 97.8  $ \pm $  0.1 | 4                  |
| 6.8                                 | 4         | 98.4  $ \pm $  0.2 | 4                  | 87.6  $ \pm $  0.2 |"
216,"| Architecture                          | IE-LIF | FGD | Weight Bits | Acc.(%) |
| ------------------------------------- | ------ | --- | ----------- | ------- |
| SD-Transformer v2 (Yao et al., 2023a) | -      | -   | 4           | 70.0    |
| ✓                                     | -      | 4   | 75.8        |         |
| ✓                                     | ✓      | 4   | 77.5        |         |
| ✓                                     | ✓      | 3   | 76.9        |         |
| ✓                                     | ✓      | 2   | 75.0        |         |
| Spikformer (Zhou et al., 2023b)       | -      | -   | 4           | 64.1    |
| ✓                                     | -      | 4   | 70.1        |         |
| ✓                                     | ✓      | 4   | 75.5        |         |
| ✓                                     | ✓      | 3   | 74.1        |         |
| ✓                                     | ✓      | 2   | 73.1        |         |"
217,"|          | VITATECS | MVBench | TempCompass       | ReXTime | TOMATO |
| -------- | -------- | ------- | ----------------- | ------- | ------ |
| # Frames | 16[S]    | 16      | $ \tau \uparrow $ | 16[S]   | 16     |
| Average  | 84.5     | 87.2    | 3.2               | 59.3    | 62.7   |"
217,"|          | VITATECS | MVBench | TempCompass         | ReXTime | TOMATO (200) |
| -------- | -------- | ------- | ------------------- | ------- | ------------ |
| # Frames | 1[R]     | 1[H]    | $ \rho \downarrow $ | 1[R]    | 1[H]         |
| Average  | 70.7     | 86.0    | 21.6                | 47.1    | 58.8         |"
218,"| Method            | Dataset |
| ----------------- | ------- |
| HIM-100k          | SMPMat  |
| SAD               | MSE     |
| Instance-agnostic |         |
| FBA(+Mask RCNN)   | 38.25   |
| FBA(+SOLO)        | 38.18   |
| FBA(+EVA)         | 37.76   |
| MG(+Mask RCNN)    | 40.51   |
| MG(+SOLO)         | 39.26   |
| MG(+EVA)          | 38.19   |
| Instance-aware    |         |
| InstMatte         | 37.34   |
| E2E-HIM           | 32.22   |
| Maggie            | 29.48   |
| MP-Mat (Ours)     | 26.75   |"
218,"| Background Estimation | Color Estimation | Refinement | SAD   | MSE  |
| --------------------- | ---------------- | ---------- | ----- | ---- |
| ✓                     |                  |            | 29.46 | 0.60 |
| ✓                     | ✓                |            | 28.53 | 0.55 |
| ✓                     | ✓                | ✓          | 27.79 | 0.52 |
| ✓                     | ✓                | ✓          | 26.75 | 0.49 |"
218,"| Editing Method   | Mean L1 Loss ( $ \downarrow $ ) | Mean L2 Loss ( $ \downarrow $ ) | PSNR ( $ \uparrow $ ) | Time per editing (s) |
| ---------------- | ------------------------------- | ------------------------------- | --------------------- | -------------------- |
| Inst-inpaint     | 12.69%                          | 2.58%                           | 23.09 dB              | 0.1982               |
| Dragon Diffusion | 12.13%                          | 2.49%                           | 22.17 dB              | 0.2139               |
| Matting Method   | Mean L1 Loss ( $ \downarrow $ ) | Mean L1 Loss ( $ \downarrow $ ) | PSNR ( $ \uparrow $ ) | Time per image (s)   |
| Ours             | 9.37%                           | 1.96%                           | 23.58 dB              | 0.1117               |
| Ours (w/ BI)     | 3.73%                           | 0.84%                           | 25.79 dB              | 0.1117               |"
218,"| Method           | Mean L1 Loss ( $ \downarrow $ ) | Mean L2 Loss ( $ \downarrow $ ) | PSNR ( $ \uparrow $ ) | Speed (s) |
| ---------------- | ------------------------------- | ------------------------------- | --------------------- | --------- |
| Inst-inpaint     | 12.65%                          | 3.42%                           | 21.2 dB               | 0.2547    |
| Dragon Diffusion | 13.85%                          | 3.66%                           | 19.82 dB              | 0.2849    |
| Ours             | 3.26%                           | 0.79%                           | 28.32 dB              | 0.1739    |"
218,"| SG-MP | Inst-MP | SAD   | MSE  |
| ----- | ------- | ----- | ---- |
|       |         | 35.85 | 0.87 |
|       | ✓       | 33.78 | 0.86 |
| ✓     |         | 29.46 | 0.60 |
| ✓     | ✓       | 26.75 | 0.49 |"
218,"| Depth | PGN | SAD   | MSE  |
| ----- | --- | ----- | ---- |
|       |     | 33.78 | 0.86 |
| ✓     |     | 31.82 | 0.74 |
| ✓     | ✓   | 26.75 | 0.49 |"
219,"| Model              | CodeBLEU           | Pass Rate           |
| ------------------ | ------------------ | ------------------- |
| BLEU ID            | Weighted N-Gram ID | Weighted N-Gram OOD |
| Proprietary Models |                    |                     |
| GPT-4o-mini        | 0.4                | 0.4                 |
| w/ ICL             | 0.5                | 0.5                 |
| GPT-4o             | 0.5                | 0.4                 |
| w/ ICL             | 0.5                | 0.5                 |
| Open-Source Models |                    |                     |
| Qwen2-7B           | 0.4                | 0.4                 |
| w/ ICL             | 0.5                | 0.5                 |
| Llama-3.1-8B       | 0.6                | 0.7                 |
| w/ ICL             | 0.7                | 0.7                 |
| Llama-3.1-70B      | 0.4                | 0.4                 |
| w/ ICL             | 0.4                | 0.4                 |
| WorkflowLlama (8B) | 9.4                | 7.0                 |"
219,"| Model                       | CodeBLEU        |
| --------------------------- | --------------- |
| BLEU                        | Weighted N-Gram |
| WorkflowLlama               | 9.4             |
| w/o Task Plan               | 9.1             |
| w/o Comment                 | 9.1             |
| w/o Task Plan &amp; Comment | 8.8             |
| w/o Synthetic Data          | 7.8             |"
220,"| Metrics     | DBLP         | BlogCat      | OGB-p        |
| ----------- | ------------ | ------------ | ------------ |
| Macro AUC   | AP           | Macro AUC    | AP           |
| Node2vec+BR | 71.22 ± 0.31 | 57.41 ± 1.29 | 53.76 ± 1.08 |
| Node2vec+CC | 72.57 ± 0.24 | 57.13 ± 1.88 | 57.97 ± 1.31 |
| GCN+ML-KNN  | 90.11 ± 1.02 | 80.01 ± 0.77 | 60.18 ± 2.33 |
| GCN+PLAIN   | 80.55 ± 1.23 | 73.44 ± 1.12 | 63.95 ± 1.85 |
| MLGW        | 73.32 ± 1.44 | 56.03 ± 0.47 | 60.02 ± 2.19 |
| ML-GCN      | 72.66 ± 2.73 | 56.71 ± 2.59 | 60.97 ± 2.21 |
| LARN        | 74.29 ± 2.53 | 58.11 ± 1.25 | 63.18 ± 1.84 |
| LANC        | 91.68 ± 0.42 | 83.50 ± 0.83 | 67.94 ± 3.30 |
| VariMul     | 92.14 ± 1.23 | 85.30 ± 0.92 | 68.71 ± 2.97 |
| GCN+Auto    | 92.13 ± 1.57 | 85.48 ± 1.32 | 66.05 ± 1.25 |
| GCN+LIP     | 94.38 ± 1.51 | 87.45 ± 1.28 | 70.21 ± 2.02 |
| Metrics     | PCG          | HumLoc       | EukLoc       |
| Macro AUC   | AP           | Macro AUC    | AP           |
| Node2vec+BR | 52.66 ± 1.73 | 15.13 ± 0.25 | 54.17 ± 1.58 |
| Node2vec+CC | 52.92 ± 0.62 | 14.54 ± 0.51 | 54.81 ± 1.29 |
| GCN+ML-KNN  | 56.11 ± 0.50 | 18.60 ± 0.82 | 57.99 ± 0.35 |
| GCN+PLAIN   | 59.75 ± 1.70 | 20.16 ± 1.00 | 60.15 ± 1.28 |
| MLGW        | 55.86 ± 3.92 | 15.59 ± 1.33 | 56.92 ± 1.03 |
| ML-GCN      | 57.24 ± 2.44 | 19.45 ± 1.13 | 60.79 ± 1.38 |
| LARN        | 57.79 ± 0.61 | 19.09 ± 0.38 | 61.48 ± 1.22 |
| LANC        | 56.58 ± 0.63 | 19.51 ± 0.93 | 59.63 ± 1.21 |
| VariMul     | 62.77 ± 0.34 | 21.93 ± 0.52 | 67.42 ± 2.44 |
| GCN+Auto    | 58.53 ± 0.47 | 19.11 ± 0.83 | 66.07 ± 1.17 |
| GCN+LIP     | 65.73 ± 0.52 | 22.97 ± 1.69 | 73.22 ± 1.76 |"
220,"| AUC    | DBLP         | BlogCat      | OGB-p        | PCG          | HumLoc       | EukLoc       |
| ------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ |
| None   | 92.83 ± 1.13 | 66.14 ± 1.74 | 71.26 ± 1.45 | 59.54 ± 0.90 | 66.57 ± 0.67 | 69.27 ± 1.97 |
| Only P | 92.08 ± 1.06 | 68.27 ± 1.88 | 73.72 ± 0.63 | 62.01 ± 1.21 | 70.17 ± 1.42 | 70.87 ± 1.64 |
| Only T | 93.94 ± 1.00 | 67.11 ± 1.51 | 73.58 ± 1.24 | 63.82 ± 1.04 | 69.30 ± 1.02 | 69.93 ± 1.01 |
| All    | 94.38 ± 1.51 | 70.21 ± 2.02 | 74.82 ± 0.34 | 65.73 ± 0.52 | 73.22 ± 1.76 | 72.92 ± 1.82 |"
222,"| Method                            | Settings | General MLLM Benchmarks | Inference Speed |
| --------------------------------- | -------- | ----------------------- | --------------- |
| Param.                            | Res.     | Data                    | MME             |
| BLIP-2 (Li et al., 2023b)         | 14B      | 224                     | 129M            |
| InstructBLIP (Dai et al., 2023)   | 14B      | 224                     | 130M            |
| QwenVL-Chat (Bai et al., 2023)    | 10B      | 448                     | 1.4B            |
| Fuyu-8B (Fuyu-8B, 2023)           | 8B       | 600                     | -               |
| mPLUG-Owl2 (Ye et al., 2023)      | 8B       | 448                     | 400M            |
| I-MoF (Tong et al., 2024)         | 13B      | 336                     | 1.2M            |
| LLaVA-1.5 (Liu et al., 2023a)     | 7B       | 336                     | 1.2M            |
| LLaVA-1.5 (Liu et al., 2023a)     | 13B      | 336                     | 1.2M            |
| LLaVA-HR                          | 7B       | 1024                    | 1.2M            |
| LLaVA-HR                          | 13B      | 1024                    | 1.2M            |
| LLaVA-HR-X                        | 14B      | 1024                    | 1.2M            |
| More Instruction Data:            |          |                         |                 |
| LLaVA-NeXT (Liu et al., 2024a)    | 7B       | 1344                    | 1.6M            |
| SPHINX-intern2 (Gao et al., 2024) | 7B       | 448                     | 16M             |
| InternLM-XC (Zhang et al., 2023)  | 7B       | 224                     | 1.1B            |
| Mini-Gemini (Li et al., 2024b)    | 7B       | 672                     | 2.7M            |
| MM1 (McKinzie et al., 2024)       | 7B       | 1792                    | 1B              |
| CogVLM-Chat (Wang et al., 2023)   | 17B      | 490                     | 1.5B            |
| InternVL-1.2 (Chen et al., 2023d) | 40B      | 448                     | 450M            |
| LLaVA-HR†                         | 7B       | 1024                    | 2.7M            |"
222,"| Method                            | Param. | Res. | Data.    | TextVQA | DocVQA | InfoVQA | AI2D | ChartQA |
| --------------------------------- | ------ | ---- | -------- | ------- | ------ | ------- | ---- | ------- |
| QwenVL (Bai et al., 2023)         | 10B    | 336  | 1.4B     | 63.8    | 65.1   | 35.4    | -    | 65.7    |
| Monkey (Li et al., 2024c)         | 10B    | 1344 | 1.4M     | 67.6    | 66.5   | 36.1    | 62.6 | -       |
| LLaVA-NeXt (Liu et al., 2024a)    | 7B     | 1344 | 1.6M     | 64.9    | -      | -       | 66.6 | 54.8    |
| TextMonkey (Liu et al., 2024b)    | 10B    | 1344 | 2.5M     | 65.9    | 73.0   | 28.6    | -    | 65.5    |
| DocOwl-1.5-Chat (Hu et al., 2024) | 8B     | 4032 | 4M       | 68.6    | 82.2   | 50.7    | -    | 70.2    |
| CogAgent Hong et al. (2023)       | 18B    | 1120 | &gt;300M | 76.1    | 81.6   | 44.5    | -    | 68.4    |
| LLaVA-HR†                         | 7B     | 1024 | 2.7M     | 73.8    | 85.8   | 52.3    | 75.3 | 77.6    |"
223,"| Dataset                              | Method                         | Overall results | F1 for each category |
| ------------------------------------ | ------------------------------ | --------------- | -------------------- |
| Accuracy                             | Macro F1                       | Kappa           | Wake                 |
| ISRUC-S3                             | FeatureNet (Jia et al., 2021b) | 0.7628          | 0.7495               |
| DeepSleepNet (Supratak et al., 2017) | 0.7426                         | 0.7135          | 0.6682               |
| AttnSleep (Eidle et al., 2021)       | 0.7656                         | 0.7480          | 0.6993               |
| DAN (Tang et al., 2022)              | 0.7720                         | 0.7431          | 0.7058               |
| STT (Li et al., 2023)                | 0.7756                         | 0.7436          | 0.7114               |
| XSleepNetPhan et al. (2021)          | 0.6705                         | 0.6440          | 0.5771               |
| SleepPrintNet (Jia et al., 2020)     | 0.7702                         | 0.7573          | 0.7043               |
| MMASleepNet (Yubo et al., 2022)      | 0.7732                         | 0.7343          | 0.7066               |
| SinCLR (Chen et al., 2020)           | 0.7738                         | 0.7297          | 0.7110               |
| DrFuse Yao et al. (2024)             | 0.7741                         | 0.7469          | 0.6991               |
| MERL (Liu et al., 2024)              | 0.7559                         | 0.7458          | 0.6876               |
| Ours                                 | 0.7930                         | 0.7815          | 0.7344               |
| MASS-SS3                             | FeatureNet (Jia et al., 2021b) | 0.8530          | 0.8042               |
| DeepSleepNet (Supratak et al., 2017) | 0.8561                         | 0.8012          | 0.7862               |
| AttnSleep (Eidle et al., 2021)       | 0.8592                         | 0.8036          | 0.7914               |
| DAN (Tang et al., 2021)              | 0.8034                         | 0.8091          | 0.8022               |
| STT (Liu &amp; Jin, 2023)            | 0.8114                         | 0.7492          | 0.7190               |
| XSleepNetPhan et al. (2021)          | 0.8066                         | 0.7464          | 0.7158               |
| SleepPrintNet (Jia et al., 2020)     | 0.8457                         | 0.7855          | 0.7656               |
| MMASleepNet (Yubo et al., 2022)      | 0.8627                         | 0.7993          | 0.7940               |
| SinCLR (Chen et al., 2020)           | 0.8607                         | 0.7911          | 0.7931               |
| DrFuse Yao et al. (2024)             | 0.8628                         | 0.8086          | 0.7964               |
| MERL (Liu et al., 2024)              | 0.8605                         | 0.8055          | 0.7915               |
| Ours                                 | 0.8686                         | 0.8193          | 0.8058               |
| ISRUC-S3                             | FeatureNet (Jia et al., 2021b) | 0.8012          | 0.7331               |
| DeepSleepNet (Supratak et al., 2017) | 0.7973                         | 0.7192          | 0.7265               |
| AttnSleep (Eidle et al., 2021)       | 0.7492                         | 0.6912          | 0.6559               |
| DAN (Tang et al., 2022)              | 0.7239                         | 0.5915          | 0.6057               |
| STT (Liu &amp; Jin, 2023)            | 0.7321                         | 0.6335          | 0.6245               |
| XSleepNetPhan et al. (2021)          | 0.7577                         | 0.6855          | 0.6631               |
| SleepPrintNet (Jia et al., 2020)     | 0.7349                         | 0.7199          | 0.7090               |
| MMASleepNet (Yubo et al., 2022)      | 0.7914                         | 0.7012          | 0.7078               |
| SimCLR (Chen et al., 2020)           | 0.8022                         | 0.7352          | 0.7267               |
| DrFuse Yao et al. (2024)             | 0.8009                         | 0.7411          | 0.7235               |
| MERL (Liu et al., 2024)              | 0.7990                         | 0.7267          | 0.7196               |
| StSTT (Liu &amp; Jin, 2023)          | 0.8158                         | 0.7558          | 0.7450               |
| XSleepNet(Phan et al., 2021)         | 0.7247                         | 0.6890          | 0.6423               |
| DrFuse Yao et al. (2024)             | 0.7444                         | 0.7226          | 0.6707               |
| MERL (Liu et al., 2024)              | 0.7441                         | 0.7215          | 0.6669               |
| Ours                                 | 0.7710                         | 0.7462          | 0.7018               |"
223,"| Modality                             | Method                         | Overall results | F1 for each category |
| ------------------------------------ | ------------------------------ | --------------- | -------------------- |
| Accuracy                             | Macro F1                       | Kappa           | Wake                 |
| EEG                                  | FeatureNet (Jia et al., 2021b) | 0.7277          | 0.7104               |
| DeepSleepNet (Supratak et al., 2017) | 0.6904                         | 0.6507          | 0.6057               |
| AttnSleep (Eijcle et al., 2021)      | 0.7338                         | 0.7105          | 0.6592               |
| DAN-Ent (Jia et al., 2022)           | 0.7212                         | 0.6791          | 0.6400               |
| BSTT-Liu &amp; Jin (2023)            | 0.7191                         | 0.6921          | 0.6371               |
| XSleepNet-Phan et al. (2021)         | 0.6555                         | 0.6322          | 0.5614               |
| SleepPrintNet (Jia et al., 2020)     | 0.5459                         | 0.4862          | 0.3924               |
| MMASleepNet (Yubo et al., 2022)      | 0.6313                         | 0.5975          | 0.5150               |
| SimCLR (Chen et al., 2020)           | 0.7338                         | 0.7163          | 0.6598               |
| DrFuse (Yao et al., 2024)            | 0.7532                         | 0.7138          | 0.6818               |
| MERL-Liu et al. (2024)               | 0.7467                         | 0.7295          | 0.6758               |
| Ours                                 | 0.7646                         | 0.7397          | 0.6969               |
| EOG                                  | FeatureNet (Jia et al., 2021b) | 0.7210          | 0.6932               |
| DeepSleepNet (Supratak et al., 2017) | 0.7234                         | 0.6902          | 0.6388               |
| AttnSleep (Eijcle et al., 2021)      | 0.7226                         | 0.6992          | 0.6416               |
| SimCLR (Chen et al., 2022)           | 0.7136                         | 0.6967          | 0.6407               |
| BSTT-Liu &amp; Jin (2023)            | 0.4700                         | 0.3163          | 0.2790               |
| XSleepNet-Phan et al. (2021)         | 0.6288                         | 0.6071          | 0.5233               |
| SleepPrintNet (Jia et al., 2020)     | 0.3745                         | 0.2531          | 0.1788               |
| MMASleepNet (Yubo et al., 2022)      | 0.2096                         | 0.1745          | 0.0619               |
| SimCLR (Chen et al., 2020)           | 0.7246                         | 0.7007          | 0.6458               |
| DrFuse (Yao et al., 2024)            | 0.6947                         | 0.6799          | 0.6078               |
| MERL-Liu et al. (2024)               | 0.6976                         | 0.6741          | 0.6132               |
| Ours                                 | 0.7444                         | 0.7168          | 0.6697               |
| EMG                                  | FeatureNet (Jia et al., 2021b) | 0.4040          | 0.3731               |
| DeepSleepNet (Supratak et al., 2017) | 0.4166                         | 0.3704          | 0.2404               |
| AttnSleep (Eijcle et al., 2021)      | 0.3915                         | 0.3814          | 0.2191               |
| DAN-Ent (Jia et al., 2022)           | 0.4048                         | 0.3101          | 0.2404               |
| BSTT-Liu &amp; Jin (2023)            | 0.3046                         | 0.0934          | 0.0000               |
| XSleepNet-Phan et al. (2021)         | 0.3660                         | 0.3484          | 0.1935               |
| SleepPrintNet (Jia et al., 2020)     | 0.3319                         | 0.2313          | 0.0939               |
| MMASleepNet (Yubo et al., 2022)      | 0.2517                         | 0.1969          | 0.1062               |
| SimCLR (Chen et al., 2020)           | 0.4177                         | 0.3906          | 0.2435               |
| DrFuse (Yao et al., 2024)            | 0.3857                         | 0.3789          | 0.2318               |
| MERL-Liu et al. (2024)               | 0.3981                         | 0.3907          | 0.2348               |
| Ours                                 | 0.4384                         | 0.4075          | 0.2693               |"
223,"| Dataset      | U   | C                        | Accuracy / Macro F1 / Kappa |
| ------------ | --- | ------------------------ | --------------------------- |
| multimodal   | EEG | EOG                      | EMG                         |
| ISRUC-S3     | ✗   | ✗                        | 0.7628 / 0.7495 / 0.6975    |
| ✗            | ✓   | 0.7918 / 0.7767 / 0.7329 | 0.7609 / 0.7358 / 0.6932    |
| ✓            | ✓   | 0.7930 / 0.7815 / 0.7344 | 0.7646 / 0.7397 / 0.6969    |
| MASS-SS3     | ✗   | ✗                        | 0.8530 / 0.8042 / 0.7835    |
| ✗            | ✓   | 0.8680 / 0.8185 / 0.8045 | 0.8455 / 0.7725 / 0.7703    |
| ✓            | ✓   | 0.8686 / 0.8193 / 0.8058 | 0.8517 / 0.7871 / 0.7798    |
| Sleep-EDF-78 | ✗   | ✗                        | 0.8012 / 0.7331 / 0.7257    |
| ✗            | ✓   | 0.8146 / 0.7552 / 0.7434 | 0.7894 / 0.7162 / 0.7088    |
| ✓            | ✓   | 0.8158 / 0.7558 / 0.7450 | 0.7959 / 0.7217 / 0.7169    |"
226,"|                                                 | % matching     | Gold reward of                      |
| ----------------------------------------------- | -------------- | ----------------------------------- |
|                                                 | human feedback | aligned model  $ \pi_{\theta}^{*} $ |
| $ \mathcal{D}_{\text{weak}} $                   | 60.6%          | 4.84 (+2.61)                        |
| $ \mathcal{D}_{\text{match}}^{\text{weak}} $    | 100%           | 4.78 (+2.56)                        |
| $ \mathcal{D}_{\text{mismatch}}^{\text{weak}} $ | 0%             | 4.01 (+1.79)                        |"
226,"|                                   | Chosen | Rejected | $ \Delta $ |
| --------------------------------- | ------ | -------- | ---------- |
| $ \mathcal{D}_{\text{human}} $    | 5.77   | 4.23     | 1.52       |
| $ \mathcal{D}_{\text{weak}} $     | 5.63   | 4.39     | 1.23       |
| $ \mathcal{D}_{\text{match}} $    | 5.93   | 3.66     | 2.27       |
| $ \mathcal{D}_{\text{mismatch}} $ | 5.17   | 5.53     | -0.36      |"
227,"| Statistics                              | Number        |
| --------------------------------------- | ------------- |
| Total unique question instances         | 700           |
| - Questions with api or textual-context | 300 (42.86%)  |
| - Questions with visual-context         | 400 (57.14%)  |
| Total unique countries                  | 54            |
| Total unique cities                     | 180           |
| Maximum textual-context length          | 1500          |
| Maximum question length                 | 107           |
| Average textual-context length          | 435.63        |
| Average question length                 | 21.41         |
| Unique number of textual-context        | 215           |
| Unique number of visual-context         | 270           |
| Min, Max, Avg questions from a country  | 6, 132, 12.96 |
| Min, Max, Avg questions from a city     | 1, 44, 3.89   |
| Min, Max, Avg choices                   | 2, 7, 4.004   |
| Max zoom of visual-context              | 21.0          |
| Min zoom of visual-context              | 8.0           |
| Average zoom of visual-context          | 15.26         |"
227,"|                                   | Model             | Textual (#300) | API (#300) | Visual (#400) |
| --------------------------------- | ----------------- | -------------- | ---------- | ------------- |
| Close-Source (Proprietary) Models |                   |                |            |               |
| 1                                 | Claude-3.5-Sonnet | 66.33          | 64.00      | 61.65         |
| 2                                 | Gemini-1.5-Pro    | 66.33          | 43.33      | 56.14         |
| 3                                 | GPT-4o            | 63.33          | 48.67      | 58.90         |
| 4                                 | GPT-4-Turbo       | 62.33          | 53.67      | 55.89         |
| 5                                 | Gemini-1.5-Flash  | 58.67          | 41.67      | 51.94         |
| 6                                 | GPT-4o-mini       | 51.00          | 23.00      | 50.13         |
|                                   | Human             | 86.67          | —          | 82.23         |"
227,"| Model                             | Straight-Line Distance (#47) | Cardinal Direction (#24) |
| --------------------------------- | ---------------------------- | ------------------------ |
| API                               | API + Calculator             | API                      |
| Close-Source (Proprietary) Models |                              |                          |
| Claude-3.5-Sonnet                 | 51.06                        | 85.11                    |
| GPT-4o                            | 46.81                        | 70.21                    |
| GPT-4-Turbo                       | 40.43                        | 76.59                    |
| Gemini-1.5-Pro                    | 38.29                        | 72.34                    |
| Gemini-1.5-Flash                  | 46.81                        | 63.83                    |
| GPT-4o-mini                       | 34.04                        | 78.72                    |
| GPT-3.5-Turbo                     | 19.15                        | 55.32                    |"
229,"| Method                                      | Noise  | Env.    | Rep.L | TS     | Local updates | Personalized | Linear speedup |
| ------------------------------------------- | ------ | ------- | ----- | ------ | ------------- | ------------ | -------------- |
| FedTD &amp; FedQ (Khodadadian et al., 2022) | Markov | Homo.   | ✗     | Single | ✓             | ✗            | ✓              |
| FedTD (Dal Fabbro et al., 2023)             | Markov | Homo.   | ✗     | Single | ✗             | ✗            | ✓              |
| FedTD (Wang et al., 2023a)                  | Markov | Hetero. | ✗     | Single | ✓             | ✗            | ✓              |
| QAvg &amp; PAvg (Jin et al., 2022)          | i.i.d. | Hetero. | ✗     | Single | ✗             | ✗            | ✗              |
| FedQ (Woo et al., 2023)                     | Markov | Hetero. | ✗     | Single | ✓             | ✗            | ✓              |
| A3C (Shen et al., 2023)                     | Markov | Homo.   | ✗     | Two    | ✗             | ✗            | ✓              |
| FedSARSA (Zhang et al., 2024)               | Markov | Hetero. | ✗     | Single | ✓             | ✗            | ✓              |
| PFEDRL-REP                                  | Markov | Hetero. | ✓     | Two    | ✓             | ✓            | ✓              |"
230,"| References                     | Running Time                 |
| ------------------------------ | ---------------------------- |
| [Ban10]                        | $ mn^{4.5} $                 |
| [ES18] + LP Solver of [LS14]   | $ m^{\omega} + m^{2+1/18} $  |
| [ES18] + LP Solver of [BLSS20] | $ mn + n^{3} $               |
| [ES18] + LP Solver of [JSWZ21] | $ nnz(A)\sqrt{n} + n^{2.5} $ |
| [Lar23]                        | $ mn^{2} + n^{3} $           |
| Ours (without FMM)             | $ nnz(A) + n^{3} $           |
| Ours (with FMM)                | $ nnz(A) + n^{2.53} $        |"
231,"| Method                                      | Controllable Statistics | Applied Geometries                                                             | Incorporated by GyroBN |
| ------------------------------------------- | ----------------------- | ------------------------------------------------------------------------------ | ---------------------- |
| SPDBN
(Brooks et al., 2019)                 | M                       | SPD manifolds under AIM                                                        | ✓                      |
| SPDBN
(Kobler et al., 2022b)                | M+V                     | SPD manifolds under AIM                                                        | ✓                      |
| SPDDSMBN
(Kobler et al., 2022a)             | M+V                     | SPD manifolds under AIM                                                        | ✓                      |
| ManifoldNorm
(Chakraborty, 2020, Algs. 1-2) | N/A                     | Riemannian homogeneous space                                                   | ✗                      |
| ManifoldNorm
(Chakraborty, 2020, Algs. 3-4) | M+V                     | Matrix Lie groups under the distance  $ d(X, Y) = \| \text{mlog}(X^{-1}Y) \| $ | ✓                      |
| RBN
(Lou et al., 2020, Alg. 2)              | N/A                     | Geodesically complete manifolds                                                | ✗                      |
| LieBN
(Chen et al., 2024b)                  | M+V                     | General Lie groups                                                             | ✓                      |
| GyroBN                                      | M+V                     | Pseudo-reductive gyrogroups with gyro isometric gyrations                      | N/A                    |"
231,"| Geometry                 | Symbol               | P ⊕ Q or x ⊕ y                    | E       | ⊕P or ⊕x                | Lie group                 | Gyrogroup     | References             |
| ------------------------ | -------------------- | --------------------------------- | ------- | ----------------------- | ------------------------- | ------------- | ---------------------- |
| AIM  $ S^{n}_{++} $      | ⊕AI                  | P 1/2 QP 1/2                      | I_n     | P−1                     | ✗                         | ✓             | (Nguyen, 2022b)        |
| LEM  $ S^{n}_{++} $      | ⊕LE                  | mexp(mlog(P) + mlog(Q))           | I_n     | P−1                     | ✓                         | ✓             | (Arsigny et al., 2005) |
| (Nguyen, 2022b)          |                      |                                   |         |                         |                           |               |                        |
| LCM  $ S^{n}_{++} $      | ⊕LC                  | ψ−1LC(ψLC(P) + ψLC(Q))            | I_n     | ψLC(−ψLC(P))            | ✓                         | ✓             | (Lin, 2019)            |
| (Nguyen, 2022b)          |                      |                                   |         |                         |                           |               |                        |
| (Chen et al., 2024e)     |                      |                                   |         |                         |                           |               |                        |
| Gr(p,n)                  | ⊕Gr                  | mexp(Ω)Q mexp(−Ω)                 | I_{p,n} | mexp(−Ω)I_{p,n} mexp(Ω) | ✗                         | Non-reductive | (Nguyen, 2022a)        |
| Gr(p,n)                  | ⊕Gr                  | mexp(Ω)V                          | I_{p,n} | mexp(−Ω)I_{p,n}         | (Nguyen &amp; Yang, 2023) |               |                        |
| M_K                      | ⊕K                   | (1−2K(x,y)−K||y||2)x+(1+K||x||2)y | 0       | −x                      | ✗(✓ for K=0)              | ✓             | (Ungar, 2009)          |
| 1−2K(x,y)+K2||x||2||y||2 | (Ganea et al., 2018) |                                   |         |                         |                           |               |                        |
| (Skopek et al., 2019)    |                      |                                   |         |                         |                           |               |                        |"
231,"| Operator                                                                   | Gr(p,n)                                                                 | $ \mathbb{P}_{K}^{n} $                                                                                          |
| -------------------------------------------------------------------------- | ----------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |
| Identity element                                                           | I_{p,n}                                                                 | 0 \in \mathbb{R}^{n}                                                                                            |
| P \oplus \mathbb{Gr} Q or x \oplus_{K} y                                   | mexp(\Omega)V                                                           | $ \frac{(1-2K\langle x,y\rangle-K\|y\|^{2})x+(1+K\|x\|^{2})y}{1-2K\langle x,y\rangle+K^{2}\|x\|^{2}\|y\|^{2}} $ |
| \ominus \mathbb{Gr} P or \ominus_{K} x                                     | mexp(-\Omega)I_{p,n}                                                    | -x                                                                                                              |
| t \odot \mathbb{Gr} P or t \odot_{K} x                                     | mexp(t\Omega)I_{p,n}                                                    | $ \frac{1}{\sqrt{|K|}}\tanh\left(t\tanh^{-1}(\sqrt{|K|}\|x\|)\right)\frac{x}{\|x\|} $                           |
| \mathrm{Bar}_{\gamma}^{\mathrm{Gr}}(Q,P) or \mathrm{Bar}_{\gamma}^{K}(y,x) | \mathrm{Exp}_{P}^{\mathrm{Gr}}(\gamma\mathrm{Log}_{P}^{\mathrm{Gr}}(Q)) | x \oplus_{K}(-x \oplus_{K} y) \odot_{K} t                                                                       |
| Fréchet Mean                                                               | Karcher Flow (Karcher, 1977)                                            | (Lou et al., 2020, Alg. 1)                                                                                      |"
231,"| BN     | None       | ManifoldNorm-Gr | RBN-Gr     | GyroBN-Gr |
| ------ | ---------- | --------------- | ---------- | --------- |
| Acc.   | Mean±std   | Max             | Mean±std   | Max       |
| HDM05  | 48.97±0.24 | 49.23           | 49.67±0.76 | 50.41     |
| NTU60  | 70.13±0.16 | 70.32           | 68.56±0.43 | 69.14     |
| NTU120 | 53.76±0.18 | 53.96           | 51.41±0.38 | 51.92     |"
231,"|              | HDM05  | NTU60  | NTU120 |
| ------------ | ------ | ------ | ------ |
| Architecture | 1Block | 2Block | 3Block |
| GyroGr       | 49.23  | 49.09  | 47.02  |
| GyroGrBN     | 52.43  | 50.62  | 51.56  |"
231,"| Dataset | HNN                | HNN-RBN-H          | HNN-GyroBN-H       |
| ------- | ------------------ | ------------------ | ------------------ |
| Cora    | 89.0  $ \pm $  0.1 | 93.5  $ \pm $  0.5 | 94.3  $ \pm $  0.2 |
| Disease | 75.1  $ \pm $  0.3 | 76.6  $ \pm $  2.2 | 81.2  $ \pm $  0.9 |
| Airport | 90.8  $ \pm $  0.2 | 94.2  $ \pm $  0.4 | 95.4  $ \pm $  0.2 |
| Pubmed  | 94.9  $ \pm $  0.1 | 93.4  $ \pm $  0.2 | 95.8  $ \pm $  0.1 |"
346,"| Training Data                         | Accuracy | Balanced Accuracy |
| ------------------------------------- | -------- | ----------------- |
| All                                   | Rare     | All               |
| CSDD $ ^{*} $  (Dencker et al., 2020) | 58.43%   | 25.84%            |
| +SD- $ \bar{Y} $  data                | 61.56%   | 38.56%            |
| +CN- $ \bar{Y} $  data                | 64.14%   | 53.17%            |"
231,"| Methods             | HDM05(47  $ \times $  10) | NTU60(75  $ \times $  10) | NTU120(75  $ \times $  10) |
| ------------------- | ------------------------- | ------------------------- | -------------------------- |
| GyroGr              | 2.19                      | 50.92                     | 80.72                      |
| GyroGr-ManifoldNorm | 4.98                      | 242.12                    | 409.48                     |
| GyroGr-RBN          | 5.16                      | 242.63                    | 410.08                     |
| GyroGr-GyroBN       | 3.10                      | 59.55                     | 108.92                     |"
231,"| Methods      | Cora   | Disease | Airport | Pubmed |
| ------------ | ------ | ------- | ------- | ------ |
| HNN          | 0.0323 | 0.0271  | 0.054   | 0.1253 |
| HNN-RBN-H    | 0.0905 | 0.0883  | 0.1215  | 0.3416 |
| HNN-GyroBN-H | 0.0757 | 0.0842  | 0.119   | 0.3351 |"
232,"| Features          | Cybench | Fang et al. $ ^{1,2} $ | CVE-Bench |
| ----------------- | ------- | ---------------------- | --------- |
| # Vulnerability   | 40      | 25                     | 40        |
| Real-world Vul.   | ✗       | ✓                      | ✓         |
| Critical-Severity | ✗       | ☑                      | ✓         |
| Diverse Attacks   | ✗       | ☑                      | ✓         |"
232,"| Application            | # Vulnerabilities |
| ---------------------- | ----------------- |
| Content Management     | 12                |
| AI App                 | 7                 |
| Business Management    | 6                 |
| Operational Monitoring | 4                 |
| Web Infra              | 3                 |
| Library                | 3                 |
| E-commerce             | 2                 |
| Computing Management   | 1                 |
| Mail Server            | 1                 |
| Web Portal             | 1                 |"
232,"| Agent                               | Cy-Agent | AutoGPT | T-Agent  |
| ----------------------------------- | -------- | ------- | -------- |
| Scenario                            | Zero-day | One-day | Zero-day |
| Limited Task Understanding (\%)^{*} | 30.0     | 20.0    | 15       |
| Incorrect Focus (%)                 | 0        | 0       | 0        |
| Insufficient Exploration (%)        | 67.5     | 37.5    | 72.5     |
| Tool Misuse (%)                     | 47.5     | 27.5    | 5.0      |
| Inadequate Reasoning (%)            | 10.0     | 7.5     | 7.5      |"
233,"| Methods                      | COCO          |
| ---------------------------- | ------------- |
| LLaVA-7B                     | MiniGPT-4     |
| $ CHAIR_{S} $                | $ CHAIR_{I} $ |
| Greedy $ ^{\dagger} $        | 51.8          |
| DoLA $ ^{\dagger} $          | 53.8          |
| OPERA $ ^{\dagger} $         | 50.2          |
| VCD $ ^{\dagger} $           | 55.4          |
| LURE $ ^{\dagger} $          | 51.2          |
| HALC $ ^{\dagger} $          | 50.2          |
| AD-HH $ ^{\dagger} $  (Ours) | 29.6          |
| TF-HH $ ^{*} $  (Ours)       | 35.0          |"
234,"| Method                         | Proc. &amp; Year | Backbone   | Utilizing Diffusion | synthetic-to-real | real-to-real |
| ------------------------------ | ---------------- | ---------- | ------------------- | ----------------- | ------------ |
| G→C                            | G→B              | G→M        | Avg.                | C→B               | C→M          |
| ResNet based:                  |                  |            |                     |                   |              |
| WildNet (Lee et al., 2022)     | CVPR2022         | RN101      | ✗                   | 44.6              | 38.4         |
| SHADE (Zhao et al., 2022)      | ECCV2022         | RN101      | ✗                   | 46.7              | 43.7         |
| SAW (Peng et al., 2022)        | CVPR2022         | RN50       | ✗                   | 39.8              | 37.3         |
| TLDR (Kim et al., 2023)        | ICCV2023         | RN101      | ✗                   | 47.6              | 44.9         |
| DIDEX (Niemeijer et al., 2024) | WACV2024         | RN101      | ✓                   | 52.4              | 40.9         |
| BlindNet (Ahn et al., 2024)    | CVPR2024         | RN50       | ✗                   | 45.7              | 41.3         |
| FAMix (Fahes et al., 2024)     | CVPR2024         | RN101      | ✗                   | 49.5              | 46.4         |
| QueryDiff (Ours)               | -                | RN50       | ✓                   | 52.5              | 49.2         |
| Transformer based:             |                  |            |                     |                   |              |
| HRDA (Hoyer et al., 2022)      | ECCV2022         | MiT-B5     | ✗                   | 57.4              | 49.1         |
| HGFormer (Ding et al., 2023)   | CVPR2023         | Swin-T     | ✗                   | -                 | -            |
| DGInStyle (Jia et al., 2023)   | ECCV2024         | MiT-B5     | ✓                   | 58.6              | 52.3         |
| QueryDiff (Ours)               | -                | MiT-B5     | ✓                   | 61.9              | 55.4         |
| VFM based:                     |                  |            |                     |                   |              |
| CLOUDS (Benigmim et al., 2024) | CVPR2024         | ConvNext-L | ✓                   | 60.2              | 57.4         |
| Rein (Wei et al., 2024)        | CVPR2024         | DINOv2-L   | ✗                   | 66.4              | 60.4         |
| QueryDiff (Ours)               | -                | DINOv2-L   | ✓                   | 69.1              | 62.3         |"
234,"| Method                           | Backbone             | normal-to-adverse    |
| -------------------------------- | -------------------- | -------------------- |
| C $ \rightarrow $ AF             | C $ \rightarrow $ AN | C $ \rightarrow $ AR |
| ResNet based:                    |                      |                      |
| Mask2Former (Cheng et al., 2022) | RN50                 | 54.1                 |
| HGFormer (Ding et al., 2023)     | RN50                 | 56.5                 |
| QueryDiff (Ours)                 | RN50                 | 59.5                 |
| Transformer based:               |                      |                      |
| ISSA (Li et al., 2023)           | MiT-B5               | 67.5                 |
| Mask2Former (Cheng et al., 2022) | Swin-L               | 69.1                 |
| HGFormer (Ding et al., 2023)     | Swin-L               | 69.9                 |
| QueryDiff (Ours)                 | MiT-B5               | 73.1                 |
| VFM based:                       |                      |                      |
| Rein (Wei et al., 2024)          | DINOv2-L             | 76.4                 |
| QueryDiff (Ours)                 | DINOv2-L             | 78.5                 |"
234,"| Backbone            | Training Method     | Trainable Params*   | mIoU |
| ------------------- | ------------------- | ------------------- | ---- |
| G $ \rightarrow $ C | G $ \rightarrow $ B | G $ \rightarrow $ M | Avg. |
| CLIP                | Full                | 304.15M             | 51.3 |
| Freeze              | 0.00M               | 53.7                | 48.7 |
| Rein                | 2.99M               | 57.1                | 54.7 |
| QueryDiff           | 5.73M               | 58.9                | 56.0 |
| SAM                 | Full                | 632.18M             | 57.6 |
| Freeze              | 0.00M               | 57.0                | 47.1 |
| Rein                | 4.51M               | 59.6                | 52.0 |
| QueryDiff           | 7.25M               | 61.4                | 53.7 |
| DINOV2              | Full                | 304.20M             | 63.7 |
| Freeze              | 0.00M               | 63.3                | 56.1 |
| Rein                | 2.99M               | 66.4                | 60.4 |
| QueryDiff           | 5.73M               | 69.1                | 62.3 |"
234,"|   | AQ | $ \mathcal{L}_{\text{sup}} $ | $ \mathcal{L}_{\text{dist}} $ | G $ \rightarrow $ C | G $ \rightarrow $ B | G $ \rightarrow $ M | Avg. | $ \Delta $ mIoU |
| - | -- | ---------------------------- | ----------------------------- | ------------------- | ------------------- | ------------------- | ---- | --------------- |
| 1 |    |                              |                               | 66.4                | 60.4                | 66.1                | 64.3 | -               |
| 2 | ✓  |                              |                               | 68.0                | 60.9                | 67.4                | 65.4 | +1.1            |
| 3 | ✓  | ✓                            |                               | 68.8                | 61.7                | 67.9                | 66.1 | +1.8            |
| 4 | ✓  | ✓                            | ✓                             | 69.1                | 62.3                | 68.6                | 66.7 | +2.4            |"
362,"| Model            | Grid Size           | Log-Lik.  $ \uparrow $ | FPT  $ \downarrow $ |
| ---------------- | ------------------- | ---------------------- | ------------------- |
| Swin-TNP NN (PT) | 64  $ \times $  128 | 1.636                  | 144                 |
| Swin-TNP NN (PT) | 64  $ \times $  128 | 1.819                  | 127                 |"
235,"|                              | ETTh1 | ETTh2 | ETTm1 | ETTm2 | ECL     | Traffic   | Weather | Solar | Average | Promotion |
| ---------------------------- | ----- | ----- | ----- | ----- | ------- | --------- | ------- | ----- | ------- | --------- |
| TimeMixer++                  | 0.419 | 0.339 | 0.369 | 0.269 | 0.165   | 0.416     | 0.226   | 0.203 | 0.300   | -         |
| w/o channel mixing           | 0.424 | 0.346 | 0.374 | 0.271 | 0.197   | 0.442     | 0.233   | 0.245 | 0.317   | 5.36%     |
| w/o time image decomposition | 0.441 | 0.358 | 0.409 | 0.291 | 0.198   | 0.445     | 0.251   | 0.241 | 0.329   | 8.81%     |
| w/o multi-scale mixing       | 0.447 | 0.361 | 0.391 | 0.284 | 0.172   | 0.427     | 0.239   | 0.234 | 0.320   | 6.25%     |
| w/o multi-resolution mixing  | 0.431 | 0.350 | 0.374 | 0.280 | 0.181   | 0.432     | 0.241   | 0.233 | 0.316   | 5.10%     |
| SMD                          | MSL   | SMAP  | SWAT  | PSM   | Average | Promotion |         |       |         |           |
| TimeMixer++                  | 86.50 | 85.82 | 73.10 | 94.64 | 97.60   | 87.47     | -       |       |         |           |
| w/o channel mixing           | 84.51 | 74.03 | 70.91 | 90.41 | 96.17   | 83.21     | 4.94%   |       |         |           |
| w/o time image decomposition | 81.21 | 72.43 | 66.02 | 82.41 | 92.53   | 78.92     | 9.84%   |       |         |           |
| w/o multi-scale mixing       | 82.37 | 75.12 | 92.79 | 86.48 | 94.53   | 86.26     | 1.46%   |       |         |           |
| w/o multi-resolution mixing  | 83.37 | 79.24 | 77.49 | 88.46 | 96.02   | 86.26     | 2.99%   |       |         |           |"
235,"| Models | TimeMixer++ (Ours) | TimeMixer iTransformer (2024b) | TiDE (2023a) | SCINet Crossformer (2022a) | PatchTST TimesNet (2023) | MICN (2023a) | DLinear FEDformer (2023) | Stationary Autoformer (2022b) | Autoformer (2021) |
| ------ | ------------------ | ------------------------------ | ------------ | -------------------------- | ------------------------ | ------------ | ------------------------ | ----------------------------- | ----------------- |
| MAE    | 15.91              | 17.41                          | 19.87        | 21.86                      | 19.12                    | 19.03        | 23.01                    | 20.54                         | 19.34             |
| MAPE   | 10.08              | 10.59                          | 12.55        | 13.80                      | 12.24                    | 12.22        | 14.95                    | 12.69                         | 12.38             |
| RMSE   | 27.06              | 28.01                          | 31.29        | 34.42                      | 30.12                    | 30.17        | 36.05                    | 33.25                         | 30.40             |"
235,"| Models | TimeMixer++ (Ours) | TimeMixer iTransformer (2024b) | TiDE (2024) | TimesNet (2023a) | N-HiTS (2023) | N-BEATS (2019) | PatchTST (2023) | MICN (2023a) | FiLM (2022a) | LightTS (2022a) | DLinear (2023) | FED. (2022b) | Stationary (2022c) | Auto. (2021) |
| ------ | ------------------ | ------------------------------ | ----------- | ---------------- | ------------- | -------------- | --------------- | ------------ | ------------ | --------------- | -------------- | ------------ | ------------------ | ------------ |
| SMAPE  | 11.448             | 11.723                         | 12.684      | 13.950           | 11.829        | 11.927         | 11.851          | 13.152       | 19.638       | 14.863          | 13.525         | 13.639       | 12.840             | 12.780       |
| MASE   | 1.487              | 1.559                          | 1.764       | 1.940            | 1.585         | 1.613          | 1.559           | 1.945        | 5.947        | 2.207           | 2.111          | 2.095        | 1.701              | 1.756        |
| OWA    | 0.821              | 0.840                          | 0.929       | 1.020            | 0.851         | 0.861          | 0.855           | 0.998        | 2.279        | 1.125           | 1.051          | 1.051        | 0.918              | 0.930        |"
236,"| Dataset Class          | Emotion6 6               | EmoSet 8                 | WebEmo7 7                  | WebEmo25 25              | Abstract 8               | Average                  |
| ---------------------- | ------------------------ | ------------------------ | -------------------------- | ------------------------ | ------------------------ | ------------------------ |
| LLaVA-7b               |                          |                          |                            |                          |                          |                          |
| Zero-shot              | 48.32                    | 52.77                    | 25.56                      | 15.71                    | 27.86                    | 34.04                    |
| Zero-shot-CoT          | 48.65                    | 51.67                    | 22.93                      | 15.52                    | 26.43                    | 33.04                    |
| SparseVLM              | 49.83                    | 54.00                    | 26.64                      | 15.68                    | 26.79                    | 34.59                    |
| SEPM ( $ \beta=0.1 $ ) | 54.21  $ \uparrow $ 5.89 | 56.04  $ \uparrow $ 3.27 | 42.39  $ \uparrow $ 16.83  | 18.26  $ \uparrow $ 2.55 | 29.29  $ \uparrow $ 1.43 | 40.04  $ \uparrow $ 6.00 |
| SEPM ( $ \beta=0.2 $ ) | 54.04  $ \uparrow $ 5.72 | 56.24  $ \uparrow $ 3.47 | 42.75  $ \uparrow $ 17.19  | 18.34  $ \uparrow $ 2.63 | 28.21  $ \uparrow $ 0.35 | 39.92  $ \uparrow $ 5.88 |
| VILA-8b                |                          |                          |                            |                          |                          |                          |
| Zero-shot              | 47.47                    | 44.45                    | 41.90                      | 15.35                    | 20.00                    | 33.83                    |
| Zero-shot-CoT          | 44.28                    | 43.20                    | 41.08                      | 13.57                    | 21.07                    | 32.64                    |
| SEPM ( $ \beta=0.1 $ ) | 51.35  $ \uparrow $ 3.88 | 53.11  $ \uparrow $ 8.66 | 44.30  $ \uparrow $ 2.40   | 17.36  $ \uparrow $ 2.01 | 25.71  $ \uparrow $ 5.71 | 38.37  $ \uparrow $ 4.54 |
| SEPM ( $ \beta=0.2 $ ) | 52.02  $ \uparrow $ 4.55 | 53.11  $ \uparrow $ 8.66 | 39.10  $ \downarrow $ 2.80 | 19.39  $ \uparrow $ 4.04 | 26.43  $ \uparrow $ 6.43 | 38.01  $ \uparrow $ 4.18 |"
236,"| Dataset       | Emotion6      | EmoSet        |
| ------------- | ------------- | ------------- |
| Drop rate     | $ \beta=0.1 $ | $ \beta=0.2 $ |
| Random        | 53.20         | 51.85         |
| Query-related | 53.54         | 53.54         |
| FoE-related   | 54.21         | 54.04         |"
236,"| CCI | FP | VTA | Emotion6                  | EmoSet                    |
| --- | -- | --- | ------------------------- | ------------------------- |
|     |    |     | 48.32                     | 52.77                     |
| ✓   |    |     | 51.68 $ ^{\uparrow3.36} $ | 53.98 $ ^{\uparrow1.21} $ |
|     | ✓  |     | 51.52 $ ^{\uparrow3.20} $ | 54.28 $ ^{\uparrow1.51} $ |
| ✓   | ✓  |     | 53.03 $ ^{\uparrow4.71} $ | 56.10 $ ^{\uparrow3.33} $ |
| ✓   | ✓  | ✓   | 54.04 $ ^{\uparrow5.72} $ | 56.24 $ ^{\uparrow3.47} $ |"
237,"|        | Private                 | Useful                   | Results on CelebA      | “Hidden” Useful             |
| ------ | ----------------------- | ------------------------ | ---------------------- | --------------------------- |
| Method | Male ( $ \downarrow $ ) | Smiling ( $ \uparrow $ ) | Young ( $ \uparrow $ ) | Attractive ( $ \uparrow $ ) |
| ADV    | 99.9 $ \pm $ 0.1        | 98.8 $ \pm $ 0.1         | 97.0 $ \pm $ 0.9       | 94.6 $ \pm $ 0.4            |
| GAP    | 83.0 $ \pm $ 1.1        | 75.9 $ \pm $ 1.3         | 45.4 $ \pm $ 3.0       | 77.6 $ \pm $ 1.1            |
| MSDA   | 91.6 $ \pm $ 0.7        | 99.8 $ \pm $ 0.2         | 92.4 $ \pm $ 2.4       | 89.9 $ \pm $ 1.0            |
| BDQ    | 99.7 $ \pm $ 0.1        | 98.8 $ \pm $ 0.2         | 96.3 $ \pm $ 0.8       | 94.1 $ \pm $ 0.6            |
| PPDAR  | 99.7 $ \pm $ 0.1        | 98.9 $ \pm $ 0.3         | 97.2 $ \pm $ 1.2       | 94.4 $ \pm $ 0.6            |
| MaSS   | 96.9 $ \pm $ 0.1        | 97.2 $ \pm $ 0.2         | 86.2 $ \pm $ 1.4       | 90.6 $ \pm $ 0.3            |
| PASS   | 4.9 $ \pm $ 0.5         | 98.3 $ \pm $ 0.1         | 78.6 $ \pm $ 0.8       | 58.1 $ \pm $ 2.8            |"
245,"| Editor            | Prefix Distraction | Multi-hop Reasoning | Subject Specificity | Relation Specificity |
| ----------------- | ------------------ | ------------------- | ------------------- | -------------------- |
| DP $ \downarrow $ | CAP $ \uparrow $   | EOS $ \uparrow $    | DP $ \downarrow $   | OAP $ \downarrow $   |
| GPT-2 XL          | 5.01               | 13.30               | 54.58               | 0.27                 |
| FT                | 22.97              | 9.05                | 23.40               | 4.57                 |
| FT-L              | 12.87              | 11.00               | 40.09               | 1.60                 |
| MEND              | 46.57              | 1.93                | 17.88               | 3.27                 |
| ROME              | 44.99              | 6.62                | 15.22               | 23.32                |
| ROME-LTI          | 19.53              | 9.88                | 28.17               | 10.08                |
| MEMIT             | 32.19              | 7.63                | 20.55               | 16.75                |
| MEMIT-LTI         | 18.76              | 8.78                | 26.02               | 8.05                 |
| GPT-J             | 4.60               | 17.08               | 64.81               | 0.27                 |
| FT                | 77.43              | 3.61                | 4.58                | 42.51                |
| FT-L              | 7.05               | 17.99               | 56.30               | 3.28                 |
| MEND              | 36.31              | 11.27               | 29.40               | 3.94                 |
| ROME              | 26.13              | 12.62               | 32.57               | 41.03                |
| ROME-LTI          | 8.93               | 15.48               | 49.73               | 11.12                |
| MEMIT             | 18.30              | 14.77               | 39.32               | 25.80                |
| MEMIT-LTI         | 10.98              | 16.43               | 48.56               | 16.35                |"
238,"| Method            | PBMC-M  | BMMC    | Pancreas | Immune Cell Atlas |
| ----------------- | ------- | ------- | -------- | ----------------- |
| Bio               | Batch   | Total   | Bio      | Batch             |
| SimCLR            | 0.877   | 0.434   | 0.700    | 0.877             |
| ± 0.020           | ± 0.001 | ± 0.012 | ± 0.025  | ± 0.002           |
| MoCo              | 0.786   | 0.581   | 0.704    | 0.647             |
| ± 0.005           | ± 0.016 | ± 0.003 | ± 0.048  | ± 0.024           |
| VICReg            | 0.814   | 0.405   | 0.651    | 0.832             |
| ± 0.039           | ± 0.026 | ± 0.013 | ± 0.051  | ± 0.009           |
| CLEAR             | —       | —       | —        | —                 |
|                   |         |         |          |                   |
| CLAIRE            | —       | —       | —        | —                 |
| scGPT (finetuned) | —       | —       | —        | —                 |
| —                 | —       | —       | —        | —                 |
| totalVI / scVI    | 0.702   | 0.305   | 0.543    | 0.755             |
| ± 0.002           | ± 0.002 | ± 0.001 | ± 0.002  | ± 0.001           |"
239,"| METHODS | NYC    | CHI    | SIP   | SD    |
| ------- | ------ | ------ | ----- | ----- |
| MAE     | RMSE   | MAPE   | MAE   | RMSE  |
| STGCN   | 6.774  | 15.853 | 0.374 | 1.518 |
| STGODE  | 9.522  | 22.555 | 0.481 | 1.543 |
| GWN     | 10.263 | 24.535 | 0.546 | 1.520 |
| STTN    | 7.962  | 19.544 | 0.435 | 1.494 |
| AGCRN   | 8.254  | 19.301 | 0.488 | 1.543 |
| ASTGCN  | 10.323 | 25.070 | 0.519 | 1.536 |
| CMUST   | 6.576  | 14.954 | 0.459 | 1.498 |
| SYNEVO  | 6.494  | 14.885 | 0.358 | 1.486 |"
239,"| Methods | GPU Cost |
| ------- | -------- |
| NYC     | CHI      |
| CMuST   | 4034MB   |
| SynEVO  | 2170MB   |"
239,"|           |            | NYC   | CHI   | SIP   |
| --------- | ---------- | ----- | ----- | ----- |
| TEMPORAL  | SYNEVO MAE | 6.278 | 1.457 | 0.683 |
| CMUST MAE | 6.457      | 1.472 | 0.716 |       |
| SOURCE    | SYNEVO MAE | 6.494 | 1.486 | 0.697 |
| CMUST MAE | 6.576      | 1.498 | 0.737 |       |"
239,"|        |        | NYC    | CHI   | SIP    | SD     |
| ------ | ------ | ------ | ----- | ------ | ------ |
| SynEVO | MAE    | 13.420 | 1.995 | 0.775  | 16.981 |
| RMSE   | 32.059 | 3.769  | 1.510 | 25.197 |        |
| MAPE   | 0.668  | 0.369  | 0.217 | 0.214  |        |
| GWN    | MAE    | 17.091 | 4.109 | 1.039  | 21.446 |
| RMSE   | 40.802 | 8.385  | 2.056 | 30.736 |        |
| MAPE   | 0.856  | 0.742  | 0.264 | 0.271  |        |"
240,"| Algorithm | HalfCheetah-v3 | Hopper-v3        | Walker2d-v3 |
| --------- | -------------- | ---------------- | ----------- |
| Nominal   | Noisy          | Nominal          | Noisy       |
| PPO       | 4820           | 3688 $ (-1132) $ | 3150        |
| RNAC      | 5423           | 4088 $ (-1335) $ | 3211        |
| RARL      | 5620           | 4617 $ (-1003) $ | 3124        |
| SAM+PPO   | 6530           | 5990 $ (-540) $  | 3505        |"
241,"| Head                     | 0    | 4    | 8    | 12   | 16   | 20   | 24   | 28   |
| ------------------------ | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| POCP                     | 0.20 | 0.22 | 0.20 | 0.28 | 0.30 | 0.28 | 0.52 | 0.25 |
| Original attention score | 21   | 30   | 14   | 42   | 8.0  | 7.0  | -3.0 | 17   |
| POCP in low freq         | 0.25 | 0.13 | 0.44 | 0.25 | 0.50 | 0.63 | 0.50 | 0.13 |
| POCP in high freq        | 0.13 | 0.31 | 0.06 | 0.38 | 0.19 | 0.06 | 0.56 | 0.38 |"
241,"| Head                     | 0    | 4     | 8    | 12    | 16    | 20   | 24    | 28    |
| ------------------------ | ---- | ----- | ---- | ----- | ----- | ---- | ----- | ----- |
| Original attention score | 4.69 | -3.09 | 1.78 | -2.33 | -1.55 | 0.31 | -2.12 | -1.21 |"
242,"|             | r=2     | r=16    | r=32    | r=128    |
| ----------- | ------- | ------- | ------- | -------- |
| MODEL       | PARA    | LORA    | PARA    | LORA     |
| MODEL SIZE  | 1.8 MB  | 4.8 MB  | 13 MB   | 33 MB    |
| TRAIN. TIME | 5.8 MIN | 6.2 MIN | 6.9 MIN | 10.4 MIN |"
243,"| Type                     | Model      | ABIDE            | ADNI             |
| ------------------------ | ---------- | ---------------- | ---------------- |
|                          |            | AUC $ \uparrow $ | ACC $ \uparrow $ |
| Graph Neural Networks    | GCN        | 59.59±3.44       | 59.30±3.38       |
| GAT                      | 60.43±3.88 | 60.10±4.13       | 59.26±5.51       |
| BrainGNN                 | 64.42±3.57 | 63.09±1.35       | 65.65±2.88       |
| BrainGB                  | 70.32±3.66 | 65.12±3.90       | 67.01±10.00      |
| FBNETGEN                 | 74.55±3.77 | 67.09±3.37       | 64.71±9.85       |
| A-GCL                    | 73.86±2.91 | 71.04±2.40       | 71.42±3.03       |
| Graph Transformer Models | SAN        | 71.35±2.18       | 65.34±2.91       |
| Graphormer               | 63.91±4.05 | 61.88±6.85       | 66.30±9.98       |
| GraphTrans               | 60.13±6.73 | 57.83±4.71       | 65.70±10.3       |
| BrainNETTF               | 77.93±1.41 | 69.26±2.26       | 65.92±8.60       |
| ContrastPool             | 57.36±0.87 | 57.44±0.69       | 57.66±6.85       |
| ALTER                    | 77.99±2.21 | 70.10±2.26       | 72.84±7.40       |
| BioBGT                   | 69.96±1.18 | 69.70±2.90       | 67.04±3.41       |
| BQN (Ours)               | 79.85±1.27 | 72.53±1.41       | 73.26±5.99       |"
244,"| Method                                  | mIoUc | Full-view Accc | mAcc  | Partial-view |
| --------------------------------------- | ----- | -------------- | ----- | ------------ |
| mIoUc                                   | Acc   | mAcc           |       |              |
| TZSLPC (Cheraghian et al., 2020)        | 3.86  | -              | 10.37 | 4.14         |
| 3DGenZ (Michele et al., 2021)           | 6.46  | -              | 18.33 | 6.03         |
| ZSLPC (Cheraghian et al., 2019)         | 9.97  | -              | 18.70 | 9.52         |
| ShapeLLM* (Qi et al., 2024)             | 0.88  | 0.28           | 0.99  | 1.49         |
| OpenAD-PointNet++ (Nguyen et al., 2023) | 13.53 | 3.97           | 16.40 | 11.29        |
| OpenAD-DGCNN (Nguyen et al., 2023)      | 11.15 | 3.84           | 13.86 | 8.04         |
| IAGNet (Yang et al., 2023)              | 16.16 | 19.07          | 23.92 | 14.36        |
| LASO (Li et al., 2024)                  | 22.41 | 15.90          | 30.22 | 20.06        |
| Ours-Qwen                               | 24.43 | 23.90          | 35.45 | 26.25        |
| Ours-Phi                                | 30.43 | 29.36          | 47.78 | 27.25        |"
244,"|                | Openset-mIoU $ ^{c} $ | Closeset-mIoU $ ^{c} $ |
| -------------- | --------------------- | ---------------------- |
| DICE &amp; BCE | 30.43                 | 42.35                  |
| DICE           | 31.00                 | 38.65                  |
| BCE            | 15.99                 | 31.14                  |"
244,"| Model           | mIoUc | Accc  | mAccc |
| --------------- | ----- | ----- | ----- |
| 3D-ADLLM        | 30.43 | 29.36 | 47.78 |
| 3D-ADLLM w/o PC | 24.82 | 20.54 | 36.73 |
| 3D-ADLLM w/o UL | 25.35 | 26.84 | 40.69 |"
384,"| Main idea: | Quantify prediction uncertainty using calibration          |
| ---------- | ---------------------------------------------------------- |
| Input:     | ML model trained on problems of interest, error statistics |
| Output:    | Algorithm with strong expected performance                 |"
247,"| Name                                                            | Riemannian Metric  $ g_{P}(V,W) $                                                                             | Riemannian Logarithm  $ \log_{P}Q $                                                                                                     | Deformation  $ (\theta \neq 0) $                                              |
| --------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| $ (\alpha,\beta) $ -LEM
(Thanwerdas &amp; Pennec, 2023)         | $ \langle \text{mlog}_{*,P}(V), \text{mlog}_{*,P}(W) \rangle^{(\alpha,\beta)} $                               | $ (mlog_{*,P})^{-1} [\text{mlog}(Q) - \text{mlog}(P)] $                                                                                 | $ \frac{1}{\theta^{2}} \text{Pow}_{\theta}^{*} g^{(\alpha,\beta)-\text{LE}} $ |
| $ (\alpha,\beta) $ -AIM
(Thanwerdas &amp; Pennec, 2023)         | $ \langle P^{-1}V, WP^{-1} \rangle^{(\alpha,\beta)} $                                                         | $ P^{1/2} \text{mlog} \left( P^{-1/2}QP^{-1/2} \right) P^{1/2} $                                                                        | $ \frac{1}{\theta^{2}} \text{Pow}_{\theta}^{*} g^{(\alpha,\beta)-\text{AI}} $ |
| $ (\alpha,\beta) $ -EM
(Thanwerdas &amp; Pennec, 2023)          | $ \langle V, W \rangle^{(\alpha,\beta)} $                                                                     | Q - P                                                                                                                                   | $ \frac{1}{\theta^{2}} \text{Pow}_{\theta}^{*} g^{(\alpha,\beta)-\text{E}} $  |
| $ (\theta_{1},\theta_{2}) $ -EM
(Thanwerdas &amp; Pennec, 2022) | $ \frac{1}{\theta_{1}\theta_{2}} \langle \text{Pow}_{\theta_{1},P}(V), \text{Pow}_{\theta_{2},P}(W) \rangle $ | $ (Pow_{\theta,P})^{-1} (Q^{\theta} - P^{\theta}), \text{with} \theta = (\theta_{1} + \theta_{2})/2 $                                   | N/A                                                                           |
| LCM (Lin, 2019)                                                 | $ \sum_{i &gt; j} \tilde{V}_{ij} \tilde{W}_{ij} + \sum_{j=1}^{n} \tilde{V}_{jj} \tilde{W}_{jj} L_{jj}^{-2} $  | $ (Chol^{-1})_{*,L} \left[ \lfloor K \rfloor - \lfloor L \rfloor + \mathbb{D}(L) \text{Dlog}(\mathbb{D}(L)^{-1}\mathbb{D}(K)) \right] $ | $ \frac{1}{\theta^{2}} \text{Pow}_{\theta}^{*} g^{\text{LC}} $                |
| BWM (Bhatia et al., 2019)                                       | $ \frac{1}{2} \langle \mathcal{L}_{P}[V], W \rangle $                                                         | $ (PQ)^{1/2} + (QP)^{1/2} - 2P $                                                                                                        | $ \frac{1}{4\theta^{2}} \text{Pow}_{2\theta}^{*} g^{\text{BW}} $              |
| GBWM (Han et al., 2023)                                         | $ \frac{1}{2} \langle \mathcal{L}_{P,M}[V], W \rangle $                                                       | $ M \left( M^{-1}PM^{-1}Q \right)^{1/2} + \left( QM^{-1}PM^{-1} \right)^{1/2} M - 2P $                                                  | $ \frac{1}{4\theta^{2}} \text{Pow}_{2\theta}^{*} g^{M-\text{BW}} $            |"
247,"| Metric                                                  | Log_{I} P                                                                                                                   | Metric                                                                                                           | Log_{I} P                                  |
| ------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- | ------------------------------------------ |
| $ (\alpha,\beta) $ -LEM  $ (\theta,\alpha,\beta) $ -AIM | $ \text{mlog}(P) $                                                                                                          | $ (\theta,\alpha,\beta) $ -EM  $ (\theta_{1},\theta_{2}) $ -EM  $ 2\theta $ -BWM  $ (2\theta,P^{2\theta}) $ -BWM | $ \frac{1}{\theta_{0}}(P^{\theta_{0}}-I) $ |
| $ \theta $ -LCM                                         | $ \frac{1}{\theta}\left[\lfloor\tilde{L}\rfloor+\lfloor\tilde{L}\rfloor^{\top}+2\text{Dlog}(\mathbb{D}(\tilde{L}))\right] $ |                                                                                                                  |                                            |"
247,"| Method        | ImageNet-1k   | Cars          |
| ------------- | ------------- | ------------- |
| Top-1 Acc (%) | Top-5 Acc (%) | Top-1 Acc (%) |
| Pow-TMLR      | 71.62         | 89.73         |
| Pow-EMLR      | 73            | 90.91         |"
247,"|             | Log-EMLR                                                                                             | Pow-EMLR                                                                                       | ScalePow-EMLR                                                                                                  | Pow-TMLR                                                                                                                              | Cho-TMLR                                                                                      |
| ----------- | ---------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| Expression  | $ f_{\mathrm{s}}\left(\mathcal{F}\left(f_{\mathrm{vec}}\left(\mathrm{mlog}(S)\right)\right)\right) $ | $ f_{\mathrm{s}}\left(\mathcal{F}\left(f_{\mathrm{vec}}\left(S^{\theta}\right)\right)\right) $ | $ f_{\mathrm{s}}\left(\mathcal{F}\left(f_{\mathrm{vec}}\left(\frac{1}{\theta}S^{\theta}\right)\right)\right) $ | $ f_{\mathrm{s}}\left(\mathcal{F}\left(f_{\mathrm{vec}}\left(\frac{1}{\theta_{0}}\left(S^{\theta_{0}}-I\right)\right)\right)\right) $ | $ f_{\mathrm{s}}\left(\mathcal{F}\left(f_{\mathrm{vec}}\left(\tilde{V}\right)\right)\right) $ |
| Explanation | SPD MLR                                                                                              | SPD MLR                                                                                        | SPD MLR                                                                                                        | Tangent Classifier                                                                                                                    | Tangent Classifier                                                                            |
| Metrics     | LEM                                                                                                  | $ (\theta,1,0) $ -EM                                                                           | $ (\theta,1,0) $ -EM                                                                                           | $ (\theta,\alpha,\beta) $ -EM,  $ (\theta_{1},\theta_{2}) $ -EM,  $ 2\theta $ -BWM,  $ (2\theta,\phi_{2\theta}(S)) $ -BWM             | $ \theta $ -LCM                                                                               |
| Used in GCP | $ \checkmark $ (Eq. (4))                                                                             | $ \checkmark $                                                                                 | ✗                                                                                                              | ✗                                                                                                                                     | ✗                                                                                             |
| Reference   | (Chen et al., 2024a, Prop. 5.1)                                                                      | Thm. 2                                                                                         | Thm. 2                                                                                                         | Tab. 2                                                                                                                                | Tab. 2                                                                                        |"
247,"| Classifier    | ImageNet-1k   | Aircrafts     | Birds         | Cars          |
| ------------- | ------------- | ------------- | ------------- | ------------- |
| Top-1 Acc (%) | Top-5 Acc (%) | Top-1 Acc (%) | Top-5 Acc (%) | Top-1 Acc (%) |
| Cho-TMLR      | N/A           | N/A           | 78.97         | 91.81         |
| Pow-TMLR      | 71.62         | 89.73         | 69.58         | 88.68         |
| ScalePow-EMLR | 72.43         | 90.44         | 71.05         | 89.86         |
| Pow-EMLR      | 73            | 90.91         | 72.07         | 89.83         |"
247,"| Classifier         | Aircrafts     | Cars          |
| ------------------ | ------------- | ------------- |
| Top-1 Acc (%)      | Top-5 Acc (%) | Top-1 Acc (%) |
| Pow-TMLR-0.25      | 65.41         | 86.71         |
| ScalePow-EMLR-0.25 | $ 72.76 $     | $ 90.31 $     |
| Pow-EMLR-0.25      | 71.47         | 90.04         |
| Pow-TMLR-0.5       | 67.9          | 88.75         |
| ScalePow-EMLR-0.5  | 74.29         | 91.12         |
| Pow-EMLR-0.5       | 74.17         | 91.21         |
| Pow-TMLR-0.7       | 65.92         | 87.49         |
| ScalePow-EMLR-0.7  | $ 74.26 $     | $ 91.15 $     |
| Pow-EMLR-0.7       | 74.17         | 90.49         |"
247,"| Dataset       | Result        | Pow-TMLR | Pow-EMLR |
| ------------- | ------------- | -------- | -------- |
| Aircrafts     | Top-1 Acc (%) | 38.01    | 65.02    |
| Top-5 Acc (%) | 74.4          | 87.79    |          |
| Cars          | Top-1 Acc (%) | 28.57    | 59.13    |
| Top-5 Acc (%) | 59.51         | 82.04    |          |"
247,"| Classifier    | Top-1 Acc (%) | Top-5 Acc (%) |
| ------------- | ------------- | ------------- |
| Pow-TMLR      | 75.79         | 92.91         |
| ScalePow-EMLR | $ 76.14 $     | $ 93.18 $     |
| Pow-EMLR      | 76.11         | 93.05         |"
249,"| Models            | Methods | CNN/DM                       | GSM8K      | TinyStories                  | Speed (tokens/s) | Overall Speedup              |
| ----------------- | ------- | ---------------------------- | ---------- | ---------------------------- | ---------------- | ---------------------------- |
| $ M $             | Speedup | $ M $                        | Speedup    | $ M $                        | Speedup          |                              |
| LLaMA-2-13B       | Vanilla | 1.00                         | 1.00\times | 1.00                         | 1.00\times       | 1.00                         |
| PARALLEL          | 1.04    | 0.95\times                   | 1.11       | 0.99\times                   | 1.06             | 0.97\times                   |
| Lookahead         | 1.38    | 1.16\times                   | 1.50       | 1.29\times                   | 1.62             | 1.37\times                   |
| SWIFT             | 4.34    | 1.37\times $ \dagger $       | 3.13       | 1.31\times $ \dagger $       | 8.21             | 1.53\times $ \dagger $       |
| LLaMA-2-13B -Chat | Vanilla | 1.00                         | 1.00\times | 1.00                         | 1.00\times       | 1.00                         |
| PARALLEL          | 1.06    | 0.96\times                   | 1.08       | 0.97\times                   | 1.10             | 0.98\times                   |
| Lookahead         | 1.35    | 1.15\times                   | 1.57       | 1.31\times                   | 1.66             | 1.40\times                   |
| SWIFT             | 3.54    | 1.28\times                   | 2.95       | 1.25\times                   | 7.42             | 1.50\times $ \dagger $       |
| LLaMA-2-70B       | Vanilla | 1.00                         | 1.00\times | 1.00                         | 1.00\times       | 1.00                         |
| PARALLEL          | 1.05    | 0.95\times                   | 1.07       | 0.97\times                   | 1.05             | 0.96\times                   |
| Lookahead         | 1.36    | 1.15\times                   | 1.54       | 1.30\times                   | 1.59             | 1.35\times                   |
| SWIFT             | 3.85    | 1.43 $ \times $  $ \dagger $ | 2.99       | 1.39 $ \times $  $ \dagger $ | 6.17             | 1.62 $ \times $  $ \dagger $ |"
249,"| Datasets            | Methods    | CodeLLaMA-13B | CodeLLaMA-34B |
| ------------------- | ---------- | ------------- | ------------- |
| $ M $               | $ \alpha $ | Acc.          | Speedup       |
| HumanEval (pass@1)  | Vanilla    | 1.00          | -             |
| SWIFT               | 4.75       | 0.98          | 0.311         |
| HumanEval (pass@10) | Vanilla    | 1.00          | -             |
| SWIFT               | 3.55       | 0.93          | 0.628         |"
250,"| Method      | Venue   | Backbone | ZSL  | GZSL |
| ----------- | ------- | -------- | ---- | ---- |
| AWA2        | CUB     | SUN      | AWA2 | CUB  |
| T1          | T1      | T1       | U    | S    |
| CLIP*       | ICML21  | ViT      | -    | -    |
| CoOp*       | IJCV22  | ViT      | -    | -    |
| ICIS        | ICCV23  | Res101   | 64.6 | 60.6 |
| ReZSL       | TIP23   | Res101   | 70.9 | 80.9 |
| PSVMA       | CVPR23  | ViT      | -    | -    |
| ZSLViT      | CVPR24  | ViT      | 70.7 | 78.9 |
| f-CLSWGAN†  | CVPR18  | Res101   | 75.9 | 84.5 |
| f-VAEGAN†   | CVPR19  | Res101   | 75.8 | 85.1 |
| TFVAEGAN†   | ECCV20  | Res101   | 72.4 | 85.8 |
| SDVAE†      | ICCV21  | Res101   | 69.3 | 85.1 |
| CEGAN†      | CVPR21  | Res101   | 72.8 | 84.6 |
| DFCAflow†   | TCSVT23 | Res101   | 74.4 | 83.9 |
| CoOp+SHIP*‡ | ICCV23  | ViT      | -    | -    |
| DSP‡        | ICML23  | Res101   | -    | -    |
| DML         | ACCV24  | Res101   | -    | -    |
| VADS‡       | CVPR24  | ViT      | 82.5 | 86.8 |
| ZeroDiff†   | Ours    | Res101   | 87.3 | 87.5 |"
307,"| Model     | Situational analysis | Project-based learning | Multi-step reasoning | AVG   |
| --------- | -------------------- | ---------------------- | -------------------- | ----- |
| MiniGPT-5 | 47.63                | 55.12                  | 42.17                | 50.92 |
| EMU-2     | 39.65                | 46.12                  | 50.75                | 45.33 |
| GILL      | 46.72                | 57.57                  | 39.33                | 51.58 |
| Anole     | 48.95                | 59.05                  | 51.72                | 55.22 |"
250,"| Method                         | AWA2                     | CUB                      | SUN                      |
| ------------------------------ | ------------------------ | ------------------------ | ------------------------ |
| $ 30\%\mathcal{D}^{tr} $       | $ 10\%\mathcal{D}^{tr} $ | $ 30\%\mathcal{D}^{tr} $ | $ 10\%\mathcal{D}^{tr} $ |
| T1                             | H                        | T1                       | H                        |
| f-CLSWGAN (Xian et al., 2018b) | 68.9                     | 57.8                     | 54.0                     |
| f-VAEGAN (Xian et al., 2019)   | 81.2                     | 64.9                     | 73.1                     |
| CEGAN (Han et al., 2021)       | 72.2                     | 70.4                     | 69.0                     |
| DFCAFlow (Su et al., 2023)     | 74.5                     | 72.6                     | 77.9                     |
| ZeroDiff (Ours)                | 84.9                     | 80.2                     | 83.3                     |"
252,"| Family                 | Method                    | Meta-Llama-3-8B      | Meta-Llama-3-70B       |
| ---------------------- | ------------------------- | -------------------- | ---------------------- |
| Wiki( $ \downarrow $ ) | Reasoning( $ \uparrow $ ) | MMLU( $ \uparrow $ ) | Wiki( $ \downarrow $ ) |
| Llama 3                | 16-bit                    | 6.1                  | 67.1                   |
| QUIK                   | 14.2                      | 51.6                 | 32.7                   |
| QuaRot                 | 7.8                       | 62.1                 | 53.2                   |
| SpinQuant              | 7.4                       | 63.8                 | 56.2                   |
| ResQ                   | 7.1                       | 63.9                 | 57.2                   |
| Family                 | Method                    | Llama-3.2-1B         | Llama-3.2-3B           |
| Wiki( $ \downarrow $ ) | Reasoning( $ \uparrow $ ) | MMLU( $ \uparrow $ ) | Wiki( $ \downarrow $ ) |
| Llama 3.2              | 16-bit                    | 9.8                  | 54.9                   |
| QUIK                   | 21.8                      | 44.3                 | 25.1                   |
| QuaRot                 | 14.3                      | 49.0                 | 25.5                   |
| SpinQuant              | 13.6                      | 48.8                 | 25.6                   |
| ResQ                   | 12.4                      | 50.1                 | 29.4                   |
| Family                 | Method                    | Qwen2.5-3B           | Qwen2.5-70B            |
| Wiki( $ \downarrow $ ) | Reasoning( $ \uparrow $ ) | MMLU( $ \uparrow $ ) | Wiki( $ \downarrow $ ) |
| Qwen 2.5               | 16-bit                    | 8.0                  | 63.8                   |
| QUIK                   | 15.5                      | 51.2                 | 39.4                   |
| QuaRot                 | 68.8                      | 47.7                 | 28.9                   |
| ResQ                   | 9.0                       | 61.1                 | 61.2                   |"
252,"| Method                 | Qwen2.5-1.5B              | Qwen2.5-3B           |
| ---------------------- | ------------------------- | -------------------- |
| Wiki( $ \downarrow $ ) | Reasoning( $ \uparrow $ ) | MMLU( $ \uparrow $ ) |
| QuaRot                 | 6599.9                    | 38.3                 |
| SpinQuant              | 5938.3                    | 38.1                 |
| ResQ                   | 187.3                     | 46.6                 |
| Method                 | Qwen2.5-7B                | Qwen2.5-14B          |
| Wiki( $ \downarrow $ ) | Reasoning( $ \uparrow $ ) | MMLU( $ \uparrow $ ) |
| QuaRot                 | 4035.9                    | 38.4                 |
| SpinQuant              | 3395.4                    | 38.6                 |
| ResQ                   | 34.2                      | 56.2                 |
| Method                 | Qwen2.5-32B               | Qwen2.5-72B          |
| Wiki( $ \downarrow $ ) | Reasoning( $ \uparrow $ ) | MMLU( $ \uparrow $ ) |
| QuaRot                 | 6.1                       | 67.8                 |
| SpinQuant              | 6.0                       | 67.9                 |
| ResQ                   | 5.9                       | 69.1                 |"
252,"| Model            | Method       | GSM8K 5-shot( $ \uparrow $ ) | LongBench( $ \uparrow $ ) |
| ---------------- | ------------ | ---------------------------- | ------------------------- |
| flexible extract | strict match | qmsum                        | samsum                    |
| Meta-Llama-3-8B  | 16-bit       | 51.0                         | 50.6                      |
| QUIK             | 2.3          | 0.0                          | 10.5                      |
| QuaRot           | 27.6         | 27.1                         | 22.0                      |
| SpinQuant        | 29.8         | 29.6                         | 23.0                      |
| ResQ             | 33.6         | 33.2                         | 23.1                      |
| Llama-3.2-3B     | 16-bit       | 25.1                         | 24.9                      |
| QUIK             | 2.5          | 0.0                          | 15.9                      |
| QuaRot           | 10.1         | 9.1                          | 20.6                      |
| SpinQuant        | 11.6         | 11.4                         | 21.7                      |
| ResQ             | 17.1         | 16.7                         | 21.7                      |"
252,"| W/A/KV (bit)         | Method               | Model |
| -------------------- | -------------------- | ----- |
| Qwen2-VL-2B-Instruct | Qwen2-VL-7B-Instruct |       |
| 16/16/16             | Baseline             | 39.6  |
| 4/4/4                | RTN                  | 25.0  |
| GPTQ                 | 27.7                 | 24.9  |
| 4.5/4.5/4.5          | QuaRot               | 24.0  |
| QUIK                 | 26.3                 | 28.9  |
| ResQ                 | 29.7                 | 47.0  |
| 4/8/4                | RTN                  | 24.9  |
| GPTQ                 | 23.4                 | 24.3  |
| 4.5/8/4.5            | QuaRot               | 26.5  |
| QUIK                 | 28.4                 | 26.4  |
| ResQ                 | 34.0                 | 48.8  |"
252,"|              | ResQ      | Removed Projections |
| ------------ | --------- | ------------------- |
| $ U_{D} $    | $ U_{A} $ | $ U_{B} $           |
| Llama-2-7B   | 5.8       | 1550                |
| Llama-3-8B   | 7.1       | 1607                |
| Llama-3.2-3B | 8.8       | 279.2               |"
252,"| Model       | Seq_len | FP16 | INT4 | ResQ | Improv. over FP16 |
| ----------- | ------- | ---- | ---- | ---- | ----------------- |
| Llama-3-8B  | 8192    | 21.9 | 11.4 | 11.9 | 1.84              |
| 512         | 15.4    | 5.6  | 6.1  | 2.54 |                   |
| Llama-2-7B  | 8192    | 18.1 | 6.2  | 6.8  | 2.95              |
| 512         | 12.9    | 3.7  | 4.2  | 3.08 |                   |
| Qwen2.5-14B | 8192    | OOM  | 19.5 | 21.3 | -                 |
| 512         | OOM     | 12.6 | 13.5 | -    |                   |"
252,"| Batch size | Seq_len | FP16  | ResQ | Improv. over FP16 |
| ---------- | ------- | ----- | ---- | ----------------- |
| 3          | 10240   | 20783 | 4242 | 4.90x             |
| 3          | 8192    | 16373 | 3361 | 4.87x             |
| 3          | 4096    | 7871  | 1609 | 4.89x             |
| 3          | 2048    | 3999  | 806  | 4.82x             |
| 6          | 2048    | 7733  | 1560 | 4.96x             |
| 9          | 2048    | 11493 | 2309 | 4.98x             |"
253,"| Model          | # Param | VQAv2 | OK-VQA | GQA  | VizWizQA | TextVQA |
| -------------- | ------- | ----- | ------ | ---- | -------- | ------- |
| CogVLM         | 17B     | 82.3  | 64.8   | -    | -        | 70.4    |
| EVLM-Chat      | 32B     | 81.9  | -      | 64.4 | 47.3     | 67.5    |
| Flamingo       | 80B     | 81.3  | 50.6   | -    | 57.2     | 54.7    |
| 8B-level MLLMs |         |       |        |      |          |         |
| Qwen-VL-Chat   | 9B      | 78.2  | 56.6   | 57.5 | 38.9     | 63.8    |
| Idefics1       | 9B      | 68.8  | 50.4   | -    | -        | 39.3    |
| Flamingo       | 9B      | 51.8  | 44.7   | -    | -        | 46.3    |
| InstructBLIP   | 7B      | 75.2  | 45.2   | 49.2 | 34.5     | 33.6    |
| mPLUG-Owl2     | 8B      | 79.4  | 57.7   | 56.1 | 54.5     | 58.2    |
| LLAVA-1.5      | 8B      | 78.5  | -      | 62.0 | 50.0     | 58.2    |
| LLAVA-Next     | 8B      | 81.8  | -      | 64.2 | 57.6     | 64.9    |
| VILA-1.5       | 8B      | 80.9  | -      | 61.9 | 58.7     | 66.3    |
| Idefics2       | 8B      | 80.8  | 53.5   | -    | -        | 70.4    |
| Mantis-SigLIP  | 8B      | 74.9  | 55.4   | -    | -        | 59.2    |
| mPLUG-Owl3     | 8B      | 82.1  | 60.1   | 65.0 | 63.5     | 69.0    |"
253,"| Model            | # Param | NextQA | MVBench | VideoMME w/o sub | LongVideoBench-val |
| ---------------- | ------- | ------ | ------- | ---------------- | ------------------ |
| VideoChat2       | 8B      | 68.6   | 51.9    | 43.8             | 36.0               |
| Video-LLaMA2     | 8B      | -      | 54.6    | 47.9             | -                  |
| Video-ChatGPT    | 8B      | -      | 32.7    | -                | -                  |
| ShareGPT4Video   | 8B      | -      | -       | 39.9             | 39.7               |
| PLLaVA           | 8B      | -      | 46.6    | -                | 40.2               |
| Idefics2         | 8B      | -      | 29.7    | -                | 49.7               |
| Mantis-SigLIP    | 8B      | -      | 50.2    | -                | 47.0               |
| LLAVA-Interleave | 8B      | 78.2   | 53.1    | -                | -                  |
| mPLUG-Owl3       | 8B      | 78.6   | 54.5    | 53.5             | 52.1               |"
253,"| Model               | # Param | MMB-EN | MMB-CN | MM-Vet | POPE | AI2D |
| ------------------- | ------- | ------ | ------ | ------ | ---- | ---- |
| CogVLM              | 17B     | 65.8   | 69.8   | 52.8   | 88.0 | 63.3 |
| EVLM-Chat           | 32B     | 76.9   | 76.9   | -      | 89.7 | 76.0 |
| InstructBLIP        | 13B     | 38.3   | -      |        | 81.5 | -    |
| 8B-level MLLMs      |         |        |        |        |      |      |
| LLAVA-1.5           | 8B      | 64.3   | 58.3   | 31.1   | 85.9 | 55.5 |
| OpenFlamingo        | 9B      | 32.4   | 14.4   | 24.8   | -    | 31.7 |
| mPLUG-Owl2          | 8B      | 64.5   | -      | 36.2   | -    | 55.7 |
| LLAVA-Next          | 8B      | 67.4   | 60.6   | 43.9   | 86.5 | 66.6 |
| LLAVA-Interleave    | 8B      | -      | -      | -      | 86.8 | 73.9 |
| VILA1.5             | 8B      | 72.3   | 66.2   | 38.3   | 84.4 | -    |
| Idefics2            | 8B      | 75.7   | 68.6   | 34.0   | 86.2 | 72.3 |
| Cambrian            | 8B      | 74.6   | 67.9   | -      | -    | 74.6 |
| MiniCPM-Llama3-V2.5 | 8B      | 77.6   | 73.8   | -      | -    | 78.4 |
| Mantis-SigLIP       | 8B      | 68.7   | -      | -      | -    | -    |
| mPLUG-Owl3          | 8B      | 77.6   | 74.3   | 40.1   | 88.2 | 73.4 |"
253,"| Model            | # Param | NLVR2 | Mantis-Eval | MathVerse-mv | SciVerse-mv | BLINK | Q-Bench2 |
| ---------------- | ------- | ----- | ----------- | ------------ | ----------- | ----- | -------- |
| Qwen-VL-Chat     | 8B      | 58.7  | 39.2        | -            | -           | 31.2  | 45.9     |
| InstructBLIP     | 8B      | 60.3  | 45.6        | -            | -           | 42.2  | 44.3     |
| CogVLM           | 17B     | 58.6  | 45.2        | -            | -           | 41.5  | 53.2     |
| VideoLLaVA       | 8B      | 56.5  | 35.9        | -            | -           | 38.9  | 45.7     |
| VILA             | 8B      | 76.5  | 51.2        | -            | -           | 39.3  | 45.7     |
| Idefics2         | 8B      | 86.9  | 48.9        | -            | -           | 45.2  | 57.0     |
| Mantis-SigLIP    | 8B      | 87.4  | 59.5        | -            | -           | 46.4  | 69.9     |
| LLAVA-Interleave | 8B      | 88.8  | 62.7        | 32.8         | 31.6        | 52.6  | 74.2     |
| mPLUG-Owl3       | 8B      | 90.8  | 63.1        | 65.0         | 86.2        | 50.3  | 74.0     |"
253,"| Adaptive Gating | Shared LayerNorm | MI-Rope | GQA      | TextVQA  | MvBench  | VideoMME | NLVR2    | Mantis   |
| --------------- | ---------------- | ------- | -------- | -------- | -------- | -------- | -------- | -------- |
|                 |                  |         | 53.3     | 44.6     | 40.2     | 38.1     | 52.7     | 41.9     |
| ✓               |                  |         | 55.7     | 49.3     | $ 43.2 $ | $ 40.1 $ | 53.4     | 47.9     |
| ✓               | ✓                |         | $ 58.1 $ | 49.7     | 42.8     | 38.4     | 54.9     | 46.1     |
| ✓               | ✓                | ✓       | 57.6     | $ 50.0 $ | 42.8     | 39.4     | $ 59.5 $ | $ 51.6 $ |"
253,"| Attention Structure  | GQA  | TextVQA | MvBench | VideoMME | NLVR2 | Mantis-Eval |
| -------------------- | ---- | ------- | ------- | -------- | ----- | ----------- |
| Concatenate          | 59.0 | 51.6    | 22.4    | 25.1     | 55.7  | 38.7        |
| Pre-Cross-Attention  | 53.8 | 45.2    | 43.0    | 38.9     | 55.3  | 44.7        |
| Post-Cross-Attention | 48.9 | 40.9    | 38.3    | 37.0     | 54.0  | 47.0        |
| Hyper Attention      | 57.6 | 50.0    | 42.8    | 39.4     | 59.5  | 51.6        |"
255,"| Methods         | SPRING              | CHARGED         | PENDULUM            |
| --------------- | ------------------- | --------------- | ------------------- |
| In-Distribution | Out-of-Distribution | In-Distribution | Out-of-Distribution |
| RMSE            | MAPE                | RMSE            | MAPE                |
| VAR             | 9.881↑3.210         | 24.55↑18.01     | 13.11↑5.729         |
| LSTM            | 7.992↑1.321         | 20.31↑4.232     | 10.22↑2.839         |
| LatentODE       | 6.671               | 6.544           | 7.381               |
| HODEN           | 5.029↓1.642         | 8.327↑1.783     | 8.139↑0.758         |
| TRS-ODEN        | 5.456↓1.215         | 4.801↓1.743     | 7.120↓0.261         |
| LG-ODE          | 4.307↓2.364         | 2.918↓3.626     | 4.346↑3.035         |
| SocialODE       | 4.452↓2.219         | 3.520↓3.024     | 4.902↓2.479         |
| PGODE           | 3.901↓2.770         | 2.562↓3.982     | 4.021↓3.360         |
| TREAT           | 3.892↓2.779         | 2.748↓3.796     | 4.213↓1.368         |
| GREAT           | 3.687↓2.984         | 2.348↓4.196     | 3.619↓3.762         |"
256,"| Method           | CIFAR-10         | CIFAR-20           | STL-10           | ImageNet-10      | ImageNet-Dogs      | Tiny-ImageNet    |
| ---------------- | ---------------- | ------------------ | ---------------- | ---------------- | ------------------ | ---------------- |
| ACC $ \uparrow $ | ARI $ \uparrow $ | ECE $ \downarrow $ | ACC $ \uparrow $ | ARI $ \uparrow $ | ECE $ \downarrow $ | ACC $ \uparrow $ |
| K-means          | 22.9             | 4.9                | N/A              | 13.0             | 2.8                | N/A              |
| MoCo-v2          | 82.9             | 64.9               | N/A              | 50.7             | 26.2               | N/A              |
| Simsiam          | 70.7             | 53.1               | N/A              | 33.0             | 16.2               | N/A              |
| BYOL             | 57.0             | 47.6               | N/A              | 34.7             | 21.2               | N/A              |
| DMICC            | 82.8             | 69.0               | N/A              | 46.8             | 29.1               | N/A              |
| ProPos           | 94.3             | 88.4               | N/A              | 61.4             | 45.1               | N/A              |
| CoNR             | 93.2             | 86.1               | N/A              | 60.4             | 44.3               | N/A              |
| DivClust         | 81.9             | 68.1               | -                | 43.7             | 28.3               | -                |
| CC               | 85.2             | 72.8               | 6.2              | 42.4             | 28.4               | 29.7             |
| TCC              | 90.6             | 73.3               | -                | 49.1             | 31.2               | -                |
| TCL              | 88.7             | 78.0               | -                | 53.1             | 35.7               | -                |
| SeCu-Size        | 90.0             | 81.5               | 8.1              | 52.9             | 38.4               | 13.1             |
| SeCu             | 92.6             | 85.4               | 4.9              | 52.7             | 39.7               | 41.8             |
| SCAN-2           | 84.1             | 74.1               | 10.9             | 50.0             | 34.7               | 37.1             |
| SCAN-3           | 90.3             | 80.8               | 6.7              | 51.2             | 35.6               | 39.0             |
| SPICE-2          | 84.4             | 70.9               | 15.4             | 47.6             | 30.3               | 52.3             |
| SPICE-3          | 91.5             | 83.4               | 7.8              | 58.4             | 42.2               | 40.6             |
| CDC-Clu (Ours)   | 94.9             | 89.4               | 1.4              | 61.9             | 46.7               | 28.0             |
| CDC-Cal (Ours)   | 94.9             | 89.5               | 1.1              | 61.7             | 46.6               | 4.9              |
| Supervised       | 89.7             | 78.9               | 4.0              | 71.7             | 50.2               | 11.0             |
| +MoCo-v2         | 94.1             | 87.5               | 2.4              | 83.2             | 68.4               | 6.7              |"
260,"|             | To Similar Architecture | To Different Architecture |
| ----------- | ----------------------- | ------------------------- |
| Teacher     | resnet32x4              | WRN_40_2                  |
| Student     | resnet8x4               | WRN_40_1                  |
| Teacher     | 79.42                   | 75.61                     |
| Student     | 72.50                   | 71.98                     |
| KD          | 73.33                   | 73.69                     |
| + TeKAP (L) | 74.79                   | 73.80                     |
| CRD         | 75.51                   | 74.14                     |
| + TeKAP (F) | 75.65                   | 74.21                     |
| TeKAP (F+L) | 75.98                   | 74.41                     |"
257,"| Method                        | B@4 ( $ \uparrow $ ) | METEOR( $ \uparrow $ ) | CIDEr ( $ \uparrow $ ) | SPICE ( $ \uparrow $ ) | ROUGE-L ( $ \uparrow $ ) | WMD ( $ \uparrow $ ) |
| ----------------------------- | -------------------- | ---------------------- | ---------------------- | ---------------------- | ------------------------ | -------------------- |
| ClipCap (Mokady et al., 2021) | 32.15                | 27.10                  | 108.35                 | 20.12                  | -                        | -                    |
| ClipCap + GMAIL (ours)        | 38.12                | 31.67                  | 119.53                 | 23.75                  | 56.27                    | 62.16                |
| IFCap (Lee et al., 2024)      | 33.25                | 28.60                  | 115.27                 | 21.58                  | 51.35                    | 56.72                |
| IFCap + GAMIL (ours)          | 39.32                | 32.07                  | 127.86                 | 23.98                  | 59.83                    | 63.51                |
| LLaVA (Liu et al., 2023)      | 39.67                | 32.38                  | 134.29                 | 24.17                  | 61.36                    | 65.78                |
| LLaVA + GMAIL (ours)          | 43.26                | 34.89                  | 146.38                 | 27.23                  | 65.25                    | 71.39                |
| Llama3 (Meta, 2024)           | 47.36                | 35.21                  | 158.13                 | 28.35                  | 68.32                    | 75.13                |
| Llama3 + GMAIL (ours)         | 50.21                | 38.59                  | 168.53                 | 32.58                  | 73.29                    | 80.25                |"
257,"| Method                      | DTD   | Stanford Cars | SUN397 | Food 101 | Aircraft | Oxford Pets | Caltech 101 | ImageNet   |
| --------------------------- | ----- | ------------- | ------ | -------- | -------- | ----------- | ----------- | ---------- |
| CLIP (Radford et al., 2021) | 55.20 | 77.53         | 69.31  | 93.08    | 32.88    | 93.33       | 93.24       | 75.54      |
| CLIP + GMAIL (ours)         | 65.26 | 81.32         | 75.53  | 95.21    | 37.85    | 95.23       | 95.57       | 77.68      |
| SynCLR (Tian et al., 2024)  | 79.90 | 93.80         | 76.20  | 91.60    | 81.70    | 93.60       | 95.30       | 85.80 (ft) |
| SynCLR + GMAIL (ours)       | 83.67 | 96.56         | 81.25  | 96.38    | 86.75    | 95.70       | 98.35       | 87.95 (ft) |"
257,"| Method               | Accuracy (%) |           | Method               | Accuracy (%) |      |
| -------------------- | ------------ | --------- | -------------------- | ------------ | ---- |
| LLaVA                | 85.2         |           | LLaVA                | 44.7         |      |
| LLaVA + GMAIL (ours) | 87.6         | ScienceQA | LLaVA + GMAIL (ours) | 48.3         | MMMU |"
257,"| Train Data           | Image-to-Text        | Text-to-Image         |
| -------------------- | -------------------- | --------------------- |
| R@1 ( $ \uparrow $ ) | R@5 ( $ \uparrow $ ) | R@10 ( $ \uparrow $ ) |
| COCO                 | 47.1                 | 71.2                  |
| CC3M                 | 48.6                 | 73.6                  |
| CC12M                | 50.9                 | 75.3                  |"
257,"| Alignment | B@4 ( $ \uparrow $ ) | METEOR( $ \uparrow $ ) | CIDEr ( $ \uparrow $ ) | SPICE ( $ \uparrow $ ) | ROUGE-L ( $ \uparrow $ ) | WMD ( $ \uparrow $ ) |
| --------- | -------------------- | ---------------------- | ---------------------- | ---------------------- | ------------------------ | -------------------- |
| ✗         | 36.15                | 30.32                  | 115.35                 | 22.95                  | 55.12                    | 61.08                |
| ✓         | 38.12                | 31.67                  | 119.53                 | 23.75                  | 56.27                    | 62.16                |"
259,"| State 3: Uncertain Category/Attribute | Action 3: Object crop          |
| ------------------------------------- | ------------------------------ |
| State 4: Uncertain Relation/Accessory | Action 4: Extended object crop |
| State 5: Uncertain Location/Behavior  | Action 5: Object Highlight     |"
259,"| Test subset                      | Training data type | #Img | AP-des | AP-des-pos | AP-des-S | AP-des-M | AP-des-L |
| -------------------------------- | ------------------ | ---- | ------ | ---------- | -------- | -------- | -------- |
| COCO                             | raw expr (A)       | 933k | 21.2   | 59.4       | 31.3     | 21.1     | 18.6     |
| raw expr w.filter (B)            | 695k               | 22.2 | 59.4   | 32.4       | 21.9     | 19.4     |          |
| raw expr w.filter + Real-LOD (C) | 863k               | 24.2 | 59.6   | 35.2       | 24.2     | 21.1     |          |
| O365                             | raw expr (A)       | 933k | 27.6   | 43.1       | 39.8     | 25.5     | 17.9     |
| raw expr w.filter (B)            | 695k               | 28.5 | 43.7   | 40.9       | 26.2     | 18.5     |          |
| raw expr w.filter + Real-LOD (C) | 863k               | 32.4 | 48.5   | 47.5       | 30.0     | 21.3     |          |
| OI                               | raw expr (A)       | 933k | 30.5   | 43.0       | 37.2     | 30.3     | 23.2     |
| raw expr w.filter (B)            | 695k               | 31.4 | 43.5   | 38.1       | 31.2     | 24.0     |          |
| raw expr w.filter + Real-LOD (C) | 863k               | 33.5 | 44.9   | 42.2       | 32.9     | 24.8     |          |"
259,"| Subset                       | LOD method                  | Backbone                | Source              | #Img | AP-des | AP-des-pos | AP-des-S | AP-des-M | AP-des-L |
| ---------------------------- | --------------------------- | ----------------------- | ------------------- | ---- | ------ | ---------- | -------- | -------- | -------- |
| COCO                         | MDETR (Kamath et al., 2021) | ENB3                    | COCO, VG, Flickr30K | 0.3M | 13.2   | 31.6       | 15.4     | 13.5     | 12.4     |
| GLIP (Li et al., 2022)       | Swin-L                      | O365, OI, RefC/g/+, etc | 17.5M               | 13.9 | 36.8   | 28.9       | 12.9     | 11.5     |          |
| mm-GDINO (Zhao et al., 2024) | Swin-B                      | GoldG, O365, COCO, etc  | 12M                 | 15.2 | 47.0   | 29.3       | 14.9     | 15.1     |          |
| FIBER (Dou et al., 2022)     | Swin-B                      | COCO, CC3M, SBU, etc    | 4M                  | 14.3 | 38.8   | 31.3       | 12.7     | 16.1     |          |
| Real-Model                   | Swin-B                      | Real-Data               | 0.18M               | 26.2 | 59.7   | 39.4       | 25.4     | 24.3     |          |
| O365                         | MDETR (Kamath et al., 2021) | ENB3                    | COCO, VG, Flickr30K | 0.3M | 3.2    | 5.9        | 3.0      | 3.2      | 2.7      |
| GLIP (Li et al., 2022)       | Swin-L                      | O365, OI, RefC/g/+, etc | 17.5M               | 24.0 | 35.2   | 44.5       | 20.5     | 11.8     |          |
| mm-GDINO (Zhao et al., 2024) | Swin-B                      | GoldG, O365, COCO, etc  | 12M                 | 19.6 | 31.0   | 32.3       | 17.8     | 12.4     |          |
| FIBER (Dou et al., 2022)     | Swin-B                      | COCO, CC3M, SBU, etc    | 4M                  | 25.9 | 38.2   | 44.7       | 22.5     | 14.1     |          |
| Real-Model                   | Swin-B                      | Real-Data               | 0.18M               | 36.0 | 52.1   | 55.7       | 32.3     | 23.7     |          |
| OI                           | MDETR (Kamath et al., 2021) | ENB3                    | COCO, VG, Flickr30K | 0.3M | 6.1    | 10.6       | 9.6      | 5.7      | 4.1      |
| GLIP (Li et al., 2022)       | Swin-L                      | O365, OI, RefC/g/+, etc | 17.5M               | 20.1 | 31.2   | 33.3       | 18.7     | 10.3     |          |
| mm-GDINO (Zhao et al., 2024) | Swin-B                      | GoldG, O365, COCO, etc  | 12M                 | 23.2 | 34.5   | 32.3       | 23.8     | 16.9     |          |
| FIBER (Dou et al., 2022)     | Swin-B                      | COCO, CC3M, SBU, etc    | 4M                  | 20.1 | 30.9   | 34.1       | 18.5     | 10.5     |          |
| Real-Model                   | Swin-B                      | Real-Data               | 0.18M               | 40.5 | 51.4   | 54.9       | 37.8     | 30.6     |          |
| ALL                          | MDETR (Kamath et al., 2021) | ENB3                    | COCO, VG, Flickr30K | 0.3M | 4.7    | 9.1        | 6.4      | 4.6      | 4.0      |
| GLIP (Li et al., 2022)       | Swin-L                      | O365, OI, RefC/g/+, etc | 17.5M               | 21.2 | 33.2   | 37.7       | 18.9     | 10.8     |          |
| mm-GDINO (Zhao et al., 2024) | Swin-B                      | GoldG, O365, COCO, etc  | 12M                 | 20.8 | 33.1   | 31.9       | 19.8     | 14.1     |          |
| FIBER (Dou et al., 2022)     | Swin-B                      | COCO, CC3M, SBU, etc    | 4M                  | 22.3 | 34.8   | 38.6       | 19.5     | 12.4     |          |
| Real-Model                   | Swin-B                      | Real-Data               | 0.18M               | 36.5 | 52.1   | 54.4       | 33.2     | 25.5     |          |"
259,"| LOD method                     | Backbone | Source                    | #Img  | Full | Presence | Absence |
| ------------------------------ | -------- | ------------------------- | ----- | ---- | -------- | ------- |
| OWL-V2 (Minderer et al., 2023) | ViT-L    | WebLI                     | 10B   | 9.6  | 10.7     | 6.4     |
| UNINEXT (Yan et al., 2023)     | ViT-H    | O365, RefC/g/+            | 0.7M  | 20.0 | 20.6     | 18.1    |
| GDINO (Liu et al., 2024b)      | Swin-B   | CC4M, O365, RefC/g/+, etc | 5.8M  | 20.1 | 20.7     | 22.5    |
| mm-GDINO (Zhao et al., 2024)   | Swin-B   | GoldG, O365, COCO, etc    | 12M   | 24.2 | 23.9     | 25.9    |
| OFA-DOD (Xie et al., 2023)     | RN101    | CC12M, SBU, VG, etc       | 16M   | 21.6 | 23.7     | 15.4    |
| APE-B (Shen et al., 2024)      | ViT-L    | LVIS, O365, RefC/g/+, etc | 2.6M  | 30.0 | 29.9     | 30.3    |
| Real-Model                     | Swin-B   | Real-Data                 | 0.18M | 34.1 | 34.4     | 33.2    |"
260,"| Baselines       | Teacher Student | resnet32x4 | WRN_40_2 |
| --------------- | --------------- | ---------- | -------- |
| resnet8x4       | WRN_40_1        |            |          |
| Teacher Student | 79.42           | 75.61      |          |
| 72.50           | 71.98           |            |          |
| Single Teacher  | DKD             | 76.32      | 74.81    |
| + TeKAP         | 76.59           | 75.33      |          |
| MLKD            | 77.08           | 75.35      |          |
| + TeKAP         | 77.36           | 75.67      |          |
| Multi-Teacher   | TAKD            | 73.93      | 73.83    |
| + TeKAP         | 74.81           | 74.37      |          |
| CA-MKD          | 75.90           | 74.56      |          |
| + TeKAP         | 76.34           | 74.98      |          |
| DGKD            | 75.31           | 74.23      |          |
| + TeKAP         | 76.17           | 75.14      |          |"
260,"| Teacher Student | resnet32x4 | WRN_40_2 | WRN_40_2 | VGG13    | resnet56     | resnet32x4   | resnet32x4   | WRN-40-2 |
| --------------- | ---------- | -------- | -------- | -------- | ------------ | ------------ | ------------ | -------- |
| resnet8x4       | WRN_40_1   | WRN_16_2 | VGG8     | resnet20 | ShuffleNetV1 | ShuffleNetV2 | ShuffleNetV1 |          |
| TAKD            | 73.81      | 73.78    | 75.12    | 73.23    | 70.83        | 74.53        | 74.82        | 75.34    |
| TeKAP (L)       | 74.79      | 73.80    | 75.21    | 74.00    | 71.32        | 74.92        | 75.43        | 76.75    |
| TeKAP (F+L)     | 75.98      | 74.41    | 76.20    | 74.42    | 71.92        | 75.60        | 77.38        | 76.59    |"
260,"| Teacher             | Accuracy |
| ------------------- | -------- |
| 1 Original + 3 AugT | 75.98    |
| 2 Original + 3 AugT | 76.12    |
| 3 Original + 3 AugT | 76.19    |"
261,"| Train DMQ Categories                      | Test DMQ Categories  | ASR    |
| ----------------------------------------- | -------------------- | ------ |
| 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 15, 16    | 8, 13, 14, 17, 18    | 0.905  |
| 2, 3, 5, 6, 9, 10, 11, 12, 14, 15, 16, 17 | 1, 4, 7, 8, 13, 18   | 0.933  |
| 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 14, 15   | 4, 8, 16, 18, 13, 17 | 0.921  |
| 1, 2, 3, 5, 6, 7, 10, 11, 12, 13, 15, 16  | 1, 2, 9, 11, 14, 18  | 0.925  |
| 1, 2, 3, 5, 6, 7, 8, 10, 12, 14, 15, 18   | 4, 13, 16, 9, 17, 11 | 0.923  |
| Average Cross-Validation ASR              |                      | 0.9214 |"
261,"| Method                     | ASR  | CPP (s) |
| -------------------------- | ---- | ------- |
| ReNeLLM Ding et al. (2023) | 0.47 | 132     |
| AUTODAN Liu et al. (2024)  | 0.38 | 955     |
| PAIR Chao et al. (2023)    | 0.35 | 146     |
| GCG Wallace et al. (2019)  | 0.32 | 564     |
| Proposed SoC Attack        | 0.95 | 15      |"
262,"| Models→                       | Llama2-13B | HUMPA (Llama2-7B) | Llama3-70B | HUMPA (Llama3-8B) | Mixtral-8x7B | HUMPA (Mistral-7B) |
| ----------------------------- | ---------- | ----------------- | ---------- | ----------------- | ------------ | ------------------ |
| The Generation Utility        |            |                   |            |                   |              |                    |
| SBert                         | 0.8278     | 0.8206            | 0.8276     | 0.8126            | 0.8342       | 0.8281             |
| ROUGE-1                       | 0.2780     | 0.2517            | 0.2926     | 0.2685            | 0.2911       | 0.2663             |
| ROUGE-2                       | 0.0972     | 0.0798            | 0.1019     | 0.0929            | 0.1045       | 0.0818             |
| ROUGE-L                       | 0.1995     | 0.1779            | 0.2078     | 0.1800            | 0.2140       | 0.1863             |
| The White-box Setting         |            |                   |            |                   |              |                    |
| Likelihood                    | 0.9995     | 0.8610            | 0.9995     | 0.9070            | 0.8932       | 0.4730             |
| LogRank                       | 0.9993     | 0.8424            | 0.9991     | 0.8769            | 0.9072       | 0.4598             |
| LRR                           | 0.8547     | 0.6634            | 0.8512     | 0.5084            | 0.9039       | 0.3656             |
| NPR                           | 0.9908     | 0.9148            | 0.9836     | 0.8387            | 0.8747       | 0.6629             |
| DNA-GPT                       | 0.9815     | 0.6656            | 0.9908     | 0.8410            | 0.8217       | 0.3388             |
| DetectGPT                     | 0.8915     | 0.8430            | 0.8313     | 0.7906            | 0.6353       | 0.5348             |
| Fast-DetectGPT                | 0.9949     | 0.9262            | 0.9952     | 0.8864            | 0.9933       | 0.4590             |
| The Black-box Setting         |            |                   |            |                   |              |                    |
| Roberta-base                  | 0.9044     | 0.7902            | 0.8225     | 0.5600            | 0.6992       | 0.5506             |
| Roberta-large                 | 0.8874     | 0.7862            | 0.8130     | 0.5556            | 0.7016       | 0.6233             |
| Likelihood(Neo-2.7)           | 0.9961     | 0.6971            | 0.9699     | 0.7338            | 0.8156       | 0.3542             |
| LogRank(Neo-2.7)              | 0.9850     | 0.6976            | 0.9733     | 0.7203            | 0.8254       | 0.3519             |
| LRR(Neo-2.7)                  | 0.9843     | 0.7773            | 0.9537     | 0.5361            | 0.8210       | 0.3309             |
| NPR(Neo-2.7)                  | 0.8586     | 0.6881            | 0.8574     | 0.5893            | 0.7076       | 0.3639             |
| DNA-GPT(Neo-2.7)              | 0.7116     | 0.4295            | 0.7567     | 0.5296            | 0.5983       | 0.1907             |
| DetectGPT(T5-3B/Neo-2.7)      | 0.8282     | 0.6209            | 0.7696     | 0.5853            | 0.6760       | 0.3811             |
| Fast-DetectGPT(GPT-J/Neo-2.7) | 0.9961     | 0.7389            | 0.9885     | 0.7720            | 0.8758       | 0.2590             |
| Binoculars(Falcon-7B)         | 1.0000     | 0.7896            | 0.9989     | 0.8596            | 0.9353       | 0.3053             |"
262,"|                   | Llama2-13B | Llama3-70B | Mixtral-8x7B | HUMPA(Llama2-7B) | HUMPA(Llama3-8B) | HUMPA(Mistral-7B) |
| ----------------- | ---------- | ---------- | ------------ | ---------------- | ---------------- | ----------------- |
| Sampling (hrs)    | 18.61      | 41.64      | 63.52        | 13.87            | 10.97            | 10.83             |
| Fine-tuning (hrs) | 3.09       | 9.54       | 13.58        | 2.04             | 3.20             | 1.39              |
| Inference (secs)  | 12.88      | 27.87      | 32.51        | 39.87            | 47.22            | 36.75             |"
263,"|              | RNSA  | Brain Tumor | ISIC  |
| ------------ | ----- | ----------- | ----- |
| AUC          | F1    | ACC         | AUC   |
| AE           | 68.33 | 67.85       | 52.90 |
| UAE          | 84.36 | 79.47       | 77.55 |
| MorphAEus    | 80.87 | 75.74       | 72.80 |
| GAN Ensemble | 82.10 | 75.30       | 74.30 |
| MemAE        | 68.65 | 67.95       | 53.45 |
| PatchCore    | 86.33 | 80.86       | 79.40 |
| SQUID        | 70.38 | 72.40       | 65.95 |
| FastFlow     | 76.00 | 73.68       | 67.95 |
| CFLOW-AD     | 70.26 | 70.20       | 62.05 |
| CutPaste     | 55.86 | 66.69       | 50.05 |
| NSA          | 82.13 | 75.87       | 82.13 |
| RD4AD        | 84.29 | 78.09       | 76.80 |
| RD++         | 88.00 | 81.57       | 81.20 |
| ReContrast   | 87.53 | 81.24       | 80.70 |
| UniAD        | 73.69 | 69.71       | 65.85 |
| SimpleNet    | 69.06 | 68.99       | 62.70 |
| EfficientAD  | 74.88 | 73.12       | 68.20 |
| UCAD         | 70.89 | 69.88       | 62.45 |
| Ours         | 91.01 | 84.05       | 83.40 |"
263,"| CRD | SAM | RSNA  | Brain Tumor | ISIC  |
| --- | --- | ----- | ----------- | ----- |
| AUC | F1  | ACC   | AUC         | F1    |
| -   | -   | 84.29 | 78.09       | 76.80 |
| ✓   | -   | 89.87 | 83.69       | 83.20 |
| ✓   | ✓   | 91.01 | 84.05       | 83.40 |"
263,"| Noise Type | RSNA  | Brain Tumor | ISIC  |
| ---------- | ----- | ----------- | ----- |
| AUC        | F1    | ACC         | AUC   |
| Gaussian   | 90.70 | 84.11       | 84.05 |
| Simplex    | 91.01 | 84.05       | 83.40 |"
263,"| Backbone       | RSNA  | Brain Tumor | ISIC  |
| -------------- | ----- | ----------- | ----- |
| AUC            | F1    | ACC         | AUC   |
| Resnet-34      | 85.20 | 78.96       | 77.25 |
| Resnet-50      | 88.45 | 82.15       | 81.25 |
| Wide ResNet-50 | 91.01 | 84.05       | 83.40 |"
263,"| Conclusion                                                                                                                                                                                                                                                                                                                                                                                                                       | References                                                                                                                                                                                                                                                                                                        |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| · Our approach employs a contrastive student-teacher framework with clean and noisy teacher encoders, a student decoder, and a scale adaptation mechanism, enabling robust feature learning and handling of anomaly scale variation.· Extensive experiments on benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance, underscoring its efficacy in unsupervised anomaly detection tasks. | [1]https://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection[2]https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset[3]https://challenge.isic-archive.com/data/#2018[4]Hanqiu Deng and Xingyu Li. Anomaly detection via reverse distillation from one-class embedding. In CVPR, 2022. |"
288,"| Phrasebook  $ \pi_{1} $ BC  $ \rightarrow $  eb DE  $ \rightarrow $  hd FG  $ \rightarrow $  af HA  $ \rightarrow $  gc  $ \cdots $ |
| ----------------------------------------------------------------------------------------------------------------------------------- |
| Circular Shift                                                                                                                      |
| Phrasebook  $ \pi_{2} $ bh  $ \rightarrow $  γβ da  $ \rightarrow $  απ fg  $ \rightarrow $  θδ ce  $ \rightarrow $  ζμ  $ \cdots $ |"
264,"| Models                            | T=3      | T=4   | T=5   | T=6      |
| --------------------------------- | -------- | ----- | ----- | -------- |
| SR↑                               | mAcc↑    | mIoU↑ | SR↑   | mAcc↑    |
| Random                            | &lt;0.01 | 0.94  | 1.66  | &lt;0.01 |
| Retrieval-Based                   | 8.05     | 23.30 | 32.06 | 3.95     |
| WLTDO (Ehsani et al., 2018)       | 1.87     | 21.64 | 31.70 | 0.77     |
| UAAA (Abu Farha &amp; Gall, 2019) | 2.15     | 20.21 | 30.87 | 0.98     |
| UPN (Srinivas et al., 2018)       | 2.89     | 24.39 | 31.56 | 1.19     |
| DDN (Chang et al., 2020)          | 12.18    | 31.29 | 47.48 | 5.97     |
| PlaTe (Sun et al., 2022)          | 16.00    | 36.17 | 65.91 | 14.00    |
| Ext-GAIL (Bi et al., 2021)        | 21.27    | 49.46 | 61.70 | 16.41    |
| P3IV (Zhao et al., 2022)          | 23.34    | 49.96 | 73.89 | 13.40    |
| EGPP (Wang et al., 2023a)         | 26.40    | 53.02 | 74.05 | 16.49    |
| PDPP† (Wang et al., 2023b)        | 37.2     | 64.67 | 66.57 | 21.48    |
| KEPP† (Nagasinghe et al., 2024)   | 38.12    | 64.74 | 67.15 | 24.15    |
| SCHEMA† (Niu et al., 2024)        | 38.93    | 63.80 | 79.82 | 24.50    |
| MTID (Ours)†                      | 40.45    | 67.19 | 69.17 | 24.76    |"
264,"| Models | COIN     | NIV      |
| ------ | -------- | -------- |
| T=3    | T=4      | T=3      |
| SR↑    | mAcc↑    | mIoU↑    |
| Random | &lt;0.01 | &lt;0.01 |
| DDN    | 13.90    | 20.19    |
| P3IV   | 15.40    | 21.67    |
| EGPP   | 19.57    | 31.42    |
| PDPP   | 19.42    | 43.44    |
| KEPP   | 20.25    | 39.87    |
| SCHEMA | 32.09    | 49.84    |
| MTID   | 30.44    | 51.70    |"
267,"|           | DRRN  | KG-A2C-Hard | MC!Q*BERT | MPRC-DQN | RC-DQN | BiKE + CBR | MC-DML        |
| --------- | ----- | ----------- | --------- | -------- | ------ | ---------- | ------------- |
| Zork1     | 32.6  | 40.2 ± 0.4  | 41.6      | 38.3     | 38.8   | 44.3       | 48.66 ± 1.89  |
| Deephome  | 1     | 20 ± 2.1    | 8         | 1        | 1      | 1          | 67 ± 1.41     |
| Ludicorp  | 13.8  | 19.8 ± 1.0  | 22.8      | 19.7     | 17     | 23.8       | 19.67 ± 1.7   |
| Pentari   | 27.2  | 44 ± 0.9    | 58        | 44.4     | 43.8   | 52.1       | 70 ± 0.0      |
| Detective | 197.8 | 338 ± 3.4   | 330       | 317.7    | 291.3  | 326.1      | 346.67 ± 9.43 |
| Library   | 17    | 17 ± 0.0    | 19        | 17.7     | 18.1   | 22.3       | 21 ± 0.0      |
| Balances  | 10    | 10          | 10        | 10       | 10     | 11.9       | 10 ± 0.0      |
| Temple    | 7.4   | 8 ± 0.0     | 8         | 8        | 8      | 7.8        | 8 ± 0.0       |
| Ztuu      | 21.6  | 5 ± 0.0     | 11.8      | -        | -      | -          | 23.67 ± 1.9   |"
267,"|           | LLM | Reflection | PUCT-RL            | MC-LAVE-RL         | MC-DML                |
| --------- | --- | ---------- | ------------------ | ------------------ | --------------------- |
| Zork1     | 0   | 5          | 38.2  $ \pm $  0.8 | 45.2  $ \pm $  1.2 | 48.66  $ \pm $  1.89  |
| Deephome  | 1   | 1          | 28.6  $ \pm $  2.9 | 35  $ \pm $  0.6   | 67  $ \pm $  1.41     |
| Ludicorp  | 4   | 4          | 18  $ \pm $  0.0   | 22.8  $ \pm $  0.2 | 19.67  $ \pm $  1.7   |
| Pentari   | 5   | 5          | 64                 | 68                 | 70  $ \pm $  0.0      |
| Detective | 30  | 30         | 322  $ \pm $  2.0  | 330  $ \pm $  0.0  | 346.67  $ \pm $  9.43 |
| Library   | 6   | 6          | 19  $ \pm $  0.0   | 19  $ \pm $  0.0   | 21  $ \pm $  0.0      |
| Balances  | 10  | 10         | 10  $ \pm $  0.0   | 10  $ \pm $  0.0   | 10  $ \pm $  0.0      |
| Temple    | 8   | 8          | 8  $ \pm $  0.0    | 8  $ \pm $  0.0    | 8  $ \pm $  0.0       |
| Ztuu      | 0   | 5          | 5  $ \pm $  0.0    | 7  $ \pm $  2.7    | 23.67  $ \pm $  1.9   |"
267,"|           | MC-DML        | w.o.  $ \mathcal{M}_{c} $ | w.o. DP    | w.o.  $ \mathcal{M}_{c} $ , DP | w.o.  $ \mathcal{M}_{c} $ ,  $ \mathcal{M}_{i} $ , DP |
| --------- | ------------- | ------------------------- | ---------- | ------------------------------ | ----------------------------------------------------- |
| Zork1     | 48.66 ± 1.89  | 38.33 ± 2.89              | 48 ± 2.45  | 38 ± 5.2                       | 31.67 ± 4.7                                           |
| Deephome  | 67 ± 1.41     | 62.66 ± 0.94              | 67.4 ± 0.8 | 64.33 ± 0.94                   | 51 ± 14.9                                             |
| Detective | 346.67 ± 9.43 | 326.67 ± 4.71             | 334 ± 4.9  | 323.33 ± 4.7                   | 320 ± 0.0                                             |
| Ztuu      | 23.67 ± 1.9   | 20.66 ± 0.47              | 7.8 ± 0.56 | 7 ± 0.81                       | 6.33 ± 0.94                                           |"
268,"| Case      | Task    | Model    | nRMSE  $ \downarrow $ | MSE  $ \downarrow $ | MAE  $ \downarrow $ | Correlation  $ \uparrow $ | nER  $ \downarrow $ |
| --------- | ------- | -------- | --------------------- | ------------------- | ------------------- | ------------------------- | ------------------- |
| Cy. Flow  | FI      | Interp.  | 0.274                 | 2.29E-05            | 3.06E-03            | 0.843                     | 5.90E-02            |
| E2E+PF    | 0.094   | 3.42E-06 | 1.08E-03              | 0.972               | 8.32E-03            |                           |                     |
| PIDM      | 0.261   | 2.14E-05 | 3.05E-03              | 0.766               | 1.60E-02            |                           |                     |
| SB        | 0.063   | 1.36E-06 | 7.18E-04              | 0.994               | 1.50E-02            |                           |                     |
| PalSB     | 0.062   | 1.42E-06 | 7.04E-04              | 0.974               | 1.22E-03            |                           |                     |
| RI        | Interp. | 0.301    | 2.80E-05              | 3.10E-03            | 0.898               | 1.70E+00                  |                     |
| E2E+PF    | 0.100   | 3.81E-06 | 1.17E-03              | 0.906               | 9.48E-03            |                           |                     |
| PIDM      | 0.121   | 4.97E-06 | 1.45E-03              | 0.894               | 4.50E-02            |                           |                     |
| SB        | 0.092   | 2.92E-06 | 1.04E-03              | 0.972               | 2.10E-02            |                           |                     |
| PalSB     | 0.090   | 2.75E-06 | 1.02E-03              | 0.955               | 8.74E-04            |                           |                     |
| Kol. Flow | FI      | Interp.  | 0.538                 | 6.58E-01            | 1.88E+00            | 0.857                     | 1.40E-01            |
| E2E+PF    | 0.288   | 1.91E-01 | 1.04E+00              | 0.959               | 3.91E-01            |                           |                     |
| PIDM      | 0.512   | 5.95E-01 | 1.77E+00              | 0.871               | 3.45E-01            |                           |                     |
| SB        | 0.077   | 1.41E-01 | 2.24E-01              | 0.996               | 7.99E-02            |                           |                     |
| PalSB     | 0.081   | 1.56E-01 | 2.66E-01              | 0.997               | 2.40E-02            |                           |                     |
| RI        | Interp. | 0.582    | 7.69E-01              | 1.88E+00            | 0.841               | 6.22E+02                  |                     |
| E2E+PF    | 0.466   | 5.00E-01 | 1.72E+00              | 0.908               | 2.68E+00            |                           |                     |
| PIDM      | 0.381   | 3.31E-01 | 1.34E+00              | 0.934               | 2.73E+01            |                           |                     |
| SB        | 0.230   | 1.23E-01 | 7.16E-01              | 0.973               | 1.95E+00            |                           |                     |
| PalSB     | 0.253   | 1.48E+00 | 8.83E-01              | 0.973               | 5.65E-01            |                           |                     |
| RDGS      | FI      | Interp.  | 0.282                 | 2.59E-02            | 1.01E-01            | 0.864                     | 3.68E-05            |
| E2E+PF    | 0.261   | 2.18E-02 | 1.03E-01              | 0.883               | 8.17E-05            |                           |                     |
| PIDM      | 0.268   | 2.30E-02 | 1.06E-01              | 0.870               | 1.23E-03            |                           |                     |
| SB        | 0.107   | 4.25E-03 | 2.66E-02              | 0.980               | 1.58E-06            |                           |                     |
| PalSB     | 0.100   | 3.76E-03 | 2.46E-02              | 0.981               | 8.91E-08            |                           |                     |
| RI        | Interp. | 0.292    | 2.78E-02              | 9.72E-02            | 0.855               | 2.95E-02                  |                     |
| E2E+PF    | 0.394   | 3.21E-02 | 1.25E-01              | 0.883               | 5.37E-05            |                           |                     |
| PIDM      | 0.277   | 1.54E-02 | 9.18E-02              | 0.918               | 3.83E-03            |                           |                     |
| SB        | 0.194   | 1.34E-02 | 5.76E-02              | 0.942               | 1.86E-06            |                           |                     |
| PalSB     | 0.193   | 1.35E-02 | 5.79E-02              | 0.941               | 1.69E-07            |                           |                     |"
268,"| Case      | Task       | Model      | nRMSE ↓  | MSE ↓    | MAE ↓    | Correlation ↑ | nER ↓    |
| --------- | ---------- | ---------- | -------- | -------- | -------- | ------------- | -------- |
| Cy. Flow  | FI         | Full model | 0.062    | 1.42E-06 | 7.04E-04 | 0.974         | 1.22E-03 |
| w/o PF    | 0.063      | 1.36E-06   | 7.18E-04 | 0.994    | 1.54E-02 |               |          |
| w/o DS    | 0.062      | 1.42E-06   | 7.04E-04 | 0.974    | 1.22E-03 |               |          |
| w/o ES    | 0.107      | 3.72E-06   | 1.23E-03 | 0.957    | 2.68E-03 |               |          |
| w/o BA    | -          | -          | -        | -        | -        |               |          |
| RI        | Full model | 0.090      | 2.75E-06 | 1.02E-03 | 0.955    | 8.74E-04      |          |
| w/o PF    | 0.092      | 2.92E-06   | 1.04E-03 | 0.972    | 2.05E-02 |               |          |
| w/o DS    | 0.092      | 2.85E-06   | 1.03E-03 | 0.944    | 9.10E-04 |               |          |
| w/o ES    | 0.129      | 5.50E-06   | 1.45E-03 | 0.921    | 1.98E-03 |               |          |
| w/o BA    | -          | -          | -        | -        | -        |               |          |
| Kol. Flow | FI         | Full model | 0.081    | 1.56E-01 | 2.66E-01 | 0.997         | 2.40E-01 |
| w/o PF    | 0.077      | 1.41E-01   | 2.24E-01 | 0.997    | 7.99E-01 |               |          |
| w/o DS    | 0.083      | 1.64E-01   | 2.72E-01 | 0.997    | 2.57E-01 |               |          |
| w/o ES    | 0.340      | 2.65E+00   | 1.10E+00 | 0.950    | 3.86E-01 |               |          |
| w/o BA    | 0.109      | 2.74E-01   | 3.24E-01 | 0.994    | 7.51E-01 |               |          |
| RI        | Full model | 0.253      | 1.48E+00 | 8.83E-01 | 0.975    | 5.65E-01      |          |
| w/o PF    | 0.230      | 1.23E+00   | 7.16E-01 | 0.975    | 1.95E+00 |               |          |
| w/o DS    | 0.259      | 1.54E+00   | 9.02E-01 | 0.974    | 6.30E-01 |               |          |
| w/o ES    | 0.375      | 3.21E+00   | 1.33E+00 | 0.954    | 9.08E-01 |               |          |
| w/o BA    | 0.276      | 1.74E+00   | 9.51E-01 | 0.971    | 1.12E+00 |               |          |
| RDGS      | FI         | Full model | 0.100    | 3.76E-03 | 2.46E-02 | 0.981         | 8.91E-08 |
| w/o PF    | 0.107      | 4.25E-03   | 2.66E-02 | 0.980    | 1.58E-06 |               |          |
| w/o DS    | 0.099      | 3.74E-03   | 2.46E-02 | 0.981    | 9.51E-08 |               |          |
| w/o ES    | 0.214      | 1.52E-02   | 6.64E-02 | 0.929    | 9.00E-07 |               |          |
| w/o BA    | 0.103      | 3.98E-03   | 2.52E-02 | 0.980    | 9.31E-08 |               |          |
| RI        | Full model | 0.193      | 1.35E-02 | 5.79E-02 | 0.941    | 1.69E-07      |          |
| w/o PF    | 0.194      | 1.34E-02   | 5.76E-02 | 0.942    | 1.86E-06 |               |          |
| w/o DS    | 0.193      | 1.33E-02   | 5.78E-02 | 0.942    | 1.90E-07 |               |          |
| w/o ES    | 0.184      | 1.19E-02   | 5.62E-02 | 0.946    | 7.44E-07 |               |          |
| w/o BA    | 0.196      | 1.39E-02   | 5.91E-02 | 0.940    | 1.69E-07 |               |          |"
289,"| Method                              | Gemma-2-9B | Llama-3-8B |
| ----------------------------------- | ---------- | ---------- |
| GSM8K                               | BBH        | FOLIO      |
| ZS-CoT (Kojima et al., 2022)        | 88.6       | 71.7       |
| APE (Zhou et al., 2022)             | 88.6       | 71.7       |
| APO (Pryzant et al., 2023)          | 88.6       | 72.3       |
| PE2 (Ye et al., 2023)               | 88.6       | 68.9       |
| TextGrad (Yuksekgonul et al., 2024) | 87.8       | 72.9       |
| GREATER                             | 89.4       | 76.6       |"
270,"|                    | Method             | BracketIRE         | BracketIRE+        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| Synthetic          | Real-World         | Synthetic          | Real-World         |
| PSNR↑/SSIM↑/LPIPS↓ | CLIPIQA↑/MANIQA↑   | PSNR↑/SSIM↑/LPIPS↓ | CLIPIQA↑/MANIQA↑   |
| Burst              | DBSR               | 35.13/0.9092/0.188 | 0.1359/0.1653      |
| MFIR               | 35.64/0.9161/0.177 | 0.2192/0.2310      | 30.06/0.8591/0.319 |
| Processing         | BIPNet             | 36.92/0.9331/0.148 | 0.2234/0.2348      |
| Networks           | Burstormer         | 37.06/0.9344/0.151 | 0.2399/0.2390      |
| RBSR               | 39.10/0.9498/0.117 | 0.2074/0.2341      | 30.49/0.8713/0.275 |
| HDR                | AHDRNet            | 36.68/0.9279/0.158 | 0.2010/0.2259      |
| HDRGAN             | 35.94/0.9177/0.181 | 0.1995/0.2178      | 30.00/0.8590/0.337 |
| Reconstruction     | HDR-Tran.          | 37.62/0.9356/0.129 | 0.2043/0.2142      |
| Networks           | SCTNet             | 37.47/0.9443/0.122 | 0.2348/0.2260      |
| Kim et al.         | 39.09/0.9494/0.115 | 0.2467/0.2388      | 30.28/0.8658/0.268 |
| Our TMRNet         | w/o Ada.           | 39.35/0.9516/0.112 | 0.2003/0.2181      |
| w/ Ada.            | -                  | 0.2537/0.2422      | -                  |"
273,"| Model  | Creation | Math | Code |
| ------ | -------- | ---- | ---- |
| GPT-4o | 55.6     | 74.8 | 68.1 |
| SM     | 69.4     | 84.8 | 69.4 |
| Con-J  | 72.4     | 85.0 | 70.1 |"
274,"| Reward Model                | Training Methods              | Test Settings               |
| --------------------------- | ----------------------------- | --------------------------- |
| Helpful                     | Harmless                      |                             |
| Avg_Reward ( $ \uparrow $ ) | Var_Reward ( $ \downarrow $ ) | Avg_Reward ( $ \uparrow $ ) |
| 0%                          | 25%                           | 0%                          |
| —                           | SFT (Gemma-7B)                | 0.62                        |
| Gemma-2B                    | PPO                           | 0.62                        |
| LCB                         | 0.59                          | 0.54                        |
| UWO                         | 0.56                          | 0.51                        |
| RLR                         | —                             | 0.69                        |
| UGDA                        | —                             | 0.83                        |
| Gemma-7B                    | PPO                           | 0.64                        |
| LCB                         | 0.70                          | 0.73                        |
| UWO                         | 0.73                          | 0.68                        |
| RLR                         | —                             | 0.79                        |
| UGDA                        | —                             | 0.91                        |"
274,"| Reward Model                 | Training Methods               | Test Settings                |
| ---------------------------- | ------------------------------ | ---------------------------- |
| Helpful                      | Harmless                       |                              |
| Avg. Reward ( $ \uparrow $ ) | Var. Reward ( $ \downarrow $ ) | Avg. Reward ( $ \uparrow $ ) |
| Gemma-2B                     | PPO                            | 0.51                         |
| LCB                          | 0.54                           | 0.18                         |
| UWO                          | 0.60                           | 0.18                         |
| RLR                          | 0.57                           | 0.15                         |
| UGDA                         | 0.75                           | 0.13                         |
| Gemma-7B                     | PPO                            | 0.58                         |
| LCB                          | 0.61                           | 0.14                         |
| UWO                          | 0.64                           | 0.17                         |
| RLR                          | 0.67                           | 0.12                         |
| UGDA                         | 0.81                           | 0.15                         |"
275,"| Prompt | Type  | SSIM↑
(Baseline) | SSIM
(Ours) | PSNR↑
(Baseline) | PSNR
(Ours) | VIFp↑
(Baseline) | VIFp
(Ours) | FSIM↑
(Baseline) | FSIM
(Ours) |
| ------ | ----- | ---------------- | ----------- | ---------------- | ----------- | ---------------- | ----------- | ---------------- | ----------- |
| P1     | Train | 0.6889           | 0.548       | 22.0869          | 18.6758     | 0.1862           | 0.1093      | 0.8285           | 0.7461      |
|        | Test  | 0.6991           | 0.5458      | 22.3317          | 18.4373     | 0.1932           | 0.1087      | 0.8355           | 0.7422      |
| P2     | Train | 0.7369           | 0.5326      | 23.3199          | 18.8712     | 0.2178           | 0.092       | 0.852            | 0.7446      |
|        | Test  | 0.745            | 0.5433      | 23.4913          | 18.8056     | 0.2251           | 0.0928      | 0.8582           | 0.7563      |
| P3     | Train | 0.7157           | 0.5168      | 22.3027          | 18.0698     | 0.2081           | 0.1031      | 0.8367           | 0.7258      |
|        | Test  | 0.7241           | 0.5201      | 22.2055          | 17.8572     | 0.2169           | 0.1032      | 0.8399           | 0.7289      |
| P4     | Train | 0.8181           | 0.5718      | 25.8903          | 19.7084     | 0.29             | 0.1229      | 0.9085           | 0.7697      |
|        | Test  | 0.8249           | 0.5704      | 26.1777          | 19.6394     | 0.2965           | 0.1202      | 0.9117           | 0.7683      |"
276,"| Model              | Security Reminder | $ \Delta $ pass@1 | $ \Delta $ sec\_pass@1 |
| ------------------ | ----------------- | ----------------- | ---------------------- |
| Base               | Agent             | Base              | Agent                  |
| CLAUDE-3.5 -SONNET | None              | 53.6%             | 59.5%                  |
| Generic            | 45.5%             | 47.7%             | 34.5%                  |
| Oracle             | 41.0%             | 45.9%             | 35.4%                  |"
276,"| Model             | $ \Delta $ pass@1 | $ \Delta $ sec\_pass@1 |
| ----------------- | ----------------- | ---------------------- |
| OpenAI o3-MINI    | +7.9%             | +3.0%                  |
| GPT-4o            | +9.6%             | +1.9%                  |
| Claude-3.5 Sonnet | +5.8%             | +3.9%                  |"
277,"| Models                             | Avg. Success  $ \uparrow $ | Avg. Rank  $ \downarrow $ | Close Drawer         | Close Jar Banana      | Close Jar Distractor  | Condition Block       | Meat On Grill         | Open Drawer Small     | Stack cups blocks    | Push Buttons Light    |
| ---------------------------------- | -------------------------- | ------------------------- | -------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | -------------------- | --------------------- |
| Voxposer (Huang et al. 2023)       | 34.29                      | 2.6                       | 96.00  $ \pm $  4.00 | 17.33  $ \pm $  19.73 | 22.67  $ \pm $  10.07 | 25.00  $ \pm $  23.26 | 38.67  $ \pm $  12.22 | 6.67  $ \pm $  2.31   | 0.00  $ \pm $  0.00  | 68.00  $ \pm $  18.33 |
| Act3D (Gervet et al. 2023)         | 17.83                      | 3.5                       | 66.67  $ \pm $  9.24 | 29.33  $ \pm $  9.24  | 41.33  $ \pm $  4.62  | 0.00  $ \pm $  0.00   | 1.33  $ \pm $  2.31   | 2.67  $ \pm $  4.62   | 0.00  $ \pm $  0.00  | 1.33  $ \pm $  2.31   |
| 3D Diffuser Actor (Ke et al. 2024) | 29.38                      | 2.9                       | 81.33  $ \pm $  6.11 | 48.00  $ \pm $  4.00  | 42.67  $ \pm $  4.62  | 27.00  $ \pm $  10.15 | 0.00  $ \pm $  0.00   | 2.67  $ \pm $  4.62   | 2.67  $ \pm $  2.31  | 30.67  $ \pm $  12.86 |
| GravMAD (VLM)                      | 62.92                      | 1.0                       | 97.33  $ \pm $  2.31 | 84.00  $ \pm $  00.00 | 86.67  $ \pm $  2.31  | 74.00  $ \pm $  11.14 | 45.33  $ \pm $  4.62  | 21.33  $ \pm $  12.86 | 18.67  $ \pm $  2.31 | 76.00  $ \pm $  8.00  |
| Performance gain                   | +28.63                     | -                         | +1.33                | +36.00                | +44.00                | +47.00                | +6.66                 | +14.66                | +16.00               | +8.00                 |"
277,"| Models                             | Avg. Success  $ \uparrow $ | Avg. Rank  $ \downarrow $ | Close Jar           | Open Drawer        | Meat off Grill      | Slide Block        | Put in Drawer       |
| ---------------------------------- | -------------------------- | ------------------------- | ------------------- | ------------------ | ------------------- | ------------------ | ------------------- |
| Voxposer (Huang et al. 2023)       | 15.11                      | 4.5                       | 12.00 $ \pm $ 10.58 | 10.67 $ \pm $ 8.33 | 45.33 $ \pm $ 24.44 | 0.00 $ \pm $ 0.00  | 0.00 $ \pm $ 0.00   |
| Act3D (Gervet et al. 2023)         | 34.11                      | 4.3                       | 61.33 $ \pm $ 4.62  | 41.33 $ \pm $ 4.62 | 60.00 $ \pm $ 6.92  | 78.67 $ \pm $ 2.31 | 49.33 $ \pm $ 10.07 |
| 3D Diffuser Actor (Ke et al. 2024) | 55.81                      | 2.3                       | 66.67 $ \pm $ 2.31  | 88.00 $ \pm $ 6.93 | 88.00 $ \pm $ 4.00  | 84.00 $ \pm $ 0.00 | 94.67 $ \pm $ 2.31  |
| GravMAD (Manual)                   | 69.17                      | 1.3                       | 100.00 $ \pm $ 0.00 | 76.67 $ \pm $ 4.62 | 89.33 $ \pm $ 2.31  | 93.33 $ \pm $ 2.31 | 78.67 $ \pm $ 6.11  |
| Performance gain                   | +13.36                     | -                         | +33.33              | -13.33             | +1.33               | +9.33              | -16.00              |
| GravMAD (VLM)                      | 56.72                      | 2.1                       | 100.00 $ \pm $ 0.00 | 58.67 $ \pm $ 2.31 | 70.67 $ \pm $ 2.31  | 80.00 $ \pm $ 0.00 | 61.33 $ \pm $ 9.24  |
| Performance gain                   | +0.91                      | -                         | +33.33              | -29.33             | -17.33              | -4.00              | -33.34              |"
277,"| Models                              | Push Buttons | Stack Blocks | Place Cups | Place Wine | Screw Bulb | Insert Peg | Stack Cups |
| ----------------------------------- | ------------ | ------------ | ---------- | ---------- | ---------- | ---------- | ---------- |
| Voxposer (Huang et al., 2023)       | 80.00±13.86  | 16.00±12.00  | 6.67±8.33  | 5.33±2.31  | 4.00±4.00  | 0.00±0.00  | 1.33±2.31  |
| Act3D (Gervet et al., 2023)         | 66.67±2.31   | 0.00±0.00    | 0.00±0.00  | 45.33±2.31 | 6.67±2.31  | 0.00±0.00  | 0.00±0.00  |
| 3D Diffuser Actor (Ke et al., 2024) | 94.67±2.31   | 13.67±2.89   | 5.33±6.11  | 82.67±2.31 | 29.33±2.31 | 2.67±4.62  | 20.00±0.00 |
| GravMAD (Manual)                    | 98.67±2.31   | 56.67±4.62   | 5.33±2.31  | 77.33±4.62 | 66.67±6.11 | 32.00±6.93 | 57.33±2.31 |
| Performance gain                    | +4.00        | +40.67       | -1.34      | -5.34      | +37.34     | +29.33     | +37.33     |
| GravMAD (VLM)                       | 97.33±2.31   | 51.33±6.11   | 5.33±4.62  | 33.33±4.62 | 54.67±6.11 | 18.67±4.62 | 49.33±2.31 |
| Performance gain                    | +2.66        | +35.33       | -1.34      | -49.34     | +25.34     | +16.00     | +29.33     |"
277,"| Real-world Task | Open Drawer | Toy in Drawer | Mouse on Pad     | Stack Cup          | Stack Block Same      |
| --------------- | ----------- | ------------- | ---------------- | ------------------ | --------------------- |
| GravMAD (\%)    | 80          | 90            | 100              | 60                 | 50                    |
| Real-world Task | Place Cup   | Stack Block   | Stack Cup Blocks | Wired Mouse on Pad | Colored Toy in Drawer |
| GravMAD (\%)    | 10          | 40            | 40               | 100                | 70                    |"
278,"| Equation             | Evaluation Metric | RL2           | RMSE   |
| -------------------- | ----------------- | ------------- | ------ |
| Methods              | Nc                | 16³           | 32³    |
| Helmholtz            | PINN              | JCP&#x27;2019 | 0.9819 |
| gPINN                | CMAME&#x27;2022   | 0.3852        | 0.3255 |
| AHD-PINN             | IJCAI&#x27;2024   | 0.2108        | 0.1903 |
| SPINN                | NeurIPS&#x27;2023 | 0.1177        | 0.0809 |
| SPINN (m)            | NeurIPS&#x27;2023 | 0.1161        | 0.0595 |
| RoPINN               | NeurIPS&#x27;2024 | 0.4059        | 0.3338 |
| FPINN                | NN&#x27;2025      | 0.3862        | 0.3502 |
| CoPINN               | Ours              | 0.0172        | 0.0050 |
| IMP.                 | -                 | 85%           | 92%    |
| (2+1)-d Klein-Gordon | PINN              | JCP&#x27;2019 | 0.0343 |
| gPINN                | CMAME&#x27;2022   | 0.0108        | 0.0025 |
| SPINN                | NeurIPS&#x27;2023 | 0.0193        | 0.0060 |
| SPINN (m)            | NeurIPS&#x27;2023 | 0.0062        | 0.0020 |
| AHD-PINN             | IJCAI&#x27;2024   | 0.0133        | 0.0082 |
| RoPINN               | NeurIPS&#x27;2024 | 0.1925        | 0.1806 |
| FPINN                | NN&#x27;2025      | 0.0331        | 0.0213 |
| CoPINN               | Ours              | 0.0016        | 0.0006 |
| IMP.                 | -                 | 74%           | 70%    |
| Diffusion            | PINN              | JCP&#x27;2019 | 0.0129 |
| gPINN                | CMAME&#x27;2022   | 0.0144        | O/M    |
| SPINN                | NeurIPS&#x27;2023 | 0.0151        | 0.0086 |
| SPINN (m)            | NeurIPS&#x27;2023 | 0.0078        | 0.0065 |
| AHD-PINN             | IJCAI&#x27;2024   | 0.0109        | 0.0107 |
| RoPINN               | NeurIPS&#x27;2024 | 0.0135        | O/M    |
| FPINN                | NN&#x27;2025      | 0.0152        | O/M    |
| CoPINN               | Ours              | 0.0041        | 0.0027 |
| IMP.                 | -                 | 47%           | 58%    |
| PINN                 | JCP&#x27;2019     | 0.0095        | 0.0082 |
| gPINN                | CMAME&#x27;2022   | 0.0099        | 0.0087 |
| SPINN                | NeurIPS&#x27;2023 | 0.0447        | 0.0115 |
| SPINN (m)            | NeurIPS&#x27;2023 | 0.0390        | 0.0067 |
| AHD-PINN             | IJCAI&#x27;2024   | 0.0102        | 0.0080 |
| RoPINN               | NeurIPS&#x27;2024 | 0.0412        | 0.0308 |
| FPINN                | NN&#x27;2025      | 0.0058        | 0.0048 |
| CoPINN               | Ours              | 0.0058        | 0.0038 |
| IMP.                 | -                 | 0%            | 21%    |"
279,"| Method           | Current Task Acc. | Old Task Acc. | Unconf. |
| ---------------- | ----------------- | ------------- | ------- |
| T1               | T2                | T3            | T1@T3   |
| Naive Sequential | 100.0             | 92.2          | 100.0   |
| EWC              | 100.0             | 99.9          | 100.0   |
| FROMP            | 100.0             | 99.8          | 99.4    |
| ER               | 100.0             | 99.6          | 100.0   |
| DER              | 100.0             | 99.9          | 100.0   |
| BGS              | 100.0             | 99.9          | 100.0   |
| GDUMB            | 64.1              | 62.0          | 69.5    |
| PNNs             | 100.0             | 99.8          | 100.0   |
| Shuffled         | 100.0             | 99.9          | 100.0   |
| Cumulative       | 100.0             | 100.0         | 100.0   |
| Joint            | 100.0             | 99.9          | 100.0   |"
279,"| Method           | Current Task Acc. | Old Task Acc. | Unconf. |
| ---------------- | ----------------- | ------------- | ------- |
| T1               | T2                | T3            | T1@T3   |
| Naive Sequential | 100.0             | 100.0         | 100.0   |
| EWC              | 100.0             | 99.9          | 100.0   |
| FROMP            | 100.0             | 59.9          | 69.2    |
| ER               | 100.0             | 97.7          | 96.4    |
| DER              | 100.0             | 98.7          | 96.5    |
| BGS              | 100.0             | 99.8          | 99.6    |
| GDUMB            | 92.5              | 62.7          | 53.6    |
| PNNs             | 100.0             | 99.8          | 100.0   |
| Shuffled         | 79.9              | 93.4          | 95.3    |
| Cumulative       | 100.0             | 87.0          | 88.8    |
| Joint            | 98.2              | 99.2          | 98.9    |"
290,"|         | Img Params | Txt Params | Total Params | Img GFLOPs | Txt GFLOPs | Total GFLOPs |
| ------- | ---------- | ---------- | ------------ | ---------- | ---------- | ------------ |
| CLIP    | 87.85M     | 63.43M     | 151.28M      | 8.82       | 5.96       | 14.78        |
| Student | 11.68M     | 0M         | 11.68M       | 1.82       | 0          | 1.82         |"
291,"| Method               | K  | $ z^{c}\uparrow $ | $ z^{s}\downarrow $ |
| -------------------- | -- | ----------------- | ------------------- |
| V3                   | 20 | 40.6              | 18.5                |
| MINE-based           | 20 | 36.0              | 20.8                |
| Cycle loss           | 20 | 16.8              | 21.2                |
| $ \beta $ -VAE       | -  | 21.8              |                     |
| Raw input            | -  | 21.4              |                     |
| EC $ ^{2} $ -VAE (c) | -  | 97.0              | 21.2                |"
291,"| Method         | K  | z^{c} \uparrow | z^{s} \downarrow |
| -------------- | -- | -------------- | ---------------- |
| V3             | 80 | 52.1           | 40.4             |
| MINE-based     | 80 | 28.6           | 51.6             |
| Cycle loss     | 80 | 16.1           | 50.5             |
| $ \beta $ -VAE | -  | 11.0           |                  |
| Raw input      | -  | 31.8           |                  |
| EC^{2}-VAE (c) | -  | 78.1           | 18.2             |"
280,"|                                | AP(%)                                 | AP50(%) | AP25(%) | RC(%) | RC50(%) | RC25(%) | PR(%) | PR50(%) | PR25(%) |
| ------------------------------ | ------------------------------------- | ------- | ------- | ----- | ------- | ------- | ----- | ------- | ------- |
| 3D Supervised                  | Mask3D (Schult et al., 2023)          | 82.9    | 94.4    | 97.0  | -       | -       | -     | -       | -       |
| 2D Foundation                  | OpenIns3D (Huang et al., 2024)        | 66.7    | 82.4    | 85.7  | -       | -       | -     | -       | -       |
| Model Supervised               | SAI3D (Yin et al., 2024)              | 38.5    | 62.5    | 81.2  | 54.3    | 79.9    | 95.4  | 38.1    | 56.4    |
| Unsupervised                   | Unscene3D (Rozenberszki et al., 2024) | 37.2    | 62.4    | 79.2  | 51.7    | 70.4    | 84.1  | 18.7    | 26.3    |
| Part2Object (Shi et al., 2024) | 34.4                                  | 56.8    | 73.9    | 46.4  | 65.5    | 78.5    | 45.5  | 65.4    | 76.7    |
| EFEM (Lei et al., 2023)        | 24.6                                  | 50.8    | 61.3    | -     | -       | -       | -     | -       | -       |
| EFEM $ _{mask3d} $             | 38.8                                  | 55.1    | 63.8    | 52.4  | 68.7    | 80.8    | 18.8  | 27.1    | 29.1    |
| GrabS (Ours-VAE)               | 46.7                                  | 71.5    | 82.9    | 53.2  | 74.5    | 85.2    | 52.1  | 76.4    | 83.0    |
| GrabS (Ours-Diffusion)         | 47.1                                  | 70.6    | 81.1    | 52.9  | 73.3    | 82.9    | 54.9  | 79.2    | 85.7    |"
280,"|                                | AP(%)                                 | AP50(%) | AP25(%) | RC(%) | RC50(%) | RC25(%) | PR(%) | PR50(%) | PR25(%) |
| ------------------------------ | ------------------------------------- | ------- | ------- | ----- | ------- | ------- | ----- | ------- | ------- |
| 3D Supervised                  | Mask3D (Schult et al., 2023)          | 84.1    | 96.0    | 96.7  | 87.1    | 96.2    | 96.9  | 89.5    | 98.9    |
| Unsupervised &amp; Real2Syn    | Unscene3D (Rozenberszki et al., 2024) | 37.7    | 59.7    | 76.2  | 50.5    | 70.4    | 83.9  | 8.2     | 14.8    |
| Part2Object (Shi et al., 2024) | 46.1                                  | 69.3    | 81.5    | 53.1  | 70.9    | 83.4    | 10.2  | 15.0    | 19.1    |
| Unsupervised                   | HDBSCAN (McInnes &amp; Healy, 2017)   | 7.6     | 12.5    | 23.4  | 10.8    | 15.8    | 24.7  | 36.6    | 58.5    |
| EFEM (Lei et al., 2023)        | 20.7                                  | 34.1    | 46.6    | 23.3  | 34.7    | 46.7    | 53.3  | 90.6    | 98.7    |
| GrabS (Ours-VAE)               | 58.7                                  | 85.0    | 90.6    | 71.6  | 87.9    | 91.1    | 76.0  | 93.7    | 96.3    |
| GrabS (Ours-Diffusion)         | 58.5                                  | 85.9    | 91.5    | 72.4  | 88.7    | 91.7    | 77.9  | 95.7    | 98.5    |"
280,"|                                       | AP(%) | AP50(%) |
| ------------------------------------- | ----- | ------- |
| (1) Replace VAE by AE                 | 32.0  | 57.1    |
| (2) Remove Orientation Estimator      | 35.3  | 56.3    |
| (3) Remove Object Discovery Branch    | 34.2  | 56.7    |
| (4) Remove Object Segmentation Branch | 25.7  | 47.9    |
| (5)  $ \Delta s = 0.2 $               | 43.6  | 61.3    |
| (6)  $ \Delta s = 0.3 $               | 46.7  | 71.5    |
| (7)  $ \Delta s = 0.4 $               | 40.1  | 58.1    |
| (8)  $ \Delta s = 0.5 $               | 38.1  | 56.5    |
| (9)  $ \alpha = 1/3 $                 | 40.8  | 61.4    |
| (10)  $ \alpha = 1/4 $                | 46.7  | 71.5    |
| (11)  $ \alpha = 1/5 $                | 43.3  | 61.9    |
| (12)  $ \delta_{d} = 0.01 $           | 40.1  | 59.3    |
| (13)  $ \delta_{d} = 0.02 $           | 46.7  | 71.5    |
| (14)  $ \delta_{d} = 0.05 $           | 43.4  | 62.8    |
| (15)  $ \delta_{d} = 0.10 $           | 42.9  | 61.4    |"
282,"| Model                                 | PESQ (VB-DMD)  $ \uparrow $ | Parameters | FLOPs / sec |
| ------------------------------------- | --------------------------- | ---------- | ----------- |
| Real-time Baselines (with processing) |                             |            |             |
| FRCRN                                 | 3.21                        | 6.9M       | 76.2G       |
| DeepFilterNet3                        | 3.16                        | 2.13M      | 0.688G      |
| SEMamba (causal)                      | 2.76                        | 3.60M      | 0.76G       |
| PercepNet                             | 2.73                        | 8.00M      | 1.60G       |
| DEMUCS                                | 2.65                        | 33.53M     | 15.44G      |
| Centaurus (DWS)                       | -                           | 0.71M      | 0.39G       |
| Centaurus (DWS data-gated, or S6)     | -                           | 0.66M      | 0.50G       |
| Centaurus (bottleneck)                | -                           | 0.87M      | 1.15G       |
| Centaurus (PW bottleneck)             | 3.06                        | 0.83M      | 0.65G       |
| Centaurus (full)                      | 3.04                        | 2.53M      | 1.12G       |
| Centaurus (hybrid)                    | 3.12                        | 0.51M      | 0.29G       |
| Centaurus (hybrid, with Causal Conv)  | 3.25                        | 0.51M      | 0.29G       |"
282,"| Model                            | test  $ \downarrow $ | dev  $ \downarrow $ | Parameters (M) | FLOPs (G) |
| -------------------------------- | -------------------- | ------------------- | -------------- | --------- |
| Offline (full-context)           |                      |                     |                |           |
| Full Conv                        | 3.3 / 10.5           | 3.1 / 9.9           |                |           |
| Transformer                      | 3.1 / 7.3            |                     | 29             |           |
| ContextNet                       | 2.4 / 5.4            |                     | 31.4           |           |
| Wav2Vec2                         | 2.1 / 4.8            | 1.8 / 4.7           | 94.4           | 285       |
| Conformer                        | 2.0 / 4.3            |                     | 30.7           | 45.2      |
| Online (streaming)               |                      |                     |                |           |
| Transformer                      | 5.0 / 11.6           |                     | 18.9           |           |
| ContextNet                       | 4.5 / 10.0           |                     | 31.4           |           |
| Conformer                        | 4.6 / 9.9            |                     | 30.7           |           |
| Centaurus (online, no attention) |                      |                     |                |           |
| Base (full SSM)                  | 6.0 / 13.1           | 5.9 / 13.1          | 12.4           | 20.6      |
| with FFN                         | 5.4 / 11.5           | 5.2 / 11.3          | 18.0           | 32.1      |
| with causal conv-block           | 4.8 / 10.6           | 4.8 / 10.5          | 23.6           | 43.7      |
| with Mamba macro-block           | 4.4 / 10.2           | 4.3 / 9.9           | 29.9           | 46.9      |"
307,"| Models               | Cosine Similarity | MSE    | MAE   | Pearson |
| -------------------- | ----------------- | ------ | ----- | ------- |
| Text-Image CLIPScore | 0.639             | 7.312  | 2.251 | 0.023   |
| InternVL-2.0-4B      | 0.736             | 15.962 | 3.165 | 0.083   |
| Anole                | 0.805             | 3.969  | 1.600 | 0.048   |
| GPT-4o               | 0.733             | 3.724  | 1.573 | 0.042   |
| Ours                 | 0.873             | 3.300  | 1.444 | 0.113   |"
283,"| Noise Strength                                                                | Classical Bounds                                                           | Quantum Bounds                                       | Speedup in  $ d $ |
| ----------------------------------------------------------------------------- | -------------------------------------------------------------------------- | ---------------------------------------------------- | ----------------- |
| $ \nu=\Omega(\epsilon^{1.5}) $                                                | Unsolvable (Jin et al., 2018a)                                             | Unsolvable (Theorem 3.2)                             | N/A               |
| $ \nu=O(\epsilon^{1.5}),\nu=\tilde{\Omega}(\epsilon^{1.5}/d) $                | $ O(\exp(d)),\Omega(d^{\log d}) $  (Jin et al., 2018a)                     | $ \Omega(d^{\log d}) $  (Theorem 3.1)                | N/A               |
| $ \nu=O(\epsilon^{1.5}/d),\nu=\tilde{\Omega}(\epsilon^{6}/d^{4}) $            | $ \tilde{O}(d^{4}/\epsilon^{5}) $  (Jin et al., 2018a; Zhang et al., 2017) | $ \tilde{O}(d^{2.5}/\epsilon^{3.5}) $  (Theorem 2.5) | Polynomial        |
| $ \nu=\tilde{O}(\epsilon^{6}/d^{4}),\nu=\tilde{\Omega}(\epsilon^{10}/d^{5}) $ | $ \Omega(d/\log d) $  (Theorem 3.3)                                        | $ O(\log^{4}d/\epsilon^{2}) $  (Theorem 2.3)         | Exponential       |
| $ \nu=\tilde{O}(\epsilon^{10}/d^{5}) $                                        | $ \Omega(d/\log d)^{*} $  (Theorem 3.3)                                    | $ O(\log d/\epsilon^{1.75}) $  (Theorem 2.2)         | Exponential*      |"
283,"| Noise Strength                                                            | Classical Bounds                               | Quantum Bounds                                 | Speedup in  $ d $ |
| ------------------------------------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ----------------- |
| $ \tilde{\nu}=\Omega(\epsilon) $                                          | Unsolvable (Theorem 3.4)                       | Unsolvable (Theorem 3.4)                       | N/A               |
| $ \tilde{\nu}=O(\epsilon), \tilde{\nu}=\tilde{\Omega}(\epsilon/d^{0.5}) $ | $ \Omega(d^{\log d}) $  (Theorem 3.4)          | $ \Omega(d^{\log d}) $  (Theorem 3.4)          | N/A               |
| $ \tilde{\nu}=\Theta(\epsilon/d^{0.5}) $                                  | $ O(d^{3}/\epsilon^{4}) $  (Jin et al., 2018a) | $ O(d^{2}/\epsilon^{3}) $  (Theorem 2.6)       | Polynomial        |
| $ \tilde{\nu}=O(\epsilon/d^{0.5+\zeta}) $                                 | $ O(\log^{4}d/\epsilon^{2}) $  (Corollary 2.4) | $ O(\log^{4}d/\epsilon^{2}) $  (Corollary 2.4) | No                |"
283,"| Input Oracle | Noise Strength                                     | Deterministic Classical Lower Bounds        | Randomized Classical and Quantum Lower Bounds |
| ------------ | -------------------------------------------------- | ------------------------------------------- | --------------------------------------------- |
| Zeroth-order | $ \nu = 0 $                                        | $ \Omega(\epsilon^{-12/7}) $                | N/A                                           |
| Zeroth-order | $ \nu = \Omega(\epsilon^{-16/7}/d) $               | (Carmon et al., 2021)                       | $ \Omega(\epsilon^{-12/7}) $  (Theorem 3.5)   |
| First-order  | $ \tilde{\nu} = 0 $                                | N/A                                         |                                               |
| First-order  | $ \tilde{\nu} = \Omega(\epsilon^{-8/7}/\sqrt{d}) $ | $ \Omega(\epsilon^{-12/7}) $  (Theorem 3.5) |                                               |"
284,"| Model       | Method      | IN-1K | Target |
| ----------- | ----------- | ----- | ------ |
| ResNet-18   | Pre-trained | 69.76 | -      |
| Standard FT | 19.58       | 89.07 |        |"
284,"| Model       | Method      | IN-1K | Target |
| ----------- | ----------- | ----- | ------ |
| ResNet-50   | Pre-trained | 79.02 | -      |
| Standard FT | 36.91       | 91.78 |        |"
284,"| Method                       | Commonsense | MMLU  | MBPP  | GSM8K | Average |
| ---------------------------- | ----------- | ----- | ----- | ----- | ------- |
| Pre-trained                  | 57.23       | 49.59 | 28.40 | 24.49 | 40.79   |
| Standard FT                  | 55.07       | 45.59 | 16.80 | 63.38 | 46.31   |
| WiSE-FT ( $ \alpha = 0.5 $ ) | 57.28       | 50.13 | 25.60 | 53.30 | 47.60   |
| LoRA ( $ r = 64 $ )          | 55.67       | 44.28 | 25.80 | 60.43 | 47.05   |
| $ \ell_{2} $  Reg.           | 57.01       | 48.43 | 24.80 | 62.85 | 49.19   |
| FLOW (Ours)                  | 57.59       | 49.31 | 26.80 | 62.55 | 49.98   |
| Pre-trained                  | 54.48       | 54.34 | 38.00 | 26.01 | 44.28   |
| Standard FT                  | 50.68       | 45.29 | 17.80 | 66.95 | 46.10   |
| WiSE-FT ( $ \alpha = 0.5 $ ) | 54.54       | 53.33 | 34.60 | 57.01 | 50.75   |
| LoRA ( $ r = 64 $ )          | 53.10       | 50.95 | 34.00 | 63.84 | 51.66   |
| $ \ell_{2} $  Reg.           | 53.60       | 51.28 | 33.60 | 66.87 | 52.30   |
| FLOW (Ours)                  | 54.30       | 51.86 | 36.00 | 65.58 | 52.87   |"
284,"| Method             | IN-1K | Target | Avg.  |
| ------------------ | ----- | ------ | ----- |
| Pre-trained        | 69.76 | -      | -     |
| Standard FT        | 19.58 | 89.07  | 54.60 |
| Linear Probe       | 69.76 | 73.57  | 71.63 |
| $ \ell_{2} $  Reg. | 34.78 | 88.12  | 61.45 |
| WiSE-FT            | 54.15 | 80.23  | 67.19 |
| FLOW (Ours)        | 65.21 | 83.93  | 74.57 |"
284,"| Method             | IN-1K | Target | Avg.  |
| ------------------ | ----- | ------ | ----- |
| Pre-trained        | 79.02 | -      | -     |
| Standard FT        | 36.91 | 91.78  | 64.34 |
| Linear Probe       | 79.02 | 76.45  | 77.73 |
| $ \ell_{2} $  Reg. | 44.78 | 91.58  | 68.18 |
| WiSE-FT            | 61.65 | 81.38  | 71.52 |
| FLOW (Ours)        | 76.09 | 86.25  | 81.17 |"
284,"| Method        | Commonsense | MMLU  | MBPP  | GSM8K | Avg.  |
| ------------- | ----------- | ----- | ----- | ----- | ----- |
| $ \ell_{2} $  | 57.01       | 48.43 | 24.80 | 62.85 | 49.19 |
| $ \ell_{2}+ $ | 57.53       | 49.38 | 26.60 | 62.02 | 49.79 |
| LoRA          | 55.67       | 44.28 | 25.80 | 60.43 | 47.05 |
| LoRA+         | 56.74       | 47.68 | 28.80 | 61.49 | 49.31 |"
287,"| Method                      | Objective  $ \mathcal{L} $                                                                                                                                        | $ \mathcal{C}(y_{w},y_{l},\pi_{\theta},\pi_{\mathrm{ref}}) $                                                                                                                                   |
| --------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| SLiC-HF (Zhao et al., 2023) | $ \max(0,\delta-\log\pi_{\theta}(y_{w}|x)+\log\pi_{\theta}(y_{l}|x)) $                                                                                            | $ f(x)=\begin{cases}1,&amp;\text{if}\log\pi_{\theta}(y_{w}|x)-\log\pi_{\theta}(y_{l}|x)&lt;\delta\\0,&amp;\text{if}\log\pi_{\theta}(y_{w}|x)-\log\pi_{\theta}(y_{l}|x)&gt;=\delta\end{cases} $ |
| DPO (Rafailov et al., 2023) | $ -\log\sigma\left(\beta\log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)}-\beta\log\frac{\pi_{\theta}(y_{l}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}\right) $ | $ \beta\cdot\sigma\left(\beta\log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}-\beta\log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}\right) $                         |
| IPO (Azar et al., 2023)     | $ \left(\log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{w}|x)}-\log\frac{\pi_{\theta}(y_{l}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}-\frac{1}{2\tau}\right)^{2} $  | $ 2\cdot\left(\frac{1}{2\tau}+\log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}-\log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{\mathrm{ref}}(y_{l}|x)}\right) $                             |
| SimPO (Meng et al., 2024)   | $ -\log\sigma\left(\frac{\beta}{|y_{w}|}\log\pi_{\theta}(y_{w}|x)-\frac{\beta}{|y_{l}|}\log\pi_{\theta}(y_{l}|x)-\gamma\right) $                                  | $ \sigma\left(\gamma+\frac{\beta}{|y_{l}|}\log\pi_{\theta}(y_{l}|x)-\frac{\beta}{|y_{w}|}\log\pi_{\theta}(y_{w}|x)\right) $                                                                    |"
287,"| Method | Wild-Bench | Arena-Hard | GSM8K    | MATH     | CRUX     | MMLU     | Average  |
| ------ | ---------- | ---------- | -------- | -------- | -------- | -------- | -------- |
| Elo    | Score      | LC (%)     | WR(%)    | Acc (%)  | Acc (%)  | Acc (%)  | Acc (%)  |
| Base   | 1160 (4)   | 42.7 (4)   | 54.9 (4) | 54.5 (4) | 87.9 (3) | 19.9 (3) | 44.6 (2) |
| DPO    | 1181 (2)   | 53.2 (2)   | 77.3 (2) | 81.3 (1) | 88.5 (1) | 20.2 (2) | 44.3 (3) |
| SimPO  | 1181 (2)   | 53.3 (1)   | 72.6 (3) | 75.4 (3) | 87.7 (4) | 18.2 (4) | 41.4 (4) |
| BNF    | 1186 (1)   | 53.2 (2)   | 77.5 (1) | 80.8 (2) | 88.0 (2) | 21.8 (1) | 45.3 (1) |"
291,"| Pretraining          | Continuous Training | PhoneNums   | InsNotes   |
| -------------------- | ------------------- | ----------- | ---------- |
| Method               | Supervision         | Supervision | Self-boost |
| V3                   | No                  | No          | No         |
| EC $ ^{2} $ -VAE (c) | Yes                 | No          | No         |
| EC $ ^{2} $ -VAE (c) | Yes                 | No          | Yes        |
| CNN Classifier       | Yes                 | No          | No         |
| CNN Classifier       | Yes                 | No          | Yes        |
| EC $ ^{2} $ -VAE (c) | Yes                 | Yes         | No         |
| CNN Classifier       | Yes                 | Yes         | No         |"
293,"| Dataset       | Dynamic Ans. | &gt;2-hop | Multi-modal | Human | Bilingual | Year |
| ------------- | ------------ | --------- | ----------- | ----- | --------- | ---- |
| VQNet         | ✗            | ✗         | ✗           | ✓     | ✗         | 2017 |
| CK-VQA        | ✗            | ✓         | ✗           | ✓     | ✗         | 2019 |
| InfSoK        | ✗            | ✓         | ✗           | ✗     | ✗         | 2023 |
| A-CKVQA       | ✗            | ✓         | ✗           | ✓     | ✗         | 2022 |
| Dyn-VQA (0ms) | ✓            | ✓         | ✓           | ✓     | ✓         | 2016 |"
293,"| Type                      | Count (%)   |
| ------------------------- | ----------- |
| Total Questions           | 1452        |
| Fast-updating Answer      | 385 (26.5%) |
| Multi-hop Reasoning       | 387 (26.7%) |
| External Visual Knowledge | 865 (59.6%) |
| Bilingual (ZH/EN)         | 737 / 715   |
| Avg. Q / A Length         | 12.5 / 4.3  |"
293,"| Model | Income | Outliers | Recon | Recon | Mean | Std Dev | Mean |
| ----- | ------ | -------- | ----- | ----- | ---- | ------- | ---- |"
294,"| Network and dataset   | Quantization scheme | Top-1 (%)      |
| --------------------- | ------------------- | -------------- |
| Ours                  | Best SoTA           | Full-precision |
| ResNet-18 on ImageNet | Binary              | 62.8           |
| Ternary               | 65.3                | 68.1           |
| ResNet-20 on CIFAR-10 | Binary              | 90.3           |
| Ternary               | 91.0                | --             |"
295,"| Model              | Tokenizer       | Objective           | Type              | Time $ \downarrow $ | Generative | Discriminative |
| ------------------ | --------------- | ------------------- | ----------------- | ------------------- | ---------- | -------------- |
| FID $ \downarrow $ | IS $ \uparrow $ | sFID $ \downarrow $ | Pre. $ \uparrow $ | Rec. $ \uparrow $   | ACC1       | ACC5           |
| LlamaGen           | VQGAN           | Cat.                | AR                | 0.13                | 3.81       | 248.28         |
| S0                 | B-AE            | Cat.                | AR                | 0.15                | 3.21       | 239.17         |
| S1                 | B-AE            | Cat.                | Mask              | 0.10                | 3.85       | 261.81         |
| S2                 | B-AE            | Bin.                | AR                | 1.04                | 7.50       | 164.31         |
| S3 (Ours)          | B-AE            | Bin.                | Mask              | 0.69                | 3.17       | 262.14         |"
296,"| Model             | Param (B) | FLOPs (G) | Val. loss $ \downarrow $ | GPQA $ \uparrow $ | TriviaQA $ \uparrow $ | BBH cot $ \uparrow $ | Hella Swag $ \uparrow $ | Wino Grande $ \uparrow $ | DROP $ \uparrow $ | Avg $ \uparrow $ |
| ----------------- | --------- | --------- | ------------------------ | ----------------- | --------------------- | -------------------- | ----------------------- | ------------------------ | ----------------- | ---------------- |
| Dense-151M        | 0.15      | 0.30      | 2.96                     | 19.98             | 12.67                 | 22.57                | 35.07                   | 52.49                    | 13.60             | 26.06            |
| PKM-151M-x12      | 2.04      | 0.35      | 2.76                     | 17.30             | 24.66                 | 23.14                | 42.25                   | 51.38                    | 13.10             | 28.64            |
| MoE-151M-2in32    | 2.04      | 0.35      | 2.63                     | 17.30             | 33.27                 | 23.24                | 48.44                   | 55.96                    | 18.57             | 33.20            |
| UltraMem-151M-x12 | 2.03      | 0.35      | 2.67                     | 19.42             | 28.97                 | 22.65                | 43.96                   | 50.83                    | 14.08             | 29.99            |
| Dense-680M        | 0.68      | 1.36      | 2.64                     | 21.09             | 27.16                 | 24.65                | 48.83                   | 54.93                    | 22.97             | 33.27            |
| PKM-680M-x12      | 8.95      | 1.50      | 2.46                     | 20.65             | 46.31                 | 26.97                | 57.32                   | 61.72                    | 25.20             | 39.70            |
| MoE-680M-2in33    | 8.95      | 1.50      | 2.39                     | 20.54             | 34.19                 | 26.63                | 62.71                   | 59.98                    | 26.54             | 38.43            |
| UltraMem-680M-x12 | 8.93      | 1.49      | 2.37                     | 21.99             | 55.17                 | 26.62                | 64.15                   | 60.54                    | 25.14             | 42.27            |
| Dense-1.6B        | 1.61      | 3.21      | 2.49                     | 21.76             | 39.65                 | 26.41                | 58.6                    | 61.72                    | 22.63             | 38.46            |
| PKM-1.6B-x12      | 21.13     | 3.48      | 2.34                     | 22.99             | 48.92                 | 28.98                | 65.45                   | 63.93                    | 27.55             | 42.97            |
| MoE-1.6B-2in34    | 21.36     | 3.52      | 2.30                     | 21.32             | 59.56                 | 29.46                | 67.34                   | 63.93                    | 28.81             | 45.07            |
| UltraMem-1.6B-x12 | 21.41     | 3.50      | 2.24                     | 24.66             | 66.38                 | 30.63                | 71.52                   | 66.38                    | 29.99             | 48.26            |
| Dense-6.5B        | 6.44      | 12.88     | 2.30                     | 19.98             | 57.28                 | 31.14                | 69.73                   | 65.9                     | 33.12             | 46.19            |"
299,"| Property | Mmultivariate time series anomaly detection | Time-frequency granularity alignment | Handle high-frequency information | Capture channel correlations |
| -------- | ------------------------------------------- | ------------------------------------ | --------------------------------- | ---------------------------- |
| SR-CNN   | ✗                                           | ✗                                    | ✗                                 | ✗                            |
| PFT      | ✗                                           | ✗                                    | ✗                                 | ✗                            |
| TFAD     | ✓                                           | ✗                                    | ✗                                 | ✗                            |
| Dual-TF  | ✓                                           | ✓                                    | ✗                                 | ✗                            |
| CATCH    | ✓                                           | ✓                                    | ✓                                 | ✓                            |"
299,"| Dataset    | Metric | CATCH | Modern | iTrans | DualITF | ATrans | DC    | TsNet | Patch | DLin  | NLin  | AE    | Ocsvm | IF    | PCA   | HBOS  | TFAD  |
| ---------- | ------ | ----- | ------ | ------ | ------- | ------ | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| CICIDS     | Aff-F  | 0.787 | 0.654  | 0.708  | 0.692   | 0.560  | 0.664 | 0.657 | 0.660 | 0.669 | 0.669 | 0.243 | 0.693 | 0.604 | 0.619 | 0.542 | 0.579 |
| A-R        | 0.795  | 0.697 | 0.692  | 0.603  | 0.528   | 0.638  | 0.732 | 0.716 | 0.751 | 0.691 | 0.629 | 0.537 | 0.787 | 0.601 | 0.760 | 0.504 |       |
| CalIt2     | Aff-F  | 0.835 | 0.780  | 0.812  | 0.751   | 0.729  | 0.697 | 0.794 | 0.793 | 0.793 | 0.757 | 0.587 | 0.783 | 0.402 | 0.768 | 0.756 | 0.744 |
| A-R        | 0.838  | 0.676 | 0.791  | 0.574  | 0.533   | 0.527  | 0.771 | 0.808 | 0.752 | 0.695 | 0.767 | 0.804 | 0.775 | 0.790 | 0.798 | 0.504 |       |
| Credit     | Aff-F  | 0.750 | 0.744  | 0.713  | 0.663   | 0.650  | 0.632 | 0.744 | 0.746 | 0.738 | 0.742 | 0.561 | 0.714 | 0.634 | 0.710 | 0.695 | 0.600 |
| A-R        | 0.958  | 0.957 | 0.934  | 0.703  | 0.552   | 0.504  | 0.957 | 0.957 | 0.954 | 0.948 | 0.909 | 0.953 | 0.860 | 0.871 | 0.951 | 0.500 |       |
| GECCO      | Aff-F  | 0.908 | 0.893  | 0.839  | 0.701   | 0.782  | 0.687 | 0.894 | 0.906 | 0.893 | 0.882 | 0.823 | 0.666 | 0.424 | 0.785 | 0.708 | 0.627 |
| A-R        | 0.970  | 0.952 | 0.795  | 0.714  | 0.516   | 0.555  | 0.954 | 0.949 | 0.947 | 0.936 | 0.769 | 0.804 | 0.619 | 0.711 | 0.557 | 0.499 |       |
| Genesis    | Aff-F  | 0.896 | 0.833  | 0.891  | 0.810   | 0.856  | 0.776 | 0.864 | 0.856 | 0.856 | 0.829 | 0.854 | 0.677 | 0.788 | 0.814 | 0.721 | 0.535 |
| A-R        | 0.974  | 0.676 | 0.690  | 0.937  | 0.947   | 0.659  | 0.913 | 0.685 | 0.696 | 0.755 | 0.931 | 0.733 | 0.549 | 0.815 | 0.897 | 0.497 |       |
| MSL        | Aff-F  | 0.740 | 0.726  | 0.710  | 0.588   | 0.692  | 0.694 | 0.734 | 0.724 | 0.725 | 0.723 | 0.625 | 0.641 | 0.584 | 0.678 | 0.680 | 0.665 |
| A-R        | 0.664  | 0.633 | 0.611  | 0.576  | 0.508   | 0.507  | 0.613 | 0.637 | 0.624 | 0.592 | 0.562 | 0.524 | 0.524 | 0.552 | 0.574 | 0.500 |       |
| NYC        | Aff-F  | 0.994 | 0.769  | 0.684  | 0.708   | 0.853  | 0.862 | 0.794 | 0.776 | 0.828 | 0.819 | 0.689 | 0.667 | 0.648 | 0.680 | 0.675 | 0.689 |
| A-R        | 0.816  | 0.466 | 0.640  | 0.633  | 0.671   | 0.549  | 0.791 | 0.709 | 0.768 | 0.671 | 0.504 | 0.456 | 0.475 | 0.666 | 0.446 | 0.502 |       |
| PSM        | Aff-F  | 0.859 | 0.825  | 0.854  | 0.725   | 0.710  | 0.682 | 0.842 | 0.831 | 0.831 | 0.843 | 0.707 | 0.531 | 0.620 | 0.702 | 0.658 | 0.628 |
| A-R        | 0.652  | 0.593 | 0.592  | 0.600  | 0.514   | 0.501  | 0.592 | 0.586 | 0.580 | 0.585 | 0.650 | 0.619 | 0.542 | 0.648 | 0.620 | 0.500 |       |
| SMD        | Aff-F  | 0.847 | 0.840  | 0.827  | 0.679   | 0.724  | 0.675 | 0.831 | 0.845 | 0.841 | 0.844 | 0.439 | 0.742 | 0.626 | 0.738 | 0.629 | 0.660 |
| A-R        | 0.811  | 0.722 | 0.745  | 0.631  | 0.508   | 0.502  | 0.727 | 0.736 | 0.728 | 0.738 | 0.774 | 0.602 | 0.664 | 0.679 | 0.626 | 0.500 |       |
| ASD        | Aff-F  | 0.804 | 0.782  | 0.780  | 0.605   | 0.674  | 0.702 | 0.800 | 0.777 | 0.782 | 0.766 | 0.731 | 0.617 | 0.781 | 0.656 | 0.669 | 0.630 |
| A-R        | 0.824  | 0.692 | 0.759  | 0.579  | 0.506   | 0.520  | 0.805 | 0.760 | 0.739 | 0.690 | 0.704 | 0.588 | 0.618 | 0.656 | 0.603 | 0.502 |       |
| Contextual | Aff-F  | 0.823 | 0.619  | 0.802  | 0.635   | 0.601  | 0.597 | 0.666 | 0.766 | 0.780 | 0.700 | 0.755 | 0.696 | 0.679 | 0.475 | 0.481 | 0.569 |
| A-R        | 0.910  | 0.562 | 0.905  | 0.598  | 0.546   | 0.525  | 0.908 | 0.854 | 0.700 | 0.530 | 0.896 | 0.711 | 0.821 | 0.538 | 0.464 | 0.504 |       |
| Global     | Aff-F  | 0.949 | 0.748  | 0.922  | 0.649   | 0.656  | 0.567 | 0.910 | 0.940 | 0.928 | 0.808 | 0.919 | 0.849 | 0.912 | 0.704 | 0.528 | 0.566 |
| A-R        | 0.997  | 0.873 | 0.976  | 0.595  | 0.564   | 0.514  | 0.989 | 0.992 | 0.979 | 0.675 | 0.996 | 0.996 | 0.938 | 0.758 | 0.608 | 0.500 |       |
| Seasonal   | Aff-F  | 0.997 | 0.681  | 0.992  | 0.776   | 0.788  | 0.859 | 0.992 | 0.989 | 0.993 | 0.951 | 0.927 | 0.805 | 0.938 | 0.637 | 0.673 | 0.686 |
| A-R        | 0.998  | 0.512 | 0.946  | 0.701  | 0.584   | 0.644  | 0.958 | 0.922 | 0.823 | 0.623 | 0.949 | 0.829 | 0.918 | 0.437 | 0.516 | 0.502 |       |
| Shapelet   | Aff-F  | 0.985 | 0.675  | 0.961  | 0.692   | 0.699  | 0.737 | 0.941 | 0.933 | 0.961 | 0.759 | 0.871 | 0.771 | 0.887 | 0.683 | 0.640 | 0.684 |
| A-R        | 0.970  | 0.522 | 0.864  | 0.573  | 0.519   | 0.597  | 0.877 | 0.818 | 0.684 | 0.563 | 0.865 | 0.655 | 0.748 | 0.517 | 0.337 | 0.503 |       |
| Trend      | Aff-F  | 0.916 | 0.734  | 0.901  | 0.677   | 0.584  | 0.765 | 0.897 | 0.888 | 0.721 | 0.830 | 0.699 | 0.691 | 0.914 | 0.693 | 0.669 | 0.642 |
| A-R        | 0.892  | 0.612 | 0.847  | 0.524  | 0.500   | 0.569  | 0.858 | 0.835 | 0.671 | 0.642 | 0.482 | 0.471 | 0.878 | 0.484 | 0.468 | 0.502 |       |
| Mixture    | Aff-F  | 0.892 | 0.856  | 0.862  | 0.652   | 0.641  | 0.709 | 0.863 | 0.879 | 0.727 | 0.839 | 0.673 | 0.676 | 0.881 | 0.676 | 0.667 | 0.710 |
| A-R        | 0.931  | 0.763 | 0.854  | 0.570  | 0.522   | 0.516  | 0.861 | 0.863 | 0.767 | 0.749 | 0.493 | 0.475 | 0.911 | 0.517 | 0.531 | 0.501 |       |"
300,"| Datasets | Cora (Homophily) | Citeseer (Homophily) |
| -------- | ---------------- | -------------------- |
| Models   | Grid             | Random               |
| MLP      | 59.41 ± 0.94     | 58.32 ± 1.21         |
| GCN      | 82.04 ± 0.96     | 81.52 ± 0.71         |
| SAGE     | 80.58 ± 1.04     | 80.49 ± 0.77         |
| APPNP    | 81.91 ± 0.90     | 81.69 ± 0.62         |
| GAT      | 81.10 ± 0.52     | 81.05 ± 0.82         |
| ChebNet  | 81.83 ± 0.46     | 81.51 ± 0.85         |
| H2GCN    | 82.05 ± 0.81     | 81.67 ± 0.71         |
| SGC      | 81.89 ± 0.94     | 82.09 ± 0.55         |
| GPRGNN   | 81.03 ± 0.65     | 81.34 ± 0.63         |
| MixHop   | 80.08 ± 1.43     | 79.39 ± 0.77         |"
300,"| Models       | GCN                  | SAGE                 |
| ------------ | -------------------- | -------------------- |
| Methods      | GNN-Diff             | p-diff               |
| Cora         | 82.33  $ \pm $  0.17 | 81.96  $ \pm $  0.31 |
| Actor        | 31.24  $ \pm $  0.26 | 30.96  $ \pm $  0.30 |
| Flickr       | 52.84  $ \pm $  0.04 | 52.70  $ \pm $  0.11 |
| PascalVOC-SP | 23.52  $ \pm $  0.08 | 23.49  $ \pm $  0.09 |"
302,"| POISSON SQUARE (CF. FIGURE 1) |
| ----------------------------- |
| MODEL                         |
| MA                            |
| UM2N                          |
| G-ADAPT                       |
| POISSON CONVEX POLYGON        |
| MA                            |
| UM2N                          |
| G-ADAPT                       |"
302,"| Viscous Burgers’ equation | Navier-Stokes equation |
| ------------------------- | ---------------------- |
| Model                     | Error Red. (%)         |
| MA                        | 25.78  $ \pm $  0.00   |
| UM2N                      | -11.24  $ \pm $  2.52  |
| G-Adapt                   | 27.17  $ \pm $  0.34   |"
302,"| SCALE    | MODEL   | ERROR RED. (%)                       | TIME (ms)   |
| -------- | ------- | ------------------------------------ | ----------- |
| 150X150  | MA      | 17.96  $ \pm $  25.70  $ \pm $  1.51 | 115395 2555 |
| 10X10X10 | MA      | 12.71  $ \pm $  0.00                 | 41049       |
|          | G-ADAPT | 28.08  $ \pm $  0.36                 | 494         |"
304,"| Model  | ADNI $ ^{1} $ | FICO $ ^{2} $ | LIFE $ ^{3} $ | NHANES $ ^{4} $ |
| ------ | ------------- | ------------- | ------------- | --------------- |
| AUC    | $ \rho $      | AUC           | $ \rho $      | AUC             |
| LR     | 72.0          | 64.1          | 79.1          | 75.4            |
| MA-LR  | 68.4          | 11.8          | 75.8          | 5.7             |
| DT     | 72.7          | 11.7          | 73.8          | 5.7             |
| MA-DT  | 74.1          | 0.1           | 73.8          | 5.7             |
| RF     | 75.0          | 66.1          | 77.3          | 65.4            |
| MA-RF  | 78.0          | 3.2           | 76.3          | 5.8             |
| GBT    | 79.0          | 60.9          | 78.4          | 69.3            |
| MA-GBT | 78.5          | 1.6           | 75.6          | 5.9             |"
305,"| Method                  | CO-DESIGN 1                  | PMPNN 1                    |
| ----------------------- | ---------------------------- | -------------------------- |
| DES-aa ( $ \uparrow $ ) | DIV-str/seq ( $ \uparrow $ ) | NOV-str ( $ \downarrow $ ) |
| Protpardelle*           | 30.00%                       | 15 / 26                    |
| ProteinGenerator        | 43.14%                       | 83 / 706                   |
| Multiflow*              | 62.74%                       | 134 / 1042                 |
| RFdiffusion             | N/A                          | N/A                        |
| Pallatom                | 85.03%                       | 291 / 1466                 |"
305,"| Length  | 150   | 200   | 250   | 300   | 350   | 400   |
| ------- | ----- | ----- | ----- | ----- | ----- | ----- |
| DES-aa  | 88%   | 93%   | 79%   | 93%   | 69%   | 81%   |
| DIV-str | 131   | 125   | 153   | 184   | 149   | 190   |
| NOV-str | 0.650 | 0.656 | 0.618 | 0.636 | 0.594 | 0.610 |
| HHH(%)  | 44%   | 53%   | 53%   | 64%   | 54%   | 70%   |
| HEL(%)  | 53%   | 44%   | 47%   | 36%   | 46%   | 30%   |
| EEE(%)  | 2.8%  | 2.7%  | 0.0%  | 0.4%  | 0.0%  | 0.4%  |"
305,"| Method   | DES-aa | DES-bb(w) | DES-bb(wo) |
| -------- | ------ | --------- | ---------- |
| atom14   | 87%    | 95%       | 95%        |
| hybrid14 | 5%     | 68%       | 82%        |
| woRC     | 21%    | 93%       | 95%        |"
306,"| Model                    | IFEval | FollowBench (SSR) |
| ------------------------ | ------ | ----------------- |
| P (L)                    | I (L)  | P (S)             |
| LLaMA3-8B Models         |        |                   |
| LLaMA3-8B-Instruct       | 77.6   | 84.5              |
| AutoIF-8B $ ^{\dagger} $ | 43.1   | 56.0              |
| SELF                     | 78.2   | 84.5              |
| Humpback                 | 72.5   | 80.2              |
| Self-Rewarding           | 77.3   | 84.2              |
| Meta-Rewarding           | 77.8   | 84.1              |
| SPAR-8B-SFT              | 75.4   | 82.5              |
| SPAR-8B-DPO-iter1        | 78.0   | 84.7              |
| SPAR-8B-DPO-iter2        | 78.9   | 85.0              |
| SPAR-8B-DPO-iter3        | 79.9   | 85.4              |
| w/ tree search           | 82.4   | 87.5              |"
306,"| GLM-4-9B-Chat     | 71.5 | 79.9 | 68.0 | 77.2 | 74.2 | 80.8 | 75.1 | 67.4 | 64.3 | 65.4 | 70.6 |
| ----------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| SPAR-9B-SFT       | 71.5 | 80.5 | 68.8 | 78.1 | 74.7 | 79.4 | 70.9 | 68.2 | 65.1 | 63.7 | 69.5 |
| SPAR-9B-DPO-iter3 | 77.3 | 84.1 | 73.6 | 81.4 | 79.1 | 82.7 | 76.7 | 67.9 | 68.3 | 64.2 | 72.0 |"
306,"| LLaMA3-70B-Instruct       | 83.7 | 88.9 | 77.1 | 83.8 | 83.4 | 77.1 | 72.5 | 69.4 | 68.7 | 66.3 | 70.8 |
| ------------------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| AutoIF-70B $ ^{\dagger} $ | 85.6 | 90.4 | 80.2 | 86.7 | 85.7 | 71.0 | 67.2 | 66.2 | 64.6 | 63.5 | 66.5 |
| SPAR-70B-DPO-iter3        | 85.6 | 90.2 | 81.3 | 87.3 | 86.1 | 80.3 | 75.7 | 71.4 | 73.7 | 70.5 | 74.3 |"
307,"| Statistic                         | Number   | Percentage |
| --------------------------------- | -------- | ---------- |
| Total questions                   | 20103    | -          |
| - Situational analysis            | 5005     | 24.89%     |
| - Project-based learning          | 11482    | 57.12%     |
| - Multi-step reasoning            | 3616     | 17.99%     |
| Total Categories/Fields/Subfields | 3/12/102 | -          |
| Formats:                          |          |            |
| - Multiple-Choice Questions       | 663      | 3.40%      |
| - Open-Ended Questions            | 19340    | 96.60%     |
| Questions with Images             | 20103    | 100%       |
| Questions with answer label       | 20103    | 100%       |
| Average question length           | 76.0     | -          |
| Average images per question       | 1.32     | -          |"
307,"| Question: How to Take Ex Method 1: Preparation...   |
| --------------------------------------------------- |
| Answer: The best way to take exam note is visual... |"
307,"| LVL M        | T2I Model   | Situational analysis | Project-based learning | Multi-step reasoning | AVG   |
| ------------ | ----------- | -------------------- | ---------------------- | -------------------- | ----- |
| GPT-4o       | Openjourney | 53.05                | 71.40                  | 53.67                | 63.65 |
| SD-3         | 53.00       | 71.20                | 63.52                  |                      |       |
| SD-XL        | 56.12       | 73.25                | 65.47                  |                      |       |
| Flux         | 54.97       | 68.80                | 62.63                  |                      |       |
| Gemini-1.5   | Openjourney | 48.08                | 67.93                  | 60.05                | 61.57 |
| SD-3         | 47.48       | 68.70                | 61.87                  |                      |       |
| SD-XL        | 49.43       | 71.85                | 64.15                  |                      |       |
| Flux         | 47.07       | 68.33                | 61.55                  |                      |       |
| LLaVA-34b    | Openjourney | 54.12                | 73.47                  | 47.28 $ ^{*} $       | 63.93 |
| SD-3         | 54.72       | 72.55                | 63.57                  |                      |       |
| SD-XL        | 55.97       | 74.60                | 65.05                  |                      |       |
| Flux         | 54.23       | 71.32                | 62.73                  |                      |       |
| Qwen2-VL-72b | Openjourney | 52.73                | 71.63                  | 55.63                | 64.05 |
| SD-3         | 54.98       | 71.87                | 64.75                  |                      |       |
| SD-XL        | 52.58       | 73.57                | 65.12                  |                      |       |
| Flux         | 54.23       | 69.47                | 63.18                  |                      |       |"
308,"|                | Glycolytic | Toggle   | Repressilator | AgeSIR   |
| -------------- | ---------- | -------- | ------------- | -------- |
| Vanilla NODE   | 1.51e-03   | 8.00e-04 | 2.25e-02      | 7.54e-03 |
| TayNODE        | 3.20e-03   | 1.37e-02 | 1.43e-01      | 3.18e-01 |
| Latent ODE 1   | 2.21e-01   | 6.57e-01 | 2.33e+01      | 4.18e+01 |
| ODE-RNN        | 8.80e-05   | 1.63e-03 | 1.41e-01      | 6.75e+00 |
| Latent ODE 2   | 1.00e-01   | 6.31e-01 | 7.47e-01      | 1.36e+09 |
| NCDE           | 3.49e-02   | 3.96e-02 | 1.51e+00      | 1.22e+01 |
| ResNet Flow    | 2.84e-01   | 7.09e-01 | 1.03e+01      | 2.50e+00 |
| GRU Flow       | 3.80e-01   | 2.45e+00 | 7.45e+00      | 4.19e+01 |
| VF-NODE (Ours) | 6.35e-05   | 1.69e-04 | 1.92e-02      | 7.39e-03 |"
308,"|                | Japan    | Italy    | Norway   | India    |
| -------------- | -------- | -------- | -------- | -------- |
| Vanilla NODE   | 1.42e+00 | 1.35e-02 | 1.03e-03 | 8.86e-04 |
| TayNODE        | 2.02e+00 | 3.88e-02 | 5.57e-04 | 1.18e-02 |
| Latent ODE 1   | 1.02e+01 | 6.56e-01 | 1.83e-01 | 5.69e-01 |
| ODE-RNN        | 8.43e+00 | 1.10e-01 | 5.58e-03 | 2.09e-01 |
| Latent ODE 2   | 1.12e+01 | 3.36e-01 | 4.12e-01 | 4.07e-01 |
| NCDE           | 1.14e+01 | 8.72e-01 | 1.27e-02 | 3.94e-01 |
| ResNet Flow    | 9.33e-01 | 3.69e-02 | 2.51e-02 | 2.04e-02 |
| GRU Flow       | 1.39e+00 | 8.63e-03 | 3.07e-03 | 2.52e-03 |
| VF NODE (Ours) | 1.87e-01 | 1.64e-03 | 3.43e-04 | 5.68e-04 |"
310,"| Model                     | Dataset         | ETTm1   | ETTm2 | Weather | Solar-Energy | Electricity | Traffic |
| ------------------------- | --------------- | ------- | ----- | ------- | ------------ | ----------- | ------- |
| Method                    | MSE             | MAE     | MSE   | MAE     | MSE          | MAE         | MSE     |
| Point forecasting         | NSformer(2022b) | 0.440   | 0.430 | 0.277   | 0.343        | 0.226       | 0.270   |
| TimesNet(2023)            | 0.374           | 0.387   | 0.249 | 0.309   | 0.219        | 0.261       | 0.296   |
| DLinear(2023)             | 0.380           | 0.389   | 0.284 | 0.362   | 0.237        | 0.296       | 0.320   |
| PatchTST(2023)            | 0.370           | 0.390   | 0.251 | 0.312   | 0.223        | 0.258       | 0.259   |
| SparseVQ(2024)            | 0.363           | 0.380   | 0.242 | 0.302   | 0.225        | 0.258       | 0.256   |
| iTransformer(2024)        | 0.377           | 0.391   | 0.250 | 0.309   | 0.221        | 0.254       | 0.233   |
| Probabilistic forecasting | TimeGrad(2021)  | 1.716   | 1.057 | 1.385   | 0.732        | 0.885       | 0.551   |
| CSDI(2021)                | 0.867           | 0.690   | 1.291 | 0.576   | 0.842        | 0.523       | 0.848   |
| TimeDiff(2023)            | 0.796           | 0.577   | 0.284 | 0.342   | 0.277        | 0.331       | 1.169   |
| TMDM(2024)                | 0.607           | 0.558   | 0.524 | 0.493   | 0.244        | 0.286       | 0.295   |
| ours                      | 0.363           | 0.386   | 0.241 | 0.302   | 0.222        | 0.264       | 0.237   |
| Model                     | Dataset         | ETTm1   | ETTm2 | Weather | Solar-Energy | Electricity | Traffic |
| Method                    | CRPS            | CRPSsum | CRPS  | CRPSsum | CRPS         | CRPSsum     | CRPS    |
| Probabilistic Forecasting | TimeGrad(2021)  | 0.665   | 0.996 | 0.785   | 1.051        | 0.482       | 0.503   |
| CSDI(2021)                | 0.773           | 0.852   | 0.625 | 0.782   | 0.508        | 0.465       | 0.649   |
| TimeDiff(2023)            | 0.454           | 0.846   | 0.316 | 0.180   | 0.293        | 0.400       | 0.900   |
| TMDM(2024)                | 0.429           | 0.633   | 0.380 | 0.226   | 0.226        | 0.292       | 0.375   |
| ours                      | 0.285           | 0.574   | 0.243 | 0.141   | 0.207        | 0.283       | 0.186   |"
310,"| Mode         | TMDM  | TMDM (D^{3}U) |
| ------------ | ----- | ------------- |
| Datasets     | MSE   | MAE           |
| ETTm1        | 0.607 | 0.558         |
| ETTm2        | 0.524 | 0.493         |
| Weather      | 0.244 | 0.286         |
| Solar-Energy | 0.295 | 0.317         |
| Electricity  | 0.222 | 0.329         |
| Traffic      | 0.721 | 0.411         |"
310,"| Ablation Study                        | Mode ( $ f_{\theta} + g_{\phi} $ ) | ETTm1 | Solar-Energy | Traffic |
| ------------------------------------- | ---------------------------------- | ----- | ------------ | ------- |
| MSE                                   | MAE                                | CRPS  | MSE          | MAE     |
| Denoise Network                       | SVQ + MLP $ ^{a} $                 | 0.372 | 0.396        | 0.294   |
| SVQ + UNet $ ^{b} $                   | 0.385                              | 0.410 | 0.301        | 0.267   |
| Structure Design                      | SVQ + PatchDN(CAttn) $ ^{c} $      | 0.370 | 0.390        | 0.295   |
| SVQ + PatchDN(InC) $ ^{d} $           | 0.366                              | 0.385 | 0.290        | 0.238   |
| Framework Design                      | SVQ + PatchDN(All) $ ^{e} $        | 0.859 | 0.699        | 0.516   |
| SVQ + PatchDN( $ \hat{y} $ ) $ ^{f} $ | 0.408                              | 0.421 | 0.312        | 0.259   |
|                                       | Ours                               | 0.361 | 0.385        | 0.284   |"
310,"| Dataset           | ETTm1 | Solar-Energy | Traffic |
| ----------------- | ----- | ------------ | ------- |
| Method            | MSE   | MAE          | CRPS    |
| NSformer          | 0.440 | 0.430        | -       |
| NSformer (D^{3}U) | 0.436 | 0.427        | 0.317   |
| PatchTST          | 0.370 | 0.390        | -       |
| PatchTST (D^{3}U) | 0.387 | 0.405        | 0.299   |
| SparseVQ          | 0.363 | 0.380        | -       |
| SparseVQ (D^{3}U) | 0.361 | 0.385        | 0.284   |"
316,"| Category           | Method  | GSM8K    | GSM-Hard | SVAMP    | StrategyQA |
| ------------------ | ------- | -------- | -------- | -------- | ---------- |
| Result (%)         | RAP     | 80.7     | 32.7     | 87.9     | 73.4       |
| Soft Reasoning     | 84.3    | 35.7     | 90.2     | 75.6     |            |
| Input Token Count  | RAP     | 25710.8k | 33152.1k | 15058.5k | 17426.2k   |
| Soft Reasoning     | 1457.1k | 1847.8k  | 1172.8k  | 1180.6k  |            |
| Output Token Count | RAP     | 334.1k   | 402.5k   | 241.2k   | 274.1k     |
| Soft Reasoning     | 211.9k  | 262.5k   | 162.4k   | 155.4k   |            |
| Time (min)         | RAP     | 184.5    | 234.1    | 142.5    | 149.7      |
| Soft Reasoning     | 23.2    | 28.4     | 18.4     | 17.4     |            |"
326,"|     | $ V_{1} $   | ... | $ X $     | ... | $ Y $     | ... | $ V_{m-1} $   | $ V_{m} $   |
| --- | ----------- | --- | --------- | --- | --------- | --- | ------------- | ----------- |
| 1   | $ V_{1,1} $ | ... | $ X_{1} $ | ... | $ y_{1} $ | ... | $ V_{1,m-1} $ | $ V_{1,m} $ |
| 2   | $ V_{2,1} $ | ... | $ X_{2} $ | ... | $ y_{2} $ | ... | $ V_{2,m-1} $ | $ V_{2,m} $ |
| ... | ...         | ... | ...       | ... | ...       | ... | ...           | ...         |
| n   | $ V_{n,1} $ | ... | $ X_{n} $ | ... | $ y_{n} $ | ... | $ V_{n,m-1} $ | $ V_{n,m} $ |"
311,"| Models            | SummEval (Coh)  | NovelEval    | CaTeRS      |
| ----------------- | --------------- | ------------ | ----------- |
| H.                | $ s_{tran}(5) $ | $ s_{comm} $ | $ s_{neg} $ |
| Direct Judgements |                 |              |             |
| Llama-2-7B        | 57.5            | 88.3         | 57.5        |
| Llama-2-13B       | 58.3            | 86.6         | 59.3        |
| Llama-3-8B        | 67.8            | 91.0         | 76.1        |
| Mistral-7B        | 63.6            | 95.1         | 59.9        |
| Zephyr-7B-beta    | 61.3            | 87.8         | 52.8        |
| Phi-3-mini        | 65.6            | 92.8         | 66.9        |
| Phi-3-medium      | 68.8            | 96.2         | 71.0        |
| Gemma-2-9B        | 73.8            | 94.8         | 78.1        |
| GPT-3.5-0125      | 66.3            | 82.5         | 67.5        |
| Deepseek-chat     | 72.7            | 93.1         | 72.8        |
| CoT Prompting     |                 |              |             |
| Llama-2-7B        | 55.0            | 42.6         | 45.7        |
| Llama-2-13B       | 66.6            | 55.8         | 63.2        |
| Llama-3-8B        | 67.2            | 65.0         | 64.8        |
| Mistral-7B        | 61.6            | 64.7         | 65.3        |
| Zephyr-7B-beta    | 58.4            | 46.4         | 51.7        |
| Phi-3-mini        | 65.1            | 67.9         | 61.9        |
| Phi-3-medium      | 69.1            | 82.3         | 72.5        |
| Gemma-2-9B        | 73.5            | 74.0         | 71.9        |
| GPT-3.5-0125      | 63.9            | 63.3         | 64.8        |
| Deepseek-chat     | 74.9            | 79.4         | 76.1        |"
311,"| Data                | $ s_{tran}(5) $ | $ s_{comm} $ | # Data | AvgComp/Inst |
| ------------------- | --------------- | ------------ | ------ | ------------ |
| Raw data            | 98.4            | -            | 14.8K  | 6.29         |
| Perturbed data      | 87.6            | -            | 14.8K  | 6.29         |
| REPAIR-ed data      | 1               | 1            | 30.9K  | 13.2         |
| REPAIR-ed data+Neg. | 1               | 1            | 61.8K  | 26.4         |"
311,"| Models                | Summarize from Feedback |
| --------------------- | ----------------------- |
| H.                    | $ s_{tran}(5) $         |
| Zero-shot inference   | 64.3                    |
| Perturbed data        | 70.3                    |
| REPAIR-ed data        | 70.1                    |
| REPAIR-ed data + Neg. | 69.7                    |"
311,"| Models        | SummEval (Coh)  |
| ------------- | --------------- |
| H.            | $ s_{tran}(5) $ |
| Mistral-7B    | 63.6            |
| Phi-3-mini    | 65.6            |
| GPT-3.5-turbo | 66.3            |
| Llama-3-8B    | 67.8            |
| Phi-3-medium  | 68.8            |"
312,"| Method                 | Natural Images             | Medical                        | Agriculture                 | Remote Sensing      |
| ---------------------- | -------------------------- | ------------------------------ | --------------------------- | ------------------- |
| $ S_{\alpha}\uparrow $ | CAMO  $ E_{\phi}\uparrow $ | $ F_{\beta}^{\omega}\uparrow $ | ISIC 2017 Jac  $ \uparrow $ | Dice  $ \uparrow $  |
| SAM decoder-only       | 79.7±0.02 84.9±0.38        | 88.8±0.09 92.7±0.34            | 79.6±0.01 81.8±0.33         | 61.0±0.12 85.9±0.34 |
| BitFit                 | 87.5±0.13                  | 94.5±0.08                      | 85.3±0.48                   | 87.7±0.14           |
| AdaptFormer            | 87.9±0.10                  | 94.8±0.21                      | 86.2±0.19                   | 87.6±0.24           |
| LoRA                   | 87.7±0.59                  | 94.6±0.50                      | 85.1±0.64                   | 87.8±0.24           |
| Adapter                | 88.2±0.44                  | 94.8±0.34                      | 86.7±0.92                   | 87.7±0.23           |
| HQ-SAM                 | 85.1±0.10                  | 92.6±0.10                      | 81.0±0.61                   | 86.3±0.32           |
| SU-SAM                 | 88.3±0.21                  | 95.0±0.22                      | 86.2±0.59                   | 87.8±0.18           |
| ConvLoRA-SAM           | 87.5±0.39                  | 94.5±0.17                      | 85.4±0.41                   | 87.7±0.22           |
| LoRA+Ours              | 88.3±0.05                  | 95.2±0.00                      | 85.8±0.59                   | 88.1±0.08           |
| Adapter+Ours           | 88.6±0.09                  | 95.1±0.05                      | 87.1±0.37                   | 88.0±0.05           |"
312,"| L_{r}               | L_{d}      | Medical     | Agriculture | Remote Sensing |
| ------------------- | ---------- | ----------- | ----------- | -------------- |
| S_{\alpha} (Kvasir) | IoU (Leaf) | IoU (Road)  |             |                |
|                     |            | 93.4        | 74.4        | 60.5           |
|                     | ✓          | 93.6 (+0.2) | 75.2 (+0.8) | 61.0 (+0.5)    |
| ✓                   | ✓          | 94.4 (+1.0) | 75.6 (+1.2) | 61.4 (+0.9)    |"
312,"| METHOD    | FROZEN                   | MEDICAL             | AGRICULTURE         |
| --------- | ------------------------ | ------------------- | ------------------- |
| RM FROM   | $ S_{\alpha} $  (Kvasir) | IoU (Leaf)          |                     |
| InfoSAM   | -                        | 94.4  $ \pm $  0.12 | -                   |
| InfoSAM-T | Leaf                     | 93.7  $ \pm $  0.24 | -                   |
| InfoSAM   | -                        | -                   | 75.6  $ \pm $  0.30 |
| InfoSAM-T | Kvasir                   | -                   | 75.4  $ \pm $  0.45 |"
313,"| Tactic | Candidate theorems | Stage one | Stage two | Expansion     | Conversion Ratio |
| ------ | ------------------ | --------- | --------- | ------------- | ---------------- |
| rw     | 110,657            | 5,081,544 | 2,830,817 | $ \times $ 25 | 56%              |
| apply  | 78,871             | 9,483,504 | 3,495,832 | $ \times $ 44 | 37%              |"
318,"| Methods                                              | GQA  | SQA   | MMMU | TextVQA | Average |
| ---------------------------------------------------- | ---- | ----- | ---- | ------- | ------- |
| Acc.                                                 | Skip | Acc.  | Skip | Acc.    | Skip    |
| LLaVA-HR (Luo et al., 2024)                          | 64.2 | 0%    | 67.9 | 0%      | 34.6    |
| MoD layer:                                           |      |       |      |         |         |
| All layers                                           | 45.9 | 38.2% | 42.6 | 33.7%   | 25.9    |
| 1 MoD per 2 layers                                   | 57.8 | 19.1% | 52.3 | 16.5%   | 26.9    |
| 2 MoDs per 3 layers                                  | 38.1 | 26.8% | 46.5 | 24.6%   | 24.3    |
| ARank-based deployment                               | 63.7 | 40.7% | 68.5 | 35.9%   | 35.6    |
| Masked token:                                        |      |       |      |         |         |
| None                                                 | 63.2 | 52.0% | 66.8 | 46.9%   | 33.9    |
| Q                                                    | 63.7 | 40.7% | 68.5 | 35.9%   | 35.6    |
| Q + A                                                | 62.8 | 38.8% | 68.6 | 30.5%   | 34.7    |
| Shared router:                                       |      |       |      |         |         |
| Not Share                                            | 60.6 | 55.8% | 64.5 | 48.2%   | 32.1    |
| Share                                                | 63.1 | 60.3% | 67.9 | 56.9%   | 34.7    |
| Threshold:                                           |      |       |      |         |         |
| $ \delta_{s} $                                       | 63.1 | 60.3% | 67.9 | 56.9%   | 34.7    |
| $ \delta_{s} \cdot (1 + \log(\text{SE}/\text{SE})) $ | 62.9 | 56.3% | 67.7 | 55.8%   | 35.4    |
| $ \delta_{s} \cdot (1 + \log(\text{AR}/\text{AR})) $ | 63.2 | 57.9% | 68.1 | 54.2%   | 34.3    |
| Routing ratio:                                       |      |       |      |         |         |
| 17%                                                  | 63.6 | 18.9% | 68.9 | 15.5%   | 34.7    |
| 34%                                                  | 63.7 | 40.7% | 68.5 | 35.9%   | 35.6    |
| 51%                                                  | 63.1 | 60.3% | 67.9 | 56.9%   | 34.7    |
| 68%                                                  | 59.1 | 77.8% | 70.1 | 73.5%   | 33.7    |"
347,"| Method                   | Mortality     | Readmission   |
| ------------------------ | ------------- | ------------- |
| AUC-ROC                  | AUC-PRC       | AUC-ROC       |
| CM-AE (ICML &#x27;11)    | 0.7873 ± 0.40 | 0.3620 ± 0.22 |
| SMIL (AAAI &#x27;21)     | 0.7981 ± 0.11 | 0.3536 ± 0.12 |
| MT (CVPR &#x27;22)       | 0.8176 ± 0.10 | 0.3467 ± 0.06 |
| Grape (NeurIPS &#x27;20) | 0.7657 ± 0.16 | 0.3733 ± 0.09 |
| M3Care (SIGKDD &#x27;22) | 0.8265 ± 0.09 | 0.3830 ± 0.07 |
| ShaSpec (CVPR &#x27;23)  | 0.8100 ± 0.13 | 0.3630 ± 0.09 |
| MUSE (ICLR&#x27;24)      | 0.8236 ± 0.09 | 0.3987 ± 0.05 |
| EBR (Ours)               | 0.8533 ± 0.09 | 0.4277 ± 0.02 |"
313,"| Methods                                          | random        | novel_premises | Search Budget     |
| ------------------------------------------------ | ------------- | -------------- | ----------------- |
| tidy                                             | 23.8          | 5.3            | -                 |
| GPT-4                                            | 29.0          | 7.4            | 1  $ \times $  35 |
| Reprover (Yang et al., 2023)                     | 47.6          | 23.2           | 1  $ \times $  64 |
| w/ retrieval                                     | 51.2          | 26.3           | 1  $ \times $  64 |
| llmstep (Pythia 2.8b) (Welleck &amp; Saha, 2023) | 47.6          | -              | 1  $ \times $  32 |
|                                                  | 50.1          | -              | 2  $ \times $  32 |
| Llama3-8b                                        | 58.22         | 38.52          | 1  $ \times $  32 |
| Mathlib-train + rw                               | 59.62 (+1.40) | 42.13 (+3.62)  | 1  $ \times $  32 |
| Mathlib-train + apply                            | 58.84 (+0.62) | 41.29 (+2.77)  | 1  $ \times $  32 |
| Mathlib-train + rw + apply                       | 59.82 (+1.60) | 43.22 (+4.70)  | 1  $ \times $  32 |
| deepseek-coder-7b-base-v1.5                      | 57.7          | 39.24          | 1  $ \times $  32 |
| Mathlib-train + rw                               | 59.25 (+1.55) | 42.98 (+3.74)  | 1  $ \times $  32 |
| Mathlib-train + apply                            | 58.68 (+0.98) | 40.51 (+1.27)  | 1  $ \times $  32 |
| Mathlib-train + rw + apply                       | 60.39 (+2.69) | 43.46 (+4.22)  | 1  $ \times $  32 |"
313,"| Methods                    | miniF2F-test  | Correct/Total | rw    | apply | norm_num | linarith |
| -------------------------- | ------------- | ------------- | ----- | ----- | -------- | -------- |
| Mathlib-train              | 34.01         | 83/244        | 16.10 | 0.00  | 27.12    | 16.95    |
| Mathlib-train + rw         | 35.24         | 86/244        | 18.75 | 0.78  | 14.84    | 21.88    |
| Mathlib-train + apply      | 36.07         | 88/244        | 8.87  | 2.42  | 20.16    | 15.63    |
| Mathlib-train + rw + apply | 36.48 (+2.47) | 89/244        | 12.31 | 0.77  | 26.92    | 16.92    |"
314,"| Dataset                   | ETH/UCY                  | SDD                 | NBA                  | JRDB                |
| ------------------------- | ------------------------ | ------------------- | -------------------- | ------------------- |
| Previous SOTA (ADE/FDE)   | SocialCircle (0.20/0.33) | TUTR (7.90/12.96)   | LED (0.82/1.15)      | LED (0.18/0.28)     |
| Speed / # person in scene | $ \sim $ 17.4ms / 57     | -                   | $ \sim $ 65.7ms / 11 | $ \sim $ 118ms / 80 |
| Ours                      | 0.19/0.32                | 7.20/11.29          | 0.75/0.97            | 0.15/0.23           |
| Speed / # person in scene | $ \sim $ 9.8ms / 57      | $ \sim $ 5.6ms / 14 | $ \sim $ 19.3ms / 11 | $ \sim $ 6.8ms / 80 |"
314,"| Noise Type                         | ADE                               | FDE                               |
| ---------------------------------- | --------------------------------- | --------------------------------- |
| Gaussian Noise (100%, scale = 0.1) | 0.27 (4 frames) / 0.28 (9 frames) | 0.49 (4 frames) / 0.50 (9 frames) |
| Gaussian Noise (100%, scale = 0.2) | 0.32 (4 frames) / 0.34 (9 frames) | 0.55 (4 frames) / 0.57 (9 frames) |
| Gaussian Noise (100%, scale = 0.5) | 0.71 (4 frames) / 0.88 (9 frames) | 1.18 (4 frames) / 1.44 (9 frames) |
| Dropped History (20%, 5 frames)    | 0.26                              | 0.48                              |
| Dropped History (50%, 5 frames)    | 0.26                              | 0.49                              |
| Dropped History (50%, 7 frames)    | 0.29                              | 0.52                              |
| Dropped History (80%, 7 frames)    | 0.30                              | 0.54                              |
| Ours (full performance)            | 0.26                              | 0.48                              |"
315,"| Model           | SHD            | Model               | SHD            |
| --------------- | -------------- | ------------------- | -------------- |
| ADAM            | 2  $ \pm $  2  | CDHRL               | 10  $ \pm $  4 |
| ADAM w/o TM, SD | 19  $ \pm $  6 | CDHRL w/ SD         | 6  $ \pm $  2  |
| Reflexion       | 24  $ \pm $  9 | React w/ TM, SD     | 5  $ \pm $  2  |
| AutoGPT         | 24  $ \pm $  6 | Reflexion w/ TM, SD | 4  $ \pm $  2  |
| Empty Graph     | 32             | AutoGPT w/ TM, SD   | 4  $ \pm $  2  |"
315,"| Framework             | Wooden Tool           | Stone Tool            | Iron Tool            | Diamond               |
| --------------------- | --------------------- | --------------------- | -------------------- | --------------------- |
| React w/ TM w/ SD     | 91  $ \pm $  34(2/3)  | 139(1/3)              | N/A (0/3)            | N/A (0/3)             |
| Reflexion w/ TM w/ SD | 76  $ \pm $  28(2/3)  | 120  $ \pm $  40(2/3) | N/A (0/3)            | N/A (0/3)             |
| AutoGPT w/ TM w/ SD   | 82  $ \pm $  25(2/3)  | 124(1/3)              | N/A (0/3)            | N/A (0/3)             |
| VOYAGER               | 95  $ \pm $  33(2/3)  | 152  $ \pm $  43(2/3) | N/A (0/3)            | N/A (0/3)             |
| VOYAGER Guided        | 108  $ \pm $  35(2/3) | 176(1/3)              | N/A (0/3)            | N/A (0/3)             |
| ADAM                  | 28  $ \pm $  4(3/3)   | 52  $ \pm $  14(3/3)  | 94  $ \pm $  27(3/3) | 109  $ \pm $  34(2/3) |
| ADAM Parallel         | 15  $ \pm $  2(3/3)   | 31  $ \pm $  7(3/3)   | 54  $ \pm $  14(3/3) | 61  $ \pm $  18(2/3)  |"
316,"| Model              | Method   | GSM8K     | GSM-Hard | SVAMP     | StrategyQA |
| ------------------ | -------- | --------- | -------- | --------- | ---------- |
| Zero Shot          | Few Shot | Zero Shot | Few Shot | Zero Shot | Few Shot   |
| LLaMA3-8B-Ins      | COT      | 53.0±0.0  | 77.4±0.0 | 14.0±0.0  | 28.0±0.0   |
| SC( $ \tau=0.4 $ ) | 73.0±1.6 | 80.4±1.4  | 25.7±0.4 | 31.8±1.8  | 79.1±1.2   |
| SC( $ \tau=0.6 $ ) | 73.6±2.5 | 80.6±1.5  | 24.5±1.1 | 31.2±1.3  | 76.1±3.9   |
| SC( $ \tau=0.8 $ ) | 65.0±2.0 | 81.1±1.1  | 21.8±1.3 | 30.8±0.9  | 69.6±2.0   |
| FIRE               | 73.8±2.3 | 79.6±2.9  | 25.2±3.0 | 25.7±2.1  | 81.5±0.8   |
| CoT-Decoding       | 73.9±1.9 | 80.3±1.7  | 24.8±1.3 | 30.3±1.3  | 83.2±1.2   |
| RAP                | -        | 80.7±1.4  | -        | 32.7±1.2  | -          |
| Ours               | 79.4±1.2 | 84.3±1.4  | 28.2±1.8 | 35.7±1.0  | 88.2±1.3   |
| Qwen2-7B-Ins       | COT      | 64.5±0.0  | 82.5±0.0 | 40.0±0.0  | 55.5±0.0   |
| SC( $ \tau=0.4 $ ) | 81.2±0.6 | 85.7±1.5  | 47.5±1.4 | 55.4±0.7  | 72.3±2.0   |
| SC( $ \tau=0.6 $ ) | 80.2±1.9 | 85.4±0.9  | 46.2±1.9 | 53.4±0.6  | 77.3±1.2   |
| SC( $ \tau=0.8 $ ) | 80.0±0.9 | 85.1±1.6  | 47.3±1.3 | 55.4±0.9  | 78.6±2.1   |
| FIRE               | 81.0±1.8 | 83.0±1.3  | 45.1±2.0 | 51.0±1.8  | 76.3±2.2   |
| CoT-Decoding       | 82.0±2.8 | 84.5±2.1  | 46.7±2.3 | 52.1±1.0  | 78.6±1.6   |
| RAP                | -        | 86.2±1.2  | -        | 56.2±0.8  | -          |
| Ours               | 88.6±1.2 | 90.0±1.4  | 53.7±1.6 | 58.7±0.5  | 83.4±2.4   |
| Mistral-7B-Ins     | COT      | 42.0±0.0  | 54.0±0.0 | 14.5±0.0  | 24.0±0.0   |
| SC( $ \tau=0.4 $ ) | 52.9±0.5 | 58.3±1.5  | 19.5±1.0 | 26.1±1.5  | 67.4±2.5   |
| SC( $ \tau=0.6 $ ) | 55.1±3.6 | 57.4±1.0  | 20.7±1.5 | 25.3±1.6  | 69.7±1.6   |
| SC( $ \tau=0.8 $ ) | 50.2±2.6 | 57.7±2.6  | 19.1±2.0 | 26.6±1.1  | 68.3±0.9   |
| FIRE               | 47.2±2.9 | 56.1±3.2  | 18.1±1.9 | 26.3±1.4  | 67.1±1.9   |
| CoT-Decoding       | 47.3±3.0 | 58.2±2.3  | 16.6±0.7 | 27.4±1.6  | 69.4±2.5   |
| RAP                | -        | 58.6±1.8  | -        | 27.6±1.2  | -          |
| Ours               | 61.4±2.5 | 62.7±1.0  | 25.8±1.8 | 32.5±1.5  | 72.2±2.2   |"
316,"| Shot | Iteration        | GSM8K            | GSM-Hard         | SVAMP            | StrategyQA       |
| ---- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- |
| Zero | 1                | 65.0 $ \pm $ 2.9 | 70.3 $ \pm $ 3.0 | 64.8 $ \pm $ 1.9 | 58.8 $ \pm $ 2.3 |
| 2    | 30.1 $ \pm $ 2.9 | 24.4 $ \pm $ 3.0 | 28.9 $ \pm $ 1.8 | 31.1 $ \pm $ 2.3 |                  |
| 3    | 4.1 $ \pm $ 1.6  | 4.9 $ \pm $ 1.6  | 5.4 $ \pm $ 1.3  | 8.8 $ \pm $ 1.8  |                  |
| 4    | 0.8 $ \pm $ 0.8  | 0.4 $ \pm $ 0.3  | 0.9 $ \pm $ 0.5  | 1.4 $ \pm $ 0.5  |                  |
| Few  | 1                | 76.8 $ \pm $ 2.2 | 77.4 $ \pm $ 2.2 | 79.7 $ \pm $ 2.0 | 66.7 $ \pm $ 3.7 |
| 2    | 20.7 $ \pm $ 0.8 | 20.6 $ \pm $ 2.7 | 19.1 $ \pm $ 2.8 | 26.6 $ \pm $ 3.7 |                  |
| 3    | 2.5 $ \pm $ 1.4  | 1.8 $ \pm $ 0.6  | 1.2 $ \pm $ 1.0  | 6.1 $ \pm $ 2.1  |                  |
| 4    | 0.0 $ \pm $ 0.0  | 0.2 $ \pm $ 0.3  | 0.0 $ \pm $ 0.0  | 0.6 $ \pm $ 0.4  |                  |"
318,"| Methods               | Training Time  $ \downarrow $ | Inference Throughput  $ \uparrow $ | Inference Memory  $ \downarrow $ | Inference TFlops  $ \downarrow $ | Avg. Acc.  $ \uparrow $ |
| --------------------- | ----------------------------- | ---------------------------------- | -------------------------------- | -------------------------------- | ----------------------- |
| LLaVA-HR              | 20.7 h                        | 4.7 samples/s                      | 19 G                             | 19.2                             | 58.5                    |
| + $ \gamma $ -MoD-0.3 | 15.4 h                        | 5.9 samples/s                      | 15 G                             | 12.6                             | 58.3                    |
| + $ \gamma $ -MoD-0.5 | 14.3 h                        | 7.2 samples/s                      | 14 G                             | 9.3                              | 57.6                    |
| Gains                 | -31.0%                        | +53.2%                             | -26.3%                           | -51.6%                           | -1.5%                   |"
318,"| Model                             | Training Time Reduction | Inference Time Reduction | Accuracy |
| --------------------------------- | ----------------------- | ------------------------ | -------- |
| $ \gamma $ -MoD-LLaVA-HR-7B       | 31.0%                   | 53.2%                    | -1.5%    |
| $ \gamma $ -MoD-Mini-Gemini-HD-7B | 41.0%                   | 58.1%                    | -1.0%    |"
319,"|                                    | Mutag                 | NCI1                  | MCF-7H                | Ogbg-Molhiv           |
| ---------------------------------- | --------------------- | --------------------- | --------------------- | --------------------- |
| Train loss                         | 1.890  $ \pm $  0.078 | 0.535  $ \pm $  0.007 | 0.327  $ \pm $  0.026 | 0.144  $ \pm $  0.006 |
| Test loss                          | 1.546  $ \pm $  0.054 | 0.588  $ \pm $  0.151 | 0.325  $ \pm $  0.010 | 0.151  $ \pm $  0.005 |
| Gener. gap                         | 0.344  $ \pm $  0.095 | 0.053  $ \pm $  0.151 | 0.002  $ \pm $  0.028 | 0.007  $ \pm $  0.008 |
| Our bound (opt.  $ \varepsilon $ ) | 0.705                 | 0.060                 | 0.007                 | 0.015                 |
| Our bound ( $ \varepsilon = 0 $ )  | 0.946                 | 0.079                 | 0.008                 | 0.019                 |"
320,"| Method               | CDR-H1                 | CDR-H2                     | CDR-H3               |
| -------------------- | ---------------------- | -------------------------- | -------------------- |
| AAR(%)  $ \uparrow $ | scRMSD  $ \downarrow $ | Plausibility  $ \uparrow $ | AAR(%)  $ \uparrow $ |
| Grafting             | 58.05                  | 0.83                       | -0.597               |
| ProteinMPNN          | 58.58                  | 0.64                       | -0.603               |
| ESM-IF1              | 53.80                  | 0.66                       | -0.610               |
| Diffab-fix           | 74.93                  | 0.66                       | -0.512               |
| AbMPNN*              | 72.83                  | 1.09                       | -0.664               |
| RADAb                | 76.57                  | 0.61                       | -0.505               |
| Method               | CDR-L1                 | CDR-L2                     | CDR-L3               |
| AAR(%)  $ \uparrow $ | scRMSD  $ \downarrow $ | Plausibility  $ \uparrow $ | AAR(%)  $ \uparrow $ |
| Grafting             | 68.53                  | 0.85                       | -0.506               |
| ProteinMPNN          | 45.60                  | 0.59                       | -0.612               |
| ESM-IF1              | 40.97                  | 0.61                       | -0.650               |
| Diffab-fix           | 79.78                  | 0.56                       | -0.386               |
| AbMPNN*              | 75.06                  | 0.73                       | -0.543               |
| RADAb                | 83.72                  | 0.54                       | -0.379               |"
320,"| Method      | $ \Delta\Delta G\downarrow $ | $ \Delta\Delta G $ -seq  $ \downarrow $ | IMP-seq(\%)  $ \uparrow $ |
| ----------- | ---------------------------- | --------------------------------------- | ------------------------- |
| Grafting    | 135.17                       | 40.22                                   | 32.69                     |
| ProteinMPNN | 127.14                       | 24.72                                   | 35.51                     |
| ESM-IF1     | 162.09                       | 42.28                                   | 33.33                     |
| Diffab-fix  | 116.36                       | 14.05                                   | 34.52                     |
| RADAb       | 109.16                       | 7.06                                    | 37.30                     |"
320,"| G | Ablation | AAR(%) | scRMSD | Plausibility |
| - | -------- | ------ | ------ | ------------ |
| R | E        |        |        |              |
| ✓ | ✓        | ✓      | 70.56  | 2.13         |
| ✗ | ✗        | ✓      | 51.36  | 2.23         |
| ✗ | ✓        | ✗      | 52.15  | 2.39         |
| ✗ | ✗        | ✗      | 49.17  | 2.24         |
| ✗ | ✓        | ✓      | 57.02  | 2.23         |"
323,"| Which of the following is not a way to form recombinant DNA? |
| ------------------------------------------------------------ |
| Choices:                                                     |
| Q. Translation                                               |
| Z. Conjugation                                               |
| R. Specialized transduction                                  |
| X. Transformation                                            |
| The correct answer is: Q                                     |"
323,"| A banana is yellow. What color is a banana? | A banana is yellow. What color is a banana? | A banana is yellow. What color is a banana? |
| ------------------------------------------- | ------------------------------------------- | ------------------------------------------- |
| Choices: A. pink B. yellow C. black D. blue | Choices: A. yellow B. pink C. black D. blue | Choices: Q. yellow Z. pink R. black X. blue |
| The correct answer is: B                    | The correct answer is: A                    | The correct answer is: Q                    |"
324,"| ALGORITHM         | SOLVE TIME  | SAMPLE ( $ \times 10^{4} $ ) |
| ----------------- | ----------- | ---------------------------- |
| DRO-BAS $ _{PE} $ | 0.01 (0.01) | N.A.                         |
| DRO-BAS $ _{PP} $ | 1.34 (0.22) | 3.48 (0.91)                  |
| BDRO              | 4.47 (0.62) | 44.45 (3.11)                 |"
325,"| Helpful ↑ | Helpful ↓ | Example                |
| --------- | --------- | ---------------------- |
| Safe      | Unsafe    | (a)(c), (a)(d), (b)(d) |
| Safe      | Safe      | (a)(b)                 |
| Unsafe    | Unsafe    | (c)(d)                 |
| Unsafe    | Safe      | (c)(b)                 |"
362,"| Model         | Grid Size          | Log-Lik.  $ \uparrow $ | FPT  $ \downarrow $ |
| ------------- | ------------------ | ---------------------- | ------------------- |
| Swin-TNP (KI) | 24  $ \times $  60 | 7.603                  | 355                 |
| Swin-TNP (PT) | 24  $ \times $  60 | 8.603                  | 375                 |"
327,"| Attack (Dataset)                  | Model         | No Defense | DiffPure | System Prompt | DiffPure + System Prompt | R2D2  | CAT   | VLGuard | BlueSuffix |
| --------------------------------- | ------------- | ---------- | -------- | ------------- | ------------------------ | ----- | ----- | ------- | ---------- |
| VAA (Harmful_Instructions)        | LLaVA-v1.5-7B | 57.50      | 42.50    | 50.00         | 35.00                    | 17.50 | 5.00  | 10.00   | 0.00       |
| MiniGPT-4                         | 47.50         | 40.00      | 27.50    | 20.00         | 5.00                     | 10.00 | -     | 0.00    |            |
| InstructionBLIP                   | 42.50         | 37.50      | 37.50    | 17.50         | 15.00                    | 5.00  | -     | 0.00    |            |
| Gemini-1.5-flash(MiniGPT-4)       | 10.00         | 0.00       | 10.00    | 12.50         | -                        | -     | -     | 0.00    |            |
| Gemini-1.5-flash(LLaVA)           | 2.50          | 0.00       | 2.50     | 5.00          | -                        | -     | -     | 0.00    |            |
| Gemini-1.5-flash(InstructionBLIP) | 5.00          | 0.00       | 2.50     | 5.00          | -                        | -     | -     | 0.00    |            |
| ImgJP (AdvBench)                  | LLaVA-v1.5-7B | 75.00      | 45.00    | 34.00         | 19.00                    | 63.00 | 17.00 | 57.00   | 0.00       |
| MiniGPT-4                         | 66.00         | 41.00      | 24.00    | 16.00         | 17.00                    | 21.00 | -     | 0.00    |            |
| InstructionBLIP                   | 45.00         | 32.00      | 27.00    | 17.00         | 23.00                    | 13.00 | -     | 0.00    |            |
| Gemini-1.5-flash                  | 8.00          | 4.00       | 5.00     | 2.00          | -                        | -     | -     | 0.00    |            |
| GCG (AdvBench)                    | LLaVA-v1.5-7B | 60.00      | 59.00    | 21.00         | 21.00                    | 35.00 | 7.00  | 58.00   | 0.00       |
| MiniGPT-4                         | 46.00         | 44.00      | 16.00    | 15.00         | 8.00                     | 15.00 | -     | 0.00    |            |
| InstructionBLIP                   | 58.00         | 58.00      | 22.00    | 21.00         | 30.00                    | 15.00 | -     | 0.00    |            |
| Gemini-1.5-flash                  | 7.00          | 7.00       | 2.00     | 1.00          | -                        | -     | -     | 0.00    |            |
| AutoDAN (AdvBench)                | LLaVA-v1.5-7B | 80.00      | 80.00    | 27.00         | 25.00                    | 26.00 | 30.00 | 71.00   | 0.00       |
| MiniGPT-4                         | 59.00         | 58.00      | 19.00    | 19.00         | 28.00                    | 11.00 | -     | 0.00    |            |
| InstructionBLIP                   | 60.00         | 61.00      | 22.00    | 20.00         | 45.00                    | 26.00 | -     | 0.00    |            |
| Gemini-1.5-flash                  | 8.00          | 7.00       | 2.00     | 1.00          | -                        | -     | -     | 0.00    |            |
| BAP Attack (MM-SafetyBench)       | LLaVA-v1.5-7B | 61.02      | 32.47    | 28.36         | 11.84                    | 46.55 | 41.85 | 21.67   | 4.65       |
| MiniGPT-4                         | 62.26         | 22.83      | 21.42    | 16.07         | 34.94                    | 40.89 | -     | 9.37    |            |
| InstructionBLIP                   | 58.48         | 25.25      | 22.68    | 19.29         | 35.00                    | 25.89 | -     | 6.78    |            |
| Gemini-1.5-flash(LLaVA)           | 40.98         | 2.73       | 2.22     | 2.92          | -                        | -     | -     | 0.47    |            |
| Gemini-1.5-flash(MiniGPT-4)       | 41.07         | 1.43       | 1.77     | 2.10          | -                        | -     | -     | 0.30    |            |
| Gemini-1.5-flash(InstructionBLIP) | 40.71         | 2.62       | 2.08     | 2.80          | -                        | -     | -     | 0.42    |            |"
328,"|              | LPIPS  $ \downarrow $ | CLIP  $ \uparrow $ | FVD  $ \downarrow $ |
| ------------ | --------------------- | ------------------ | ------------------- |
| DG4D         | 0.1748                | 0.915              | 856.86              |
| Consistent4D | 0.1729                | 0.865              | 1072.94             |
| SC4D         | 0.1659                | 0.915              | 879.66              |
| STAG4D       | 0.1506                | 0.885              | 972.73              |
| Ours         | 0.1216                | 0.948              | 846.32              |"
329,"| Method                 | LIBERO-Object | LIBERO-Spatial | LIBERO-Goal | LIBERO-Long | LIBERO-90  | Overall    |
| ---------------------- | ------------- | -------------- | ----------- | ----------- | ---------- | ---------- |
| Octo $ ^{\dagger} $    | 85.7 ± 0.9    | 78.9 ± 1.0     | 84.6 ± 0.9  | 51.1 ± 1.3  | -          | 75.1 ± 0.6 |
| OpenVLA $ ^{\dagger} $ | 88.4 ± 0.8    | 84.7 ± 0.9     | 79.2 ± 1.0  | 53.7 ± 1.3  | -          | 76.5 ± 0.6 |
| ResNet-T               | 78.9 ± 1.4    | 75.7 ± 1.9     | 52.7 ± 2.4  | 45.0 ± 1.1  | 83.9 ± 1.5 | 67.3 ± 0.9 |
| Diffusion Policy       | 62.6 ± 2.8    | 69.5 ± 1.8     | 54.6 ± 0.5  | 51.2 ± 3.0  | 75.3 ± 0.7 | 62.6 ± 0.6 |
| ACT                    | 78.8 ± 1.2    | 82.0 ± 0.5     | 66.1 ± 1.6  | 44.0 ± 0.5  | 63.4 ± 5.8 | 66.8 ± 1.1 |
| VQ-BeT                 | 90.3 ± 1.5    | 88.7 ± 2.0     | 61.3 ± 1.0  | 59.7 ± 0.2  | 84.2 ± 0.3 | 76.8 ± 0.5 |
| QueST                  | 90.0 ± 1.1    | 84.5 ± 0.2     | 76.7 ± 0.9  | 69.1 ± 1.0  | 87.4 ± 0.4 | 81.5 ± 0.6 |
| Ours                   | 98.3 ± 0.2    | 95.5 ± 0.6     | 95.0 ± 0.7  | 88.5 ± 0.3  | 90.8 ± 0.2 | 93.6 ± 0.1 |"
330,"| Dataset      | Norm              | Robust Test Accuracy (%) | FAA (%) | FF (%)  |
| ------------ | ----------------- | ------------------------ | ------- | ------- |
| stage 1      | stage 2           | stage 3                  | stage 4 | stage 5 |
| CIFAR-10     | $ \ell_{\infty} $ | 88.36                    | 91.43   | 89.21   |
| $ \ell_{2} $ | 89.34             | 90.67                    | 90.56   | 91.84   |
| CIFAR-100    | $ \ell_{\infty} $ | 77.74                    | 87.87   | 88.33   |
| $ \ell_{2} $ | 78.89             | 89.34                    | 88.21   | 88.50   |
| SVHN         | $ \ell_{\infty} $ | 88.61                    | 92.52   | 92.45   |
| $ \ell_{2} $ | 86.72             | 92.66                    | 90.57   | 90.47   |
| TinyImageNet | $ \ell_{\infty} $ | 58.98                    | 81.53   | 80.70   |
| $ \ell_{2} $ | 65.18             | 81.85                    | 79.70   | 82.37   |"
330,"| Dataset                         | Method                     | Architecture     | Clean         | PGD-20        | CW-20         | AA    |
| ------------------------------- | -------------------------- | ---------------- | ------------- | ------------- | ------------- | ----- |
| CIFAR-10                        | AVMixup (Lee et al., 2020) | WideResNet-34-10 | 92.56         | 59.75         | 54.34         | 39.70 |
| Gowal et al. (2020)             | WideResNet-70-16           | 85.29            | 58.22*        | -             | 57.20         |       |
| MART (Wang et al., 2019)        | WideResNet-34-10           | 83.51            | 58.31         | 54.33         | 51.10         |       |
| ES (Rice et al., 2020)          | WideResNet-34-20           | 85.34            | -             | -             | 53.42         |       |
| FAB (Zhang et al., 2020)        | WideResNet-34-10           | 84.52            | -             | -             | 53.51         |       |
| LS (Pang et al., 2021)          | WideResNet-34-20           | 86.43            | 57.91**       | -             | 54.39         |       |
| S²O (Jin et al., 2022)          | WideResNet-34-20           | 86.01            | 61.12         | 57.93         | 55.90         |       |
| RAT (Jin et al., 2023)          | WideResNet-34-10           | 85.98            | 58.47         | 56.13         | 54.20         |       |
| S²O (Jin et al., 2022)          | WideResNet-34-10           | 85.58            | 59.43         | 55.66         | 53.93         |       |
| S²O+AMS                         | WideResNet-34-10           | 85.83 (↑0.25)    | 61.29 (↑1.86) | 57.78 (↑2.12) | 55.92 (↑1.99) |       |
| Wang et al. (2023) (1M)         | WideResNet-34-10           | 91.18            | 68.11         | 65.20         | 63.31         |       |
| Wang et al. (2023) (1M)+AMS     | WideResNet-34-10           | 90.79 (↓0.39)    | 69.32 (↑1.21) | 66.03 (↑0.83) | 63.97 (↑0.66) |       |
| TRADES (Zhang et al., 2019b)    | WideResNet-34-10           | 84.65            | 56.68         | 54.49         | 53.00         |       |
| TRADES+EMA (Gowal et al., 2020) | WideResNet-34-10           | 84.78            | 57.23         | 55.12         | 53.76         |       |
| TRADES+AMS                      | WideResNet-34-10           | 85.37 (↑0.72)    | 58.76 (↑2.08) | 56.43 (↑1.94) | 54.31 (↑1.31) |       |
| TRADES+AWP (Wu et al., 2020)    | WideResNet-34-10           | 84.99            | 59.67         | 57.41         | 56.17         |       |
| TRADES+AWP+AMS                  | WideResNet-34-10           | 85.21 (↑0.22)    | 61.45 (↑1.78) | 58.56 (↑1.14) | 57.36 (↑1.19) |       |
| CIFAR-100                       | Gowal et al. (2020)        | WideResNet-70-16 | 60.86         | 31.47*        | -             | 30.03 |
| Rebuffi et al. (2021a) (1M)     | WideResNet-28-10           | 62.41            | -             | -             | 32.06         |       |
| LBGAT (Cui et al., 2021)        | WideResNet-34-10           | 60.43            | 35.50         | 31.50         | 29.34         |       |
| RAT (Jin et al., 2023)          | WideResNet-34-10           | 62.93            | 33.36         | 29.61         | 27.90         |       |
| TRADES (Zhang et al., 2019b)    | WideResNet-34-10           | 60.22            | 32.11         | 28.93         | 26.90         |       |
| TRADES+EMA (Gowal et al., 2020) | WideResNet-34-10           | 61.43            | 32.76         | 29.41         | 27.19         |       |
| TRADES+AMS                      | WideResNet-34-10           | 62.56 (↑2.34)    | 33.81 (↑1.70) | 30.47 (↑1.54) | 28.25 (↑1.35) |       |
| Sehwag et al. (2022) (1M)       | WideResNet-34-10           | 65.76            | 36.33         | 32.97         | 31.20         |       |
| Sehwag et al. (2022) (1M)+AMS   | WideResNet-34-10           | 65.55 (↓0.21)    | 36.42 (↑0.09) | 33.56 (↑0.59) | 31.46 (↑0.26) |       |
| SVHN                            | Gowal et al. (2021)        | WideResNet-28-10 | 92.87         | -             | -             | 56.83 |
| Gowal et al. (2021) (1M)        | WideResNet-28-10           | 94.15            | -             | -             | 60.90         |       |
| Rebuffi et al. (2021a) (1M)     | WideResNet-28-10           | 94.39            | -             | -             | 61.09         |       |
| Wang et al. (2023) (1M)         | WideResNet-28-10           | 95.08            | 65.38         | 62.98         | 61.73         |       |
| Wang et al. (2023) (1M)+AMS     | WideResNet-28-10           | 95.26 (↑0.18)    | 66.67 (↑1.29) | 64.21 (↑1.23) | 63.15 (↑1.42) |       |
| TinyImageNet                    | Gowal et al. (2021)        | WideResNet-28-10 | 51.56         | -             | -             | 21.56 |
| Gowal et al. (2021) (1M)        | WideResNet-28-10           | 60.95            | -             | -             | 26.66         |       |
| Wang et al. (2023) (1M)         | WideResNet-28-10           | 64.83            | 32.21         | 31.65         | 30.76         |       |
| Wang et al. (2023) (1M)+AMS     | WideResNet-28-10           | 64.20 (↓0.63)    | 33.46 (↑1.25) | 32.54 (↑0.89) | 31.52 (↑0.76) |       |"
331,"| Datasets    | Methods | F1-Score | ACC    | H_ACC  |
| ----------- | ------- | -------- | ------ | ------ |
| Data2016a   | DAELSTM | 0.7766   | 78.55% | 80.84% |
| FEAT        | 0.7544  | 76.67%   | 78.31% |        |
| MCLDNN      | 0.6772  | 69.66%   | 70.98% |        |
| ThreeStream | 0.7683  | 78.11%   | 80.35% |        |
| Resnet      | 0.7347  | 74.48%   | 77.29% |        |
| Ours        | 0.8091  | 81.68%   | 84.90% |        |
| Data2016b   | DAELSTM | 0.9139   | 91.63% | 93.02% |
| FEAT        | 0.8186  | 83.10%   | 83.66% |        |
| MCLDNN      | 0.9065  | 91.16%   | 92.02% |        |
| ThreeStream | 0.9035  | 90.76%   | 92.34% |        |
| Resnet      | 0.8919  | 89.52%   | 90.69% |        |
| Ours        | 0.9210  | 92.62%   | 93.58% |        |
| Data2018    | DAELSTM | 0.7535   | 75.81% | 89.33% |
| FEAT        | 0.7129  | 72.33%   | 79.52% |        |
| MCLDNN      | 0.7895  | 80.28%   | 91.62% |        |
| ThreeStream | 0.8197  | 82.08%   | 93.74% |        |
| Resnet      | 0.7267  | 73.67%   | 86.46% |        |
| Ours        | 0.8666  | 86.75%   | 96.62% |        |"
331,"| Methods        | Data2018 | Data2016a | Data2016b |
| -------------- | -------- | --------- | --------- |
| F1-Score       | ACC      | H_ACC     | F1-Score  |
| DAELSTM        | 0.7535   | 75.81%    | 89.33%    |
| DAELSTM+FR     | 0.7684   | 78.08%    | 92.02%    |
| $ \Delta\% $   | +1.49%   | +2.27%    | +2.69%    |
| FEAT           | 0.7129   | 72.33%    | 79.52%    |
| FEAT+FR        | 0.7275   | 74.36%    | 81.89%    |
| $ \Delta\% $   | +1.46%   | +2.03%    | +2.37%    |
| MCLDNN         | 0.7895   | 80.28%    | 91.62%    |
| MCLDNN+FR      | 0.8026   | 81.48%    | 92.29%    |
| $ \Delta\% $   | +1.31%   | +1.20%    | +0.67%    |
| ThreeStream    | 0.8197   | 82.08%    | 93.74%    |
| ThreeStream+FR | 0.8666   | 86.75%    | 96.62%    |
| $ \Delta\% $   | +4.69%   | +4.67%    | +2.88%    |
| Resnet         | 0.7267   | 73.67%    | 86.46%    |
| Resnet+FR      | 0.7442   | 74.98%    | 87.39%    |
| $ \Delta\% $   | +1.75%   | +1.31%    | +0.93%    |"
362,"| Model         | Grid Size            | Log-Lik.  $ \uparrow $ | FPT  $ \downarrow $ |
| ------------- | -------------------- | ---------------------- | ------------------- |
| ViTNP (PT)    | 144  $ \times $  288 | 1.808                  | 215                 |
| Swin-TNP (PT) | 64  $ \times $  128  | 1.819                  | 127                 |"
333,"| Server Room A         | PID                   | Ours (4 ACUs)         | Ours (6 ACUs)              | Ours (all ACUs)            |
| --------------------- | --------------------- | --------------------- | -------------------------- | -------------------------- |
| May 5th 11:00 - 17:30 | May 6th 09:50 - 17:20 | May 7th 11:00 - 17:30 | May 8th 09:50 - 17:20      | May 9th 09:50 - 17:20      |
| Server AEP (kW)       | 555.31                | 552.17                | 548.61                     | 549.28                     |
| Server EC (kWh)       | 3610.15               | 4141.34               | 3566.65                    | 4120.42                    |
| ACU AEP (kW)          | 24.53                 | 23.82                 | 19.9                       | 20.19                      |
| ACU EC (kWh)          | 159.42                | 178.73                | 129.24                     | 151.44                     |
| ACLF (%)              | 4.42                  | 4.32                  | 3.62 ( $ \downarrow $ 18%) | 3.68 ( $ \downarrow $ 15%) |"
333,"| Server Room B         | PID                   | Our (4 ACUs)          | Ours (6 ACUs)              | Ours (all ACUs)            |
| --------------------- | --------------------- | --------------------- | -------------------------- | -------------------------- |
| May 5th 11:00 - 17:30 | May 6th 09:50 - 17:20 | May 7th 11:00 - 17:30 | May 8th 09:50 - 17:20      | May 9th 09:50 - 17:20      |
| Server AEP (kW)       | 617.18                | 602.04                | 593.28                     | 610.57                     |
| Server EC (kWh)       | 4010.83               | 4520.42               | 3853.19                    | 4579.69                    |
| ACU AEP (kW)          | 37.2                  | 36.38                 | 30.58                      | 31.66                      |
| ACU EC (kWh)          | 241.79                | 272.9                 | 198.75                     | 237.43                     |
| ACLF (%)              | 6.03                  | 6.04                  | 5.16 ( $ \downarrow $ 14%) | 5.18 ( $ \downarrow $ 14%) |"
335,"| Methods                           | Performance | Inference Cost |
| --------------------------------- | ----------- | -------------- |
| Diffusion-based (e.g. DiffSmooth) | Strong      | High           |
| Traditional (e.g. Consistency)    | Poor        | Low            |
| Ours rRCM                         | Strong      | Low            |"
335,"| Method                                                | Latency $ ^{1} $ | Certified Accuracy at  $ r $  (%) |
| ----------------------------------------------------- | ---------------- | --------------------------------- |
| 0.0                                                   | 0.5              | 1.0                               |
| Gaussian (Salman et al., 2019a)                       | 1min 20s         | 67.0                              |
| Consistency (Jeong &amp; Shin, 2020)                  | 1min 20s         | 55.0                              |
| SmoothAdv (Salman et al., 2019a)                      | 1min 20s         | 67.0                              |
| Boosting (Horváth et al., 2021)                       | 4min             | 65.6                              |
| MACER (Zhai et al., 2020)                             | 1min 20s         | 68.0                              |
| SmoothMix (Jeong et al., 2021) $ ^{2} $               | 1min 20s         | 55.0                              |
| Denoised (Salman et al., 2020)                        | -                | 60.0                              |
| DDS $ ^{\dagger} $  (Carlini et al., 2022)            | 3min 52s         | 76.2                              |
| DensePure $ ^{\dagger} $  (Xiao et al., 2022) K=1     | 17min 8s         | 76.6                              |
| K=5                                                   | 52min 20s        | 77.8                              |
| DiffSmooth $ ^{\dagger} $  (Zhang et al., 2023) m = 5 | 4min 41s         | 70.1                              |
| m = 10                                                | 5min 10s         | 70.0                              |
| m = 15                                                | 5min 35s         | 69.8                              |
| rRCM-B $ ^{\dagger} $                                 | 6s               | 76.6                              |
| rRCM-B                                                | 53s $ ^{4} $     | 76.8                              |
| rRCM-B-Deep                                           | 1min 41s         | 77.4                              |"
336,"| Robust Methods | Nat                  | Robust Test Acc (%)  |
| -------------- | -------------------- | -------------------- |
| Final          | Best                 | Diff                 |
| CIFAR-10       |                      |                      |
| PGD-AT         | 84.80  $ \pm $  0.14 | 45.16  $ \pm $  0.19 |
| UDR-AT         | 83.87  $ \pm $  0.26 | 46.60  $ \pm $  0.27 |
| HR             | 83.95  $ \pm $  0.32 | 47.32  $ \pm $  0.59 |
| Ours           | 83.34  $ \pm $  0.16 | 48.58  $ \pm $  0.21 |
| CIFAR-100      |                      |                      |
| PGD-AT         | 57.42  $ \pm $  0.28 | 21.87  $ \pm $  0.20 |
| UDR-AT         | 56.20  $ \pm $  0.54 | 22.07  $ \pm $  0.12 |
| HR             | 56.69  $ \pm $  0.43 | 21.15  $ \pm $  0.20 |
| Ours           | 56.71  $ \pm $  0.08 | 23.09  $ \pm $  0.20 |"
336,"| $ \varepsilon $ | 8/255          | 6/255          | 4/255          |
| --------------- | -------------- | -------------- | -------------- |
| PGD-200         | AA             | PGD-200        | AA             |
| PGD-AT          | 42.86 \pm 0.27 | 41.62 \pm 0.25 | 54.14 \pm 0.18 |
| UDR-AT          | 44.59 \pm 0.27 | 42.81 \pm 0.24 | 55.29 \pm 0.18 |
| HR              | 45.27 \pm 0.52 | 41.99 \pm 0.38 | 56.66 \pm 0.30 |
| Ours            | 46.79 \pm 0.11 | 44.06 \pm 0.32 | 57.57 \pm 0.53 |"
337,"| Cue       | Description                                                         | Retrieved trace                                          | Template question (corresponding to ★)                                                                                                                                                                                                                                                                                                 |
| --------- | ------------------------------------------------------------------- | -------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| (t,*,*,*) | Events at a specific time                                           | - Spaces
- Entities ★
- Contents                         | Consider all events that happened on {t}. Provide a list of all protagonists involved in any of these events, without describing the events themselves.
Reflect on {e}s&#x27;s experiences at {s}. Describe all the key events they&#x27;ve been involved in at this location, focusing on what happened rather than when it occurred. |
| (*,s,e,*) | Events involving entities at a specific location                    | - Times
- Contents ★                                     |                                                                                                                                                                                                                                                                                                                                        |
| (*,s,e,c) | Events with specific location, entities, and content                | - Times ★                                                | Consider all events involving both {e} and {c} at {s}. Provide a list of all dates when these events occurred, without describing the events.                                                                                                                                                                                          |
| (t,s,e,c) | Events with specific time, location, entities, and content          | - Full event details ★                                   | Provide a comprehensive account of what happened involving {e} and {c} at {s} on {t}. Include all relevant details about the event(s), including what occurred and any other pertinent information.                                                                                                                                    |
| (*,*,e,*) | Retrieves the most recent known location of an entity               | - Times [latest]
- Spaces [latest] ★
- Contents [latest] | What is the most recent location where {e} was observed in the story&#x27;s chronological timeline?                                                                                                                                                                                                                                    |
| (*,*,e,*) | Retrieves a chronological list of dates when an entity was observed | - Times [chrono] ★
- Spaces [chrono]
- Contents [chrono] | Provide a chronological list of all dates when {e} was observed, from earliest to latest in the story&#x27;s timeline.                                                                                                                                                                                                                 |"
337,"| Quality assessment                                                             | Model       | Simple Recall | Chronological Awareness |
| ------------------------------------------------------------------------------ | ----------- | ------------- | ----------------------- |
| On the default 10K tokens book, recent models reach 99% in-context performance | deepseek-r1 | 0.988         | 0.964                   |
| o1                                                                             | 0.978       | 0.948         |                         |
| gemini-2-flash-thinking                                                        | 0.962       | 0.967         |                         |
| gemini-2-pro                                                                   | 0.950       | 0.186         |                         |
| o3-mini                                                                        | 0.945       | 0.809         |                         |
| o1-mini                                                                        | 0.935       | 0.809         |                         |
| gemini-2-flash                                                                 | 0.915       | 0.163         |                         |"
337,"| In-context        | gpt-4o        | 0.84 | 0.81 | 0.60 | 0.57 | 0.53 |
| ----------------- | ------------- | ---- | ---- | ---- | ---- | ---- |
| llama-3.1         | 0.80          | 0.49 | 0.38 | 0.40 | 0.45 |      |
| cl-3-haiku        | 0.84          | 0.39 | 0.37 | 0.37 | 0.38 |      |
| gpt-4o-mini       | 0.51          | 0.54 | 0.44 | 0.47 | 0.50 |      |
| cl-3.5-sonnet     | 0.92          | 0.35 | 0.35 | 0.32 | 0.41 |      |
| o1-mini           | 0.97          | 0.05 | 0.12 | 0.12 | 0.24 |      |
| RAG               | cl-3.5-sonnet | 0.91 | 0.59 | 0.59 | 0.59 | 0.62 |
| gpt-4o            | 0.82          | 0.60 | 0.55 | 0.55 | 0.59 |      |
| gpt-4o-mini       | 0.63          | 0.60 | 0.60 | 0.59 | 0.62 |      |
| cl-3-haiku        | 0.71          | 0.57 | 0.59 | 0.58 | 0.59 |      |
| $ \theta $        | gpt-4o-mini   | 0.00 | 0.83 | 0.37 | 0.28 | 0.19 |
| matching event(s) | 0             | 1    | 2    | 3-5  | 6+   |      |"
337,"| Model                   | Simple Recall | Chronological Awareness |
| ----------------------- | ------------- | ----------------------- |
| gemini-2-pro            | 0.708         | 0.290                   |
| gemini-2-flash-thinking | 0.708         | 0.288                   |
| gpt-4o                  | 0.670         | 0.204                   |
| deepseek-v3             | 0.600         | 0.103                   |
| gemini-2-flash          | 0.596         | 0.173                   |
| deepseek-r1             | 0.572         | 0.147                   |
| llama-3.1-405b          | 0.504         | 0.129                   |
| gpt-4o-mini             | 0.492         | 0.077                   |
| claude-3-haiku          | 0.470         | 0.109                   |
| claude-3-5-sonnet       | 0.470         | 0.090                   |
| o3-mini                 | 0.424         | 0.044                   |
| o1                      | 0.384         | 0.052                   |
| o1-mini                 | 0.300         | 0.033                   |"
337,"| Chapter 1                                                                                                                        | Chapter2                            | ... | Chapter N_events                    | gpt4o-mini Finetuning                      |
| -------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------- | --- | ----------------------------------- | ------------------------------------------ |
| (t₁,s₁,e₁,c₁)                                                                                                                    | (t₂,s₁,e₁,c₁)                       | ... | (t₂,s₂,e₂,c₁)                       | (tₙ,sₙ,e₁,cₙ)                              |
| All questions that match this event                                                                                              | All questions that match this event | ... | All questions that match this event | All 3199 questions with one matching event |
| Despite knowing each event separately, model fails to connect them → Naïve fine-tuning is inadequate to embed episodic knowledge |                                     |     |                                     |                                            |"
338,"| Category                                   | Method                     | Data Acquisition Difficulty                 | Supported Tasks             | Multi-tasks support? | Multi-modal support? | Generalist support? |
| ------------------------------------------ | -------------------------- | ------------------------------------------- | --------------------------- | -------------------- | -------------------- | ------------------- |
| Computer Vision                            | HAT (Chen et al., 2023b)   | Low-cost                                    | Image super-resolution (SR) | ✗                    | ✗                    | ✗                   |
| IPT (Chen et al., 2021)                    | Low-cost                   | Image restoration, Derain, Dehaze           | ✓                           | ✗                    | Requires fine-tuning |                     |
| Painter (Wang et al., 2023a)               | Low-cost                   | Segmentation, Keypoint detection            | ✓                           | ✗                    | ✓                    |                     |
| PromptGIP (Liu et al., 2023a)              | Low-cost                   | Image restoration, Derain, Dehaze           | ✓                           | ✗                    | ✓                    |                     |
| GenLV (Chen et al., 2024a)                 | Low-cost                   | Image restoration, enhancement, translation | ✓                           | ✗                    | ✓                    |                     |
| Earth Science                              | Prediff (Gao et al., 2024) | High-cost                                   | Weather forecasting         | ✗                    | ✗                    | ✗                   |
| Cascast (Gong et al., 2024)                | High-cost                  | Post-processing                             | ✗                           | ✗                    | ✗                    |                     |
| Climax (Nguyen et al., 2023)               | High-cost                  | Weather forecasting, Super-resolution       | ✓                           | ✗                    | Requires fine-tuning |                     |
| Aurora (Bodnar et al., 2024)               | High-cost                  | Weather forecasting                         | ✗                           | ✓                    | Requires fine-tuning |                     |
| WeatherGFM (ours)                          | High-cost                  | Atmospheric chemistry prediction            | ✓                           | ✓                    | ✓                    |                     |
| Weather forecasting, Weather image SR      | ✓                          | ✓                                           | ✓                           |                      |                      |                     |
| Weather image translation, Post-processing | ✓                          | ✓                                           | ✓                           |                      |                      |                     |"
338,"| Task                    | Dataset | Prompt Format   |
| ----------------------- | ------- | --------------- |
| Radar Spatial SR        | Sevir   | Single Modal    |
| Satellite Spatial SR    | Sevir   | Single Modal    |
| Radar Temporal SR       | Sevir   | Single Modal    |
| Deblur                  | Sevir   | Single Modal    |
| GEOS-IR2Radar           | Sevir   | Cross Modal     |
| GEOS-IR2GEOS-IR         | Sevir   | Cross Modal     |
| GEOS-IR2GEOS-Vis        | Sevir   | Cross Modal     |
| GEOS2POES-NO2           | POMINO  | Cross Modal     |
| Satellite extrapolation | Sevir   | Time-series Mod |
| Radar extrapolation     | Sevir   | Time-series Mod |
| Weather Forecasting T2M | ERA5    | Cross Modal     |
| Weather Forecasting U10 | ERA5    | Cross Modal     |"
338,"| Task name               | Weather super-resolution (SR) |
| ----------------------- | ----------------------------- |
| Satellite Spatial SR    | Radar Temporal SR             |
| Metrics                 | RMSE                          |
| UNet#                   | 0.932                         |
| ViT#                    | 0.047                         |
| WeatherGFM†             | 0.042                         |
| Task name               | Weather Forecasting           |
| Satellite extrapolation | Radar extrapolation           |
| Metrics                 | RMSE                          |
| UNet#                   | 1.033                         |
| ViT#                    | 0.408                         |
| WeatherGFM†             | 0.347                         |
| Task name               | Weather image translation     |
| GOES2Radar              | GOES-IR2GOES-IR               |
| Metrics                 | RMSE                          |
| UNet#                   | 0.821                         |
| ViT#                    | 0.445                         |
| WeatherGFM†             | 0.436                         |
| Task name               | GOES-IR2GOES-Visible          |
| Metrics                 | RMSE                          |
| UNet#                   | 0.915                         |
| ViT#                    | 0.448                         |
| WeatherGFM†             | 0.439                         |
| Task name               | Weather Forecasting T2M       |
| Lead Time [hr.]         | 6h                            |
| IFS                     | 0.97                          |
| ClimaX                  | 1.11                          |
| WeatherGFM*             | 1.08                          |"
338,"| Tasks                | IR107 Satellite extrapolation | IR107 2 IR069 |
| -------------------- | ----------------------------- | ------------- |
| Metrics              | RMSE                          | CSI/-4K       |
| UNet $ ^{\#} $       | 0.991                         | 0.695         |
| ViT $ ^{\#} $        | 0.413                         | 0.899         |
| WeatherGFM $ ^{\#} $ | 0.389                         | 0.903         |
| Tasks                | Temporal SR at 15min          |               |
| Metrics              | RMSE                          | CSI/16        |
| UNet $ ^{\#} $       | 0.676                         | 0.211         |
| ViT $ ^{\#} $        | 0.218                         | 0.838         |
| WeatherGFM $ ^{\#} $ | 0.272                         | 0.814         |"
339,"| Model          | Context Length |
| -------------- | -------------- |
| 8K             | 16K            |
| Mamba-130m     | 0.17           |
| DeciMamba-130m | 0.14           |"
345,"| Method Type                                    | Category                          | Models                                        |
| ---------------------------------------------- | --------------------------------- | --------------------------------------------- |
| Two-Step                                       | Graph kernel with detector (GK-D) | PK-SVM, PK-iF, WL-SVM, WL-iF                  |
| Self-supervised learning with detector (SSL-D) | IG-SVM, IG-iF, GCL-SVM, GCL-iF    |                                               |
| End-to-End                                     | Graph neural network-based GLAD   | OCGIN, GLADC, GLocalKD, OCGTL, SIGNET, CVTGAD |
| Graph neural network-based GLOD                | GOOD-D, GraphDE, AAGOD, GOODAT    |                                               |"
340,"|                                   | Linear Pattern Matching (LPM)                 | Nonlinear Robust Pattern Matching (NRPM)                                                         |
| --------------------------------- | --------------------------------------------- | ------------------------------------------------------------------------------------------------ |
| Formula                           | $ z_{LPM} = \sum_{d=1}^{D} a_d x_d $          | $ z_{NRPM}^{(k+1)} = D \cdot \frac{\sum_{d=1}^{D} w_d^{(k)} a_d x_d}{\sum_{d=1}^{D} w_d^{(k)}} $ |
| Optimization                      | $ \min_{z} \sum_{d=1}^{D} (z/D - a_d x_d)^2 $ | $ \min_{z} \sum_{d=1}^{D} |z/D - a_d x_d| $                                                      |
| Algorithm                         | Closed-form Optimal Solution                  | Newton-IRLS Algorithm                                                                            |
| Model (One Layer)                 | $ x_1 \bullet a_1 $                           | $ x_1 \bullet a_1 \bullet w_1 $                                                                  |
| $ x_2 \bullet a_2 $               | $ x_2 \bullet a_2 \bullet w_2 $               |                                                                                                  |
| $ x_3 \bullet a_3 $               | $ x_3 \bullet a_3 \bullet w_3 $               |                                                                                                  |
| $ \vdots \quad \vdots \quad a_D $ | $ \vdots \quad \vdots \quad w_D $             |                                                                                                  |
| $ x_D \bullet $                   | $ x_D \bullet a_D \bullet $                   |                                                                                                  |"
340,"| Method / Budget                   | Natural | 0.05 | 0.1  | 0.15 | 0.2  | 0.25 | 0.3             | $ [\lambda_{1}, \lambda_{2}, \lambda_{3}] $ |
| --------------------------------- | ------- | ---- | ---- | ---- | ---- | ---- | --------------- | ------------------------------------------- |
| Normal-train                      | 90.8    | 31.8 | 2.6  | 0.0  | 0.0  | 0.0  | 0.0             | \                                           |
| Adv-train                         | 76.4    | 66.0 | 57.6 | 46.9 | 35.0 | 23.0 | 9.1             | \                                           |
| Paradigm 1 (without tuning)       | 90.8    | 31.8 | 2.6  | 0.0  | 0.0  | 0.0  | 0.0             | [1.0, 1.0, 1.0]                             |
| 90.8                              | 56.6    | 17.9 | 8.5  | 4.6  | 3.0  | 2.3  | [0.9, 0.9, 0.9] |                                             |
| 90.4                              | 67.1    | 30.8 | 17.4 | 10.6 | 6.5  | 4.5  | [0.8, 0.8, 0.8] |                                             |
| 89.7                              | 73.7    | 43.5 | 25.5 | 16.9 | 11.7 | 9.2  | [0.7, 0.7, 0.7] |                                             |
| 88.1                              | 75.3    | 49.0 | 31.0 | 22.0 | 15.5 | 12.4 | [0.6, 0.6, 0.6] |                                             |
| 84.1                              | 74.4    | 50.0 | 31.9 | 22.8 | 18.1 | 14.3 | [0.5, 0.5, 0.5] |                                             |
| 78.8                              | 70.4    | 48.3 | 33.9 | 24.1 | 18.4 | 14.6 | [0.4, 0.4, 0.4] |                                             |
| 69.5                              | 62.6    | 45.2 | 31.5 | 23.1 | 19.0 | 15.5 | [0.3, 0.3, 0.3] |                                             |
| 58.5                              | 53.2    | 38.2 | 27.6 | 22.2 | 16.4 | 12.9 | [0.2, 0.2, 0.2] |                                             |
| 40.7                              | 38.3    | 29.7 | 22.8 | 16.8 | 12.9 | 11.1 | [0.1, 0.1, 0.1] |                                             |
| 18.8                              | 17.6    | 16.4 | 14.6 | 12.4 | 10.7 | 9.4  | [0.0, 0.0, 0.0] |                                             |
| Paradigm 2 (tuning  $ \lambda $ ) | 81.5    | 75.3 | 61.2 | 44.7 | 33.7 | 26.0 | 20.1            | [0.459, 0.033, 0.131]                       |
| Paradigm 3 (tuning all)           | 86.1    | 81.7 | 75.8 | 66.7 | 58.7 | 50.1 | 39.8            | [0.925, 0.119, 0.325]                       |"
340,"| Method            | Natural | PGD   | FGSM  | C&amp;W | AA    | DeepFool | SPSA  | AVG   |
| ----------------- | ------- | ----- | ----- | ------- | ----- | -------- | ----- | ----- |
| PGD-AT            | 80.90   | 44.35 | 58.41 | 46.72   | 42.14 | 14.81    | 62.92 | 44.89 |
| TRADES-2.0        | 82.80   | 48.32 | 51.67 | 40.65   | 36.40 | 25.91    | 64.29 | 44.54 |
| TRADES-0.2        | 85.74   | 32.63 | 44.26 | 26.70   | 19.00 | 12.98    | 57.79 | 32.23 |
| MART              | 79.03   | 48.90 | 60.86 | 45.92   | 43.88 | 25.63    | 56.55 | 46.96 |
| SAT               | 63.28   | 43.57 | 50.13 | 47.47   | 39.72 | 22.34    | 53.47 | 42.78 |
| AWP               | 81.20   | 51.60 | 55.30 | 48.00   | 46.90 | 26.25    | 61.37 | 48.24 |
| Consistency       | 84.37   | 45.19 | 53.84 | 43.75   | 40.88 | 21.27    | 65.91 | 45.14 |
| DYNAT             | 82.34   | 52.25 | 65.96 | 52.19   | 45.10 | 28.72    | 67.97 | 52.03 |
| Paradigm 3 (Ours) | 80.43   | 57.23 | 70.23 | 64.07   | 52.60 | 36.50    | 67.56 | 58.03 |"
344,"| Approach            | Corrective | No extra model | Prefill efficiency | Decode efficiency |
| ------------------- | ---------- | -------------- | ------------------ | ----------------- |
| RAG [19]            | No         | Yes            | No                 | No                |
| Corrective RAG [36] | Yes        | No             | No                 | No                |
| PCW RAG [28]        | No         | Yes            | Yes                | No                |
| Sparse RAG          | Yes        | Yes            | Yes                | Yes               |"
344,"| Dataset  | Metrics | Off-the-shelf | LLMLingua | RAG   | PCW-RAG | CRAG  | Sparse RAG |
| -------- | ------- | ------------- | --------- | ----- | ------- | ----- | ---------- |
| -        | ES      | 56.28         | -         | 56.28 | 147.58  | -     | 147.58     |
| PopQA    | EM      | 0.33          | 1.96      | 65.43 | 65.04   | 66.52 | 67.71      |
| F1       | 12.76   | 12.15         | 69.99     | 69.54 | 70.99   | 71.16 |            |
| K        | 20.00   | 7.84          | 20.00     | 20.00 | 8.9     | 7.84  |            |
| DS       | 6.65    | -             | 6.65      | 6.65  | -       | 12.28 |            |
| QMSum    | F1      | 20.37         | 22.28     | 21.43 | 20.18   | -     | 23.96      |
| RougeSum | 12.67   | 18.37         | 18.20     | 16.95 | -       | 20.10 |            |
| K        | 20.00   | 4.45          | 20.00     | 20.00 | -       | 4.45  |            |
| DS       | 6.65    | -             | 6.65      | 6.65  | -       | 16.05 |            |
| TriviaQA | EM      | 0.00          | 2.6       | 46.20 | 46.00   | -     | 47.50      |
| F1       | 12.21   | 16.30         | 53.03     | 53.20 | -       | 55.10 |            |
| K        | 20.00   | 9.90          | 20.00     | 20.00 | -       | 9.90  |            |
| DS       | 6.65    | -             | 6.65      | 6.65  | -       | 10.18 |            |
| HotpotQA | EM      | 0.00          | 1.33      | 43    | 38.83   | -     | 43.50      |
| F1       | 12.21   | 14.67         | 55.85     | 50.03 | -       | 55.36 |            |
| K        | 10.00   | 6.50          | 10.00     | 10.00 | -       | 6.50  |            |
| DS       | 10.31   | -             | 10.31     | 10.31 | -       | 13.00 |            |"
344,"| Threshold | EM    | PopQA | DS        | F1    | QMSum |
| --------- | ----- | ----- | --------- | ----- | ----- |
| F1        | K     | DS    | RougeLSum | K     | DS    |
| 0.05      | 66.95 | 70.97 | 9.75      | 10.61 | 22.85 |
| 0.1       | 66.84 | 70.66 | 8.72      | 11.70 | 23.78 |
| 0.15      | 67.17 | 71.16 | 7.84      | 12.28 | 23.43 |
| 0.2       | 66.77 | 70.54 | 7.13      | 12.88 | 23.2  |
| 0.25      | 65.75 | 69.64 | 6.56      | 13.00 | 23.96 |
| 0.3       | 63.86 | 68.2  | 5.98      | 13.08 | 23.84 |"
344,"| Approach   | Prefill Documents | EM    | F1    | K    | ES     | DS    |
| ---------- | ----------------- | ----- | ----- | ---- | ------ | ----- |
| RAG        | 10                | 64.66 | 68.67 | 10   | 80.74  | 10.31 |
| PCW RAG    | 10                | 63.9  | 68.58 | 10   | 147.48 | 10.31 |
| Sparse RAG | 10                | 65.86 | 70.2  | 7.79 | 147.48 | 12.33 |
| RAG        | 20                | 65.43 | 69.99 | 20   | 56.28  | 6.65  |
| PCW RAG    | 20                | 65.04 | 69.95 | 20   | 147.58 | 6.65  |
| Sparse RAG | 20                | 67.17 | 71.16 | 7.84 | 147.58 | 12.28 |"
346,"| Method                      | F1@20  | F1@30  | F1@40  |
| --------------------------- | ------ | ------ | ------ |
| SIFT (Lowe, 1999) + RANSAC  | 2.56%  | 3.78%  | 5.01%  |
| DINOv2 (Oquab et al., 2024) | 12.33% | 21.88% | 31.04% |
| DINOv2 + RANSAC             | 16.12% | 28.19% | 37.93% |
| DIFT (Tang et al., 2023)    | 16.14% | 25.80% | 33.79% |
| DIFT + RANSAC               | 13.13% | 21.96% | 30.15% |
| Ours (w/o refinement)       | 21.31% | 37.08% | 50.13% |
| Ours (full)                 | 27.14% | 42.09% | 52.43% |"
348,"| Method                                | Edit Data | Following | Preserving | Quality |
| ------------------------------------- | --------- | --------- | ---------- | ------- |
| Acc                                   | Score     | Acc       | Score      | Acc     |
| KOSMOS-G (Pan et al., 2023)           | 9M        | 51%       | 2.82       | 9%      |
| MagicBrush (Zhang et al., 2024b)      | 0.31M     | 51%       | 2.90       | 70%     |
| MGIE (Fu et al., 2023)                | 1M        | 40%       | 2.43       | 45%     |
| InstructDiffusion (Geng et al., 2024) | 0.86M     | 52%       | 2.87       | 54%     |
| HIVE (Zhang et al., 2024c)            | 1.1M      | 54%       | 2.93       | 56%     |
| HQ-Edit (Hui et al., 2024)            | 0.5M      | 51%       | 2.84       | 16%     |
| InsPix2Pix (Brooks et al., 2023)      | 0.3M      | 52%       | 2.94       | 53%     |
| Reward-InsPix2Pix                     | 0.32M     | 63%       | 3.39       | 58%     |
| SmartEdit (Huang et al., 2024)        | 1.17M     | 64%       | 3.50       | 66%     |
| Reward-SmartEdit                      | 1.19M     | 69%       | 3.72       | 74%     |"
348,"| Method            | CLIP-I   | CLIP-T   |
| ----------------- | -------- | -------- |
| MagicBrush        | 90.7     | $ 30.6 $ |
| MGIE              | 90.9     | 30.5     |
| InstructDiffusion | 89.2     | 30.2     |
| InsPix2Pix        | 85.4     | 29.2     |
| Reward-InsPix2Pix | 88.9     | 29.8     |
| SmartEdit         | 90.4     | 30.3     |
| Reward-SmartEdit  | $ 91.3 $ | 30.5     |"
348,"| Method           | Following | Preserving | Quality |
| ---------------- | --------- | ---------- | ------- |
| SmartEdit        | 3.09      | 3.18       | 2.58    |
| Reward-SmartEdit | 3.34      | 3.46       | 2.73    |"
349,"| Filtering Strategy                               | Data Size | MATH Val. Accuracy |
| ------------------------------------------------ | --------- | ------------------ |
| Unfiltered                                       | 128K      | 43.6  $ \pm $  1.7 |
| LLM-as-a-Judge: Prompt 1                         | 113K      | 43.6  $ \pm $  0.1 |
| LLM-as-a-Judge: Prompt 2                         | 116K      | 43.0  $ \pm $  0.8 |
| Nemotron-4-340B-Reward: Helpfulness  $ \geq $  3 | 118K      | 43.8  $ \pm $  0.4 |
| Nemotron-4-340B-Reward: Correctness  $ \geq $  3 | 120K      | 43.1  $ \pm $  0.4 |"
349,"| Category | Params | Model                         | MATH | AMC 2023 | AIME 2024 | Omni-MATH $ ^{1} $ |
| -------- | ------ | ----------------------------- | ---- | -------- | --------- | ------------------ |
| Open     | -      | Qwen2.5-Math-7B-Instruct      | 83.6 | 25/40    | 5/30      | 32.3               |
| Weight   | -      | Mathstral-7B                  | 56.6 | -        | -         | -                  |
| -        | -      | Llama3.1-8B-Instruct          | 51.8 | 9/40     | 2/30      | 12.7               |
| Open     | -      | NuminaMath-7B-CoT             | 55.2 | 11/40    | 0/30      | -                  |
| Source   | -      | OpenMath2-Llama3.1-8B (ours)  | 67.8 | 16/40    | 3/30      | 22.0               |
|          |        | + maj@2.56                    | 76.1 | 23/40    | 3/30      | 24.6               |
| Open     | -      | DS-Coder-V2-Lite-Instruct     | 61.8 | -        | 0/30      | 19.7               |
| Weight   | -      | Qwen2.5-Math-72B-Instruct     | 85.0 | 28/40    | 9/30      | 36.3               |
| -        | -      | Llama3.1-70B-Instruct         | 67.9 | 19/40    | 6/30      | 19.0               |
| Open     | 100B   | NuminaMath-72B-CoT            | 68.0 | 21/40    | 1/30      | 28.4               |
| Source   | -      | OpenMath2-Llama3.1-70B (ours) | 71.9 | 20/40    | 4/30      | 23.1               |
|          |        | + maj@2.56                    | 79.6 | 24/40    | 6/30      | 27.6               |"
351,"| Algorithm 1: LSPC Training                                                                                                        |
| --------------------------------------------------------------------------------------------------------------------------------- |
| Initialize:  $ \phi, \theta, \eta, \psi, \alpha, \beta, \delta $  for each gradient step do                                       |
| TD learning with IQL:                                                                                                             |
| $ \phi \leftarrow \phi - \nu_{\phi} \nabla_{\phi} \mathcal{L}_{V}(\phi) $                                                         |
| $ \theta \leftarrow \theta - \nu_{\theta} \nabla_{\theta} \mathcal{L}_{Q}(\theta) $                                               |
| $ \eta \leftarrow \eta - \nu_{\eta} \nabla_{\eta} \mathcal{L}_{V}^{c}(\eta) $                                                     |
| $ \psi \leftarrow \psi - \nu_{\psi} \nabla_{\psi} \mathcal{L}_{Q}^{c}(\psi) $                                                     |
| Policy extraction with AWR:                                                                                                       |
| $ \{\alpha, \beta\} \leftarrow \{\alpha, \beta\} - \nu_{\alpha\beta} \nabla_{\{\alpha,\beta\}} \mathcal{L}_{p,q}(\alpha, \beta) $ |
| $ \delta \leftarrow \delta - \nu_{\delta} \nabla_{\delta} \mathcal{L}_{\mu}(\delta) $                                             |"
351,"| Method             | BC-Safe  | CDT    | FISOR    | LSPC-S | LSPC-O   |
| ------------------ | -------- | ------ | -------- | ------ | -------- |
| Task               | reward ↑ | cost ↓ | reward ↑ | cost ↓ | reward ↑ |
| Metadrive:         |          |        |          |        |          |
| EasySparse         | 0.11     | 0.21   | 0.17     | 0.23   | 0.38     |
| EasyMean           | 0.04     | 0.29   | 0.45     | 0.54   | 0.38     |
| EasyDense          | 0.11     | 0.14   | 0.32     | 0.62   | 0.36     |
| MediumSparse       | 0.33     | 0.30   | 0.87     | 1.10   | 0.42     |
| MediumMean         | 0.31     | 0.21   | 0.45     | 0.75   | 0.39     |
| MediumDense        | 0.24     | 0.17   | 0.88     | 2.41   | 0.49     |
| HardSparse         | 0.17     | 3.25   | 0.25     | 0.41   | 0.30     |
| HardMean           | 0.13     | 0.40   | 0.33     | 0.97   | 0.26     |
| HardDense          | 0.15     | 0.22   | 0.08     | 0.21   | 0.30     |
| Average            | 0.18     | 0.58   | 0.42     | 0.80   | 0.36     |
| Safety Gym:        |          |        |          |        |          |
| CarButton1         | 0.07     | 0.85   | 0.21     | 1.6    | -0.02    |
| CarButton2         | -0.01    | 0.63   | 0.13     | 1.58   | 0.01     |
| CarGoal1           | 0.24     | 0.28   | 0.66     | 1.21   | 0.49     |
| CarGoal2           | 0.14     | 0.51   | 0.48     | 1.25   | 0.06     |
| CarPush1           | 0.14     | 0.33   | 0.31     | 0.4    | 0.28     |
| CarPush2           | 0.05     | 0.45   | 0.19     | 1.3    | 0.14     |
| SwimmerVel         | 0.51     | 1.07   | 0.66     | 0.96   | -0.04    |
| HopperVel          | 0.36     | 0.67   | 0.63     | 0.61   | 0.17     |
| HalfCheetahVel     | 0.88     | 0.54   | 1.0      | 0.01   | 0.89     |
| Walker2dVel        | 0.79     | 0.04   | 0.78     | 0.06   | 0.38     |
| AntVel             | 0.98     | 0.29   | 0.98     | 0.39   | 0.89     |
| Average            | 0.38     | 0.51   | 0.55     | 0.85   | 0.30     |
| Bullet Safety Gym: |          |        |          |        |          |
| BallRun            | 0.27     | 1.46   | 0.39     | 1.16   | 0.18     |
| CarRun             | 0.94     | 0.22   | 0.99     | 0.65   | 0.73     |
| DroneRun           | 0.28     | 0.74   | 0.63     | 0.79   | 0.30     |
| AntRun             | 0.65     | 1.09   | 0.72     | 0.91   | 0.45     |
| BallCircle         | 0.52     | 0.65   | 0.77     | 1.07   | 0.34     |
| CarCircle          | 0.5      | 0.84   | 0.75     | 0.95   | 0.40     |
| DroneCircle        | 0.56     | 0.57   | 0.60     | 0.98   | 0.48     |
| AntCircle          | 0.40     | 0.96   | 0.54     | 1.78   | 0.20     |
| Average            | 0.52     | 0.82   | 0.68     | 1.04   | 0.39     |"
352,"| Method                            | Spot. | Comp.     | Clus. | Chain.    | Overall |
| --------------------------------- | ----- | --------- | ----- | --------- | ------- |
| LLM Score                         | EM    | LLM Score | EM    | LLM Score | EM      |
| Set 1 (10K-50K Tokens)            |       |           |       |           |         |
| Long-context (Yang et al., 2024a) | 68.49 | 0.55      | 60.60 | 0.37      | 47.08   |
| RAG (Lewis et al., 2020)          | 51.08 | 0.35      | 44.53 | 0.27      | 37.96   |
| RQ-RAG (Chan et al., 2024)        | 72.31 | 0.54      | 48.16 | 0.05      | 47.44   |
| GraphRAG (Edge et al., 2024)      | 31.67 | 0.00      | 27.60 | 0.00      | 40.71   |
| StructRAG (Ours)                  | 74.53 | 0.47      | 75.58 | 0.47      | 65.13   |
| Set 2 (50K-100K Tokens)           |       |           |       |           |         |
| Long-context (Yang et al., 2024a) | 64.53 | 0.43      | 42.60 | 0.21      | 38.52   |
| RAG (Lewis et al., 2020)          | 66.27 | 0.46      | 46.28 | 0.31      | 38.95   |
| RQ-RAG (Chan et al., 2024)        | 57.35 | 0.35      | 50.83 | 0.16      | 42.85   |
| GraphRAG (Edge et al., 2024)      | 24.80 | 0.00      | 14.29 | 0.00      | 37.86   |
| StructRAG (Ours)                  | 68.00 | 0.41      | 63.71 | 0.36      | 61.40   |
| Set 3 (100K-200K Tokens)          |       |           |       |           |         |
| Long-context (Yang et al., 2024a) | 46.99 | 0.27      | 37.06 | 0.13      | 31.50   |
| RAG (Lewis et al., 2020)          | 73.69 | 0.55      | 42.20 | 0.27      | 32.78   |
| RQ-RAG (Chan et al., 2024)        | 50.50 | 0.13      | 44.62 | 0.00      | 36.98   |
| GraphRAG (Edge et al., 2024)      | 15.83 | 0.00      | 27.40 | 0.00      | 42.50   |
| StructRAG (Ours)                  | 68.62 | 0.44      | 57.74 | 0.35      | 58.27   |
| Set 4 (200K-250K Tokens)          |       |           |       |           |         |
| Long-context (Yang et al., 2024a) | 33.18 | 0.16      | 26.59 | 0.08      | 29.84   |
| RAG (Lewis et al., 2020)          | 52.17 | 0.24      | 24.60 | 0.10      | 26.78   |
| RQ-RAG (Chan et al., 2024)        | 29.17 | 0.08      | 40.36 | 0.00      | 26.92   |
| GraphRAG (Edge et al., 2024)      | 17.50 | 0.00      | 26.67 | 0.00      | 20.91   |
| StructRAG (Ours)                  | 56.87 | 0.19      | 55.62 | 0.25      | 56.59   |"
353,"| Architecture | #Depth↓            | #Epochs↓ | #Params↓ | #FLOPs↓ | Top-1 (%)↑ | Train Time Per Epoch ↓ | Train Time 300 Epochs ↓ |
| ------------ | ------------------ | -------- | -------- | ------- | ---------- | ---------------------- | ----------------------- |
| VanillaNet-6 | Chen et al. (2023) | 6        | 300      | 32.0M   | 6.00B      | 76.36                  | 8 minutes               |
| VanillaNet-8 | Chen et al. (2023) | 8        | 300      | 37.1M   | 7.70B      | 79.13                  | 11 minutes              |
| CoSNet-B1    | 26                 | 300      | 19.8M    | 3.05B   | 79.50      | 5 minutes              | 25 hours                |"
353,"| Approach                           | #Epochs | #Depth | #Params | #FLOPs | Top-1 (%) |
| ---------------------------------- | ------- | ------ | ------- | ------ | --------- |
| ResNet-50 + SE Hu et al. (2018)    | 120     | 50     | 28.0M   | 4.13B  | 76.85     |
| ResNet-50 + CBAM Woo et al. (2018) | 120     | 50     | 28.0M   | 4.13B  | 77.34     |
| • CoSNet-B1                        | 120     | 26     | 19.2M   | 3.05B  | 76.77     |
| • CoSNet-B1 + SE Hu et al. (2018)  | 120     | 26     | 20.1M   | 3.10B  | 77.85     |
| ResNet-50 + AFF Dai et al. (2021a) | 160     | 50     | 30.3M   | 4.30B  | 79.10     |
| ResNet-50 + SKNet Li et al. (2019) | 160     | 50     | 27.7M   | 4.47B  | 79.21     |
| • CoSNet-C1 + SE Hu et al. (2018)  | 160     | 28     | 25.0M   | 4.13B  | 79.51     |"
353,"| Method          | #Depth            | #Params | AP   | AP_{S} | AP_{M} | AP_{L} |
| --------------- | ----------------- | ------- | ---- | ------ | ------ | ------ |
| EfficientViT-M4 | Liu et al. (2023) | 42      | 8.8M | 32.7   | 17.6   | 35.3   |
| • CoSNet-A0     | 26                | 8.8M    | 34.3 | 19.1   | 38.0   | 49.1   |"
353,"| Method      | #Params            | mIoU   | FPS   |
| ----------- | ------------------ | ------ | ----- |
| RepVGG-B1g2 | Ding et al. (2021) | 41.36M | 78.88 |
| ResNet-50   | 25.5M              | 77.17  | 13    |
| • CosNet-B1 | 22.0M              | 79.05  | 17    |"
354,"|                            | Judgment | Multiple Choice |
| -------------------------- | -------- | --------------- |
| Video                      | Image    | 3D              |
| Human (Medium)             | 83.5     | 80.1            |
| Expert models              | 53.1     | 63.1            |
| InternLM-XComposer2.5      | 58.4     | 46.4            |
| mPLUG-Owl3-7B              | 55.3     | 45.9            |
| Qwen2-VL-7B                | 59.5     | 47.8            |
| LLaVA-OV-7B                | 56.8     | 49.8            |
| InternVL2-8B               | 60.8     | 49.7            |
| InternVL2-40B              | 62.0     | 49.6            |
| VILA1.5-40B                | 59.2     | 48.8            |
| Qwen2-VL-72B               | 59.6     | 53.2            |
| LLaVA-OV-72B               | 56.5     | 46.3            |
| Claude-3.5-Sonnet $ ^{*} $ | 61.7     | 53.6            |
| Gemini-1.5-Pro $ ^{*} $    | 58.5     | 43.5            |
| GPT-4o $ ^{*} $            | 71.3     | 63.4            |"
357,"|                           | Consistent4D | STAG4D  | DG4D    | Efficient4D | Ours    |
| ------------------------- | ------------ | ------- | ------- | ----------- | ------- |
| CLIP score  $ \uparrow $  | 0.905        | 0.920   | 0.885   | 0.917       | 0.922   |
| LPIPS  $ \downarrow $     | 12.81        | 12.78   | 16.17   | 14.66       | 12.35   |
| FVD  $ \downarrow $       | 893.67       | 855.84  | 1143.59 | 873.25      | 831.32  |
| Gen. time  $ \downarrow $ | 120 mins     | 90 mins | 11 mins | 12 mins     | 12 mins |"
360,"| Method                           | VBench $ \uparrow $ | FID-FVD $ \downarrow $ | FVD $ \downarrow $ | CD-FVD $ \downarrow $ |
| -------------------------------- | ------------------- | ---------------------- | ------------------ | --------------------- |
| Temporal Flickering              | Subject Consistency | Background Consistency | Motion Smoothness  | Dynamic Degree        |
| Stable Diffusion1.5              |                     |                        |                    |                       |
| MagicPose (Chang et al., 2023)   | 96.65               | 95.12                  | 94.55              | 98.29                 |
| Moore (Moore Threads, 2024)      | 96.86               | 95.18                  | 95.37              | 98.01                 |
| MusePose (Tong et al., 2024)     | 97.02               | 95.27                  | 95.16              | 98.45                 |
| MusePose+Ours                    | 97.63               | 95.70                  | 95.64              | 98.51                 |
| Stable Video Diffusion           |                     |                        |                    |                       |
| ControlNeXt (Peng et al., 2024)  | 97.55               | 94.58                  | 95.60              | 98.75                 |
| MimicMotion (Zhang et al., 2024) | 97.56               | 94.95                  | 95.36              | 98.67                 |
| MimicMotion+Ours                 | 97.73               | 95.72                  | 95.90              | 98.89                 |"
361,"| Method                                  | Params. (M) | FLOPs (G) | Latency (ms) | IN-1K (↑%) | IN-C (↓%)  | IN-A (↑%)  | IN-R (↑%)  | IN-SK (↑%) |
| --------------------------------------- | ----------- | --------- | ------------ | ---------- | ---------- | ---------- | ---------- | ---------- |
| LVT (Yang et al. 2022)                  | 5.5         | 0.8       | 7.7          | 74.8       | 75.0       | 7.5        | 34.6       | 23.0       |
| + AFBO (ours)                           | -0.3        | +0.0      | 9.1          | 76.1 (1.3) | 73.3 (1.7) | 8.5 (1.0)  | 35.1 (0.5) | 23.7 (0.7) |
| DeiT-T (Touvron et al. 2021a)           | 5.7         | 1.3       | 11.4         | 72.2       | 71.1       | 7.3        | 32.6       | 20.2       |
| + AFBO (ours)                           | +0.3        | +0.0      | 13.2         | 74.6 (2.4) | 66.2 (4.9) | 8.3 (1.0)  | 38.5 (5.9) | 23.1 (2.9) |
| PoolFormer-S12 (Yu et al. 2022)         | 11.9        | 1.8       | 9.9          | 77.2       | 69.8       | 7.0        | 37.7       | 25.4       |
| + AFBO (ours)                           | +0.1        | +0.1      | 11.2         | 79.1 (1.9) | 65.0 (4.8) | 8.9 (1.9)  | 42.5 (4.8) | 27.6 (2.2) |
| GC ViT-XXT (Hatamizadeh et al. 2023)    | 12.0        | 2.1       | 11.1         | 79.9       | 72.9       | 19.0       | 41.9       | 29.2       |
| + AFBO (ours)                           | +0.3        | +0.2      | 12.5         | 81.2 (1.3) | 72.5 (0.4) | 22.4 (3.4) | 43.5 (1.6) | 31.6 (1.4) |
| PVTv2-B1 (Wang et al. 2022)             | 14.0        | 2.1       | 11.5         | 78.7       | 65.1       | 14.6       | 41.7       | 28.9       |
| + AFBO (ours)                           | -0.1        | +0.2      | 13.2         | 80.1 (1.4) | 63.6 (1.5) | 16.7 (2.1) | 44.1 (2.4) | 31.1 (2.2) |
| CycleMLP-B1 (Chen et al. 2023)          | 15.0        | 2.1       | 12.1         | 78.9       | 64.5       | 11.6       | 41.6       | 29.1       |
| + AFBO (ours)                           | -0.2        | +0.1      | 13.4         | 80.0 (1.1) | 62.3 (2.2) | 14.2 (2.6) | 44.3 (2.7) | 30.4 (1.3) |
| HireMLP-Tiny (Guo et al. 2022b)         | 17.0        | 2.1       | 12.8         | 78.9       | 65.3       | 12.8       | 41.5       | 29.0       |
| + AFBO (ours)                           | -0.2        | +0.1      | 15.1         | 80.2 (1.3) | 62.9 (2.4) | 15.6 (2.8) | 43.8 (2.3) | 30.5 (1.5) |
| GC ViT-XT (Hatamizadeh et al. 2023)     | 20.0        | 2.6       | 13.2         | 82.0       | 75.3       | 26.7       | 44.3       | 32.0       |
| + AFBO (ours)                           | +0.4        | +0.2      | 14.9         | 82.5 (0.5) | 74.8 (0.5) | 29.2 (2.5) | 44.9 (0.6) | 32.6 (0.6) |
| Pyramid-VisionLLaMA-S (Chu et al. 2024) | 22.0        | 2.6       | 14.8         | 81.6       | 58.1       | 23.5       | 41.8       | 28.9       |
| + AFBO (ours)                           | +0.1        | +0.1      | 16.6         | 82.1 (0.5) | 56.1 (2.0) | 25.3 (1.8) | 45.3 (3.5) | 32.9 (3.0) |
| Poolformer-S24 (Yu et al. 2022)         | 21.4        | 3.4       | 14.3         | 80.3       | 62.2       | 14.5       | 41.4       | 28.9       |
| + AFBO (ours)                           | +0.6        | +0.3      | 16.4         | 81.5 (1.2) | 57.8 (4.4) | 18.0 (3.5) | 44.5 (3.1) | 30.8 (1.9) |
| DeiT-S (Touvron et al. 2021a)           | 22.0        | 4.6       | 12.9         | 79.8       | 54.6       | 19.8       | 41.9       | 29.4       |
| + AFBO (ours)                           | -0.3        | -0.1      | 15.2         | 81.1 (1.3) | 53.3 (1.3) | 20.8 (1.0) | 45.3 (3.4) | 31.9 (2.5) |
| Swin-T (Liu et al. 2021)                | 28.0        | 4.5       | 12.6         | 81.2       | 62.0       | 21.7       | 41.3       | 29.0       |
| + AFBO (ours)                           | -0.5        | -0.1      | 15.5         | 82.1 (0.9) | 56.4 (5.6) | 26.0 (4.3) | 45.8 (4.5) | 31.7 (2.7) |"
361,"| Backbone                          | Method | Parameter | FLOPs | IN-1K | IN-R  |
| --------------------------------- | ------ | --------- | ----- | ----- | ----- |
| DeiT-T (Touvron et al., 2021a)    | FFN    | 5.7 M     | 1.3 G | 72.2% | 32.6% |
| ConvNeXt Block (Liu et al., 2022) | 5.9 M  | 1.3 G     | 73.6% | 34.2% |       |
| IMLP (Xu et al., 2024)            | 5.0 M  | 1.1 G     | 72.6% | 33.5% |       |
| SwiGLU (Fang et al., 2024)        | 5.7 M  | 1.3 G     | 72.6% | 33.4% |       |
| ConvGLU (Shi, 2024)               | 5.9 M  | 1.4 G     | 74.2% | 37.5% |       |
| AFBO (ours)                       | 6.0 M  | 1.3 G     | 74.6% | 38.5% |       |
| DeiT-S (Touvron et al., 2021a)    | FFN    | 22.0 M    | 4.6 G | 79.8% | 41.9% |
| IMLP (Xu et al., 2024)            | 18.8 M | 3.9 G     | 80.0% | N/A   |       |
| SwiGLU (Fang et al., 2024)        | 22.0 M | 4.6 G     | 80.4% | 42.5% |       |
| ConvGLU (Shi, 2024)               | 21.6 M | 4.5 G     | 80.6% | 44.7% |       |
| AFBO (ours)                       | 21.7 M | 4.5 G     | 81.1% | 45.3% |       |
| Pool-S12 (Yu et al., 2022)        | FFN    | 11.9 M    | 1.8 G | 77.2% | 37.7% |
| IMLP (Xu et al., 2024)            | 9.8 M  | 1.5 G     | 77.2% | N/A   |       |
| SCHEME-12 (Sridhar et al., 2023)  | 16.7 M | 2.6 G     | 78.5% | N/A   |       |
| SCHEME-44 (Sridhar et al., 2023)  | 7.2 M  | 1.0 G     | 73.0% | N/A   |       |
| ConvGLU (Shi, 2024)               | 12.0 M | 1.9 G     | 78.1% | 41.7% |       |
| AFBO (ours)                       | 12.0 M | 1.9 G     | 79.1% | 42.5% |       |
| Pool-S24 (Yu et al., 2022)        | FFN    | 21.4 M    | 3.4 G | 80.3% | 41.4% |
| IMLP (Xu et al., 2024)            | 17.2 M | 2.7 G     | 80.7% | N/A   |       |
| SCHEME-12 (Sridhar et al., 2023)  | 30.8 M | 4.9 G     | 80.5% | N/A   |       |
| ConvGLU (Shi, 2024)               | 22.0 M | 3.5 G     | 81.1% | 43.8% |       |
| AFBO (ours)                       | 22.0 M | 3.7 G     | 81.5% | 44.5% |       |
| Swin-T (Liu et al., 2021)         | FFN    | 28.0 M    | 4.5 G | 81.2% | 41.3% |
| IMLP (Xu et al., 2024)            | 24.3 M | 3.9 G     | 81.5% | N/A   |       |
| SCHEME-44 (Sridhar et al., 2023)  | 19.7 M | 3.1 G     | 79.6% | N/A   |       |
| SCHEME-12 (Sridhar et al., 2023)  | 36.9 M | 5.9 G     | 81.7% | N/A   |       |
| ConvGLU (Shi, 2024)               | 28.5 M | 4.7 G     | 81.8% | 44.8% |       |
| AFBO (ours)                       | 27.5 M | 4.4 G     | 82.1% | 45.8% |       |"
362,"| Model         | Grid Size           | Log-Lik.  $ \uparrow $ | FPT  $ \downarrow $ |
| ------------- | ------------------- | ---------------------- | ------------------- |
| ConvCNP       | 64  $ \times $  128 | 1.535                  | 96                  |
| Swin-TNP (PT) | 64  $ \times $  128 | 1.819                  | 127                 |"
362,"| Model         | Grid Size          | Log-Lik.  $ \uparrow $ | FPT  $ \downarrow $ |
| ------------- | ------------------ | ---------------------- | ------------------- |
| ConvCNP       | 24  $ \times $  60 | 6.143                  | 210                 |
| Swin-TNP (PT) | 24  $ \times $  60 | 8.603                  | 375                 |"
362,"| Model         | Grid Size           | Log-Lik.  $ \uparrow $ | FPT  $ \downarrow $ |
| ------------- | ------------------- | ---------------------- | ------------------- |
| Swin-TNP (KI) | 64  $ \times $  128 | 1.683                  | 121                 |
| Swin-TNP (PT) | 64  $ \times $  128 | 1.819                  | 127                 |"
362,"| Model           | Grid Size          | Log-Lik.  $ \uparrow $ | FPT  $ \downarrow $ |
| --------------- | ------------------ | ---------------------- | ------------------- |
| Swin-TNP (s-PT) | 24  $ \times $  60 | 8.073                  | 364                 |
| Swin-TNP (m-PT) | 24  $ \times $  60 | 8.603                  | 375                 |"
362,"| Model             | Grid Size           | Log-Lik.  $ \uparrow $ | FPT  $ \downarrow $ |
| ----------------- | ------------------- | ---------------------- | ------------------- |
| Swin-TNP (PT) (T) | 64  $ \times $  128 | 1.895                  | 125                 |
| Swin-TNP (PT)     | 64  $ \times $  128 | 1.819                  | 127                 |"
362,"| Model               | Grid Size          | Log-Lik.  $ \uparrow $ | FPT  $ \downarrow $ |
| ------------------- | ------------------ | ---------------------- | ------------------- |
| Swin-TNP (m-PT) (T) | 24  $ \times $  60 | 9.972                  | 373                 |
| Swin-TNP (m-PT)     | 24  $ \times $  60 | 8.603                  | 375                 |"
363,"| Subset    | Scale     | Train  | Test   | Total  |
| --------- | --------- | ------ | ------ | ------ |
| IITC      | 4k Tokens | 208103 | 700    | 208803 |
| 8k Tokens | 196947    | 700    | 197647 |        |
| ITA       | 3 Pic C.  | 23991  | -      | 23991  |
| 3 Pic M.  | 23993     | -      | 23993  |        |
| 3 Pic E.  | 47983     | 500    | 48483  |        |
| 5 Pic C.  | 23986     | -      | 23986  |        |
| 5 Pic M.  | 23985     | -      | 23985  |        |
| 5 Pic E.  | 45226     | 486    | 45712  |        |"
364,"| Question: Agatha has $60 to spend on a new bike. She spends $15 on the frame, and $25 on the front wheel. What does she have left, in dollars, to spend on a seat and handlebar tape? Answer: Agatha spends 15+25=&lt;&lt;15+25=40&gt;&gt;40 dollars. Agatha has 60-40=&lt;&lt;60-40=20&gt;&gt;20 dollars left. Question: Mary has 6 jars of sprinkles in her pantry. Each jar of sprinkles can decorate 8 cupcakes. Mary wants to bake enough cupcakes to use up all of her sprinkles. If each pan holds 12 cupcakes, how many pans worth of cupcakes should she bake? Answer: Mary has 6 jars of sprinkles. Each jar can decorate 8 cupcakes. So she has 6 x 8 = 48 cupcakes worth of sprinkles. Each pan holds 12 cupcakes. So she needs 48 / Predicted next token | 12 |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -- |"
364,"| Experiments on various reasoning tasks | Method | AQuA  | GSM8K | CSQA  | Date  | Sport | Last letter |
| -------------------------------------- | ------ | ----- | ----- | ----- | ----- | ----- | ----------- |
| GPT2-XL                                | 22.44  | 2.27  | 16.54 | 2.0   | 55.2  | 0.0   |             |
| GPT2-XL + FAI                          | 28.74  | 2.88  | 16.63 | 2.0   | 55.2  | 0.0   |             |
| GPT-NEO                                | 22.83  | 1.59  | 22.69 | 3.2   | 54.4  | 0.0   |             |
| GPT-NEO + FAI                          | 36.22  | 2.50  | 23.26 | 3.6   | 55.2  | 0.0   |             |
| Llama3 8B                              | 40.94  | 70.32 | 71.17 | 64.00 | 95.60 | 58.67 |             |
| Llama3 8B + FAI                        | 46.85  | 71.24 | 74.28 | 65.60 | 96.00 | 62.00 |             |
| Llama3-70B                             | 66.14  | 91.28 | 77.31 | 87.60 | 97.2  | 84.00 |             |
| Llama3-70B + FAI                       | 66.53  | 91.28 | 78.62 | 88.0  | 98.0  | 85.33 |             |"
364,"|                     | number of shot methods | 1-shot    | 2-shot | 4-shot    | 6-shot | Mean Accuracy |
| ------------------- | ---------------------- | --------- | ------ | --------- | ------ | ------------- |
| Retrieval           | Random                 | Retrieval | Random | Retrieval | Random | Retrieval     |
| Robustness Analysis | Llama2 13B             | 30.55     | 32.45  | 33.59     | 32.52  | 33.51         |
| Llama2 13B + FAI    | 34.34                  | 32.45     | 33.66  | 34.27     | 34.80  | 35.86         |
| Llama3 8B           | 67.78                  | 69.29     | 68.99  | 73.62     | 71.65  | 73.09         |
| Llama3 8B + FAI     | 67.78                  | 69.90     | 71.27  | 73.77     | 73.54  | 74.30         |
| Mistral 7B          | 35.33                  | 35.86     | 36.24  | 38.06     | 38.13  | 39.73         |
| Mistral 7B + FAI    | 36.09                  | 37.15     | 39.27  | 38.59     | 41.93  | 41.55         |"
364,"| Analysis of Tokens Identified      | Llama3 8B | Mistral 7B | Token Name      | &#x27;=&#x27; | &#x27;&lt;&lt;&#x27; | &#x27;&gt;&gt;&#x27; | &#x27;/&#x27; | &#x27;$$&#x27; |  |
| ---------------------------------- | --------- | ---------- | --------------- | ------------- | -------------------- | -------------------- | ------------- | -------------- |  |
| Identified Token Number per Sample | 25.39     | 52.18      | Frequency Ratio | 3148 9.40%    | 2774 8.28%           | 1344 4.01%           | 555 1.66%     | 509 1.52%      |  |
| Demo Token Number per Sample       | 160.7     | 194.45     | Token Name      | &#x27;*&#x27; | &#x27;+&#x27;        | &#x27;of&#x27;       | 2             | &#x27;-&#x27;  |  |
| Ratio                              | 15.80%    | 26.8%      | Frequency Ratio | 431 1.29%     | 399 1.19%            | 342 1.02%            | 313 0.93%     | 238 0.71%      |  |"
365,"| Method           | MCF              | ||               | zsRE                |
| ---------------- | ---------------- | ---------------- | ------------------- |
| NS  $ \uparrow $ | PS  $ \uparrow $ | ES  $ \uparrow $ | Score  $ \uparrow $ |
| GPT2-XL          | 78.24            | 23.88            | 21.50               |
| GLOBAL-EDIT      | 65.08            | 80.66            | 89.66               |
| TIES-MERGING     | 78.46            | 26.35            | 27.16               |
| TASK-ARITHMETIC  | 66.84            | 55.19            | 61.66               |
| SIMPLE-AVERAGE   | 76.90            | 29.97            | 33.06               |
| COLLABEDIT       | 65.26            | 80.67            | 89.70               |"
365,"| Model                                   | Method                     | MCF   | ||      | zsRE  |
| --------------------------------------- | -------------------------- | ----- | ------- | ----- |
| NS ↑                                    | PS ↑                       | ES ↑  | Score ↑ | ||    |
| GPT-J                                   | Before m rounds of editing | 57.20 | 96.13   | 99.26 |
| After m rounds of editing (Immutable C) | 65.14                      | 76.94 | 84.58   | 74.68 |
| After m rounds of editing (Dynamic C)   | 58.15                      | 91.62 | 97.32   | 78.15 |
| GPT2-XL                                 | Before m rounds of editing | 65.08 | 80.66   | 89.66 |
| After m rounds of editing (Immutable C) | 64.89                      | 60.38 | 69.82   | 64.80 |
| After m rounds of editing (Dynamic C)   | 61.54                      | 74.33 | 82.30   | 71.72 |"
368,"| Method        | Split-Cifar10 | Split-Cifar100 | Split-miniImageNet |
| ------------- | ------------- | -------------- | ------------------ |
| Sequential    | 19.67         | 9.29           | 4.51               |
| Joint         | 92.38         | 73.29          | 53.55              |
| Buffer Size   | 100           | 200            | 500                |
| ER            | 36.39         | 44.79          | 57.74              |
| + STAR (ours) | 51.5          | 59.3           | 70.70              |
| ER-ACE        | 52.95         | 61.25          | 71.16              |
| + STAR (ours) | 60.69         | 67.58          | 75.44              |
| DER++         | 57.65         | 64.88          | 72.70              |
| + STAR (ours) | 61.76         | 68.60          | 76.52              |
| X-DER (RPC)   | 52.75         | 58.48          | 64.77              |
| + STAR (ours) | 58.85         | 65.94          | 69.19              |"
368,"| Method       | Split-Cifar10 | Split-Cifar100 | Split-miniImageNet |
| ------------ | ------------- | -------------- | ------------------ |
| Buffer Size  | 100           | 200            | 500                |
| ER-ACE       | 53.90         | 63.41          | 70.53              |
| +sSGD        | 56.26         | 64.73          | 71.45              |
| +oEWC        | 52.36         | 61.09          | 68.70              |
| +OCM         | 57.18         | 64.65          | 70.86              |
| +LiDER       | 56.08         | 65.32          | 71.75              |
| +DualHSIC    | 57.03         | 64.05          | 72.35              |
| +STAR (ours) | 60.69         | 67.58          | 75.44              |
| DER++        | 57.65         | 64.88          | 72.70              |
| +sSGD        | 55.81         | 64.44          | 72.05              |
| +oEWC        | 55.78         | 63.02          | 71.64              |
| +OCM         | 59.25         | 65.81          | 73.53              |
| +LiDER       | 58.43         | 66.02          | 73.39              |
| +DualHSIC    | 58.90         | 67.11          | 74.34              |
| +STAR (ours) | 61.76         | 68.60          | 76.52              |"
368,"| Method | Buffer | Current | 100   | 200   | 500   |
| ------ | ------ | ------- | ----- | ----- | ----- |
| DER++  | -      | -       | 57.52 | 65.07 | 73.05 |
| + STAR | ✗      | ✓       | 24.25 | 30.29 | 34.06 |
|        | ✓      | ✗       | 61.76 | 68.60 | 76.52 |
|        | ✓      | ✓       | 58.88 | 52.55 | 75.48 |"
368,"| Method    | $ x^{*} $  vs.  $ x $ | $ \nabla_{\theta} $  vs.  $ z $ | 100   | 200   | 500   |
| --------- | --------------------- | ------------------------------- | ----- | ----- | ----- |
| DER++     | -                     | -                               | 55.36 | 62.97 | 70.48 |
| + STAR    | $ x $                 | $ z $                           | 56.28 | 63.15 | 72.38 |
| $ x^{*} $ | $ z $                 | 56.20                           | 66.82 | 72.51 |       |
| $ x $     | $ \nabla_{\theta} $   | 60.83                           | 67.03 | 75.11 |       |
|           | $ x^{*} $             | $ \nabla_{\theta} $             | 61.76 | 68.06 | 76.52 |"
369,"| Method     | Avg. Latency  $ \downarrow $ | GPU Memory  $ \downarrow $   | $ CHAIR_{S} $   $ \downarrow $ |
| ---------- | ---------------------------- | ---------------------------- | ------------------------------ |
| Regular    | 3.44 s ( $ \times $  1.00)   | 15778 MB ( $ \times $  1.00) | 55.0                           |
| VCD        | 6.91 s ( $ \times $  2.01)   | 16634 MB ( $ \times $  1.05) | 54.4                           |
| OPERA      | 24.70 s ( $ \times $  7.18)  | 22706 MB ( $ \times $  1.44) | 52.6                           |
| Woodpecker | 10.68 s ( $ \times $  3.10)  | 22199 MB ( $ \times $  1.41) | 57.6                           |
| HALC       | 22.61 s ( $ \times $  6.51)  | 23084 MB ( $ \times $  1.46) | 51.0                           |
| Ours       | 13.89 s ( $ \times $  4.04)  | 19119 MB ( $ \times $  1.21) | 48.8                           |"
369,"| Setup              | Method              | LLaVA-1.5        | InstructBLIP       | Qwen-VL             |
| ------------------ | ------------------- | ---------------- | ------------------ | ------------------- |
| Acc.  $ \uparrow $ | Prec.  $ \uparrow $ | F1  $ \uparrow $ | Acc.  $ \uparrow $ | Prec.  $ \uparrow $ |
| Random             | Regular             | 83.13            | 81.94              | 83.44               |
| VCD                | 87.00               | 86.13            | 87.15              | 86.23               |
| M3ID               | 87.50               | 87.38            | 87.52              | 86.67               |
| RITUAL             | 88.87               | 89.23            | 88.81              | 88.83               |
| Ours               | 89.03               | 91.20            | 88.74              | 88.83               |
| Popular            | Regular             | 81.17            | 78.28              | 82.08               |
| VCD                | 83.10               | 79.96            | 83.94              | 80.07               |
| M3ID               | 84.30               | 81.58            | 84.95              | 80.97               |
| RITUAL             | 85.83               | 84.17            | 86.17              | 81.97               |
| Ours               | 86.63               | 87.75            | 86.28              | 82.73               |
| Adversarial        | Regular             | 77.43            | 73.31              | 79.26               |
| VCD                | 77.17               | 72.18            | 79.47              | 77.20               |
| M3ID               | 78.23               | 73.51            | 80.22              | 77.47               |
| RITUAL             | 78.80               | 74.43            | 80.54              | 78.73               |
| Ours               | 81.63               | 80.59            | 81.94              | 80.30               |"
369,"| Method                  | Object-level           | Attribute-level         |
| ----------------------- | ---------------------- | ----------------------- |
| Existence  $ \uparrow $ | Count  $ \uparrow $    | Position  $ \uparrow $  |
| Regular                 | 173.75 ( $ \pm $ 4.79) | 121.67 ( $ \pm $ 12.47) |
| DoLa                    | 176.67 ( $ \pm $ 2.89) | 113.33 ( $ \pm $ 10.41) |
| OPERA                   | 183.33 ( $ \pm $ 6.45) | 137.22 ( $ \pm $ 6.31)  |
| VCD                     | 186.67 ( $ \pm $ 5.77) | 125.56 ( $ \pm $ 3.47)  |
| M3ID                    | 186.67 ( $ \pm $ 5.77) | 128.33 ( $ \pm $ 10.41) |
| RITUAL                  | 187.50 ( $ \pm $ 2.89) | 139.58 ( $ \pm $ 7.64)  |
| Woodpecker              | 187.50 ( $ \pm $ 2.89) | 125.00 ( $ \pm $ 0.00)  |
| HALC                    | 183.33 ( $ \pm $ 0.00) | 133.33 ( $ \pm $ 5.77)  |
| Ours                    | 188.33 ( $ \pm $ 2.89) | 150.00 ( $ \pm $ 7.64)  |"
369,"| Method                   | LLaVA-1.5                |
| ------------------------ | ------------------------ |
| $ CHAIR_{S} \downarrow $ | $ CHAIR_{I} \downarrow $ |
| Regular                  | 26.2                     |
| VCD                      | 24.4                     |
| M3ID                     | 21.4                     |
| RITUAL                   | 22.4                     |
| Woodpecker               | 24.9                     |
| HALC                     | 21.7                     |
| Ours                     | 18.4                     |"
370,"| Demonstration                  | Semantic Accuracy | PSNR ↑  | Visual Quality |
| ------------------------------ | ----------------- | ------- | -------------- |
| V-Acc ↑                        | P-Acc ↑           | LPIPS ↓ | FID ↓          |
| Pretrain                       |                   |         |                |
| No Demonstration               | 22.9              | 29.6    | 13.20          |
| Random Demonstration           | 22.7              | 28.3    | 13.01          |
| In-class Demonstration         | 24.7              | 36.7    | 13.07          |
| Pretrain w/ Imitation Finetune |                   |         |                |
| No Demonstration               | 24.2              | 26.7    | 13.02          |
| Random Demonstration           | 23.1              | 25.6    | 13.01          |
| In-class Demonstration         | 25.7              | 40.7    | 13.08          |
| Pretrain w/ In-domain Finetune |                   |         |                |
| No Demonstration               | 25.0              | 30.8    | 13.09          |
| Random Demonstration           | 24.9              | 34.1    | 13.16          |
| In-class Demonstration         | 25.9              | 48.8    | 13.21          |"
371,"| Steps                  | Method              | Stable Diffusion-v2 |
| ---------------------- | ------------------- | ------------------- |
| Iters  $ \downarrow $  | NFE  $ \downarrow $ | CLIP $ \uparrow $   |
| 1000                   | DDPM                | 1000                |
| DDPM + ParaDiGMS       | 65                  | 2024                |
| DDPM + ParaSolver      | 32                  | 1065                |
| 50                     | DDIM                | 50                  |
| DDIM + ParaDiGMS       | 21                  | 132                 |
| DDIM + ParaSolver      | 13                  | 83                  |
| 25                     | DDIM                | 25                  |
| DDIM + ParaDiGMS       | 18                  | 53                  |
| DDIM + ParaSolver      | 11                  | 49                  |
| 50                     | DPMSolver           | 50                  |
| DPMSolver + ParaDiGMS  | 25                  | 132                 |
| DPMSolver + ParaSolver | 15                  | 96                  |
| 25                     | DPMSolver           | 25                  |
| DPMSolver + ParaDiGMS  | 15                  | 81                  |
| DPMSolver + ParaSolver | 11                  | 58                  |"
371,"| Method                  | 1 GPU                | 2 GPUs                  | 4 GPUs               | 8 GPUs                  |
| ----------------------- | -------------------- | ----------------------- | -------------------- | ----------------------- |
| Time (s) $ \downarrow $ | Speedup $ \uparrow $ | Time (s) $ \downarrow $ | Speedup $ \uparrow $ | Time (s) $ \downarrow $ |
| DDPM                    | 50.0                 | 1.0 $ \times $          | 50.0                 | 1.0 $ \times $          |
| ParaDiGMS               | 37.0                 | 1.4 $ \times $          | 23.3                 | 2.1 $ \times $          |
| Ours                    | 19.7                 | 2.5 $ \times $          | 13.0                 | 3.8 $ \times $          |"
372,"| Methods           | Challenges                                                                    |
| ----------------- | ----------------------------------------------------------------------------- |
| Multiple Choices  | - Limited by strict formats.
- Unsuitable for open-ended tasks.               |
| LLM-as-a-Judge    | - Requires powerful/costly LLMs.
- Raises reproducibility, privacy concerns.  |
| Fine-tuned Judges | - Performance capped by base model.
- May struggle with complex instructions. |"
373,"| AHMS Classification (Wrist→Wrist) |
| --------------------------------- |
| Subsequence Level                 |
| Model                             |
| SimCLR                            |
| Aug Pred                          |
| REBAR                             |
| RelCon (ours)                     |"
373,"| Gait Metric Regression (Wrist $ \rightarrow $ Waist) |
| ---------------------------------------------------- |
| Stride Velocity                                      |
| Model                                                |
| SimCLR                                               |
| Aug Pred                                             |
| REBAR                                                |
| RelCon (ours)                                        |"
373,"|                    | Opportunity (Wrist $ \rightarrow $ Wrist) | PAMAP2 (Wrist $ \rightarrow $ Wrist) |
| ------------------ | ----------------------------------------- | ------------------------------------ |
|                    | Eval Method                               | Pre-train Data                       |
| RelCon FM          | Fine-tuned                                | AHMS                                 |
| Yuan et al. (2024) | Fine-tuned                                | UKBioBank                            |
| RelCon FM          | MLP Probe                                 | AHMS                                 |
| Yuan et al. (2024) | MLP Probe                                 | UKBioBank                            |"
373,"|                    |                      | HHAR (Wrist  $ \rightarrow $  Wrist) | Motionsense (Wrist  $ \rightarrow $  Waist) | PAMAP2 (Wrist  $ \rightarrow $  Leg) |
| ------------------ | -------------------- | ------------------------------------ | ------------------------------------------- | ------------------------------------ |
|                    |                      | $ \uparrow $  FI                     | $ \uparrow $  FI                            | $ \uparrow $  FI                     |
| Self-supervised w/ | RelCon FM            | 57.63  $ \pm $  3.24                 | 80.35  $ \pm $  0.71                        | 53.98  $ \pm $  0.76                 |
| Aug Pred           | 50.95  $ \pm $  2.70 | 74.96  $ \pm $  1.37                 | 46.90  $ \pm $  1.14                        |                                      |
| SimCLR             | 55.93  $ \pm $  1.75 | 83.93  $ \pm $  1.78                 | 50.75  $ \pm $  2.97                        |                                      |
| Frozen Embedding   | SimSiam              | 45.36  $ \pm $  4.98                 | 71.91  $ \pm $  12.3                        | 47.85  $ \pm $  2.48                 |
| + Linear Probe     | BYOL                 | 40.66  $ \pm $  4.08                 | 66.44  $ \pm $  2.76                        | 43.89  $ \pm $  3.35                 |
| MAE                | 43.48  $ \pm $  2.84 | 61.14  $ \pm $  3.45                 | 42.32  $ \pm $  1.63                        |                                      |
| CPC                | 56.24  $ \pm $  0.98 | 72.89  $ \pm $  2.06                 | 45.84  $ \pm $  1.39                        |                                      |
| AE                 | 53.57  $ \pm $  1.14 | 55.12  $ \pm $  3.46                 | 50.79  $ \pm $  1.09                        |                                      |
| Fully Supervised   | DeepConvLSTM         | 54.39  $ \pm $  2.28                 | 84.56  $ \pm $  0.85                        | 51.22  $ \pm $  1.91                 |
| Conv classifier    | 55.43  $ \pm $  1.21 | 89.25  $ \pm $  0.50                 | 59.76  $ \pm $  1.53                        |                                      |
| LSTM classifier    | 37.42  $ \pm $  5.04 | 86.74  $ \pm $  0.29                 | 48.61  $ \pm $  1.82                        |                                      |"
373,"|                                                     | Stride Velocity    | Double Support Time | Subseq-level Activity | Workout-level Activity |
| --------------------------------------------------- | ------------------ | ------------------- | --------------------- | ---------------------- |
|                                                     | $ \uparrow $  Corr | $ \uparrow $  Corr  | $ \uparrow $  F1      | $ \uparrow $  F1       |
| RelCon                                              | 0.8431             | 0.7559              | 38.56                 | 55.28                  |
| w/o Augmentations                                   | -5.42%             | -13.88%             | -12.5%                | -17.93%                |
| w/o RevIN                                           | -11.66%            | -6.01%              | -3.81%                | -4.11%                 |
| w/o SparseMax                                       | -0.87%             | -3.7%               | -5.06%                | -7.11%                 |
| w/o Sampling Within-Subject                         | -1.40%             | -1.40%              | -4.49%                | -7.25%                 |
| w/ Softer Metric Loss (Kim et al., 2019)            | -3.64%             | -6.11%              | -1.45%                | 1.39%                  |
| w/ Harder Binary Contrastive Loss (Xu et al., 2024) | -6.86%             | -9.82%              | -5.63%                | -9.03%                 |"
374,"| Literature                                | Assumptions | Form        | Bound   |
| ----------------------------------------- | ----------- | ----------- | ------- |
| Condition 1                               | Condition 2 | Condition 3 | last    |
| Lip.sm                                    | C²          | Lip         | loc.Lip |
| [Ryu and Boyd 2014] Theorem 4             | ✓           |             |         |
| [Toulis et al. 2016] Theorem 1            |             | ✓           | ✓       |
| [Patrascu and Necoara 2018] Theorem 14    |             |             |         |
| [Asi and Duchi 2019] Proposition 5.3      |             |             |         |
| [Davis and Drusvyatskiy 2019] Theorem 4.5 |             |             | ✓       |
| [Patrascu 2021] Theorem 4.3               |             |             |         |
| [Yuan and Li 2023] Theorem 1              | ✓           |             |         |
| [Yuan and Li 2023] Theorem 10             | ✓           |             | ✓       |
| Our paper                                 |             |             |         |"
374,"| Assumptions                   | Relation                                                                                                                                  | Example                                                                                                                                                             |
| ----------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Condition 1                   | Lip.sm  $ \Rightarrow $  loc.Lip                                                                                                          | $ \|Ax - b\|_{1} + \lambda\|x\|_{2}^{2} $  - local Lipschitz condition; not Lipschitz smooth                                                                        |
| C²  $ \Rightarrow $  loc.Lip  | $ \|Ax - b\|_{1} + \lambda\|x\|_{2}^{2} $  - local Lipschitz condition; not C²                                                            |                                                                                                                                                                     |
| Lip  $ \Rightarrow $  loc.Lip | $ \|Ax - b\|_{2}^{2} + \lambda\|x\|_{2}^{2} $  - local Lipschitz condition on f(•; s) and r(•); not globally Lipschitz on f(•; s) or r(•) |                                                                                                                                                                     |
| Condition 2                   | s.cvx  $ \Rightarrow $  qd.grow                                                                                                           | $ \|Ax - b\|_{2}^{2} + \lambda\|x\|_{1} (A \in \mathbb{R}^{n \times d}) $  - local quadratic growth condition; with  $ \text{rank}(A) &lt; d $  not strongly convex |
| Condition 3                   | exact  $ \Rightarrow $  inexact                                                                                                           | $ \|Ax - b\|_{2}^{2} + \lambda\|x\|_{1} $  - subproblem solved inexactly; subproblem solved exactly difficult to achieve                                            |"
375,"| Method     | Pixel Acc. | mAP   | mIoU  |
| ---------- | ---------- | ----- | ----- |
| Full Model | 79.60      | 86.40 | 62.51 |
| w/o SilU   | 79.32      | 86.22 | 62.41 |
| w/o Conv1D | 70.01      | 78.87 | 50.64 |
| w/o Gate   | 75.11      | 80.12 | 55.78 |
| Only S6    | 72.39      | 80.09 | 53.19 |"
375,"| Model  | Method                                   | Pixel accuracy  $ \uparrow $ | mAP  $ \uparrow $ | mIoU  $ \uparrow $ |
| ------ | ---------------------------------------- | ---------------------------- | ----------------- | ------------------ |
| DeiT-S | Raw-Attention                            | 59.69                        | 77.25             | 36.94              |
| ViM-S  | Raw-Attention (Ali et al., 2024)         | 67.64                        | 74.88             | 45.09              |
| ViM-S  | Raw-Attention Ours                       | 67.66                        | 80.00             | 47.28              |
| DeiT-S | Attn-Rollout (Abnar &amp; Zuidema, 2020) | 66.84                        | 80.34             | 47.85              |
| ViM-S  | Attn-Rollout (Ali et al., 2024)          | 71.01                        | 80.78             | 51.51              |
| ViM-S  | Attn-Rollout Ours                        | 76.40                        | 83.90             | 58.48              |
| DeiT-S | Transformer-Attr (Chefer et al., 2021b)  | 79.26                        | 84.85             | 60.63              |
| ViM-S  | Mamba-Attr (Ali et al., 2024)            | 74.72                        | 81.70             | 54.24              |
| ViM-S  | Mamba-Attr Ours                          | 79.60                        | 86.40             | 62.51              |"
375,"|               | Positive Perturbation  $ \downarrow $ | Negative Perturbation  $ \uparrow $ |
| ------------- | ------------------------------------- | ----------------------------------- |
|               | Mamba  $ \diamond $                   | Mamba Ours                          |
| Raw-Attention | 17.268                                | 13.264                              |
| Attn-Rollout  | 18.806                                | 12.830                              |
| Attribution   | 16.619                                | 11.350                              |"
375,"|                                   | Activation Perturbation (AUAC)  $ \uparrow $ | Pruning Perturbation (AU-MSE)  $ \downarrow $ |
| --------------------------------- | -------------------------------------------- | --------------------------------------------- |
| Mamba 1.3B                        | Mamba 1.3B                                   | Mamba 2.8B                                    |
| Attribution of (Ali et al., 2024) | 0.915                                        | 0.918                                         |
| Our Attribution                   | 0.934                                        | 0.939                                         |"
375,"| Model size                   | Snarks (%) | CommonsenseQA (%) | Formal Fallacies (%) |
| ---------------------------- | ---------- | ----------------- | -------------------- |
| 1.3B                         | 44.54      | 52.15             | 40.13                |
| 1.3B + AMPLIFY  $ \diamond $ | 53.11      | 53.55             | 44.28                |
| 1.3B + AMPLIFY (Ours)        | 56.15      | 54.72             | 45.22                |
| 2.8B                         | 48.75      | 53.11             | 44.67                |
| 2.8B + AMPLIFY  $ \diamond $ | 58.10      | 56.10             | 47.80                |
| 2.8B + AMPLIFY (Ours)        | 60.02      | 56.08             | 47.86                |"
375,"| bkg  | aero    |
| ---- | ------- |
|      | bicycle |
|      | bird    |
|      | boat    |
|      | bottle  |
|      | bus     |
|      | car     |
|      | cat     |
|      | chair   |
|      | cow     |
|      | dog     |
|      | horse   |
|      | motor   |
|      | person  |
|      | plant   |
|      | sheep   |
|      | sofa    |
|      | train   |
|      | tv      |
| mIoU |         |"
376,"| Method                                  | PT Invariance | Symmetry | IS Invariance | Reflexivity | Behavior on Orthogonal Matrices         |
| --------------------------------------- | ------------- | -------- | ------------- | ----------- | --------------------------------------- |
| Linear Regression                       | ✓             | ✗        | ✓             | ✓           | Constant                                |
| CCA ( $ R^{2}_{\text{CCA}} $ )          | ✓             | ✓        | ✓             | ✗           | Constant                                |
| CCA ( $ \bar{\rho}_{\text{CCA}} $ )     | ✓             | ✓        | ✓             | ✓           | Constant                                |
| SVCCA ( $ R^{2}_{\text{SVCCA}} $ )      | ✓             | ✓        | ✓             | ✓           | Constant (assuming  $ T_{X}=T_{Y}=I $ ) |
| SVCCA ( $ \bar{\rho}_{\text{SVCCA}} $ ) | ✓             | ✓        | ✓             | ✓           | Constant (assuming  $ T_{X}=T_{Y}=I $ ) |
| Linear HSIC                             | ✓             | ✗        | ✗             | ✗           | Dimension-Dependent                     |
| Linear CKA                              | ✓             | ✓        | ✓             | ✓           | Constant                                |
| DOCS (Ours)                             | ✓             | ✓        | ✓             | ✓           | Discriminative                          |"
377,"| ASR (German)          |
| --------------------- |
| sparsity              |
| ET                    |
| 10%                   |
| 30%                   |
| 50%                   |
| 70%                   |
| 90%                   |
| ST (French-to-German) |
| sparsity              |
| ET                    |
| 10%                   |
| 30%                   |
| 50%                   |
| 70%                   |
| 90%                   |"
378,"|                 | Vina Score ( $ \downarrow $ ) | QED ( $ \uparrow $ ) | SA ( $ \uparrow $ ) | Linpiski ( $ \uparrow $ ) | logP              | High Affi. ( $ \uparrow $ ) | Comp. Rate ( $ \uparrow $ ) |
| --------------- | ----------------------------- | -------------------- | ------------------- | ------------------------- | ----------------- | --------------------------- | --------------------------- |
| Reference       | -7.84 $ \pm $ 3.11            | 0.53 $ \pm $ 0.19    | 0.67 $ \pm $ 0.15   | 4.45 $ \pm $ 0.77         | 1.64 $ \pm $ 2.17 | N/A                         | N/A                         |
| Pocket2Mol      | -5.50 $ \pm $ 1.25            | 0.54 $ \pm $ 0.12    | 0.83 $ \pm $ 0.10   | 1.70 $ \pm $ 1.52         | 4.98 $ \pm $ 0.14 | 28.6%                       | N/A                         |
| TargetDiff      | -5.09 $ \pm $ 4.81            | 0.42 $ \pm $ 0.18    | 0.53 $ \pm $ 0.13   | 3.20 $ \pm $ 2.39         | 4.32 $ \pm $ 0.81 | 44.0%                       | 68.2%                       |
| TargetDiff*     | -5.53 $ \pm $ 4.09            | 0.39 $ \pm $ 0.21    | 0.55 $ \pm $ 0.15   | 3.02 $ \pm $ 2.75         | 4.11 $ \pm $ 1.10 | 45.7%                       | 62.4%                       |
| IPDiff          | -7.55 $ \pm $ 5.59            | 0.38 $ \pm $ 0.17    | 0.50 $ \pm $ 0.14   | 4.10 $ \pm $ 0.77         | 5.07 $ \pm $ 2.94 | 51.0%                       | 67.8%                       |
| IPDiff*         | -6.23 $ \pm $ 5.26            | 0.40 $ \pm $ 0.20    | 0.55 $ \pm $ 0.15   | 3.94 $ \pm $ 1.00         | 5.04 $ \pm $ 3.34 | 49.5%                       | 67.3%                       |
| DynamicFlow-ODE | -7.28 $ \pm $ 1.98            | 0.53 $ \pm $ 0.20    | 0.61 $ \pm $ 0.14   | 3.13 $ \pm $ 2.62         | 4.45 $ \pm $ 0.81 | 51.0%                       | 70.2%                       |
| DynamicFlow-SDE | -7.65 $ \pm $ 1.59            | 0.53 $ \pm $ 0.15    | 0.53 $ \pm $ 0.17   | 4.34 $ \pm $ 2.58         | 4.25 $ \pm $ 0.90 | 52.5%                       | 73.6%                       |"
379,"| $ a_{11} $ | ... | $ a_{1m} $ |
| ---------- | --- | ---------- |
| ⋮          | ⋮   | ⋮          |
| $ a_{n1} $ | ... | $ a_{nm} $ |"
379,"| Dataset                                              | ICGNB     | WANBIA | CFWNB  | AG-NBC | AE-NBC | GNB    |     |
| ---------------------------------------------------- | --------- | ------ | ------ | ------ | ------ | ------ | --- |
| (W / T / L)                                          |           | 17/0/7 | 18/1/5 | 19/0/5 | 22/0/2 | 20/0/4 |     |
| Average                                              | 78.87     | 77.84  | 73.03  | 77.67  | 75.92  | 71.84  |     |
| ICGNB achieves SOTA accuracy with high significance. | Algorithm | ICGNB  | WANBIA | CFWNB  | AG-NBC | AE-NBC | GNB |
| ICGNB                                                | -         | ○      | ○      | ○      | ○      | ○      |     |
| WANBIA                                               | ●         | -      | ○      |        | ○      | ○      |     |
| CFWNB                                                | ●         | ●      | -      | ●      |        | ○      |     |
| AG-NBC                                               | ●         |        | ○      | -      |        | ○      |     |
| AE-NBC                                               | ●         | ●      |        |        | -      | ○      |     |
| GNB                                                  | ●         | ●      | ●      | ●      |        | -      |     |"
380,"| Report Type                                             | Field Name                                                                                                                                                                                                                                                                                                                                                               | Field Description                                                                 |
| ------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------- |
| Collected for All Flaw Reports                          | Reporter ID                                                                                                                                                                                                                                                                                                                                                              | Anonymous or real identity of flaw reporter                                       |
| Report ID                                               | Unique flaw report ID. The flaw report ID can be referenced in future submissions or mitigation efforts, similar to vulnerability identifiers such as CVE identifiers in computer security (Cybersecurity and Infrastructure Security Agency, 2022).                                                                                                                     |                                                                                   |
| System Version(s)                                       | AI system(s) and version(s) involved; multiple systems can be selected                                                                                                                                                                                                                                                                                                   |                                                                                   |
| Report Status                                           | Current status of the report, recorded with timestamps as updated by the submitter or receiving company. Initially, the status of a report is “Submitted”, but once it is submitted the status field will be updated to reflect current status of addressing the flaw (e.g., “Under investigation” or “Fixed”) (Cybersecurity and Infrastructure Security Agency, 2022). |                                                                                   |
| Session ID                                              | System session ID(s) for tracing flaw environment                                                                                                                                                                                                                                                                                                                        |                                                                                   |
| Report Timestamp                                        | Report submission timestamp                                                                                                                                                                                                                                                                                                                                              |                                                                                   |
| Flaw Timestamp(s)                                       | Time(s) where flaws occurred                                                                                                                                                                                                                                                                                                                                             |                                                                                   |
| Context Info                                            | Versions of other software or hardware systems involved                                                                                                                                                                                                                                                                                                                  |                                                                                   |
| Flaw Description                                        | Description of the flaw, its identification, reproduction, and how it violates system policies or user expectations                                                                                                                                                                                                                                                      |                                                                                   |
| Policy Violation                                        | Detail of how the expectations of the system are violated or undocumented, pointing to the terms of use, acceptable use policy, system card, or other documentation. Policies may be explicitly or implicitly violated.                                                                                                                                                  |                                                                                   |
| Developer                                               | Triage tag with name of system developer                                                                                                                                                                                                                                                                                                                                 |                                                                                   |
| System                                                  | Triage tag with name and version of system                                                                                                                                                                                                                                                                                                                               |                                                                                   |
| Severity                                                | Triage tag with worst-case scenario estimate of how negatively stakeholders will be impacted                                                                                                                                                                                                                                                                             |                                                                                   |
| Prevalence                                              | Triage tag with rough estimate of how often the flaw might be expressed across system deployments                                                                                                                                                                                                                                                                        |                                                                                   |
| Impacts                                                 | Triage tag indicating how impacted stakeholders may suffer if the flaw is not addressed                                                                                                                                                                                                                                                                                  |                                                                                   |
| Impacted Stakeholder(s)                                 | Triage tag(s) indicating who may be harmed if the flaw is not addressed                                                                                                                                                                                                                                                                                                  |                                                                                   |
| Risk Source                                             | Triage tag indicating worst-case scenario estimate of how negatively stakeholders will be impacted                                                                                                                                                                                                                                                                       |                                                                                   |
| Bounty Eligibility                                      | Triage tag indicating whether the submitter believes the flaw report meets the criteria for bounty programs                                                                                                                                                                                                                                                              |                                                                                   |
| Collected for Real-World Events                         | Description of the Incident(s)                                                                                                                                                                                                                                                                                                                                           | Details on specific real-world event(s) that have occurred                        |
| Implicated Systems                                      | Systems involved in real-world event(s) which generalized flaw reports might cover                                                                                                                                                                                                                                                                                       |                                                                                   |
| Submitter Relationship                                  | How the submitter is related to the event (e.g., “affected stakeholder” or “independent observer”)                                                                                                                                                                                                                                                                       |                                                                                   |
| Event Date(s)                                           | Date when the incident(s) occurred                                                                                                                                                                                                                                                                                                                                       |                                                                                   |
| Event Location(s)                                       | Geographical location of the incident(s)                                                                                                                                                                                                                                                                                                                                 |                                                                                   |
| Experienced Harm Types                                  | Physical; psychological; reputational; economic/property; environmental; public interest/critical infrastructure; fundamental rights; other                                                                                                                                                                                                                              |                                                                                   |
| Experienced Harm Severity                               | Maximum severity of harm experienced in the real world                                                                                                                                                                                                                                                                                                                   |                                                                                   |
| Harm Narrative                                          | Justification of why the event constitutes harm and how system flaws contributed to it                                                                                                                                                                                                                                                                                   |                                                                                   |
| Malign Actor                                            | Tactic Select Impact                                                                                                                                                                                                                                                                                                                                                     | Tactics observed or used (e.g., from MITRE&#x27;s ATLAS Matrix)                   |
| Confidentiality/privacy, integrity, availability, abuse |                                                                                                                                                                                                                                                                                                                                                                          |                                                                                   |
| Security Incident Report                                | Threat Actor Intent Detection                                                                                                                                                                                                                                                                                                                                            | Deliberate, unintentional, unknown                                                |
| Vulnerability Report                                    | Proof-of-Concept Exploit                                                                                                                                                                                                                                                                                                                                                 | How the reporter knows about the security incident, including observation methods |
| Hazard Report                                           | Examples                                                                                                                                                                                                                                                                                                                                                                 | A code and documentation archive proving the existence of a vulnerability         |
| Replication Packet                                      | A list of system inputs/outputs to help understand the replication packet                                                                                                                                                                                                                                                                                                |                                                                                   |
| Statistical Argument                                    | Files evidencing the flaw statistically, including test data, custom evaluators, and structured datasets                                                                                                                                                                                                                                                                 |                                                                                   |
| Argument supporting sufficient evidence of a flaw       |                                                                                                                                                                                                                                                                                                                                                                          |                                                                                   |"
381,"| Algorithm 11 Monte Carlo Tree Diffusion |
| --------------------------------------- |
| 1:                                      |
| 2:                                      |
| 3:                                      |
| 4:                                      |
| 5:                                      |
| 6:                                      |
| 7:                                      |
| 8:                                      |
| 9:                                      |
| 10:                                     |
| 11:                                     |
| 12:                                     |
| 13:                                     |
| 14:                                     |
| 15:                                     |
| 16:                                     |
| 17:                                     |
| 18:                                     |
| 19:                                     |
| 20:                                     |
| 21:                                     |
| 22:                                     |
| 23:                                     |
| 24:                                     |
| 25:                                     |
| 26:                                     |
| 27:                                     |"
381,"| Environment       | Dataset            | Diffuser        | Diffusion Forcing | MCTD            |
| ----------------- | ------------------ | --------------- | ----------------- | --------------- |
| Base              | Replanning         | Random Search   |                   |                 |
| pointmaze         | medium-navigate-v0 | 58  $ \pm $  6  | 60  $ \pm $  0    | 60  $ \pm $  9  |
| large-navigate-v0 | 44  $ \pm $  8     | 40  $ \pm $  0  | 34  $ \pm $  13   | 74  $ \pm $  9  |
| giant-navigate-v0 | 0  $ \pm $  0      | 0  $ \pm $  0   | 4  $ \pm $  8     | 50  $ \pm $  10 |
| antmaze           | medium-navigate-v0 | 36  $ \pm $  15 | 40  $ \pm $  18   | 48  $ \pm $  10 |
| large-navigate-v0 | 14  $ \pm $  16    | 26  $ \pm $  13 | 20  $ \pm $  0    | 57  $ \pm $  6  |
| giant-navigate-v0 | 0  $ \pm $  0      | 0  $ \pm $  0   | 4  $ \pm $  8     | 24  $ \pm $  12 |"
381,"| Dataset           | Diffuser        | Diffuser-Replanning | Diffusion Forcing | MCTD            | MCTD-Replanning                   |
| ----------------- | --------------- | ------------------- | ----------------- | --------------- | --------------------------------- |
| single-play-v0    | 78  $ \pm $  23 | 92  $ \pm $  13     | 100  $ \pm $  0   | 98  $ \pm $  6  | 100  $ \pm $  0                   |
| double-play-v0    | 12  $ \pm $  10 | 12  $ \pm $  13     | 18  $ \pm $  11   | 22  $ \pm $  11 | 50  $ \pm $  16 (78  $ \pm $  11) |
| triple-play-v0    | 8  $ \pm $  10  | 4  $ \pm $  8       | 16  $ \pm $  8    | 0  $ \pm $  0   | 6  $ \pm $  9 (40  $ \pm $  21)   |
| quadruple-play-v0 | 0  $ \pm $  0   | 0  $ \pm $  0       | 0  $ \pm $  0     | 0  $ \pm $  0   | 0  $ \pm $  0 (24  $ \pm $  8)    |"
381,"| Dataset            | Diffuser       | Diffuser-Replanning | Diffusion Forcing | MCTD            | MCTD-Replanning |
| ------------------ | -------------- | ------------------- | ----------------- | --------------- | --------------- |
| medium-navigate-v0 | 8  $ \pm $  13 | 8  $ \pm $  10      | 66  $ \pm $  32   | 82  $ \pm $  18 | 90  $ \pm $  9  |
| large-navigate-v0  | 0  $ \pm $  0  | 0  $ \pm $  0       | 8  $ \pm $  12    | 0  $ \pm $  0   | 20  $ \pm $  21 |"
383,"| Method                                        | NFE              | Validity (%)  $ \uparrow $ | Coverage (%)  $ \uparrow $ | Property  $ \downarrow $ | Stability Rate (%)  $ \uparrow $ |
| --------------------------------------------- | ---------------- | -------------------------- | -------------------------- | ------------------------ | -------------------------------- |
| Structural Composition                        | Recall Precision | wdist ( $ \rho $ )         | wdist ( $ N_{et} $ )       |                          |                                  |
| CDVAE [8]                                     | 5000             | 100.00                     | 86.70                      | 99.15                    | 99.49                            |
| DiffCSP [4]                                   | 1000             | 100.00                     | 83.25                      | 99.71                    | 99.76                            |
| FlowMM [5]                                    | 1000             | 96.85                      | 83.19                      | 99.49                    | 99.58                            |
| CrystalLLM (70B) [3]                          | -                | 99.6                       | 95.4                       | 85.8                     | 98.9                             |
| Autoregressive                                | -                | 86.43                      | 89.33                      | 63.31                    | 99.74                            |
| Perm. invariant DFM - Mask w/ Cubic           | 250              | 94.40                      | 84.40                      | 98.25                    | 99.40                            |
| Perm. invariant DFM - Mask w/ Kinetic Optimal | 250              | 95.79                      | 88.50                      | 90.11                    | 99.29                            |"
385,"| Datasets | IDOL   | TDRL   | G-CaRL | iCITRIS | β-VAE  | SlowVAE | iVAE   |
| -------- | ------ | ------ | ------ | ------- | ------ | ------- | ------ |
| A        | 0.9645 | 0.9416 | 0.9059 | 0.8219  | 0.8485 | 0.8512  | 0.6283 |
| B        | 0.9142 | 0.8727 | 0.6248 | 0.4120  | 0.4113 | 0.2875  | 0.5545 |
| C        | 0.9801 | 0.9001 | 0.5850 | 0.4234  | 0.4093 | 0.3420  | 0.6736 |
| D        | 0.9766 | 0.9796 | 0.5455 | 0.3343  | 0.2181 | 0.2641  | 0.3469 |
| E        | 0.7869 | 0.7228 | 0.5835 | 0.4646  | 0.4260 | 0.3986  | 0.6071 |
| F        | 0.9747 | 0.5899 | 0.5225 | -       | 0.4321 | 0.5157  | 0.3176 |"
386,"| Datasets | CIFAR-10 | CIFAR-20 | STL-10 |
| -------- | -------- | -------- | ------ |
| Metric   | ACC      | CAA      | NMI    |
| SimCLR   | 41.4     | 46.1     | 40.5   |
| MoCo     | 38.8     | 42.1     | 36.1   |
| BYOL     | 51.8     | 52.5     | 52.2   |
| SDCLR    | 44.1     | 50.5     | 43.4   |
| CC       | 25.2     | 20.9     | 18.3   |
| IDFD     | 56.7     | 63.0     | 51.8   |
| CoNR     | 41.4     | 43.5     | 34.9   |
| ProPos   | 51.4     | 59.2     | 52.2   |
| DMICC    | 40.6     | 42.5     | 36.9   |
| ConMix   | 61.6     | 65.4     | 59.8   |"
386,"| Datasets | CIFAR-10 | CIFAR-20 | STL-10 |
| -------- | -------- | -------- | ------ |
| Metric   | ACC      | CAA      | NMI    |
| SimCLR   | 39.4     | 42.5     | 38.4   |
| MoCo     | 37.0     | 40.8     | 34.7   |
| BYOL     | 46.0     | 45.8     | 51.8   |
| SDCLR    | 38.9     | 44.3     | 42.5   |
| CC       | 40.6     | 27.5     | 43.9   |
| IDFD     | 47.5     | 54.9     | 48.4   |
| CoNR     | 31.4     | 44.3     | 29.2   |
| ProPos   | 46.1     | 49.3     | 52.5   |
| DMICC    | 36.6     | 39.5     | 36.8   |
| ConMix   | 53.3     | 58.2     | 57.1   |"
387,"| Singular Vector | Top Tokens (L14)                                      | Interpretation             |
| --------------- | ----------------------------------------------------- | -------------------------- |
| Mean vector     | , and the - in ( &quot;.                              | Frequent tokens, stopwords |
| 1st svec        | s**t f**k ucker b****h slut F**k holes                | Toxic tokens               |
| 2nd svec        | damn really kinda stupid s**t goddamn                 | Toxic tokens               |
| 3rd svec        | disclaimer Opinion L, ^H Statement Disclaimer Brief   | Context dependent topics   |
| 4th svec        | nation globalization paradigm continent empire ocracy | Context dependent topics   |"
387,"| Category       | Method        | Toxicity  $ \downarrow $  (%) | Fluency | Noise Robustness | Low Data Requirement | Inference Time |
| -------------- | ------------- | ----------------------------- | ------- | ---------------- | -------------------- | -------------- |
| Pre-Trained    | -             | 48.00                         | ✓       | -                | ✗                    | ✓              |
| Fine-Tuned     | DPO           | 36.26                         | ✓       | ✗                | ✗                    | ✓              |
| KTO            | 41.13         | ✓                             | ✗       | ✗                | ✓                    |                |
| Decoding Based | DexPerts      | 13.87                         | ✗       | ✗                | ✗                    | ✗              |
| Editing Based  | Tox. Reversal | 27.94                         | ✓       | ✗                | ✓                    | ✗              |
| ProFS (Ours)   | 26.83         | ✓                             | ✓       | ✓                | ✓                    |                |"
389,"| Method                    | Homophily shift           | Degree shift               | Attribute + homophily shift | Attribute + degree shift  |
| ------------------------- | ------------------------- | -------------------------- | --------------------------- | ------------------------- |
| hom  $ \rightarrow $  het | het  $ \rightarrow $  hom | high  $ \rightarrow $  low | low  $ \rightarrow $  high  | hom  $ \rightarrow $  het |
| ERM                       | 73.62  $ \pm $  0.44      | 76.72  $ \pm $  0.89       | 86.47  $ \pm $  0.38        | 92.92  $ \pm $  0.43      |
| + Matcha                  | 89.71  $ \pm $  0.27      | 90.68  $ \pm $  0.26       | 88.55  $ \pm $  0.44        | 93.78  $ \pm $  0.74      |
| T3A                       | 73.85  $ \pm $  0.24      | 76.68  $ \pm $  1.08       | 86.52  $ \pm $  0.44        | 92.94  $ \pm $  0.37      |
| + Matcha                  | 90.40  $ \pm $  0.11      | 90.50  $ \pm $  0.24       | 88.42  $ \pm $  0.60        | 93.83  $ \pm $  0.41      |
| Tent                      | 74.64  $ \pm $  0.38      | 79.40  $ \pm $  0.57       | 86.49  $ \pm $  0.50        | 92.84  $ \pm $  0.18      |
| + Matcha                  | 89.93  $ \pm $  0.16      | 91.26  $ \pm $  0.08       | 89.20  $ \pm $  0.20        | 94.88  $ \pm $  0.09      |
| AdaNPC                    | 76.03  $ \pm $  0.46      | 81.66  $ \pm $  0.17       | 86.92  $ \pm $  0.38        | 91.15  $ \pm $  0.39      |
| + Matcha                  | 90.03  $ \pm $  0.33      | 90.36  $ \pm $  0.67       | 88.49  $ \pm $  0.31        | 92.84  $ \pm $  0.57      |
| GTrans                    | 74.01  $ \pm $  0.44      | 77.28  $ \pm $  0.56       | 86.58  $ \pm $  0.11        | 92.74  $ \pm $  0.13      |
| + Matcha                  | 89.47  $ \pm $  0.20      | 90.31  $ \pm $  0.31       | 87.88  $ \pm $  0.77        | 93.23  $ \pm $  0.52      |
| SOGA                      | 74.33  $ \pm $  0.18      | 83.99  $ \pm $  0.35       | 86.69  $ \pm $  0.37        | 93.06  $ \pm $  0.21      |
| + Matcha                  | 89.92  $ \pm $  0.26      | 90.69  $ \pm $  0.27       | 88.83  $ \pm $  0.32        | 94.49  $ \pm $  0.23      |
| GraphPatcher              | 79.14  $ \pm $  0.62      | 82.14  $ \pm $  1.11       | 87.87  $ \pm $  0.18        | 93.64  $ \pm $  0.45      |
| + Matcha                  | 91.28  $ \pm $  0.28      | 90.66  $ \pm $  0.15       | 88.01  $ \pm $  0.18        | 93.88  $ \pm $  0.69      |"
390,"| Base Model   | Method | Structure | CREAK | CSQA2 | StrategyQA |
| ------------ | ------ | --------- | ----- | ----- | ---------- |
| Acc.         | F1.    | Ins.      | Acc.  | F1.   | Ins.       |
| BERT-base    | +None  | -         | 69.3  | 69.1  | 70.1       |
| CoT          | Chain  | 77.7      | 76.6  | 78.7  | 71.1       |
| SCOTT        | Chain  | 84.1      | 84.2  | 83.5  | 85.2       |
| DSbS         | Chain  | 69.5      | 69.5  | 69.4  | 54.2       |
| UniCoTT      | Chain  | 92.7      | 92.8  | 93.0  | 81.5       |
| UniCoTT      | Tree   | 94.5      | 94.4  | 94.9  | 87.9       |
| UniCoTT      | Graph  | 95.8      | 95.6  | 96.0  | 83.8       |
| RoBERTa-base | +None  | -         | 71.3  | 71.3  | 71.4       |
| CoT          | Chain  | 86.5      | 86.4  | 86.7  | 72.7       |
| SCOTT        | Chain  | 90.2      | 90.2  | 90.5  | 82.3       |
| DSbS         | Chain  | 72.2      | 72.2  | 72.4  | 54.2       |
| UniCoTT      | Chain  | 93.4      | 93.4  | 93.3  | 82.2       |
| UniCoTT      | Tree   | 94.8      | 94.6  | 94.7  | 88.8       |
| UniCoTT      | Graph  | 96.8      | 96.8  | 95.9  | 84.9       |"
390,"| Base Model   | Method | Structure | Acc. | CSQA | Acc. | OBQA | Acc. | QASC |
| ------------ | ------ | --------- | ---- | ---- | ---- | ---- | ---- | ---- |
| F1           | Ins.   | F1.       | Ins. | F1.  | Ins. |      |      |      |
| BERT-base    | +None  | -         | 81.6 | 68.6 | 57.4 | 75.9 | 65.7 | 52.8 |
| CoT          | Chain  | 86.7      | 77.0 | 71.1 | 77.5 | 69.0 | 61.4 | 89.3 |
| SCOTT        | Chain  | 88.7      | 80.2 | 77.3 | 80.8 | 71.0 | 64.4 | 86.4 |
| DSbS         | Chain  | 81.3      | 68.2 | 56.2 | 76.4 | 63.2 | 52.0 | 87.1 |
| UniCoTT      | Chain  | 88.1      | 80.9 | 79.2 | 82.4 | 75.8 | 73.6 | 92.3 |
| UniCoTT      | Tree   | 90.4      | 84.9 | 84.4 | 83.8 | 75.6 | 75.2 | 93.2 |
| UniCoTT      | Graph  | 91.6      | 88.0 | 86.8 | 84.4 | 77.9 | 77.2 | 90.3 |
| RoBERTa-base | +None  | -         | 83.3 | 72.6 | 64.4 | 78.4 | 69.5 | 61.4 |
| CoT          | Chain  | 86.7      | 77.7 | 71.0 | 84.4 | 78.3 | 74.4 | 90.9 |
| SCOTT        | Chain  | 89.8      | 83.7 | 78.9 | 84.8 | 77.9 | 73.0 | 87.5 |
| DSbS         | Chain  | 82.8      | 72.4 | 64.3 | 77.6 | 69.0 | 60.0 | 87.5 |
| UniCoTT      | Chain  | 91.7      | 86.5 | 86.7 | 84.3 | 79.2 | 78.7 | 92.9 |
| UniCoTT      | Tree   | 91.7      | 86.9 | 87.5 | 87.5 | 83.4 | 82.2 | 93.6 |
| UniCoTT      | Graph  | 92.5      | 89.6 | 88.8 | 88.8 | 85.4 | 84.1 | 92.4 |"
405,"| Model       | Dataset                 | Method | #Bits  | Size(MB) | FID↓  | sFID↓ | Precision↑ | Recall↑ |
| ----------- | ----------------------- | ------ | ------ | -------- | ----- | ----- | ---------- | ------- |
| LDM-4       | LSUN-Bedrooms 256 × 256 | FP     | 32/32  | 1045.4   | 3.09  | 7.08  | 65.82      | 45.36   |
| LSQ         | 2/32                    | 69.8   | 7.49   | 12.79    | 64.02 | 37.60 |            |         |
| Baseline    | 1/32                    | 35.8   | 8.43   | 13.11    | 65.45 | 29.88 |            |         |
| BinaryDM    | 1/32                    | 35.8   | 6.99   | 12.15    | 67.51 | 36.80 |            |         |
| Q-Diffusion | 2/8                     | 69.8   | 62.01  | 33.56    | 16.48 | 14.12 |            |         |
| LSQ         | 2/8                     | 69.8   | 6.48   | 11.66    | 62.55 | 38.92 |            |         |
| Baseline    | 1/8                     | 35.8   | 9.37   | 12.10    | 64.36 | 30.76 |            |         |
| BinaryDM    | 1/8                     | 35.8   | 6.51   | 11.67    | 65.80 | 35.28 |            |         |
| Q-Diffusion | 4/4                     | 134.9  | 427.46 | 277.22   | 0.00  | 0.00  |            |         |
| EfficientDM | 4/4                     | 134.9  | 10.60  | -        | -     | -     |            |         |
| LSQ         | 2/4                     | 69.8   | 12.95  | 12.79    | 55.97 | 34.30 |            |         |
| Baseline    | 1/4                     | 35.8   | 10.87  | 15.46    | 64.05 | 26.50 |            |         |
| TDQ         | 1/4                     | 35.8   | 11.28  | 12.80    | 55.14 | 27.32 |            |         |
| ReActNet    | 1/4                     | 35.8   | 10.23  | 13.02    | 61.43 | 29.68 |            |         |
| Q-DM        | 1/4                     | 35.8   | 9.99   | 11.96    | 57.62 | 29.30 |            |         |
| INSTA-BNN   | 1/4                     | 35.8   | 9.42   | 12.39    | 60.05 | 31.08 |            |         |
| BI-DiffSR   | 1/4                     | 35.8   | 8.58   | 11.81    | 62.61 | 30.86 |            |         |
| BinaryDM    | 1/4                     | 35.8   | 7.74   | 10.80    | 64.71 | 32.98 |            |         |"
390,"| Base Model   | Methods | Structure | CoLA | RTE  | WNLI | MRPC | Average. |
| ------------ | ------- | --------- | ---- | ---- | ---- | ---- | -------- |
| BERT-base    | +None   | -         | 56.6 | 65.3 | 53.4 | 81.8 | 64.3     |
| CoT          | Chain   | 67.9      | 81.6 | 80.3 | 87.8 | 79.4 |          |
| SCOTT        | Chain   | 81.1      | 91.7 | 91.6 | 92.6 | 89.3 |          |
| UniCoTT      | Chain   | 86.4      | 89.9 | 93.0 | 95.5 | 91.2 |          |
| UniCoTT      | Tree    | 88.5      | 93.5 | 94.4 | 96.3 | 93.2 |          |
| UniCoTT      | Graph   | 90.2      | 94.6 | 93.9 | 94.1 | 93.2 |          |
| RoBERTa-base | +None   | -         | 56.7 | 78.7 | 55.5 | 86.9 | 69.5     |
| CoT          | Chain   | 69.6      | 82.3 | 81.2 | 71.3 | 76.1 |          |
| SCOTT        | Chain   | 78.5      | 89.9 | 90.8 | 90.6 | 87.5 |          |
| UniCoTT      | Chain   | 88.3      | 91.7 | 93.4 | 93.1 | 91.6 |          |
| UniCoTT      | Tree    | 91.4      | 93.0 | 95.1 | 96.3 | 94.0 |          |
| UniCoTT      | Graph   | 93.9      | 95.5 | 95.3 | 94.0 | 94.7 |          |"
391,"| Method                         | Base Model | Train-Free | CLIP-T $ \uparrow $ | CLIP-I $ \uparrow $ | DreamSim $ \downarrow $ | Steps | Memory (GB) $ \downarrow $ | Inference Time (s) $ \downarrow $ |
| ------------------------------ | ---------- | ---------- | ------------------- | ------------------- | ----------------------- | ----- | -------------------------- | --------------------------------- |
| Vanilla SD1.5                  | -          | -          | 0.8353              | 0.7474              | 0.5873                  | 50    | 4.73                       | 2.4657                            |
| Vanilla SDXL                   | -          | -          | 0.9074              | 0.8165              | 0.5292                  | 50    | 16.04                      | 13.0890                           |
| BLIP-Diffusion                 | SD1.5      | ✗          | 0.7607              | 0.8863              | 0.2830                  | 26    | 7.75                       | 1.9284                            |
| Textual Inversion              | SDXL       | ✗          | 0.8378              | 0.8229              | 0.4268                  | 40    | 32.94                      | 282.507                           |
| The Chosen One                 | SDXL       | ✗          | 0.7614              | 0.7831              | 0.4929                  | 35    | 10.93                      | 11.2073                           |
| PhotoMaker                     | SDXL       | ✗          | 0.8651              | 0.8465              | 0.3996                  | 50    | 23.79                      | 18.0259                           |
| IP-Adapter                     | SDXL       | ✗          | 0.8458              | 0.9429              | 0.1462                  | 30    | 19.39                      | 13.4594                           |
| ConsiStory                     | SDXL       | ✓          | 0.8769              | 0.8737              | 0.3188                  | 50    | 34.55                      | 34.5894                           |
| StoryDiffusion                 | SDXL       | ✓          | 0.8877              | 0.8755              | 0.3212                  | 50    | 45.61                      | 25.6928                           |
| Naive Prompt Reweighting (NPR) | SDXL       | ✓          | 0.8411              | 0.8916              | 0.2548                  | 50    | 16.04                      | 17.2413                           |
| 1Prompt1Story (Ours)           | SDXL       | ✓          | 0.8942              | 0.9117              | 0.1993                  | 50    | 18.70                      | 23.2088                           |"
392,"| Method                      | V2XSet | OPV2V  | DAIR-V2X-C |
| --------------------------- | ------ | ------ | ---------- |
| AP@0.5                      | AP@0.7 | AP@0.5 | AP@0.7     |
| DiscoNet Li et al. (2021a)  | 90.78  | 83.81  | 86.2       |
| V2X-ViT Xu et al. (2022a)   | 89.03  | 79.02  | 91.4       |
| Where2comm Hu et al. (2022) | 85.03  | 76.77  | -          |
| OPV2V Xu et al. (2022b)     | 91.88  | 84.75  | 89.9       |
| CoBEVT Xu et al. (2023a)    | 90.33  | 82.69  | -          |
| CoAlign Lu et al. (2023)    | -      | -      | 94.5       |
| Ours                        | 92.83  | 89.55  | 94.6       |"
392,"| Method        | Metric      |
| ------------- | ----------- |
| $ AP_{SP-O} $ | $ AP_{CP} $ |
| Where 2comm   | 38.54       |
| V2X-ViT       | 36.85       |
| Ours          | 40.13       |"
392,"| $ \ell_{\text{latency}} $ | Time Latency |
| ------------------------- | ------------ |
| 100                       | 200          |
| 0                         | 76.36/67.97  |
| 0.1                       | 76.34/67.93  |
| 0.2                       | 76.34/67.88  |
| 0.3                       | 76.34/67.86  |
| 0.4                       | 76.42/68.01  |
| 0.5                       | 76.44/68.01  |"
392,"| Method | Ratio |
| ------ | ----- |
| 1/128  | 1/64  |
| RPS    | 64.60 |
| FPS    | 65.39 |
| S-FPS  | 66.21 |
| D-FPS  | 65.65 |
| SD-FPS | 66.12 |"
393,"| MGDA          | FedMGDA | FedAvg      |
| ------------- | ------- | ----------- |
| Linearization | FedProx | MosT (ours) |"
393,"| Dataset        | MGDA       | Linearization | FedAvg     | FedProx    | FedMGDA+   | MosT w/o  $ R(\Gamma) $ | MosT       |
| -------------- | ---------- | ------------- | ---------- | ---------- | ---------- | ----------------------- | ---------- |
| Syn (0.0, 0.0) | 77.22±0.41 | 75.91±0.37    | 75.71±0.51 | 75.60±0.42 | 75.26±1.21 | 83.09±0.87              | 84.25±0.51 |
| Syn (0.5, 0.5) | 87.09±0.29 | 87.18±0.27    | 86.26±0.61 | 86.13±0.39 | 85.21±1.42 | 89.07±0.63              | 89.99±0.52 |
| Syn (1.0, 1.0) | 90.52±0.13 | 89.87±0.51    | 88.12±0.75 | 87.58±1.36 | 87.16±1.09 | 91.70±0.02              | 92.21±0.08 |
| FEMNIST        | 78.86±1.43 | 72.62±0.65    | 72.47±0.19 | 72.45±0.06 | 80.08±0.12 | 80.94±0.34              | 81.16±0.03 |"
393,"| Dataset   | MGDA               | Linearization      | EPO                | COSMOS             | TAG                | MosT               |
| --------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |
| Office-10 | 80.74 $ \pm $ 0.44 | 61.26 $ \pm $ 0.67 | 61.05 $ \pm $ 1.09 | 63.83 $ \pm $ 1.01 | 49.38 $ \pm $ 1.10 | 82.98 $ \pm $ 0.51 |
| DomainNet | 65.81 $ \pm $ 0.37 | 57.15 $ \pm $ 0.17 | 58.55 $ \pm $ 0.37 | 63.78 $ \pm $ 0.34 | 31.05 $ \pm $ 1.24 | 67.65 $ \pm $ 0.55 |"
393,"| Task    | MGDA               | Linearization      | MosT               |
| ------- | ------------------ | ------------------ | ------------------ |
| BoolQ   | 62.69 $ \pm $ 0.71 | 61.30 $ \pm $ 0.09 | 67.03 $ \pm $ 0.49 |
| MultiRC | 60.86 $ \pm $ 0.50 | 58.79 $ \pm $ 1.23 | 63.78 $ \pm $ 0.15 |
| WiC     | 55.28 $ \pm $ 0.89 | 57.16 $ \pm $ 1.27 | 62.38 $ \pm $ 0.31 |"
394,"| Task                       | OP Statistics              | Avg. Perf. Changes (%) |
| -------------------------- | -------------------------- | ---------------------- |
| $ P_{i,\text{low}} $       | $ P_{i,\text{mid}} $       | $ P_{i,\text{high}} $  |
| I2T                        | Image NSFW Filter          | 7.13                   |
| Text Action Number         | 59.90                      | 0.29                   |
| Language Score             | 49.90                      | 0.85                   |
| T2V                        | Video Aesthetics Score     | -0.98                  |
| Video NSFW Score           | 0.82                       | -0.05                  |
| Frames-Text Similarity     | -1.45                      | 0.23                   |
| ITP                        | CLIP Image-Text Similarity | -32.57                 |
| BLIP Image-Text Similarity | -24.28                     | 1.82                   |
| Image NSFW Score           | 12.18                      | 1.28                   |
| IC                         | Text Length                | 0.76                   |
| Image Watermark Score      | -0.64                      | -0.13                  |
| Character Repetition Ratio | 0.45                       | -0.46                  |"
395,"| Model                       | Discriminative Score | Predictive Score |          |
| --------------------------- | -------------------- | ---------------- | -------- |
| SigDiffusion (ours)         | 0.350±.080           | 0.168±.001       |          |
| DDO ( $ \gamma = 10 $ ) [3] | 0.356±.196           | 0.307±.007       |          |
| Diffusion-TS [4]            | 0.498±.003           | 0.438±.035       |          |
| CSPD-GP (Transformer) [5]   | 0.500±.000           | 0.490±.000       |          |
|                             |                      |                  |          |
| Model                       | Parameters           | Training         | Sampling |
| SigDiffusion (ours)         | 282K                 | 8 min            | 12 sec   |
| DDO ( $ \gamma = 10 $ ) [3] | 4.12M                | 3.6 h            | 42 min   |
| Diffusion-TS [4]            | 4.3 M                | 1.4 h            | 20 min   |
| CSPD-GP (Transformer) [5]   | 975 K                | 17 min           | 6 min    |"
396,"| Model            | Method   | Params(%) | BoolQ | PIQA | SIQA | HellaS. | WinoG. | ARC-e | ARC-c | OBQA | Avg. |
| ---------------- | -------- | --------- | ----- | ---- | ---- | ------- | ------ | ----- | ----- | ---- | ---- |
| ChatGPT          | -        | -         | 73.1  | 85.4 | 68.5 | 78.5    | 66.1   | 89.8  | 79.9  | 74.8 | 77.0 |
| LLaMA-7B         | Fully FT | 100       | 69.9  | 84.2 | 78.9 | 92.3    | 83.3   | 86.6  | 72.8  | 83.4 | 81.4 |
| LoRA $ _{r=4} $  | 0.10     | 2.3       | 46.1  | 18.3 | 19.7 | 55.2    | 65.4   | 51.9  | 57.0  | 39.5 |      |
| LoRA-Dash        | 0.10     | 65.2      | 79.9  | 78.3 | 82.8 | 77.1    | 78.6   | 65.4  | 78.4  | 75.7 |      |
| LoRA $ _{r=8} $  | 0.21     | 31.3      | 57.0  | 44.0 | 11.8 | 43.3    | 45.7   | 39.2  | 53.8  | 40.7 |      |
| LoRA-Dash        | 0.21     | 69.8      | 81.1  | 77.3 | 85.1 | 81.1    | 77.2   | 64.1  | 79.6  | 76.9 |      |
| LoRA $ _{r=16} $ | 0.42     | 69.9      | 77.8  | 75.1 | 72.1 | 55.8    | 77.1   | 62.2  | 78.0  | 70.9 |      |
| LoRA-Dash        | 0.42     | 66.9      | 80.2  | 77.8 | 78.8 | 79.2    | 78.0   | 61.9  | 77.4  | 75.0 |      |
| LoRA $ _{r=32} $ | 0.83     | 68.9      | 80.7  | 77.4 | 78.1 | 78.8    | 77.8   | 61.3  | 74.8  | 74.7 |      |
| LoRA-Dash        | 0.83     | 69.9      | 82.8  | 78.6 | 84.9 | 81.6    | 82.3   | 66.5  | 80.8  | 78.4 |      |
| LoRA $ _{r=64} $ | 1.66     | 66.7      | 79.1  | 75.7 | 17.6 | 78.8    | 73.3   | 59.6  | 75.2  | 65.8 |      |
| LoRA-Dash        | 1.66     | 69.6      | 79.5  | 76.0 | 82.8 | 75.8    | 81.5   | 64.7  | 81.0  | 76.4 |      |
| LLaMA2-7B        | Fully FT | 100       | 72.2  | 84.9 | 80.9 | 93.1    | 84.7   | 87.5  | 74.2  | 85.1 | 82.8 |
| LoRA $ _{r=16} $ | 0.41     | 71.7      | 81.6  | 79.5 | 89.5 | 81.9    | 82.9   | 67.9  | 79.6  | 79.3 |      |
| LoRA-Dash        | 0.41     | 70.9      | 82.2  | 80.5 | 90.2 | 80.1    | 83.5   | 68.9  | 80.8  | 79.6 |      |
| LoRA $ _{r=32} $ | 0.82     | 69.8      | 79.9  | 79.5 | 83.6 | 82.6    | 79.8   | 64.7  | 81.0  | 77.6 |      |
| LoRA-Dash        | 0.82     | 71.0      | 75.7  | 79.3 | 91.1 | 78.6    | 84.2   | 69.8  | 78.8  | 78.6 |      |
| LLaMA3-8B        | Fully FT | 100       | 75.3  | 89.9 | 81.5 | 95.8    | 87.6   | 91.6  | 79.3  | 87.4 | 86.1 |
| LoRA $ _{r=16} $ | 0.35     | 72.3      | 86.7  | 79.3 | 93.5 | 84.8    | 87.7   | 75.7  | 82.8  | 82.8 |      |
| LoRA-Dash        | 0.35     | 74.8      | 88.0  | 80.6 | 95.2 | 85.6    | 89.0   | 77.4  | 84.8  | 84.4 |      |
| LoRA $ _{r=32} $ | 0.70     | 70.8      | 85.2  | 79.9 | 91.7 | 84.3    | 84.2   | 71.2  | 79.0  | 80.8 |      |
| LoRA-Dash        | 0.70     | 75.3      | 88.5  | 80.2 | 95.7 | 86.8    | 90.7   | 80.2  | 85.6  | 85.4 |      |"
397,"| Hard Negative Type | R@100       | N@100       |
| ------------------ | ----------- | ----------- |
| Baseline           | 0.461±0.085 | 0.099±0.085 |
| Prerank_neg        | 0.575±0.095 | 0.127±0.028 |
| Coarse_neg         | 0.555±0.066 | 0.121±0.021 |
| Rank_neg           | 0.462±0.126 | 0.094±0.030 |
| Rank_pos           | 0.648±0.074 | 0.134±0.017 |
| Rerank_neg         | 0.577±0.091 | 0.119±0.019 |
| Rerank_pos         | 0.687±0.087 | 0.144±0.018 |
| Exposure_neg       | 0.603±0.093 | 0.137±0.016 |"
397,"|            | #Stage Sample | #Request      | #Users       | #Realshow_videos | #All_videos   |
| ---------- | ------------- | ------------- | ------------ | ---------------- | ------------- |
| 1st Period | 352,120,401   | 6,062,348     | 38,193       | 5,984,924        | 30,305,725    |
| 2nd Period | 1,572,217,303 | 3,308,233     | 35,073       | 3,627,694        | 55,665,503    |
| Total      | 1,924,337,704 | 9,370,581     | 42,472       | 8,773,147        | 82,216,301    |
|            | #Realshow     | #Like         | #Long_view   | #Effective_view  | #Follow       |
| 1st Period | 24,523,473    | 1,027,013     | 5,853,054    | 9,343,776        | 69,495        |
| 2nd Period | 13,721,842    | 618,158       | 3,111,439    | 5,063,751        | 37,558        |
| Total      | 38,245,315    | 1,645,171     | 8,964,493    | 14,407,527       | 107,053       |
|            | #Forward      | #Comment      | #Prerank_neg | #coarse_neg      | #Rank_pos     |
| 1st Period | 45,966        | 175,896       | 60,623,480   | 60,623,480       | 60,624,430    |
| 2nd Period | 23,769        | 114,741       | 132,329,320  | 132,329,320      | 33,082,330    |
| Total      | 69,735        | 290,637       | 192,952,800  | 192,952,800      | 93,706,760    |
|            | #Rank_neg     | #Rank         | #Rerank_pos  | #Rerank_neg      | #Re-rank      |
| 1st Period | 60,624,012    | 121,248,442   | 60,624,613   | 60,623,606       | 121,248,219   |
| 2nd Period | 33,082,330    | 1,307,558,663 | 33,082,330   | 33,082,330       | 1,307,558,663 |
| Total      | 93,706,342    | 1,428,807,105 | 93,706,943   | 93,705,936       | 1,428,806,882 |"
397,"| Neg Type   | #N          | AUC         | LogLoss     | Recall@100  | NDCG@100    |
| ---------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| Baseline   | -           | 0.718±0.001 | 0.592±0.003 | 0.271±0.027 | 0.059±0.027 |
| Coarse_neg | 1           | 0.705±0.002 | 0.608±0.006 | 0.321±0.012 | 0.072±0.027 |
| 10         | 0.633±0.016 | 0.773±0.018 | 0.392±0.012 | 0.088±0.004 |             |
| Rank_neg   | 1           | 0.704±0.002 | 0.615±0.004 | 0.353±0.013 | 0.079±0.003 |
| 10         | 0.618±0.016 | 0.825±0.027 | 0.454±0.011 | 0.102±0.005 |             |
| Rank_pos   | 1           | 0.704±0.001 | 0.603±0.001 | 0.275±0.027 | 0.061±0.004 |
| 10         | 0.623±0.020 | 0.769±0.003 | 0.290±0.002 | 0.069±0.004 |             |
| Rerank_neg | 1           | 0.702±0.001 | 0.616±0.005 | 0.337±0.007 | 0.076±0.001 |
| 10         | 0.608±0.021 | 0.821±0.019 | 0.380±0.014 | 0.084±0.003 |             |
| Rerank_pos | 1           | 0.703±0.001 | 0.607±0.003 | 0.264±0.011 | 0.060±0.003 |
| 10         | 0.618±0.024 | 0.782±0.025 | 0.285±0.015 | 0.069±0.003 |             |
| All        | 1           | 0.662±0.006 | 0.704±0.011 | 0.386±0.010 | 0.084±0.001 |
| 10         | 0.563±0.004 | 1.243±0.030 | 0.455±0.004 | 0.105±0.001 |             |"
397,"| Neg Type   | #N          | AUC         | LogLoss     | Recall@50   | NDCG@50     |
| ---------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| Baseline   | -           | 0.727±0.001 | 0.583±0.003 | 0.169±0.005 | 0.045±0.002 |
| Rank_neg   | 1           | 0.711±0.001 | 0.610±0.008 | 0.223±0.005 | 0.061±0.002 |
| 10         | 0.645±0.003 | 0.810±0.032 | 0.264±0.012 | 0.074±0.004 |             |
| Rank_pos   | 1           | 0.711±0.001 | 0.604±0.008 | 0.176±0.005 | 0.047±0.001 |
| 10         | 0.653±0.002 | 0.724±0.029 | 0.185±0.009 | 0.049±0.003 |             |
| Rerank_neg | 1           | 0.708±0.001 | 0.616±0.006 | 0.215±0.003 | 0.059±0.001 |
| 10         | 0.624±0.005 | 0.815±0.028 | 0.232±0.018 | 0.064±0.005 |             |
| Rerank_pos | 1           | 0.711±0.002 | 0.608±0.006 | 0.170±0.012 | 0.045±0.003 |
| 10         | 0.646±0.002 | 0.782±0.033 | 0.183±0.016 | 0.048±0.005 |             |
| All        | 1           | 0.675±0.003 | 0.697±0.010 | 0.234±0.005 | 0.064±0.002 |
| 10         | 0.602±0.005 | 1.076±0.049 | 0.278±0.027 | 0.078±0.007 |             |"
398,"| (single-shuffle) | (multi-shuffle) |
| ---------------- | --------------- |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | ▪               |
| ▪                | �               |"
398,"| u_0 | w_1 | w_2      | w_3      | w_4      |
| --- | --- | -------- | -------- | -------- |
| 0   | 0   | 0        | 0        | 0        |
| 1   | 0   | $ \eta $ | $ \eta $ | $ \eta $ |
| 1   | 0   | 0        | $ \eta $ | $ \eta $ |
| 1   | 0   | 0        | 0        | $ \eta $ |
| 0   | 0   | 0        | 0        | 0        |
| 1   | 0   | 0        | 0        | 0        |"
399,"| Task      | SST-2 | RTE  | CB   | BoolQ | WSC  | WiC  | MultiRC | COPA | ReCoRD | SQuAD | DROP |
| --------- | ----- | ---- | ---- | ----- | ---- | ---- | ------- | ---- | ------ | ----- | ---- |
| Zero-shot | 58.8  | 59.6 | 46.4 | 59.0  | 38.5 | 55.0 | 46.9    | 80.0 | 81.0   | 46.2  | 14.6 |
| ICL       | 87.0  | 62.1 | 57.1 | 66.9  | 39.4 | 50.5 | 53.1    | 87.0 | 82.3   | 75.9  | 29.5 |
| MeZO      | 91.3  | 68.2 | 66.1 | 68.1  | 61.5 | 59.4 | 59.4    | 88.0 | 81.3   | 81.8  | 31.3 |
| MeZO-LoRA | 89.6  | 67.9 | 67.8 | 73.5  | 63.5 | 60.2 | 61.3    | 84.0 | 81.5   | 82.1  | 31.3 |
| LOZO      | 91.7  | 70.4 | 69.6 | 71.9  | 63.5 | 60.8 | 63.0    | 89.0 | 81.3   | 84.9  | 30.7 |
| FT        | 91.8  | 70.9 | 84.1 | 76.9  | 63.5 | 70.1 | 71.1    | 79.0 | 74.1   | 84.9  | 31.3 |"
405,"| Sampler  | Method | #Bits  | IS↑    | FID↓  | sFID↓ | Prec.↑ |
| -------- | ------ | ------ | ------ | ----- | ----- | ------ |
| DDIM     | FP     | 32/32  | 235.84 | 12.96 | 25.99 | 92.63  |
| Baseline | 1/32   | 197.85 | 11.50  | 23.44 | 84.83 |        |
| BinaryDM | 1/32   | 215.55 | 10.86  | 21.10 | 88.43 |        |
| Baseline | 1/8    | 203.90 | 11.35  | 25.49 | 85.78 |        |
| BinaryDM | 1/8    | 211.43 | 11.23  | 24.12 | 88.09 |        |
| Baseline | 1/4    | 187.70 | 11.51  | 20.77 | 84.13 |        |
| BinaryDM | 1/4    | 208.42 | 10.78  | 20.40 | 87.61 |        |"
400,"| Models                 | Claimed | Short-version (16K)               | Long-version (32K) |
| ---------------------- | ------- | --------------------------------- | ------------------ |
| Length                 | CR      | STIC-1                            | STIC-2             |
|                        |         | Models with 7-10B Parameters      |                    |
| Mamba-2.8B             | 2K      | 11.3%                             | 23.8%              |
| FILM-7B                | 32K     | 36.0%                             | 22.4%              |
| Mistral-v0.2-7B        | 32K     | 81.8%                             | 25.7%              |
| Phi-3-mini-3-8B        | 128K    | 22.9%                             | 27.6%              |
| LLama3.1-8B            | 128K    | 93.5%                             | 23.4%              |
| Qwen2-7B               | 128K    | 60.0%                             | 23.3%              |
| LongWriter-llama3.1-8B | 128K    | 46.0%                             | 32.6%              |
|                        |         | Models Larger Than 20B Parameters |                    |
| Mixtral-8x7B           | 32K     | 83.0%                             | 34.4%              |
| Phi-3.5-8x7B           | 128K    | 26.9%                             | 46.4%              |
| LLama3.1-70B           | 128K    | 79.3%                             | 34.4%              |
| Qwen2-72B              | 128K    | 94.3%                             | 29.7%              |
| Closed-source Model    |         |                                   |                    |
| GPT-4o-mini            | 128K    | 97.0%                             | 34.8%              |
| GPT-4o                 | 128K    | 67.2%                             | 42.9%              |"
401,"| Task                 | $ \alpha $    | $ \Delta\epsilon $ | $ \varepsilon_{HOMO} $ | $ \varepsilon_{LUMO} $ | $ \mu $ | $ C_{v} $   | G    | H    | $ R^{2} $     | U    | $ U_{0} $ | ZPVE |
| -------------------- | ------------- | ------------------ | ---------------------- | ---------------------- | ------- | ----------- | ---- | ---- | ------------- | ---- | --------- | ---- |
| Units                | bohr $ ^{3} $ | meV                | meV                    | meV                    | D       | cal/(mol K) | meV  | meV  | bohr $ ^{3} $ | meV  | meV       | meV  |
| SchNet               | .235          | 63                 | 41                     | 34                     | .033    | .033        | 14   | 14   | .073          | 19   | 14        | 1.70 |
| DimeNet++            | .044          | 33                 | 25                     | 20                     | .030    | .023        | 8    | 7    | .331          | 6    | 6         | 1.21 |
| PaiNN                | .045          | 46                 | 28                     | 20                     | .012    | .024        | 7.35 | 5.98 | .066          | 5.83 | 5.85      | 1.28 |
| TorchMD-NET          | .059          | 36                 | 20                     | 18                     | .011    | .026        | 7.62 | 6.16 | .033          | 6.38 | 6.15      | 1.84 |
| SEGNN $ ^{\dagger} $ | .060          | 42                 | 24                     | 21                     | .023    | .031        | 15   | 16   | .660          | 13   | 15        | 1.62 |
| Equiformer           | .046          | 30                 | 15                     | 14                     | .011    | .023        | 7.63 | 6.63 | .251          | 6.74 | 6.59      | 1.26 |
| EMPP (1-Mask)        | .041          | 27                 | 14                     | 13                     | .0108   | .021        | 6.89 | 5.38 | .189          | 6.05 | 5.88      | 1.20 |
| EMPP (3-Mask)        | .039          | 26                 | 13                     | 12                     | .0096   | .019        | 6.32 | 5.02 | .154          | 5.72 | 5.25      | 1.18 |"
401,"| Methods            | Aspirin | Benzene | Ethanol | Malonaldehyde | Naphthalene | Salicylic acid | Toluene | Uracil |
| ------------------ | ------- | ------- | ------- | ------------- | ----------- | -------------- | ------- | ------ |
| energy             | forces  | energy  | forces  | energy        | forces      | energy         | forces  | energy |
| SchNet             | 16.0    | 58.5    | 3.5     | 13.4          | 3.5         | 16.9           | 5.6     | 28.6   |
| DimeNet            | 8.8     | 21.6    | 3.4     | 8.1           | 2.8         | 10.0           | 4.5     | 16.6   |
| PaiNN              | 6.9     | 14.7    | -       | -             | 2.7         | 9.7            | 3.9     | 13.8   |
| TorchMD-NET        | 5.3     | 11.0    | 2.5     | 8.5           | 2.3         | 4.7            | 3.3     | 7.3    |
| Equiformer         | 5.3     | 6.6     | 2.5     | 8.1           | 2.2         | 2.9            | 3.2     | 5.4    |
| EMPP (1-Mask)      | 5.1     | 6.4     | 2.3     | 7.5           | 2.2         | 2.7            | 3.0     | 5.2    |
| EMPP (3-Mask)      | 5.0     | 6.2     | 2.1     | 7.3           | 2.0         | 2.6            | 3.0     | 5.0    |
| EMPP (energy only) | 4.8     | 6.6     | 2.1     | 8.5           | 2.0         | 2.5            | 2.8     | 5.1    |
| EMPP (force only)  | 5.3     | 6.4     | 2.8     | 7.1           | 2.3         | 2.4            | 3.3     | 4.9    |"
401,"| Task           | $ \alpha $    | $ \Delta\epsilon $ | $ \varepsilon_{HOMO} $ | $ \varepsilon_{LUMO} $ | $ \mu $ | $ C_{v} $   | G    | H    | $ R^{2} $     | U    | $ U_{0} $ | ZPVE |
| -------------- | ------------- | ------------------ | ---------------------- | ---------------------- | ------- | ----------- | ---- | ---- | ------------- | ---- | --------- | ---- |
| Units          | bohr $ ^{3} $ | meV                | meV                    | meV                    | D       | cal/(mol K) | meV  | meV  | bohr $ ^{3} $ | meV  | meV       | meV  |
| AttrMask       | .072          | 50.0               | 31.3                   | 37.8                   | .020    | 0.062       | 11.2 | 11.4 | .423          | 10.8 | 10.7      | 1.90 |
| Transformer-M  | .041          | 27.4               | 17.5                   | 16.2                   | .037    | 0.022       | 9.63 | 9.39 | .075          | 9.41 | 9.37      | 1.18 |
| SE(3)-DDM      | .046          | 40.2               | 23.5                   | 19.5                   | .015    | .024        | 7.65 | 7.09 | .122          | 6.99 | 6.92      | 1.31 |
| 3D-EMGP        | .057          | 37.1               | 21.3                   | 18.2                   | .020    | .026        | 9.30 | 8.70 | .092          | 8.60 | 8.60      | 1.38 |
| DP-TorchMD-Net | .0517         | 31.8               | 17.7                   | 14.3                   | .012    | .020        | 6.91 | 6.45 | .4496         | 6.11 | 6.57      | 1.71 |
| Frad           | 0.037         | 27.8               | 15.3                   | 13.7                   | .010    | .020        | 6.19 | 5.55 | .342          | 5.62 | 5.33      | 1.42 |
| EMPP (3-Mask)  | .035          | 25.8               | 13.7                   | 13.4                   | .014    | .019        | 6.45 | 5.73 | .241          | 5.34 | 5.08      | 1.27 |"
403,"| Dataset          | Onestep-RL | CQL   | IQL   | IVR   | EQL   | Diffuser | DTQL  | AlignIQL | SfBC  | DQL   | IDQL-A | DAC (ours)          |
| ---------------- | ---------- | ----- | ----- | ----- | ----- | -------- | ----- | -------- | ----- | ----- | ------ | ------------------- |
| halfcheetah-m    | 48.4       | 44.0  | 47.4  | 48.3  | 48.3  | 44.2     | 57.9  | 46.0     | 45.9  | 51.1  | 51.0   | 59.1  $ \pm $  0.4  |
| hopper-m         | 59.6       | 58.5  | 66.3  | 75.5  | 74.2  | 58.5     | 99.6  | 56.1     | 57.1  | 90.5  | 65.4   | 101.2  $ \pm $  2.0 |
| walker2d-m       | 81.8       | 72.5  | 78.3  | 84.2  | 84.2  | 79.7     | 89.4  | 78.5     | 77.9  | 87.0  | 82.5   | 96.8  $ \pm $  3.6  |
| halfcheetah-m-r  | 38.1       | 45.5  | 44.2  | 44.8  | 45.2  | 42.2     | 50.9  | 41.1     | 37.1  | 47.8  | 45.9   | 55.0  $ \pm $  0.2  |
| hopper-m-r       | 97.5       | 95.0  | 94.7  | 99.7  | 100.7 | 96.8     | 100.0 | 74.8     | 86.2  | 101.3 | 92.1   | 103.1  $ \pm $  0.3 |
| walker2d-m-r     | 49.5       | 77.2  | 73.9  | 81.2  | 82.2  | 61.2     | 88.5  | 76.5     | 65.1  | 95.5  | 85.1   | 96.8  $ \pm $  1.0  |
| halfcheetah-m-e  | 93.4       | 91.6  | 86.7  | 94.0  | 94.2  | 79.8     | 92.7  | 89.1     | 92.6  | 96.8  | 95.9   | 99.1  $ \pm $  0.9  |
| hopper-m-e       | 103.3      | 105.4 | 91.5  | 111.8 | 111.2 | 107.2    | 109.3 | 107.1    | 108.6 | 111.1 | 108.6  | 111.7  $ \pm $  1.0 |
| walker2d-m-e     | 113.0      | 108.8 | 109.6 | 110.2 | 112.7 | 108.4    | 110.0 | 111.9    | 109.8 | 110.1 | 112.7  | 113.6  $ \pm $  3.5 |
| locomotion total | 684.6      | 698.5 | 749.7 | 749.7 | 752.9 | 678.0    | 798.3 | 681.1    | 680.3 | 791.2 | 739.2  | 836.4               |
| antmaze-u        | 64.3       | 74.0  | 87.5  | 93.2  | 93.8  | -        | 94.8  | 94.8     | 92.0  | 93.4  | 94.0   | 99.5  $ \pm $  0.9  |
| antmaze-u-div    | 60.7       | 84.0  | 62.2  | 74.0  | 82.0  | -        | 78.8  | 82.4     | 85.3  | 66.2  | 80.2   | 85.0  $ \pm $  7.9  |
| antmaze-m-play   | 0.3        | 61.2  | 71.2  | 80.2  | 76.0  | -        | 79.6  | 80.5     | 81.3  | 76.6  | 84.2   | 85.8  $ \pm $  5.5  |
| antmaze-m-div    | 0.0        | 53.7  | 70.0  | 79.1  | 73.6  | -        | 82.2  | 85.5     | 82.0  | 78.6  | 84.8   | 84.0  $ \pm $  6.2  |
| antmaze-l-play   | 0.0        | 15.8  | 39.6  | 53.2  | 46.5  | -        | 52.0  | 65.2     | 59.3  | 46.4  | 63.5   | 50.3  $ \pm $  8.6  |
| antmaze-l-div    | 0.0        | 14.9  | 47.5  | 52.3  | 49.0  | -        | 66.4  | 54.0     | 45.5  | 56.6  | 67.9   | 55.3  $ \pm $  10.3 |
| antmaze total    | 125.3      | 303.6 | 378.0 | 432.0 | 420.9 | -        | 441.4 | 474.8    | 445.4 | 417.8 | 474.6  | 459.9               |"
405,"| Method  | #Bits | FID↓ | sFID↓ | Prec.↑ | Recall↑ |
| ------- | ----- | ---- | ----- | ------ | ------- |
| FP      | 32/32 | 3.09 | 7.08  | 65.82  | 45.36   |
| Vanilla | 1/32  | 8.43 | 13.11 | 65.45  | 29.88   |
| +EBB    | 1/32  | 7.39 | 12.34 | 65.98  | 35.84   |
| +LRM    | 1/32  | 6.99 | 12.15 | 67.51  | 36.80   |"
405,"| Model       | Method         | #Bits | Size(MB) | OPs(×109) | FID↓ |
| ----------- | -------------- | ----- | -------- | --------- | ---- |
| LDM-4       | Full-Precision | 4/4   | 1045.4   | 96.0      | 3.09 |
| Q-Diffusion | 4/4            | 134.9 | 24.3     | 427.46    |      |
| EfficientDM | 4/4            | 134.9 | 24.3     | 10.60     |      |
| LSQ         | 2/4            | 69.8  | 12.3     | 12.95     |      |
| BinaryDM    | 1/4            | 35.8  | 6.3      | 7.74      |      |"
404,"| Metrics                    | QM9                       | DRUG                    |
| -------------------------- | ------------------------- | ----------------------- |
| Atom Sta (%)  $ \uparrow $ | Mol Sta (%)  $ \uparrow $ | Valid (%)  $ \uparrow $ |
| Methods                    | 99                        | 95.2                    |
| G-Schnet                   | 95.7                      | 68.1                    |
| GDM                        | 97                        | 63.2                    |
| GDM-AUG                    | 97.6                      | 71.6                    |
| GraphLDM                   | 97.2                      | 70.5                    |
| GraphLDM-AUG               | 97.9                      | 78.7                    |
| EDM                        | 98.7                      | 82                      |
| EDM-Bridge                 | 98.8                      | 84.6                    |
| GeoLDM                     | 98.9(0.1)                 | 89.4(0.5)               |
| GCDM                       | 98.7(0.0)                 | 85.7(0.4)               |
| ENF                        | 85                        | 4.9                     |
| EquiFM                     | 98.9(0.1)                 | 88.3(0.3)               |
| GOAT                       | 98.4                      | 84.1                    |
| GeoBFN                     | 99.08(0.03)               | 90.87(0.1)              |
| GeoRCG (EDM)               | 99.12(0.03)               | 92.32(0.06)             |"
404,"| Methods            | Atom Stab  $ \uparrow $ | Mol Stab  $ \uparrow $ | Valid  $ \uparrow $ | Energy  $ \downarrow $ | Strain  $ \downarrow $ |
| ------------------ | ----------------------- | ---------------------- | ------------------- | ---------------------- | ---------------------- |
| MiDi               | 99.8                    | 91.6                   | 77.8                | -                      | -                      |
| EQGAT-diff         | 99.8(0.01)              | 93.4(0.21)             | 94.6(0.24)          | 148.8(0.9)             | 140.2(0.7)             |
| SemlaFlow $ ^{*} $ | 99.8(0.00)              | 97.4(0.07)             | 94.4(0.17)          | 95.72(1.24)            | 56.42(1.07)            |
| GeoRCG (Semla)     | 99.8(0.00)              | 97.6(0.00)             | 95.3(0.13)          | 88.6(1.03)             | 47.64(1.10)            |"
404,"| Properties        | α          | Δε         | εHOMO      | εLUMO      | μ      | Cv           |
| ----------------- | ---------- | ---------- | ---------- | ---------- | ------ | ------------ |
| QM9 (lower bound) | 0.1        | 64         | 39         | 36         | 0.043  | 0.04         |
| Random            | 9.01       | 1470       | 646        | 1457       | 1.616  | 6.857        |
| N.atoms           | 3.86       | 866        | 426        | 813        | 1.053  | 1.971        |
| EDM               | 2.76       | 655        | 356        | 584        | 1.111  | 1.101        |
| GeoLDM            | 2.37       | 587        | 340        | 522        | 1.108  | 1.025        |
| QCDM              | 1.97       | 602        | 344        | 479        | 0.844  | 0.689        |
| EquiFM            | 2.41       | 591        | 337        | 530        | 1.106  | 1.033        |
| GOAT              | 2.74       | 605        | 350        | 534        | 1.01   | 0.883        |
| LDM-3DG*          | 12.29      | 1160       | 583        | 1093       | 1.42   | 5.74         |
| GeoBFN            | 2.34       | 577        | 328        | 516        | 0.998  | 0.949        |
| GeoBCG (EDM)      | 0.86(0.01) | 325.2(3.4) | 502.2(1.2) | 257.9(5.5) | 55.84% | 0.475(0.005) |"
404,"| Metrics      | # Steps | Atom Sta (%) ↑    | Mol Sta (%) ↑      | Valid (%) ↑      |
| ------------ | ------- | ----------------- | ------------------ | ---------------- |
| Data         | -       | 99                | 95.2               | 97.7             |
| EquiFM       | 200     | 98.9(0.1)         | 88.3(0.3)          | 94.7(0.4)        |
| GOAT         | 90      | 98.4              | 84.1               | 90.9             |
| EDM          | 50      | 97.0(0.1)         | 66.4(0.2)          | -                |
| EDM-Bridge   | 50      | 97.3(0.1)         | 69.2(0.2)          | -                |
| GeoBFN       | 50      | 98.28(0.1)        | 85.11(0.5)         | 92.27(0.4)       |
| GeoRCG (EDM) | 50      | 98.75(0.05) 1.80% | 89.08(0.52) 31.16% | 95.05(0.33)      |
| EDM          | 100     | 97.3(0.1)         | 69.8(0.2)          | -                |
| EDM-Bridge   | 100     | 97.9(0.1)         | 72.3(0.2)          | -                |
| GeoBFN       | 100     | 98.64(0.1)        | 87.21(0.3)         | 93.03(0.3)       |
| GeoRCG (EDM) | 100     | 99.08(0.03) 1.83% | 91.85(0.34) 31.59% | 96.49(0.27)      |
| EDM          | 500     | 98.5(0.1)         | 81.2(0.1)          | -                |
| EDM-Bridge   | 500     | 98.7(0.1)         | 83.7(0.1)          | -                |
| GeoBFN       | 500     | 98.78(0.8)        | 88.42(0.2)         | 93.35(0.2)       |
| GeoRCG (EDM) | 500     | 99.09(0.01) 0.60% | 91.89(0.24) 13.17% | 96.57(0.12)      |
| EDM          | 1000    | 98.7              | 82                 | 91.9             |
| EDM-Bridge   | 1000    | 98.8              | 84.6               | 92               |
| GeoBFN       | 1000    | 99.08(0.06)       | 90.87(0.2)         | 95.31(0.1)       |
| GeoRCG (EDM) | 1000    | 99.12(0.03) 0.43% | 92.32(0.06) 12.59% | 96.52(0.2) 5.03% |"
404,"| # Steps | Atom Sta (%)  $ \uparrow $ | Valid (%)  $ \uparrow $ |
| ------- | -------------------------- | ----------------------- |
|         | GeoBFN                     | GeoRCG (EDM)            |
| 50      | 75.11                      | 81.44(0.10)             |
| 100     | 78.89                      | 83.02(0.06)             |
| 500     | 81.39                      | 84.03(0.37)             |
| 1000    | 85.6                       | 84.3(0.12)              |"
404,"| # Steps        | Method      | Energy  $ \downarrow $ | Strain  $ \downarrow $ | Atom-Stab.  $ \uparrow $ | Mol.-Stab.  $ \uparrow $ | Validity   |
| -------------- | ----------- | ---------------------- | ---------------------- | ------------------------ | ------------------------ | ---------- |
|                |             | kcal·mol $ ^{-1} $     | kcal·mol $ ^{-1} $     | %                        | %                        |            |
| 100            | SemlaFlow   | 95.72(1.24)            | 56.42(1.07)            | 99.8(0.00)               | 97.4(0.07)               | 94.4(0.17) |
| GeoRCG (Semla) | 88.6(1.03)  | 47.64(1.10)            | 99.8(0.00)             | 97.6(0.00)               | 95.3(0.13)               |            |
| 50             | SemlaFlow   | 100.60(0.55)           | 60.31(0.13)            | 99.8(0.00)               | 96.9(0.11)               | 94.6(0.17) |
| GeoRCG (Semla) | 91.60(1.03) | 50.50(0.65)            | 99.8(0.00)             | 97.2(0.20)               | 95.3(0.22)               |            |
| 20             | SemlaFlow   | 117.03(1.37)           | 76.99(1.18)            | 99.7(0.01)               | 95.4(0.17)               | 93.2(0.30) |
| GeoRCG (Semla) | 99.83(0.91) | 62.88(0.56)            | 99.7(0.00)             | 95.5(0.13)               | 94.2(0.01)               |            |"
405,"| Dataset       | Method      | #Bits | Size(MB) | Time(h) | FID↓   |
| ------------- | ----------- | ----- | -------- | ------- | ------ |
| LSUN-Bedrooms | Q-Diffusion | 4/4   | 134.9    | 13.7    | 427.46 |
| BinaryDM      | 1/4         | 35.8  | 11.3     | 13.93   |        |
| LSUN-Churches | Q-Diffusion | 4/4   | 144.2    | 10.9    | 198.35 |
| BinaryDM      | 1/4         | 38.1  | 9.0      | 15.11   |        |"
407,"| Dataset      | Method | Pick Score | HPS   | Aesthetics | CLIP  | Image Reward |
| ------------ | ------ | ---------- | ----- | ---------- | ----- | ------------ |
| PickV2       | SFT    | 70.20      | 84.20 | 75.80      | 61.20 | 76.40        |
| Diff.-DPO    | 71.60  | 70.20      | 66.20 | 58.80      | 63.60 |              |
| Diff.-KTO    | 71.40  | 84.40      | 72.60 | 60.02      | 77.00 |              |
| DSPO         | 73.60  | 84.80      | 76.20 | 61.80      | 78.00 |              |
| Parti-Prompt | SFT    | 64.27      | 85.72 | 75.74      | 54.72 | 71.38        |
| Diff.-DPO    | 61.18  | 66.48      | 60.42 | 55.45      | 62.19 |              |
| Diff.-KTO    | 64.80  | 86.16      | 72.92 | 54.34      | 71.51 |              |
| DSPO         | 65.32  | 87.50      | 76.96 | 54.86      | 71.75 |              |
| HPSV2        | SFT    | 79.03      | 91.97 | 78.56      | 60.47 | 80.78        |
| Diff.-DPO    | 76.06  | 72.13      | 66.00 | 58.50      | 64.22 |              |
| Diff.-KTO    | 79.18  | 92.15      | 77.87 | 59.28      | 81.96 |              |
| DSPO         | 79.90  | 92.56      | 80.59 | 61.13      | 82.31 |              |"
407,"| Dataset      | Method | Pick Score | HPS   | Aesthetics | CLIP  | Image Reward |
| ------------ | ------ | ---------- | ----- | ---------- | ----- | ------------ |
| PickV2       | SFT    | 20.80      | 40.60 | 23.20      | 44.80 | 34.40        |
| Diff.-DPO    | 75.20  | 76.20      | 54.10 | 59.40      | 65.20 |              |
| MaPO         | 54.40  | 69.60      | 68.20 | 51.20      | 61.40 |              |
| DSPO         | 74.00  | 80.00      | 54.20 | 59.60      | 68.60 |              |
| Parti-Prompt | SFT    | 17.03      | 33.02 | 27.81      | 36.58 | 37.18        |
| Diff.-DPO    | 65.44  | 74.08      | 56.86 | 60.54      | 66.85 |              |
| MaPO         | 58.34  | 66.54      | 68.23 | 47.43      | 58.64 |              |
| DSPO         | 67.46  | 81.80      | 57.84 | 55.02      | 73.47 |              |
| HPSV2        | SFT    | 18.18      | 45.28 | 26.72      | 39.13 | 47.22        |
| Diff.-DPO    | 70.31  | 80.81      | 50.78 | 59.31      | 68.75 |              |
| MaPO         | 59.62  | 77.90      | 62.31 | 50.90      | 62.09 |              |
| DSPO         | 72.59  | 83.47      | 51.41 | 57.34      | 70.09 |              |"
408,"|          | CINIC-10 (ResNet-18), BA=80.2±0.1 | Clothing1M (ResNet-50), BA=69.0±0.3 |
| -------- | --------------------------------- | ----------------------------------- |
|          | Mean                              | GM                                  |
| RLF      | 76.9±0.4                          | 79.3±0.3                            |
| LMA      | 54.6±1.2                          | 75.0±0.6                            |
| CPA      | 45.9±0.4                          | 71.2±5.2                            |
| HIPS+LMA | 75.3±0.1                          | 68.7±0.1                            |
| HIPS+CPA | 74.2±1.1                          | 65.8±0.5                            |"
408,"|          | CIFAR-100 (WideResNet-28), BA=66.8±0.5 | CIFAR-10 (ResNet-18), BA=87.7±1.2 |
| -------- | -------------------------------------- | --------------------------------- |
|          | Mean                                   | GM                                |
| RLF      | 65.2±0.7                               | 65.2±0.3                          |
| LMA      | 41.8±4.4                               | 51.3±0.1                          |
| CPA      | 43.3±1.2                               | 56.7±0.9                          |
| HIPS+LMA | 50.3±3.3                               | 34.3±0.4                          |
| HIPS+CPA | 47.2±4.2                               | 32.6±4.5                          |"
408,"| 1: Input: Pred.  $ Y_{i}^{t+1}(\mathcal{D}_{p}) $ , weights  $ p_{i} $ ,  $ \forall i \in N $ , aggregation method AGG.                                                         |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2:  $ \sigma_{i} \leftarrow \text{AGG}(Y_{i}^{t+1}(\mathcal{D}_{p})), \forall i \in [n] $  ▷ Compute outlier scores                                                             |
| 3:  $ p_{i}^{t+1} \leftarrow p_{i} \exp(-\sigma_{i}), \forall i \in [n] $  ▷ Update weights                                                                                     |
| 4:  $ \bar{Y}_{i}^{t+1}(x) \leftarrow \frac{1}{\sum_{j=1}^{n} p_{j}^{t+1}} \sum_{i=1}^{N} p_{i}^{t+1} Y_{i}^{t+1}(x) $  ▷ Comp. weighted sum  $ \forall x \in \mathcal{D}_{p} $ |
| 5: Output:  $ \bar{Y}_{i}^{t+1}(x), p_{i}^{t+1}, \forall i \in [N] $                                                                                                            |"
409,"| Models                         | Hidden Dim | # of Classes | # of Layers | Multiple Classifiers (#Params) | Single Classifier (#Params) | #Param Saving |
| ------------------------------ | ---------- | ------------ | ----------- | ------------------------------ | --------------------------- | ------------- |
| DeiT-S(Touvron et al., 2021)   | 384        | 1,000        | 12          | 26.27M                         | 22.05M                      | 16.07%        |
| DeiT-B(Touvron et al., 2021)   | 768        | 1,000        | 12          | 95.02M                         | 86.57M                      | 8.89%         |
| GPT-2(Radford et al., 2019)    | 768        | 50,257       | 12          | 541.57M                        | 117.35M                     | 78.39%        |
| GPT-3(Brown et al., 2020)      | 12,288     | 50,257       | 96          | 233.67B                        | 175.63B                     | 25.10%        |
| LLAMA-2 (Touvron et al., 2023) | 4,096      | 32,000       | 40          | 75.11B                         | 70.35B                      | 6.81%         |"
411,"| Method               | Automatic Metric     | Human Evaluation         |
| -------------------- | -------------------- | ------------------------ |
| CLIP-F  $ \uparrow $ | CLIP-T  $ \uparrow $ | Warp-Err  $ \downarrow $ |
| FateZero             | 95.75                | 33.78                    |
| ControlVideo         | 97.71                | 34.41                    |
| TokenFlow            | 96.48                | 34.59                    |
| Ground-A-Video       | 95.17                | 35.09                    |
| DMT                  | 96.34                | 34.09                    |
| VideoGrain(ours)     | 98.63                | 36.56                    |"
412,"| Method          | Chamfer $ \downarrow $ | F-score $ \uparrow $ | RI $ \uparrow $ | VOI $ \downarrow $ | SC $ \uparrow $ | Chamfer $ \downarrow $ | F-score $ \uparrow $ | RI $ \uparrow $ | VOI $ \downarrow $ | SC $ \uparrow $ |
| --------------- | ---------------------- | -------------------- | --------------- | ------------------ | --------------- | ---------------------- | -------------------- | --------------- | ------------------ | --------------- |
| PlanarRecon     | 9.80                   | 49.0                 | 0.909           | 3.27               | 0.265           | 14.29                  | 43.8                 | 0.900           | 3.49               | 0.231           |
| AirPlanes       | 6.01                   | 55.1                 | 0.944           | 2.51               | 3.41            | 8.91                   | 44.1                 | 0.931           | 2.90               | 0.219           |
| FineRecon       | 5.16                   | 70.6                 | -               | -                  | -               | 5.52                   | 74.0                 | -               | -                  | -               |
| +Seq.RANSAC     | 5.43                   | 66.7                 | 0.941           | 2.56               | 0.276           | 5.36                   | 75.3                 | 0.929           | 2.79               | 0.252           |
| +AirPlanes      | 5.44                   | 66.2                 | 0.947           | 2.43               | 0.310           | 5.37                   | 75.5                 | 0.941           | 2.66               | 0.277           |
| NeuRIS          | 7.96                   | 63.2                 | -               | -                  | -               | 4.83                   | 81.2                 | -               | -                  | -               |
| +Seq.RANSAC     | 8.11                   | 59.3                 | 0.945           | 2.57               | 0.293           | 4.84                   | 80.9                 | 0.941           | 2.46               | 0.315           |
| +AirPlanes      | 6.17                   | 61.0                 | 0.943           | 2.55               | 0.291           | 4.69                   | 79.9                 | 0.943           | 2.53               | 0.287           |
| MonoSDF         | 5.18                   | 69.7                 | -               | -                  | -               | 4.85                   | 77.4                 | -               | -                  | -               |
| +Seq.RANSAC     | 5.67                   | 65.9                 | 0.945           | 2.38               | 0.333           | 5.09                   | 77.7                 | 0.939           | 2.47               | 0.288           |
| +AirPlanes      | 5.45                   | 66.6                 | 0.948           | 2.38               | 0.346           | 5.29                   | 74.2                 | 0.935           | 2.69               | 0.264           |
| Ours@PlaneRecTR | 5.02                   | 68.7                 | 0.949           | 2.37               | 0.364           | 6.17                   | 70.0                 | 0.939           | 2.72               | 0.301           |
| Ours@SAM        | 4.59                   | 71.2                 | 0.955           | 2.25               | 0.376           | 4.60                   | 79.7                 | 0.950           | 2.38               | 0.356           |"
413,"| Model        | Params | Tflops | L2 Loss |
| ------------ | ------ | ------ | ------- |
| GINO         | 72M    | 0.73   | 0.2445  |
| MGN          | 101M   | 32.16  | 0.2617  |
| OFormer      | 131M   | 17.34  | 0.3386  |
| LDM_{S}-FF   | 198M   | 0.81   | 0.1522  |
| LDM_{M}-FF   | 667M   | 1.16   | 0.1309  |
| LDM_{S}-Text | 313M   | 0.83   | 0.1796  |
| LDM_{M}-Text | 804M   | 1.20   | 0.1476  |"
413,"| Model        | Params | Tflops | L2 Loss         |
| ------------ | ------ | ------ | --------------- |
| FNO          | 510M   | 0.85   | 0.5126          |
| Unet         | 580M   | 21.48  | 0.3050          |
| Dil-Resnet   | 33.2M  | 58.75  | 0.4466          |
| ACDM         | 404M   | 51.23  | 0.4766          |
| LDM_{S}-FF   | 243M   | 1.41   | 0.3459          |
| LDM_{M}-FF   | 725M   | 1.68   | 0.3177          |
| LDM_{L}-FF   | 1.55B  | 2.19   | 0.2728          |
| LDM_{S}-Text | 334M   | 1.39   | 0.4290 $ ^{*} $ |
| LDM_{M}-Text | 825M   | 1.65   | 0.3158 $ ^{*} $ |
| LDM_{L}-Text | 2.69B  | 2.53   | 0.2944 $ ^{*} $ |"
414,"| Dataset      | Study        | Acc   | MF1   | Kappa |
| ------------ | ------------ | ----- | ----- | ----- |
| EDF20        | DEEPSLEEPNET | 81.9  | 76.6  | 0.760 |
| ARNN+SVM     | 79.1         | 69.8  | 0.700 |       |
| MULTITASKCNN | 83.1         | 75.0  | 0.77  |       |
| DFSC         | 84.44        | 78.25 | 0.784 |       |
| RESATTEN     | 84.3         | 79.0  | 0.78  |       |
| MISC         | 81.9         | 74.4  | 0.75  |       |
| TMCEK (Ours) | 85.0         | 80.2  | 0.80  |       |
| EDF78        | DEEPSLEEPNET | 77.8  | 73.9  | 0.73  |
| SLEEPEEGNET  | 74.2         | 69.6  | 0.66  |       |
| RESNETLSTM   | 78.9         | 71.4  | 0.71  |       |
| MULTITASKCNN | 79.6         | 72.8  | 0.72  |       |
| ATTNSLEEP    | 81.3         | 75.1  | 0.74  |       |
| MISC         | 77.4         | 69.8  | 0.68  |       |
| TMCEK (Ours) | 81.4         | 77.5  | 0.75  |       |
| SHHS         | DEEPSLEEPNET | 81.0  | 73.9  | 0.73  |
| SLEEPEEGNET  | 73.9         | 68.4  | 0.65  |       |
| RESNETLSTM   | 83.3         | 69.4  | 0.76  |       |
| MULTITASKCNN | 81.4         | 71.2  | 0.74  |       |
| ATTNSLEEP    | 84.2         | 75.3  | 0.78  |       |
| MISC         | 79.1         | 72.6  | 0.71  |       |
| TMCEK (Ours) | 84.3         | 78.0  | 0.79  |       |"
415,"| Algorithm            | Training             | ARC-e | Hella | Piqa  | Sciq  | Lamb  | Avg $ \uparrow $ |
| -------------------- | -------------------- | ----- | ----- | ----- | ----- | ----- | ---------------- |
| SMoE                 | pre-train 20B tokens | 47.14 | 35.51 | 64.69 | 76.2  | 14.61 | 47.63            |
| +sft                 | 50.93                | 35.82 | 65.61 | 74.7  | 17.81 | 48.97 |                  |
| +sft (freeze router) | 50.59                | 35.78 | 66.32 | 74.7  | 18.18 | 49.11 |                  |
| Speed: 48.87 s/step  | pre-train 40B tokens | 52.57 | 40.85 | 67.74 | 83.4  | 26.74 | 54.26            |
| +sft                 | 53.70                | 42.07 | 68.61 | 83.5  | 32.80 | 56.13 |                  |
| +sft (freeze router) | 53.45                | 41.94 | 68.88 | 83.1  | 32.06 | 55.88 |                  |
| RMoE                 | pre-train 20B tokens | 47.01 | 35.91 | 65.23 | 78.7  | 19.13 | 49.20            |
| +sft                 | 48.53                | 36.90 | 66.21 | 79.6  | 24.74 | 51.20 |                  |
| +sft (freeze router) | 49.24                | 36.79 | 66.16 | 79.7  | 24.32 | 51.24 |                  |
| Speed: 49.07 s/step  | pre-train 40B tokens | 51.18 | 41.38 | 67.79 | 83.6  | 32.58 | 55.31            |
| +sft                 | 53.20                | 43.05 | 68.55 | 83.8  | 37.16 | 57.15 |                  |
| +sft (freeze router) | 53.11                | 43.16 | 68.77 | 82.8  | 37.57 | 57.08 |                  |"
417,"| LLM                          | N-Gram       | Fixed-Key-List |
| ---------------------------- | ------------ | -------------- |
| Non                          | KGW          | Aar            |
| Water-Probe-v1 (w. prompt 1) |              |                |
| Qwen2.5-1.5B                 | 0.02 ± 0.02  | 0.37 ± 0.02    |
| OPT-2.7B                     | 0.05 ± 0.01  | 0.47 ± 0.01    |
| Llama-3.2-3B                 | 0.04 ± 0.02  | 0.53 ± 0.01    |
| Qwen2.5-3B                   | 0.03 ± 0.01  | 0.33 ± 0.02    |
| Llama2-7B                    | 0.02 ± 0.01  | 0.42 ± 0.01    |
| Mixtral-7B                   | 0.01 ± 0.02  | 0.41 ± 0.01    |
| Qwen2.5-7B                   | 0.07 ± 0.04  | 0.41 ± 0.02    |
| Llama-3.1-8B                 | 0.01 ± 0.02  | 0.41 ± 0.02    |
| Llama2-13B                   | 0.01 ± 0.03  | 0.41 ± 0.01    |
| Average                      | 0.029        | 0.418          |
| Water-Probe-v2 (w. prompt 1) |              |                |
| Qwen2.5-1.5B                 | 0.02 ± 0.02  | 0.30 ± 0.01    |
| OPT-2.7B                     | 0.04 ± 0.03  | 0.29 ± 0.02    |
| Llama-3.2-3B                 | 0.00 ± 0.01  | 0.31 ± 0.01    |
| Qwen2.5-3B                   | 0.03 ± 0.02  | 0.35 ± 0.04    |
| Llama2-7B                    | 0.04 ± 0.02  | 0.34 ± 0.01    |
| Mixtral-7B                   | 0.09 ± 0.01  | 0.34 ± 0.04    |
| Qwen2.5-7B                   | -0.01 ± 0.04 | 0.26 ± 0.02    |
| Llama-3.1-8B                 | 0.01 ± 0.00  | 0.31 ± 0.01    |
| Llama2-13B                   | 0.01 ± 0.02  | 0.35 ± 0.01    |
| Average                      | 0.026        | 0.317          |"
417,"| Model            | Similarity | Std Dev | Z-score | Watermarked? |
| ---------------- | ---------- | ------- | ------- | ------------ |
| GPT-4o-mini      | -0.005     | 0.018   | -5.984  | No           |
| GPT-4o           | 0.017      | 0.020   | -4.211  | No           |
| GPT-3.5-turbo    | 0.028      | 0.030   | -2.362  | No           |
| Gemini-1.5-flash | 0.027      | 0.049   | -1.474  | No           |
| Gemini-1.5-pro   | 0.018      | 0.038   | -2.135  | No           |"
422,"| p_{wl}       | Frequency of p_{wl}              |
| ------------ | -------------------------------- |
|              | Llama-3.1-Nemotron-70B-Reward-HF |
| (0.00, 0.05) | 1,184                            |
| [0.05, 0.10) | 363                              |
| [0.10, 0.90) | 3,636                            |
| [0.90, 0.99) | 1,574                            |
| [0.99, 1.00) | 1,795                            |
| Total        | 8,552                            |"
427,"| Method          | Bi-TSP150          | Bi-TSP200           |
| --------------- | ------------------ | ------------------- |
| HV $ \uparrow $ | Gap $ \downarrow $ | Time $ \downarrow $ |
| WS-LKH          | 0.7149             | -1.65%              |
| MOEA/D          | 0.6809             | 3.18%               |
| NSGA-II         | 0.6659             | 5.32%               |
| MOGLS           | 0.6768             | 3.77%               |
| PPLS/D-C        | 0.6784             | 3.54%               |
| DRL-MOA         | 0.6901             | 1.88%               |
| MDRL            | 0.6922             | 1.58%               |
| EMNH            | 0.6930             | 1.46%               |
| PMOCO           | 0.6910             | 1.75%               |
| GIMF-P          | 0.6958             | 1.07%               |
| CNH             | 0.6985             | 0.68%               |
| GIMF-C          | 0.6993             | 0.57%               |
| MDRL-Aug        | 0.6976             | 0.81%               |
| EMNH-Aug        | 0.6983             | 0.71%               |
| PMOCO-Aug       | 0.6967             | 0.94%               |
| GIMF-P-Aug      | 0.7003             | 0.43%               |
| CNH-Aug         | 0.7025             | 0.11%               |
| GIMF-C-Aug      | 0.7033             | 0.00%               |"
423,"| $ \mathcal{X}_{tr} \downarrow \mathcal{X}_{te} $ | dataset  $ \rightarrow $ | Navier-Stokes       |
| ------------------------------------------------ | ------------------------ | ------------------- |
| In-t                                             | Out-t                    |                     |
| $ \pi = 100\% $  regular grid                    | DeepONet                 | 4.72e-2 \pm 2.84e-2 |
| FNO                                              | 5.68e-4 \pm 7.62e-5      | 8.95e-3 \pm 1.50e-3 |
| MP-PDE                                           | 4.39e-4 \pm 8.78e-5      | 4.46e-3 \pm 1.28e-3 |
| DINO                                             | 1.27e-3 \pm 2.22e-5      | 1.11e-2 \pm 2.28e-3 |
| CORAL                                            | 1.86e-4 \pm 1.44e-5      | 1.02e-3 \pm 8.62e-5 |
| MARBLE (Ours)                                    | 3.52e-5 \pm 5.31e-6      | 5.04e-4 \pm 3.68e-5 |
| $ \pi = 20\% $  irregular grid                   | DeepONet                 | 8.37e-1 \pm 2.07e-2 |
| FNO + lin. int.                                  | 3.97e-3 \pm 8.03e-4      | 9.92e-3 \pm 2.36e-3 |
| MP-PDE                                           | 3.98e-2 \pm 1.69e-2      | 1.31e-1 \pm 5.34e-2 |
| DINO                                             | 9.99e-4 \pm 6.71e-3      | 8.27e-3 \pm 5.61e-3 |
| CORAL                                            | 2.18e-3 \pm 6.88e-4      | 6.67e-3 \pm 2.01e-3 |
| MARBLE (Ours)                                    | 1.62e-4 \pm 2.42e-5      | 9.27e-4 \pm 1.44e-4 |"
424,"| Model      | Weekdays | Months    |
| ---------- | -------- | --------- |
| Llama 3 8B | 29 / 49  | 143 / 144 |
| Mistral 7B | 31 / 49  | 125 / 144 |
| GPT-2      | 8 / 49   | 10 / 144  |"
425,"| Methods                     | Canny-to-Image     | Depth-to-Image      | Inpainting          | Outpainting        | Text-to-Image       |
| --------------------------- | ------------------ | ------------------- | ------------------- | ------------------ | ------------------- |
| FI $ \uparrow $             | FID $ \downarrow $ | CLIP-S $ \uparrow $ | RMSE $ \downarrow $ | FID $ \downarrow $ | CLIP-S $ \uparrow $ |
| MultiGen-20M                | MultiGen-20M       | Places              | Places              | COCO-30K           | Photo               |
| ControlNet-SD1.5 (2023a)    | 34.65              | 14.73               | 32.15               | 35.90              | 17.76               |
| T2I-Adapter-SD1.5 (2024)    | 23.65              | 15.96               | 31.71               | 48.40              | 22.52               |
| LDM-4 (2022b)               |                    |                     |                     |                    |                     |
| LaMa (2022)                 |                    |                     |                     |                    |                     |
| DeepFill v2 (2019)          |                    |                     |                     |                    |                     |
| MaskGIT (2022b)             |                    |                     |                     |                    |                     |
| DALL-E 2 (2021)             |                    |                     |                     |                    |                     |
| SD 1.5 (2022b)              |                    |                     |                     |                    |                     |
| PixArt- $ \alpha $  (2024b) |                    |                     |                     |                    |                     |
| Lumina-Next (2024)          |                    |                     |                     |                    |                     |
| PixWizard                   | 35.46              | 15.76               | 32.01               | 33.83              | 16.94               |"
426,"| Method   | ||      | Component  | CIFAR-100            | ImageNet-R              |
| -------- | ------- | ---------- | -------------------- | ----------------------- |
| ||       | ISA-FAM | Logit Mask | $ A_{AUC} \uparrow $ | $ F_{Last} \downarrow $ |
| Baseline | ||      |            |                      | 67.07±4.16              |
| Ours     | ✓       |            |                      | 68.97±0.85              |
|          | ✓       | ✓          | 74.84±2.99           | 11.71±1.56              |
| ✓        | ✓       | ✓          | 80.55±2.17           | 10.35±1.12              |"
426,"| Method   | Logit Mask | Naive ISA | SAM | FAM                | CIFAR-100          | ImageNet-R         |
| -------- | ---------- | --------- | --- | ------------------ | ------------------ | ------------------ |
| Baseline | ✓          |           |     |                    | 74.84 $ \pm $ 2.99 | 45.59 $ \pm $ 1.71 |
| Ours     | ✓          | ✓         |     |                    | 77.21 $ \pm $ 2.66 | 47.71 $ \pm $ 1.31 |
| ✓        | ✓          | ✓         |     | 79.13 $ \pm $ 2.38 | 49.99 $ \pm $ 1.21 |                    |
| ✓        | ✓          |           | ✓   | 80.55 $ \pm $ 2.17 | 50.89 $ \pm $ 1.03 |                    |"
426,"| Method      | PTM(IN-1k)+ISA(IN-100) | PTM(YFCC100M)+ISA(IN-1k) | PTM(IN-1k)+ISA(CUB200) |
| ----------- | ---------------------- | ------------------------ | ---------------------- |
| $ A_{AUC} $ | $ A_{Last} $           | $ A_{AUC} $              | $ A_{Last} $           |
| EWC         | 31.5±1.0               | 20.7±1.1                 | 30.6±1.3               |
| DualPrompt  | 40.1±1.2               | 29.2±4.6                 | 37.9±0.9               |
| MVP         | 40.6±1.2               | 31.9±3.0                 | 34.0±1.3               |
| Ours        | 49.5±1.3               | 43.0±0.6                 | 49.3±1.2               |"
427,"| Method     | Bi-TSP20  | Bi-TSP50  | Bi-TSP100  |
| ---------- | --------- | --------- | ---------- |
| HV↑        | Gap↓      | Time↓     | HV↑        |
| WS-LKH     | 0.6270    | 0.00%     | 10m        |
| MOEA/D     | 0.6241    | 0.46%     | 1.7h       |
| NSGA-II    | 0.6258    | 0.19%     | 6.0h       |
| MOGLS      | 0.6279    | -0.14%    | 1.6h       |
| PPLS/D-C   | 0.6256    | 0.22%     | 26m        |
| DRL-MOA    | 0.6257    | 0.21%     | 6s         |
| MDRL       | 0.6271    | -0.02%    | 5s         |
| EMNH       | 0.6271    | -0.02%    | 5s         |
| PMOCO      | 0.6259    | 0.18%     | 6s         |
| GIMF-P     | 0.6266    | 0.06%     | 7s         |
| CNH        | 0.6270    | 0.00%     | 14s        |
| GIMF-C     | 0.6270    | 0.00%     | 15s        |
| MDRL-Aug   | 0.6271    | -0.02%    | 33s        |
| EMNH-Aug   | 0.6271    | -0.02%    | 33s        |
| PMOCO-Aug  | 0.6270    | 0.00%     | 1.1m       |
| GIMF-P-Aug | 0.6271    | -0.02%    | 1.5m       |
| CNH-Aug    | 0.6271    | -0.02%    | 1.5m       |
| GIMF-C-Aug | 0.6270    | 0.00%     | 2.0m       |
| Method     | Bi-CVRP20 | Bi-CVRP50 | Bi-CVRP100 |
| HV↑        | Gap↓      | Time↓     | HV↑        |
| MOEA/D     | 0.4255    | 1.05%     | 2.3h       |
| NSGA-II    | 0.4275    | 0.58%     | 6.4h       |
| MOGLS      | 0.4278    | 0.51%     | 9.0h       |
| PPLS/D-C   | 0.4287    | 0.30%     | 1.6h       |
| DRL-MOA    | 0.4287    | 0.30%     | 10s        |
| MDRL       | 0.4291    | 0.21%     | 8s         |
| EMNH       | 0.4299    | 0.02%     | 7s         |
| PMOCO      | 0.4267    | 0.77%     | 7s         |
| GIMF-P     | 0.4287    | 0.30%     | 7s         |
| CNH        | 0.4287    | 0.30%     | 15s        |
| GIMF-C     | 0.4289    | 0.26%     | 15s        |
| MDRL-Aug   | 0.4294    | 0.14%     | 11s        |
| EMNH-Aug   | 0.4302    | -0.05%    | 11s        |
| PMOCO-Aug  | 0.4294    | 0.14%     | 14s        |
| GIMF-P-Aug | 0.4298    | 0.05%     | 17s        |
| CNH-Aug    | 0.4299    | 0.02%     | 22s        |
| GIMF-C-Aug | 0.4300    | 0.00%     | 25s        |"
427,"| Method     | Bi-KP50   | Bi-KP100  | Bi-KP200   |
| ---------- | --------- | --------- | ---------- |
| HV↑        | Gap↓      | Time↓     | HV↑        |
| WS-DP      | 0.3561    | -0.03%    | 22m        |
| MOEA/D     | 0.3540    | 0.56%     | 1.6h       |
| NSGA-II    | 0.3547    | 0.37%     | 7.8h       |
| MOGLS      | 0.3540    | 0.56%     | 5.8h       |
| PPLS/D-C   | 0.3528    | 0.90%     | 18m        |
| DRL-MOA    | 0.3559    | 0.03%     | 9s         |
| MDRL       | 0.3530    | 0.84%     | 6s         |
| EMNH       | 0.3561    | -0.03%    | 6s         |
| PMOCO      | 0.3552    | 0.22%     | 9s         |
| GIMF-P     | 0.3560    | 0.00%     | 9s         |
| CNH        | 0.3556    | 0.11%     | 18s        |
| GIMF-C     | 0.3560    | 0.00%     | 18s        |
| Method     | Tri-TSP20 | Tri-TSP50 | Tri-TSP100 |
| HV↑        | Gap↓      | Time↓     | HV↑        |
| WS-LKH     | 0.4712    | -0.13%    | 12m        |
| MOEA/D     | 0.4702    | 0.08%     | 1.9h       |
| NSGA-II    | 0.4238    | 9.94%     | 7.1h       |
| MOGLS      | 0.4701    | 0.11%     | 1.5h       |
| PPLS/D-C   | 0.4698    | 0.17%     | 1.4h       |
| DRL-MOA    | 0.4699    | 0.15%     | 6s         |
| MDRL       | 0.4699    | 0.15%     | 5s         |
| EMNH       | 0.4699    | 0.15%     | 5s         |
| PMOCO      | 0.4693    | 0.28%     | 5s         |
| GIMF-P     | 0.4702    | 0.08%     | 6s         |
| CNH        | 0.4698    | 0.17%     | 10s        |
| GIMF-C     | 0.4702    | 0.08%     | 12s        |
| MDRL-Aug   | 0.4712    | -0.13%    | 2.6m       |
| EMNH-Aug   | 0.4712    | -0.13%    | 2.6m       |
| PMOCO-Aug  | 0.4712    | -0.13%    | 5.1m       |
| GIMF-P-Aug | 0.4712    | -0.13%    | 13m        |
| CNH-Aug    | 0.4704    | 0.04%     | 8.0m       |
| GIMF-C-Aug | 0.4706    | 0.00%     | 16m        |"
438,"| Pair                | ResNet34-MobileNet | ResNet50-MobileNet |
| ------------------- | ------------------ | ------------------ |
| Keep Ratio          | $ r = 30\% $       | $ r = 50\% $       |
| Random              | 67.25↓3.47         | 69.39↓1.33         |
| Herding             | 61.96↓8.76         | 66.58↓4.14         |
| EL2N                | 64.70↓6.02         | 69.14↓1.58         |
| Moderate            | 66.79↓3.93         | 69.05↓1.67         |
| InfoBatch*          | 67.43↓3.29         | 69.91↓0.81         |
| Ours                | 67.91↓2.81         | 69.98↓0.74         |
| Full Dataset w/o KD | 69.57↓1.15         | 69.57↓1.08         |
| Full Dataset w/ KD  | 70.72              | 70.65              |"
438,"|          | Baseline | Zipf’s LS     | USKD          | Ours  |
| -------- | -------- | ------------- | ------------- | ----- |
| Acc (%)  | 69.57    | 69.59         | 70.38         | 70.92 |
| Time (h) | 39.86    | $ &gt;39.86 $ | $ &gt;39.86 $ | 36.98 |"
428,"| Method              | LLaVA Bench(2023b) | Ferret Bench(2023) | MDVP Bench            |
| ------------------- | ------------------ | ------------------ | --------------------- |
| Conversation        | Detail Description | Complex Reasoning  | Referring Description |
| Box                 | Point              | Box                | Point                 |
| LLaVA-7B(2023b)     | 85.4               | 68.3               | 92.1                  |
| SPHINX-X-13B(2024)  | -                  | -                  | -                     |
| Kosmos-2(2023)      | 71.7               | 63.4               | 74.9                  |
| Osprey-7B(2024a)    | -                  | -                  | -                     |
| Ferret-7B(2023)     | 84.4               | 79.4               | 96.3                  |
| Ferret-13B(2023)    | 85.2               | 80.9               | 96.4                  |
| Ferret-v2-13B(2024) | -                  | -                  | -                     |
| VP-SPHINX-7B        | 81.1               | 83.1               | 79.2                  |
| VP-SPHINX-13B       | 84.6               | 86.4               | 83.3                  |
| VP-LLaVA-8B         | 90.9               | 78.9               | 94.4                  |"
428,"| Method              | Semantic Similarity | Semantic IOU | Accuracy |
| ------------------- | ------------------- | ------------ | -------- |
| Point               | Box                 | Free-Form    |          |
| LLaVA-7B(2023b)     | 48.95               | 19.81        | 50.1     |
| Kosmos-2(2023)      | 38.96               | 8.67         | -        |
| Shikra-7B(2023b)    | 49.65               | 19.82        | 57.8     |
| GPT4RoI-7B(2023a)   | 51.32               | 11.99        | -        |
| ChatSpot-7B(2023)   | -                   | -            | -        |
| Osprey-7B(2024a)    | 65.24               | 38.19        | -        |
| Ferret-13B(2023)    | 64.96               | 37.82        | 68.4     |
| Ferret-v2-7B(2024)  | -                   | -            | 74.6     |
| Ferret-v2-13B(2024) | -                   | -            | 75.1     |
| VP-SPHINX-7B        | 86.02               | 61.24        | 85.44    |
| VP-SPHINX-13B       | 87.06               | 62.90        | 86.46    |
| VP-LLaVA-8B         | 86.67               | 61.52        | 85.85    |"
429,"| Method                                                        | VQA-RAD | SLAKE | PathVQA |
| ------------------------------------------------------------- | ------- | ----- | ------- |
| Open                                                          | Closed  | Avg   | Open    |
| Method Not Finetuned on the Training Set of the VQA Benchmark |         |       |         |
| GPT-4V                                                        | 39.5    | 78.9  | 59.2    |
| LLaVA-Med                                                     | 28.2    | 61.4  | 44.8    |
| LLaVA-Tri                                                     | 36.9    | 62.6  | 49.7    |
| Method Finetuned on the Training Set of the VQA Benchmark     |         |       |         |
| Clip-based                                                    |         |       |         |
| PubMedCLIP                                                    | 60.1    | 80.0  | 70.1    |
| BiomedCLIP                                                    | 67.6    | 79.8  | 73.7    |
| Non Clip-based                                                |         |       |         |
| VL Encoder-Decoder                                            | 71.5    | 82.5  | 77.0    |
| Q2ATransformer                                                | 79.2    | 81.2  | 80.2    |
| Prefix T. Medical LM                                          | -       | -     | -       |
| M2I2                                                          | 66.5    | 83.5  | 75.0    |
| LLaVA                                                         | 50.0    | 65.1  | 57.6    |
| LLaVA-Med (finetuned for 3 epochs)                            | 55.5    | 66.5  | 61.0    |
| LLaVA-Tri (finetuned for 3 epochs)                            | 77.1    | 86.0  | 81.6    |"
429,"| Model                               | Dataset Use   | VQA-RAD      | SLAKE         | PathVQA      |
| ----------------------------------- | ------------- | ------------ | ------------- | ------------ |
| open                                | close         | average      | open          | close        |
| LLaVA-Tri                           | w/o           | 64.6         | 77.0          | 70.8         |
| w/                                  | 77.1 + (12.5) | 86.0 + (9.0) | 81.6 + (10.8) | 86.2 + (6.9) |
| MiniCPM-V-2.6-8B (Yao et al., 2024) | w/o           | 48.5         | 86.4          | 67.5         |
| w/                                  | 50.5 + (2.0)  | 87.6 + (1.2) | 69.1 + (1.6)  | 65.3 + (8.1) |
| InternVL2-8B (Chen et al., 2024)    | w/o           | 38.2         | 76.2          | 57.2         |
| w/                                  | 40.7 + (2.5)  | 80.0 + (3.8) | 60.4 + (3.2)  | 66.4 + (4.7) |
| PubMedCLIP (Eslami et al., 2023)    | w/o           | 55.6         | 79.3          | 67.5         |
| w/                                  | 60.6 + (5.0)  | 79.7 + (0.4) | 70.2 + (2.7)  | -            |"
431,"| Method | Memory Cost                                                                               | Per-step Comms Cost                                                                                                           |
| ------ | ----------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| STD    | $ \mathcal{O}(\mathcal{M}) $                                                              | $ \mathcal{O}(\mathcal{M}) $                                                                                                  |
| GLOB   | $ \mathcal{O}(\mathcal{M}) $                                                              | $ \mathcal{O}\left(\frac{\mathcal{M}}{N_{\text{local}}}\right) $                                                              |
| TRIM   | $ \mathcal{O}(\mathcal{M}-(|\mathcal{V}|-\overline{|\mathcal{V}_{k}|})d_{\text{model}}) $ | $ \mathcal{O}\left(\frac{\mathcal{M}-(|\mathcal{V}|-\overline{|\mathcal{V}_{k}|})d_{\text{model}}}{N_{\text{local}}}\right) $ |
| SPEC   | $ \mathcal{O}(\mathcal{M}-(|\mathcal{V}|-\overline{|\mathcal{V}_{k}|})d_{\text{model}}) $ | $ \mathcal{O}\left(\frac{\mathcal{M}-(|\mathcal{V}|+\mathcal{L})d_{\text{model}}}{N_{\text{local}}}\right) $                  |"
439,"|          | Vanilla | Disguised Intent | Role Play    | Structured Response | Virtual AI   | Hybrid Strategies | B_{cj}       | B_{sm}       | B_{cd}       |
| -------- | ------- | ---------------- | ------------ | ------------------- | ------------ | ----------------- | ------------ | ------------ | ------------ |
| Avg. MSR | 0.25    | 0.10 (-0.15)     | 0.03 (-0.22) | 0.01 (-0.24)        | 0.14 (-0.09) | 0.07 (-0.18)      | 0.83 (+0.58) | 0.66 (+0.41) | 0.65 (+0.40) |"
439,"| Models $ \downarrow $ | Method            | Categories                       | Avg.  $ \uparrow $ |
| --------------------- | ----------------- | -------------------------------- | ------------------ |
| Physical Privacy      | Pornography Fraud | Illegal Hateful Activity Conduct | Sabotage           |
| Harm                  | Violence          | Activity Conduct                 |                    |
| GPT-4-turbo           | Vanilla           | 0.24                             | 0.03               |
| Bcj                   | 0.92              | 0.82                             | 0.56               |
| Bsm                   | 0.83              | 0.41                             | 0.39               |
| Bcd                   | 0.68              | 0.54                             | 0.54               |
| GPT-3.5-turbo         | Vanilla           | 0.43                             | 0.17               |
| Bcj                   | 0.94              | 0.85                             | 0.64               |
| Bsm                   | 0.91              | 0.44                             | 0.58               |
| Bcd                   | 0.91              | 0.75                             | 0.65               |
| GPT-4o                | Vanilla           | 0.29                             | 0.02               |
| Bcj                   | 0.72              | 0.39                             | 0.10               |
| Bsm                   | 0.78              | 0.31                             | 0.17               |
| Bcd                   | 0.73              | 0.49                             | 0.25               |
| llava-1.5-7b          | Vanilla           | 0.28                             | 0.29               |
| Bcj                   | 0.61              | 0.36                             | 0.05               |
| Bsm                   | 0.51              | 0.23                             | 0.03               |
| Bcd                   | 0.56              | 0.84                             | 0.46               |
| Yi-vision             | Vanilla           | 0.70                             | 0.50               |
| Bcj                   | 0.95              | 0.73                             | 0.60               |
| Bsm                   | 0.84              | 0.77                             | 0.46               |
| Bcd                   | 0.85              | 0.80                             | 0.67               |"
431,"| Method   | $ \overline{\left|\mathcal{V}_{k}\right|}\pm\sigma $ | $ \overline{\left|\mathcal{V}_{k}\right|}\times d_{\text{model}} $ | $ \overline{\mathcal{M}}_{k} $  ( $ \downarrow $ ) | Per-step Comms Cost ( $ \downarrow $ ) |
| -------- | ---------------------------------------------------- | ------------------------------------------------------------------ | -------------------------------------------------- | -------------------------------------- |
| STD      | 250 112                                              | 192M                                                               | 278M (1\times)                                     | 278M (1\times)                         |
| GLOB     | 250 112                                              | 192M                                                               | 278M (1\times)                                     | 0.56M (0.002\times)                    |
| TRIM     | 216 135 \pm 27 160                                   | 166M                                                               | 252M (0.92\times)                                  | 0.5M (0.002\times)                     |
| SPEC     | 216 135 \pm 27 160                                   | 166M                                                               | 252M (0.92\times)                                  | 0.17M (0.0006\times)                   |
| SPEC-OP! | 50 257 \pm 0                                         | 38.6M                                                              | 125M (0.45\times)                                  | 0.17M (0.0006\times)                   |"
431,"| Name (UNIGRAM-CE)  | DM (6.9) | EN (7.9) | EP (10) | FL (7.8) | GH (7.9) | AVG (8.1) |
| ------------------ | -------- | -------- | ------- | -------- | -------- | --------- |
| STD ( $ \tau=0 $ ) | 5.5      | 44.8     | 93.5    | 30.9     | 8.1      | 56        |
| STD ( $ \tau=1 $ ) | 5        | 30.6     | 49.5    | 20.6     | 6        | 36.9      |
| ACT                | -        | -        | -       | -        | -        | -         |
| GLOB               | 4.8      | 25.7     | 38.2    | 17.3     | 5.4      | 31.6      |
| TRIM               | 4.8      | 27.3     | 39.5    | 18.5     | 5.6      | 33.3      |
| SPEC               | 4.8      | 26.7     | 36.8    | 18.2     | 5.5      | 32.7      |
| SPEC-OPT           | 4.7      | 25.9     | 35      | 17.5     | 5.4      | 31.2      |
| Min Imp (\%)       | 3.7      | 10.6     | 20.2    | 10.1     | 7.4      | 9.7       |
| Max Imp (\%)       | 4.2      | 15.7     | 29.3    | 16.3     | 11       | 15.3      |"
431,"| Name               | Random Init |
| ------------------ | ----------- |
| RACE (ACC)         | MNLI (ACC)  |
| STD ( $ \tau=0 $ ) | 0.50        |
| STD ( $ \tau=1 $ ) | 0.46        |
| ACT                | 0.45        |
| GLOB               | 0.51        |
| TRIM               | 0.53        |
| SPEC               | 0.52        |
| SPEC-OPT           | 0.51        |
| Min Imp (%)        | 2.9%        |
| Max Imp (%)        | 5.8%        |"
432,"| Results on FlickrUser Dataset |
| ----------------------------- |
| Model                         |
| Llava-1.5 Finetuned           |
| EngageNet Finetuned (ours)    |"
433,"| Model     | Params (B) | Tokens (B) | Tokenizer | ARC-c acc_norm ↑ | ARC-e acc ↑ | HS acc_norm ↑ | LMB ppl ↓ | LMB acc ↑ | OBQA acc_norm ↑ | PIQA acc ↑ | WG acc ↑ | AVG acc* ↑ |
| --------- | ---------- | ---------- | --------- | ---------------- | ----------- | ------------- | --------- | --------- | --------------- | ---------- | -------- | ---------- |
| Mamba£    | 0.13       | 100        | NeoX      | 23.72            | 46.84       | 33.37         | 19.72     | 40.33     | 29.80           | 65.07      | 52.17    | 41.61      |
| Mamba2£   | 0.13       | 100        | NeoX      | 23.81            | 45.50       | 34.13         | 20.42     | 39.55     | 29.20           | 64.47      | 51.85    | 41.22      |
| Rodimus£  | 0.13       | 100        | NeoX      | 23.38            | 46.84       | 34.22         | 17.95     | 42.44     | 30.00           | 65.40      | 51.46    | 41.96      |
| GPT-Neo   | 0.13       | 300        | GPT2      | 23.21            | 43.69       | 30.42         | 30.26     | 37.38     | 26.40           | 62.89      | 50.51    | 39.21      |
| OPT       | 0.13       | 300        | OPT       | 22.78            | 43.52       | 31.35         | 26.02     | 37.90     | 28.00           | 63.00      | 50.36    | 39.56      |
| Pythia    | 0.16       | 300        | NeoX      | 23.72            | 43.60       | 30.19         | 38.31     | 32.80     | 26.00           | 61.59      | 50.51    | 38.34      |
| Mamba     | 0.13       | 300        | NeoX      | 24.32            | 47.90       | 35.20         | 16.05     | 44.21     | 28.60           | 64.69      | 52.33    | 42.46      |
| Mamba2    | 0.13       | 300        | NeoX      | 24.23            | 47.35       | 35.32         | 16.79     | 43.78     | 30.40           | 64.85      | 52.64    | 42.65      |
| RWKV4     | 0.17       | 300        | NeoX      | 23.55            | 47.56       | 32.27         | 31.73     | 32.64     | 27.80           | 64.25      | 50.99    | 39.87      |
| Rodimus   | 0.13       | 300        | NeoX      | 23.72            | 49.07       | 35.45         | 15.75     | 45.00     | 29.20           | 64.36      | 53.35    | 42.88      |
| Rodimus+  | 0.13       | 300        | NeoX      | 24.23            | 47.22       | 35.49         | 12.94     | 48.09     | 29.20           | 64.36      | 53.43    | 43.15      |
| OPT       | 0.35       | 300        | OPT       | 23.98            | 44.19       | 36.66         | 16.40     | 45.10     | 28.20           | 64.47      | 52.41    | 42.14      |
| Pythia    | 0.41       | 300        | NeoX      | 24.49            | 52.10       | 40.59         | 10.83     | 51.45     | 29.40           | 66.97      | 53.59    | 45.51      |
| BLOOM     | 0.56       | 300        | BLOOM     | 23.89            | 47.47       | 36.89         | 28.83     | 34.10     | 28.80           | 64.20      | 52.01    | 41.05      |
| Mamba     | 0.37       | 300        | NeoX      | 27.82            | 54.92       | 46.49         | 8.14      | 55.60     | 30.80           | 69.53      | 55.17    | 48.62      |
| Mamba2    | 0.37       | 300        | NeoX      | 26.62            | 54.67       | 46.96         | 7.98      | 55.85     | 32.60           | 70.46      | 55.64    | 48.97      |
| RWKV4     | 0.43       | 300        | NeoX      | 25.26            | 47.18       | 40.77         | 13.38     | 45.39     | 30.60           | 68.12      | 53.28    | 44.37      |
| Qwen2     | 0.5        | 7000       | Qwen2     | 28.84            | 54.97       | 49.03         | 11.66     | 50.05     | 33.20           | 69.42      | 57.62    | 49.02      |
| Rodimus   | 0.46       | 150        | Rodimus   | 27.65            | 55.77       | 48.78         | 10.17     | 50.65     | 32.60           | 70.73      | 55.41    | 48.80      |
| Rodimus+  | 0.47       | 150        | Rodimus   | 28.58            | 57.28       | 52.13         | 10.22     | 51.14     | 35.00           | 72.91      | 53.59    | 50.09      |
| GPT-Neo   | 1.3        | 300        | GPT2      | 25.77            | 56.14       | 48.91         | 7.50      | 57.21     | 33.60           | 71.22      | 55.01    | 49.69      |
| OPT       | 1.3        | 300        | OPT       | 29.52            | 56.94       | 53.75         | 6.65      | 57.89     | 33.20           | 71.60      | 59.59    | 51.78      |
| Pythia    | 1.0        | 300        | NeoX      | 27.05            | 56.94       | 47.15         | 7.92      | 56.22     | 31.40           | 70.67      | 53.51    | 48.99      |
| Pythia    | 1.4        | 300        | NeoX      | 28.41            | 60.48       | 51.98         | 6.08      | 61.60     | 33.20           | 70.84      | 57.30    | 51.97      |
| BLOOM     | 1.1        | 300        | BLOOM     | 25.51            | 51.47       | 42.97         | 17.28     | 42.64     | 29.40           | 67.19      | 55.01    | 44.88      |
| GLA       | 1.3        | 100        | Mistral   | 27.82            | 55.13       | 48.97         | 15.37     | 46.09     | 33.00           | 69.80      | 53.20    | 47.72      |
| RetNet    | 1.3        | 100        | Mistral   | 26.45            | 57.32       | 48.04         | 16.44     | 43.37     | 32.00           | 69.42      | 53.43    | 47.15      |
| HGRN      | 1.3        | 100        | Mistral   | 25.51            | 55.01       | 48.82         | 20.24     | 37.71     | 31.80           | 69.80      | 50.28    | 45.56      |
| HGRN2     | 1.3        | 100        | Mistral   | 28.16            | 58.16       | 51.74         | 11.38     | 49.97     | 32.80           | 71.27      | 52.17    | 49.18      |
| Mamba     | 1.3        | 300        | NeoX      | 32.85            | 65.49       | 59.08         | 5.04      | 64.87     | 36.40           | 74.10      | 61.40    | 56.31      |
| Mamba2    | 1.4        | 300        | NeoX      | 33.36            | 64.10       | 59.92         | 5.02      | 65.61     | 37.80           | 73.29      | 60.93    | 56.43      |
| RWKV4     | 1.5        | 300        | NeoX      | 29.01            | 60.94       | 52.90         | 7.08      | 57.19     | 33.40           | 72.09      | 55.33    | 51.55      |
| RWKV6     | 1.6        | 1420       | RWKV6     | 33.36            | 60.69       | 61.42         | 4.61      | 67.26     | 37.40           | 73.67      | 60.38    | 56.31      |
| Rec-Gemma | 2.0        | 2000       | Gemma     | 29.69            | 48.91       | 61.82         | 9.27      | 53.83     | 29.40           | 67.52      | 57.06    | 49.75      |
| Qwen2     | 1.5        | 7000       | Qwen2     | 35.75            | 65.95       | 65.51         | 5.52      | 63.61     | 36.80           | 75.19      | 65.27    | 58.30      |
| Rodimus   | 1.4        | 500        | Rodimus   | 36.09            | 68.01       | 62.44         | 6.33      | 59.62     | 35.60           | 74.32      | 59.75    | 56.55      |
| Rodimus+  | 1.6        | 1000       | Rodimus   | 36.35            | 68.35       | 64.51         | 5.38      | 63.59     | 38.80           | 76.17      | 62.51    | 58.61      |"
433,"| Models        | Test PPL | Params(M) |
| ------------- | -------- | --------- |
| Rodimus       | 21.90    | 46.68     |
| Rodimus+      | 21.56    | 48.45     |
| Mamba         | 22.58    | 46.08     |
| Mamba2        | 22.78    | 46.38     |
| Transformer++ | 22.02    | 46.24     |
| Llama3        | 22.12    | 46.48     |
| GLA           | 22.16    | 44.71     |
| HGRN2         | 22.00    | 46.20     |"
435,"| Method           | Cora        | Citeseer    | Pubmed      | Chameleon   | Amazon Photo | Amazon Ratings |
| ---------------- | ----------- | ----------- | ----------- | ----------- | ------------ | -------------- |
| Central GNN      | 82.94± 1.26 | 69.37± 1.07 | 85.12± 1.15 | 54.38± 1.96 | 94.10± 0.30  | 41.42± 0.80    |
| FedSGD GNN       | 66.00± 1.51 | 63.38± 0.76 | 84.66± 0.22 | 36.80± 1.70 | 91.55± 0.34  | 35.96± 0.46    |
| FedSage+ [1]     | 66.33± 1.69 | 63.93± 0.97 | 84.64± 0.37 | 36.32± 1.59 | 91.33± 0.47  | 35.85± 0.39    |
| FedPub [2]       | 61.82± 1.84 | 62.91± 0.76 | 82.39± 0.41 | 33.31± 1.37 | 88.05± 0.68  | 35.72± 0.60    |
| FedStruct (Deg)  | 69.89± 1.85 | 63.54± 0.64 | 84.01± 0.38 | 41.82± 1.78 | 89.72± 0.43  | 38.67± 0.66    |
| FedStruct (Fed★) | 69.61± 1.87 | 63.38± 0.90 | 84.02± 0.35 | 41.89± 1.67 | 89.78± 0.38  | 38.65± 0.44    |
| FedStruct (H2V)  | 79.27± 0.90 | 65.43± 0.98 | 85.02± 0.43 | 52.60± 1.25 | 90.93± 0.27  | 40.97± 0.64    |
| Local GNN        | 39.24± 1.64 | 39.88± 1.62 | 75.27± 0.59 | 29.60± 1.25 | 77.12± 1.75  | 32.80± 0.43    |"
436,"| T: Train via ✗ | I: Infer |
| -------------- | -------- |
| T&amp;I        | I        |
| I              | T&amp;I  |
| T&amp;I        | I        |
| I              | T&amp;I  |"
437,"| Method       | Data             | Epoch | Token | ViT-S               | ViT-B                |
| ------------ | ---------------- | ----- | ----- | ------------------- | -------------------- |
| Scratch      | -                | -     | -     | 79.9                | 81.8                 |
| MAE          | INet-1K          | 300   | ✓     | 80.6                | 82.9                 |
| iBOT         | INet-1K          | 1600  | ✓     | 81.1 $ ^{\dagger} $ | 84.0 $ ^{\ddagger} $ |
| BEiT         | INet-1K + DALL-E | 300   | ✓     | 81.3                | 82.9                 |
| AttMask      | INet-1K          | 300   | ✓     | 81.3                | N/A                  |
| MoCo V3      | INet-1K          | 600   | -     | 81.4                | 83.2                 |
| DINO         | INet-1K          | 1600  | -     | 81.5                | 82.8                 |
| MFM          | INet-1K          | 300   | -     | 81.6                | 83.1                 |
| MFM*         | INet-1K          | 300   | -     | 81.2                | 82.9                 |
| MFM + R/Com* | INet-1K          | 300   | -     | 81.4                | 83.2                 |
| FOLK         | INet-1K          | 300   | -     | 81.6                | 83.4                 |
| FOLK         | INet-1K          | 800   | -     | 82.1                | 84.0                 |"
438,"| Table 1: Top-1 accuracy±standard deviation (%) on CIFAR-100 with ResNet50-MobileNetV2 using different data pruning strategies. |
| ------------------------------------------------------------------------------------------------------------------------------ |
| Strategy                                                                                                                       |
| Random                                                                                                                         |
| Hard                                                                                                                           |
| Easy                                                                                                                           |
| Medium                                                                                                                         |
| Medium+Reshape                                                                                                                 |"
440,"|                         | Embedding Size/ # Blocks | 2     | 4     | 6     | 8     |
| ----------------------- | ------------------------ | ----- | ----- | ----- | ----- |
| GNN-tractable dataset   | GAT                      | 0.856 | 0.926 | 0.930 | 0.940 |
| GATv2                   | 0.914                    | 0.927 | 0.936 | 0.943 |       |
| Graph Transformer       | 0.875                    | 0.914 | 0.920 | 0.930 |       |
| MILP-GNN                | 0.872                    | 0.910 | 0.911 | 0.902 |       |
| GQGLA                   | 0.933                    | 0.936 | 0.942 | 0.946 |       |
| GNN-intractable dataset | GAT                      | 0.500 | 0.500 | 0.500 | 0.500 |
| GATv2                   | 0.500                    | 0.500 | 0.500 | 0.500 |       |
| Graph Transformer       | 0.500                    | 0.500 | 0.500 | 0.500 |       |
| MILP-GNN                | 0.500                    | 0.500 | 0.500 | 0.500 |       |
| GQGLA                   | 0.987                    | 0.998 | 1.000 | 1.000 |       |"
440,"|                              | Feasibility (Rate of Error  $ \downarrow $ )/10 $ ^{-2} $ | Optimal Value (MSE  $ \downarrow $ )/10 $ ^{-2} $ | Optimal Solution (MSE  $ \downarrow $ )/10 $ ^{-1} $ |
| ---------------------------- | --------------------------------------------------------- | ------------------------------------------------- | ---------------------------------------------------- |
| MILP-GNN (Chen et al., 2023) | # E.Size                                                  | 4                                                 | 6                                                    |
| Train                        | 6.72  $ \pm $  0.17                                       | 5.42  $ \pm $  0.21                               | 4.29  $ \pm $  0.12                                  |
| Test                         | 7.96  $ \pm $  0.19                                       | 6.74  $ \pm $  0.20                               | 5.62  $ \pm $  0.11                                  |
| GQGLA                        | # Block                                                   | 4                                                 | 6                                                    |
| Train                        | 7.22  $ \pm $  0.11                                       | 6.53  $ \pm $  0.13                               | 5.57  $ \pm $  0.08                                  |
| Test                         | 7.34  $ \pm $  0.12                                       | 6.65  $ \pm $  0.12                               | 5.74  $ \pm $  0.09                                  |"
440,"|       | HEA
(Kandala et al., 2017) | QGCN
(Zheng et al., 2021) | GQGLA (ours)           |
| ----- | -------------------------- | ------------------------- | ---------------------- |
| Train | 0.4613  $ \pm $  0.024     | 0.3419  $ \pm $  0.015    | 0.1086  $ \pm $  0.008 |
| Test  | 0.4665  $ \pm $  0.020     | 0.3475  $ \pm $  0.018    | 0.1127  $ \pm $  0.012 |"
440,"| # Aux. qubits | 0      | 1      | 2          | 3      |
| ------------- | ------ | ------ | ---------- | ------ |
| Train         | 0.6580 | 0.6166 | $ 0.5694 $ | 0.6099 |
| Test          | 0.6853 | 0.6410 | $ 0.5993 $ | 0.6354 |"
440,"| Repeated Encoding | Syn. Encoding Learning | Double Interaction | Train  | Test   |
| ----------------- | ---------------------- | ------------------ | ------ | ------ |
| ✗                 | ✗                      | ✗                  | 0.0924 | 0.0953 |
| ✗                 | ✗                      | ✓                  | 0.0876 | 0.0898 |
| ✓                 | ✗                      | ✓                  | 0.0708 | 0.0726 |
| ✓                 | ✗                      | ✗                  | 0.0738 | 0.0765 |
| ✓                 | ✓                      | ✗                  | 0.0623 | 0.0659 |
| ✓                 | ✓                      | ✓                  | 0.0557 | 0.0574 |"
440,"| Methods        | PROTEINS    | PTC         |
| -------------- | ----------- | ----------- |
| GraphSAGE      | 75.9 ± 3.2  | 63.9 ± 7.7  |
| GIN            | 76.20 ± 2.8 | 64.6 ± 7.0  |
| GAT            | 74.70 ± 2.2 | 66.70 ± 5.1 |
| PPGN           | 76.66 ± 5.6 | 62.94 ± 6.6 |
| QS-CNN         | 78.2 ± 4.6  | 66.0 ± 4.4  |
| Deep-WL-SGN    | 76.78 ± 2.4 | 65.88 ± 5.1 |
| U2GNN          | 78.53 ± 4.1 | 69.63 ± 3.6 |
| SEG-BERT       | 77.20 ± 3.1 | 68.86 ± 4.2 |
| S-GQGLA (Ours) | 91.64 ± 3.1 | 68.57 ± 2.9 |"
441,"| Model             | LLaVA               | LLaVA-v1.5          | mPLUG-Owl2          |
| ----------------- | ------------------- | ------------------- | ------------------- |
| CHAIR             | $ C_{S}\downarrow $ | $ C_{I}\downarrow $ | $ C_{S}\downarrow $ |
| Greedy            | 26.6                | 10.5                | 8.8                 |
| Ensembling Models |                     |                     |                     |
| MARINE            | 17.8                | 7.2                 | 6.2                 |
| Single Models     |                     |                     |                     |
| MARINE-DETR only  | 27.6                | 8.4                 | 10.5                |
| MARINE-RAM only   | 29.0                | 9.1                 | 6.6                 |"
441,"| Task                 | Metrics             | LLaVA               | mPLUG-Owl2          |
| -------------------- | ------------------- | ------------------- | ------------------- |
| ✗                    | ✓                   | ✗                   | ✓                   |
| LLaVA-QA90           | Acc  $ \uparrow $   | 5.82  $ \pm $  0.10 | 5.94  $ \pm $  0.05 |
| Detail  $ \uparrow $ | 4.59  $ \pm $  0.08 | 4.59  $ \pm $  0.08 | 5.06  $ \pm $  0.05 |
| Image Captioning     | Acc  $ \uparrow $   | 5.27  $ \pm $  0.20 | 6.11  $ \pm $  0.23 |
| Detail  $ \uparrow $ | 4.39  $ \pm $  0.29 | 4.36  $ \pm $  0.17 | 5.74  $ \pm $  0.24 |"
441,"|                                                                                           | Greedy                 | LURE                     | Woodpecker $ ^{*} $              | VCD                     | OPERA                   | MARINE (ours)           |
| ----------------------------------------------------------------------------------------- | ---------------------- | ------------------------ | -------------------------------- | ----------------------- | ----------------------- | ----------------------- |
| Training Cost                                                                             | 0                      | 10 min on A100 80G       | 0                                | 0                       | 0                       | 0                       |
| Inference Latency $ ^{\text{(ms)/token)}} $                                               | 26.3 ( $ \times $ 1.0) | 179.9 ( $ \times $ 6.84) | 94.5 ( $ \times $ 3.59) $ ^{*} $ | 53.4 ( $ \times $ 2.03) | 185.1 ( $ \times $ 7.0) | 52.2 ( $ \times $ 1.98) |
| $ ^{*} $ Woodpecker requires GPT API key access and the latency may depend on OPENAI API. |                        |                          |                                  |                         |                         |                         |"
460,"|                  | Llama2-13B | Llama3-8B | Mistral-7B |
| ---------------- | ---------- | --------- | ---------- |
|                  | GSM8K      | MATH      | AQuA       |
| Pretrained model | 29.5       | 2.0       | 21.0       |
| Prompt tuning    | 38.1       | 7.6       | 22.4       |
| ACT              | 39.2       | 7.1       | 20.1       |
| DPC              | 41.9       | 9.2       | 31.1       |"
443,"| Model                 | Metric  | Cabinet | Sweep | Sandwich | Sort | Rope |
| --------------------- | ------- | ------- | ----- | -------- | ---- | ---- |
| Central Plan (oracle) | Success | 0.90    | 1.00  | 0.96     | 0.70 | 0.50 |
| #Step                 | 4.0     | 8.4     | 8.8   | 8.6      | 2.3  |      |
| Roco Dialog           | Success | 0.75    | 0.70  | 0.70     | 0.70 | 0.70 |
| #Step                 | 4.7     | 7.9     | 9.1   | 5.4      | 2.4  |      |
| IoA                   | Success | 1.00    | 0.80  | 1.00     | 1.00 | 0.70 |
| #Step                 | 4.6     | 8.5     | 8.9   | 5.8      | 2.6  |      |"
443,"| Models          | Agent Type | Level 1 | Level 2 | Level 3 | Overall |
| --------------- | ---------- | ------- | ------- | ------- | ------- |
| GPT-4           | 👟          | 15.09   | 2.33    | 0.00    | 6.06    |
| GPT-4-Turbo     | 👟          | 20.75   | 5.81    | 0.00    | 9.70    |
| AutoGPT-4       | 👟          | 13.21   | 0.00    | 3.85    | 4.85    |
| GPT-4 + Plugins | 👟          | 30.30   | 9.70    | 0.00    | 14.60   |
| FRIDAY          | 👟          | 45.28   | 34.88   | 11.54   | 34.55   |
| AutoGen         | 👟          | 54.72   | 38.37   | 11.54   | 39.39   |
| IoA             | 👟          | 50.94   | 40.70   | 15.38   | 40.00   |"
443,"| Category                | IoA vs. Autogpt | IoA vs. Open Interpreter | IoA vs. AutoGen |
| ----------------------- | --------------- | ------------------------ | --------------- |
| Math                    | 83.3%           | 66.7%                    | 60.0%           |
| Coding &amp; Developing | 83.3%           | 53.3%                    | 53.3%           |
| Life Assistance         | 56.1%           | 63.4%                    | 43.9%           |
| Search &amp; Report     | 84.6%           | 67.3%                    | 57.7%           |
| Overall Performance     | 76.5%           | 63.4%                    | 53.6%           |"
443,"| Model                              | TriviaQA | NQ    | HotpotQA | 2WMHQA | Overall |
| ---------------------------------- | -------- | ----- | -------- | ------ | ------- |
| GPT 4                              | 0.902    | 0.692 | 0.566    | 0.284  | 0.611   |
| GPT 3.5 Turbo                      | 0.778    | 0.532 | 0.384    | 0.210  | 0.476   |
| + Zero-Shot CoT                    | 0.772    | 0.588 | 0.410    | 0.190  | 0.490   |
| + Self Consistency                 | 0.818    | 0.622 | 0.408    | 0.206  | 0.514   |
| + Reflxion                         | 0.762    | 0.586 | 0.378    | 0.254  | 0.495   |
| + Multi-Agent Debate1              | 0.798    | 0.648 | 0.394    | 0.186  | 0.507   |
| + Multi-Agent Debate2              | 0.756    | 0.576 | 0.450    | 0.334  | 0.529   |
| Apollo&#x27;s Oracle (Homogeneous) | 0.834    | 0.662 | 0.542    | 0.350  | 0.597   |
| IoA + 2 Agents (Heterogeneous)     | 0.803    | 0.708 | 0.478    | 0.449  | 0.610   |
| IoA + 2 Agents (Homogeneous)       | 0.820    | 0.671 | 0.586    | 0.530  | 0.652   |
| IoA + 3 Agents (Homogeneous)       | 0.908    | 0.682 | 0.575    | 0.519  | 0.671   |"
444,"|                                 | SAMMEME | Overall | CASME2ME | Overall |
| ------------------------------- | ------- | ------- | -------- | ------- |
| AUW-GCN (Yin et al. ICME 2023)  | 0.4293  | 0.1984  | 0.3728   | 0.4235  |
| Spot-GCN (Deng et al. FG 2024)  | 0.4631  | 0.4035  | 0.4454   | 0.4340  |
| SpotFormer (Deng et al. arxiv)  | 0.4447  | 0.4281  | 0.4401   | 0.5061  |
| LAC (Lee et al. ICCV 2021)      | 0.3714  | 0.1983  | 0.3223   | 0.3889  |
| HR-Pro (Zhang et al. AAAI 2024) | 0.3395  | 0.1667  | 0.2895   | 0.3515  |
| TSP-Net (Xia et al. CVPR 2024)  | 0.3152  | 0.1567  | 0.2703   | 0.3781  |
| Ours                            | 0.4189  | 0.2033  | 0.3587   | 0.4395  |"
445,"| Method / Budget | K=32 | K=64 | K=128 | K=256 | K=512 |
| --------------- | ---- | ---- | ----- | ----- | ----- |
| H2O             | 0%   | 1%   | 1%    | 1%    | 3%    |
| TOVA            | 0%   | 1%   | 1%    | 3%    | 8%    |
| StreamingLLM    | 1%   | 1%   | 1%    | 3%    | 5%    |
| Quest           | 65%  | 99%  | 99%   | 99%   | 100%  |
| TD+L7(Ours)     | 73%  | 92%  | 98%   | 99%   | 100%  |"
445,"| Model (ctx)        | Method | K=32 | K=64 | K=128 | K=256 | K=512 |
| ------------------ | ------ | ---- | ---- | ----- | ----- | ----- |
| Llama-3-8B (10K)   | Quest  | 74%  | 84%  | 99%   | 98%   | 100%  |
| TD+L13             | 88%    | 98%  | 100% | 100%  | 100%  |       |
| Llama-3-8B (100K)  | Quest  | 38%  | 50%  | 65%   | 87%   | 98%   |
| TD+L13             | 86%    | 92%  | 100% | 100%  | 100%  |       |
| Llama-3.1-8B (10K) | Quest  | 74%  | 86%  | 94%   | 100%  | 98%   |
| TD+L13             | 100%   | 100% | 100% | 100%  | 100%  |       |
| Llama-3.1-8B (32K) | Quest  | 78%  | 88%  | 92%   | 100%  | 100%  |
| TD+L13             | 98%    | 100% | 100% | 100%  | 100%  |       |
| Llama-3-70B (10K)  | Quest  | 68%  | 72%  | 90%   | 98%   | 100%  |
| TD+L14             | 87%    | 93%  | 100% | 100%  | 100%  |       |
| Llama-3-70B (32K)  | Quest  | 50%  | 80%  | 88%   | 92%   | 78%   |
| TD+L14             | 82%    | 98%  | 98%  | 100%  | 100%  |       |"
445,"| Task  | Full   | Token Budget: 1024 | Token Budget: 4096 |
| ----- | ------ | ------------------ | ------------------ |
| Quest | TD+L13 | Quest              | TD+L13             |
| MFQA  | 30.76  | 26.21              | 28.57              |
| NrtQA | 5.52   | 4.08               | 7.63               |
| Qasp  | 14.56  | 12.19              | 11.11              |
| 2Wiki | 13.32  | 12.61              | 13.56              |
| HotQA | 11.50  | 10.75              | 9.82               |
| QMSm  | 19.43  | 19.56              | 20.37              |
| TrQA  | 86.56  | 83.47              | 79.78              |
| PRe   | 77.00  | 63.84              | 75.17              |
| Avg   | 32.33  | 29.09              | 30.75              |"
446,"| Architecture                        | params throughput (flops/img Peak Mem) | Top-1 Acc.          |
| ----------------------------------- | -------------------------------------- | ------------------- |
| $ (\times 10^{6}) $                 | (im/s)                                 | $ (\times 10^{9}) $ |
| MLP-Mixers (ResMLP)                 |                                        |                     |
| ResMLP-L24 $ ^{1} $                 | 318.1                                  | 1107                |
| $ \mathcal{E} $ (ResMLP-L24)        | 159.2                                  | 1756                |
| ResMLP-B24                          | 115.7                                  | 2482                |
| $ \mathcal{E} $ (ResMLP-B24)        | 58.0                                   | 3459                |
| Vision Transformers (DeiT III)      |                                        |                     |
| ViT-H                               | 632.1                                  | 431                 |
| $ \mathcal{H} $ (ViT-H)             | 474.2                                  | 466                 |
| $ \mathcal{E} $ (ViT-H)             | 316.1                                  | 501                 |
| ViT-L                               | 304.4                                  | 1064                |
| $ \mathcal{H} $ (ViT-L)             | 228.3                                  | 1123                |
| $ \mathcal{E} $ (ViT-L)             | 152.2                                  | 1204                |
| ViT-B                               | 86.6                                   | 3088                |
| $ \mathcal{H} $ (ViT-B)             | 65.0                                   | 3162                |
| $ \mathcal{E} $ (ViT-B)             | 43.3                                   | 3266                |
| Convolutional Networks (ConvNeXt)   |                                        |                     |
| ConvNeXt-L (iso.)                   | 305.9                                  | 1284                |
| $ \mathcal{E} $ (ConvNeXt-L (iso.)) | 153.1                                  | 1527                |
| ConvNeXt-B (iso.)                   | 87.1                                   | 3890                |
| $ \mathcal{E} $ (ConvNeXt-B (iso.)) | 43.6                                   | 4094                |"
462,"| Safety | Eval            | ASR  |
| ------ | --------------- | ---- |
|        | XSTest          | 0.08 |
|        | HarmBench       | 0.28 |
|        | AUTOBENCH       | 0.38 |
|        | HarmBench GCG-T | 0.45 |"
464,"| General linear solution (GLS)                                                                      | Least-squares solution (LSS)                                                                                                                       | Min. representation-norm solution (MRNS)                                                                                               | Min. weight-norm solution (MWNS)                                                                                            |
| -------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| any  $ \mathbf{W}_1, \mathbf{W}_2 $  s.t.  $ \mathbf{W}_2 \mathbf{W}_1 \Sigma_{xx} = \Sigma_{yx} $ | arg min  $ \|\mathbf{W}_2 \mathbf{W}_1\|_F^2 $   $ \mathbf{W}_1, \mathbf{W}_2 $  subj. to  $ \mathbf{W}_2 \mathbf{W}_1 \Sigma_{xx} = \Sigma_{yx} $ | arg min  $ \|\mathbf{W}_1 \mathbf{X}\|_F^2 + \|\mathbf{W}_2\|_F^2 $  subj. to  $ \mathbf{W}_2 \mathbf{W}_1 \Sigma_{xx} = \Sigma_{yx} $ | arg min  $ \|\mathbf{W}_1\|_F^2 + \|\mathbf{W}_2\|_F^2 $  subj. to  $ \mathbf{W}_2 \mathbf{W}_1 \Sigma_{xx} = \Sigma_{yx} $ |
| ⇒ Least constrained solution type.                                                                 | ⇒ Constrains null-space transforms.                                                                                                                | ⇒ Constrains magnitude of activity on the training data.                                                                               | ⇒ Balanced input and output weights.                                                                                        |"
474,"| Model       | Before | After |
| ----------- | ------ | ----- |
| AAcc.       | SR     | AAcc. |
| NN $ _{1} $ | 0.134  | 0.714 |
| NN $ _{2} $ | 0.067  | 0.701 |
| NN $ _{3} $ | 0.195  | 0.584 |
| NN $ _{4} $ | 0.035  | 0.997 |
| NN $ _{5} $ | 0.059  | 0.842 |
| NN $ _{6} $ | 0.236  | 0.784 |
| NN $ _{7} $ | 0.154  | 0.933 |
| Avg         | 0.126  | 0.794 |"
447,"| Model                  | Occurrence rate (%) | Toxicity score | Coverage (%) |
| ---------------------- | ------------------- | -------------- | ------------ |
| o-                     | S-                  | R-             | I-           |
| GPT-3.5-Turbo-Instruct | 62.8                | 42.1           | 37.6         |
| GPT-4                  | 46.1                | 31.4           | 29.6         |
| GPT-3.5-Turbo          | 45.3                | 27.1           | 27.1         |
| Claude3 Opus           | 43.1                | 30.3           | 30.4         |
| GPT-4o                 | 42.1                | 30.9           | 28.6         |
| Gemini 1.5 Flash       | 42.1                | 25.9           | 27.8         |
| GPT-o1-preview         | 39.9                | 26.6           | 29.6         |
| Gemini 1.0 Pro         | 39.3                | 12.8           | 17.2         |
| Gemini 1.5 Pro         | 37.9                | 23.9           | 27.8         |
| GPT-o1-mini            | 36.9                | 16.9           | 23.4         |
| Claude3 Sonnet         | 30.5                | 18.5           | 19.9         |
| Claude 3 Haiku         | 25.1                | 13.8           | 17.8         |"
448,"| Model            | Correctness | Relevance | Scientific style | Compile Error Rate |
| ---------------- | ----------- | --------- | ---------------- | ------------------ |
| Automatikz       | 2.05        | 2.31      | 3.35             | 0.04               |
| Llama_tikz       | 1.78        | 1.94      | 2.61             | 0.29               |
| GPT-4o_tikz      | 3.50        | 3.67      | 3.75             | 0.09               |
| Llama_python     | 2.10        | 2.54      | 3.18             | 0.28               |
| GPT-4o_python    | 3.51        | 3.40      | 3.93             | 0.07               |
| Stable Diffusion | 2.19        | 2.09      | 1.96             | -                  |
| DALL·E           | 2.16        | 2.00      | 1.55             | -                  |"
448,"| Criteria         | Correctness | Relevance | Scientific style |
| ---------------- | ----------- | --------- | ---------------- |
| Language         | EN          | DE        | ZH               |
| Llama_tikz       | 1.88        | 1.48      | 1.50             |
| GPT-4o_tikz      | 3.85        | 4.03      | 3.98             |
| OpenAI-o1_tikz   | 4.43        | 3.68      | 3.83             |
| Llama_python     | 2.53        | 1.35      | 1.75             |
| GPT-4o_python    | 3.38        | 4.15      | 4.13             |
| OpenAI-o1_python | 4.28        | 3.45      | 4.10             |
| Qwen2.5_python   | 3.10        | 2.30      | 2.05             |
| DALL-E           | 1.98        | 2.15      | 1.83             |
| Average          | 3.18        | 2.82      | 2.89             |"
448,"| Model            | Attribute | Numerical | Spatial | Attribute &amp; Numerical | Attribute &amp; Spatial | Numerical &amp; Spatial | Attribute &amp; Numerical &amp; Spatial |
| ---------------- | --------- | --------- | ------- | ------------------------- | ----------------------- | ----------------------- | --------------------------------------- |
| Automatikz       | 2.42      | 1.91      | 1.71    | 2.29                      | 2.04                    | 2.13                    | 1.77                                    |
| Llama_tikz       | 2.53      | 1.55      | 1.77    | 1.69                      | 1.84                    | 1.91                    | 1.3                                     |
| GPT-4o_tikz      | 4.11      | 3.49      | 3.35    | 3.41                      | 3.53                    | 3.59                    | 3.13                                    |
| Llama_python     | 2.24      | 2.38      | 1.96    | 2.28                      | 2.28                    | 1.63                    | 1.97                                    |
| GPT-4o_python    | 3.95      | 3.92      | 3.47    | 3.46                      | 3.34                    | 3.28                    | 3.13                                    |
| Stable Diffusion | 2.75      | 1.73      | 2.06    | 2.41                      | 2.46                    | 1.96                    | 2.11                                    |
| DALL·E           | 2.68      | 1.77      | 2.13    | 2.36                      | 2.31                    | 1.94                    | 2.07                                    |
| Average          | 2.95      | 2.39      | 2.35    | 2.56                      | 2.54                    | 2.35                    | 2.21                                    |
| Sample Size      | 48        | 64        | 56      | 80                        | 40                      | 64                      | 52                                      |"
450,"| Method   | FS $ \downarrow $ | FP $ \downarrow $ | HSP $ \downarrow $ | HHP $ \downarrow $ |
| -------- | ----------------- | ----------------- | ------------------ | ------------------ |
| Baseline | 0                 | 0.0099            | 5.5119             | 0.1991             |
| ComMDM   | 0.0001            | 0.0215            | 10.5532            | 0.2712             |
| InterGen | 0.0001            | 0.0242            | 9.6035             | 0.1774             |
| Ours     | 0                 | 0.0189            | 5.7529             | 0.1687             |"
450,"| Method   | FS $ \downarrow $ | FP $ \downarrow $ | HSP $ \downarrow $ | HHP $ \downarrow $ |
| -------- | ----------------- | ----------------- | ------------------ | ------------------ |
| Real     | 0                 | 0.0037            | 0.0043             | 0.0807             |
| ComMDM   | 0                 | 0.0076            | 6.2802             | 0.1336             |
| InterGen | 0                 | 0.0049            | 6.6408             | 0.0989             |
| Ours     | 0                 | 0.0047            | 1.6950             | 0.0742             |"
451,"|             | Classical | DFT        | MLIPs         |
| ----------- | --------- | ---------- | ------------- |
| Accuracy    | Low       | High       | $ \sim $ High |
| Speed       | Fast      | Slow       | Medium        |
| Scalability | N         | N $ ^{3} $ | N             |"
451,"| $ l_{\text{max}} $ | 1  | 2  | 3  | 4  | 5  |
| ------------------ | -- | -- | -- | -- | -- |
| Sparsity (%)       | 71 | 78 | 82 | 84 | 86 |"
465,"| $ \theta(t|l)\equiv $       | 1/2.5  | 1/1.5  | 1.5   | 2.5   |
| --------------------------- | ------ | ------ | ----- | ----- |
| $ \psi(\theta) $            | 0.893  | 0.694  | 0.404 | 0.297 |
| Bias ( $ \times 10^{-2} $ ) | -0.388 | -0.139 | 0.031 | 0.039 |
| SEE ( $ \times 10^{-2} $ )  | 3.035  | 2.508  | 2.055 | 2.096 |
| SD ( $ \times 10^{-2} $ )   | 2.982  | 2.495  | 2.059 | 2.088 |
| 95% CP (%)                  | 93.9   | 94     | 94.5  | 95.2  |"
466,"| $ | a | b | b | # | a | b | b | @ |
| - | - | - | - | - | - | - | - | - |"
466,"| b | a | b | b | a | a | # | b | a | b | b | a | a | @ |
| - | - | - | - | - | - | - | - | - | - | - | - | - | - |"
466,"| Initial    | P(i) := Q_{\sigma}(i)                                 | for \sigma \in \Sigma |
| ---------- | ----------------------------------------------------- | --------------------- |
| Boolean    | P(i) := \neg P_{1}(i)P(i) := P_{1}(i) \wedge P_{2}(i) |                       |
| Constant   | P(i) := \top                                          |                       |
| Positional | P(i) := \phi(i)for \phi \in \Phi                      |                       |
| Comparison | P(i) := C_{1}(i) \leq C_{2}(i)                        |                       |"
451,"| fp32                                      |                                           | Forward                                   | Backward     | Double-Backward |
| ----------------------------------------- | ----------------------------------------- | ----------------------------------------- | ------------ | --------------- |
| Latency (ms) (Speedup over  $ e_{3nm} $ ) | Latency (ms) (Speedup over  $ e_{3nm} $ ) | Latency (ms) (Speedup over  $ e_{3nm} $ ) |              |                 |
| $ l_{\text{max}} $                        | $ e_{3nm} $                               | cuEq                                      | FlashTP      | $ e_{3nm} $     |
| 1                                         | 2.05                                      | 0.61 (3.4x)                               | 0.22 (9.4x)  | 6.18            |
| 2                                         | 6.20                                      | 1.24 (5.0x)                               | 0.81 (7.7x)  | 32.26           |
| 3                                         | 18.40                                     | 16.06 (1.1x)                              | 2.54 (7.3x)  | 85.00           |
| 4                                         | 45.58                                     | 122.20 (0.4x)                             | 9.29 (4.9x)  | 268.48          |
| 5                                         | 99.01                                     | 121.02 (0.8x)                             | 18.22 (5.4x) | 779.14          |"
451,"| Table 3. Per-epoch training time of SevenNet models on a GPU |
| ------------------------------------------------------------ |
|                                                              |
|                                                              |
| e3nn                                                         |
| cuEq                                                         |
| FlashTP                                                      |"
452,"| Defense        | Attacked by RLBMI | Attacked by LOKT | Tested |              |
| -------------- | ----------------- | ---------------- | ------ | ------------ |
| $ \downarrow $ | Acc1              | $ \downarrow $   | Acc5   | $ \uparrow $ |
| None           | 44%               | 65%              | 505    | 0.89         |
| MID            | 31%               | 53%              | 523    | 0.90         |
| BiDO           | 24%               | 44%              | 552    | 1.01         |
| LS             | 24%               | 52%              | 527    | 0.98         |
| TL             | 37%               | 55%              | 523    | 0.94         |
| SSD            | 21%               | 42%              | 572    | 1.07         |"
453,"| Name of the dataset | #Tasks | #Tables | Inductive | C1 | C2 | C3 | C4 | Task type                         |
| ------------------- | ------ | ------- | --------- | -- | -- | -- | -- | --------------------------------- |
| Movielens           | 1      | 3       | ✓         | ✓  | ✓  | ✓  | ✗  | Relation Attribute                |
| MAG                 | 3      | 5       | ✗         | ✓  | ✓  | ✓  | ✓  | Entity Attribute, FK Prediction   |
| AVS                 | 2      | 3       | ✓         | ✓  | ✓  | ✓  | ✓  | Entity Attribute                  |
| IEEE-CIS            | 1      | 2       | ✗         | ✗  | ✓  | ✓  | ✗  | Entity Attribute                  |
| Outbrain            | 1      | 8       | ✓         | ✓  | ✓  | ✓  | ✗  | Relation Attribute                |
| Dignetica           | 2      | 8       | ✓         | ✓  | ✓  | ✓  | ✓  | Relation Attribute, FK Prediction |
| RetailRocket        | 1      | 5       | ✓         | ✓  | ✓  | ✓  | ✗  | Relation Attribute                |
| Stackexchange       | 3      | 7       | ✓         | ✓  | ✓  | ✓  | ✓  | Entity Attribute                  |"
453,"| Dataset                                 | Task          | XGBOOST | DeepFM | TabGNN | Vanilla Schema | JTD Schema | AutoG  | Expert |
| --------------------------------------- | ------------- | ------- | ------ | ------ | -------------- | ---------- | ------ | ------ |
| N/A                                     | N/A           | TabGNN  | K1N    | R2NE   | R2N            | AutoG      | Expert |        |
| Datasets with a Single Downstream Task  |               |         |        |        |                |            |        |        |
| IEEE-CIS                                | Fraud (AUC)   | 90.14   | 90.28  | 75.38  | 89.17          | 89.17      | 89.17  | 87.28  |
| RetailRocket                            | CVR (AUC)     | 50.35   | 49.33  | 82.64  | 50.45          | 49.90      | 50.82  | 74.78  |
| Movielens                               | Ratings (AUC) | 53.62   | 50.93  | 55.34  | 57.34          | 61.20      | 54.55  | 69.13  |
| Outbrain                                | CTR (AUC)     | 50.05   | 51.09  | 62.12  | 49.33          | 52.06      | 49.35  | 52.23  |
| AVS                                     | Repeat (AUC)  | 52.71   | 52.88  | 54.48  | 47.35          | 48.84      | 53.27  | 56.63  |
| Datasets with Multiple Downstream Tasks |               |         |        |        |                |            |        |        |
| MAG                                     | Venue (Acc)   | 21.95   | 28.19  | 42.84  | 27.24          | 46.26      | 21.26  | 46.97  |
| Citation (MRR)                          | 3.29          | 45.06   | 70.65  | 65.29  | 65.29          | 71.53      | 81.58  |        |
| Year (Acc)                              | 28.09         | 28.42   | 52.77  | 54.89  | 30.90          | 53.07      | 53.07  |        |
| Diginetica                              | CTR (AUC)     | 53.50   | 50.57  | 50.00  | 68.44          | 65.82      | 50.09  | 75.67  |
| Purchase (MRR)                          | 3.16          | 5.02    | 5.05   | 5.64   | 7.70           | 11.37      | 15.62  |        |
| Stackerchange                           | Chem (AUC)    | 58.20   | 59.84  | 78.27  | 77.67          | 76.47      | 83.58  | 86.92  |
| Uprote (AUC)                            | 86.69         | 87.64   | 85.28  | 86.45  | 86.47          | 88.61      | 87.63  |        |
| Ranking (ROCN)                          | 5.9           | 5.2     | 4.7    | 4.2    | 3.1            | 2.0        | 2.0    |        |
| Ranking (Average)                       | 5.8           | 5.3     | 4.5    | 4.4    | 3.3            | 2.0        | 2.0    |        |"
454,"| PCK  $ \uparrow $             | AUC  $ \uparrow $ | MPJPE  $ \downarrow $ |
| ----------------------------- | ----------------- | --------------------- |
| Single Hypothesis             |                   |                       |
| P-STMO (Shan et al., 2022)    | 97.9              | 75.8                  |
| MixSTE (Zhang et al., 2022b)  | 96.9              | 75.8                  |
| D3DP (Shan et al., 2023)      | 97.7              | 77.8                  |
| CHAMP-Naive                   | 97.9              | 76.0                  |
| Multiple Hypotheses           |                   |                       |
| MHFormer (Li et al., 2022b)   | 93.8              | 63.3                  |
| DiffPose (Gong et al., 2023)  | 98.0              | 75.9                  |
| DiffPose (Feng et al., 2023)  | 94.6              | 62.8                  |
| D3DP-Agg (Shan et al., 2021)  | 97.7              | 78.2                  |
| CHAMP-Naive                   | 97.5              | 78.1                  |
| CHAMP-Naive-Agg               | 97.6              | 78.2                  |
| CHAMP                         | 97.9              | 78.4                  |
| CHAMP-Agg                     | 98.1              | 78.7                  |
| D3DP-Best (Shan et al., 2021) | 98.0              | 79.1                  |
| CHAMP-Best                    | 98.2              | 79.2                  |"
466,"| Counting    | C(i) := #[j ≤ i, ψ(i, j)] P(j) for ψ ∈ Ψ ∪ {T} |
| ----------- | ---------------------------------------------- |
| Conditional | C(i) := P(i)? C1(i) : C2(i)                    |
| Addition    | C(i) := C1(i) + C2(i)                          |
| Subtraction | C(i) := C1(i) - C2(i)                          |
| Constant    | C(i) := 1                                      |"
466,"| $ | 1 | 1 | 0 | 0 | 1 | # | 1 | @ |   |
| - | - | - | - | - | - | - | - | - | - |
| $ | 0 | 1 | 0 | 0 | 0 | 1 | # | 0 | @ |"
466,"| $ | u | r | s | # | u | r | s | @ |
| - | - | - | - | - | - | - | - | - |"
466,"| $ | a | a | a | a | a | a | # | 0 | @ |   |
| - | - | - | - | - | - | - | - | - | - | - |
| $ | a | a | a | a | a | a | a | # | 1 | @ |"
467,"|            | $ \tau=1 $        | $ \tau=2 $        | $ \tau=3 $        | $ \tau=4 $        | $ \tau=5 $        | $ \tau=6 $        | $ \tau=7 $        | $ \tau=8 $        | $ \tau=9 $        | $ \tau=10 $       |
| ---------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- |
| MSMs       | 0.37 $ \pm $ 0.01 | 0.57 $ \pm $ 0.03 | 0.74 $ \pm $ 0.06 | 0.88 $ \pm $ 0.03 | 1.14 $ \pm $ 0.10 | 1.95 $ \pm $ 1.48 | 3.44 $ \pm $ 4.57 | &gt;10.0          | &gt;10.0          | &gt;10.0          |
| RMSNs      | 0.24 $ \pm $ 0.01 | 0.47 $ \pm $ 0.01 | 0.60 $ \pm $ 0.01 | 0.70 $ \pm $ 0.02 | 0.78 $ \pm $ 0.04 | 0.84 $ \pm $ 0.05 | 0.89 $ \pm $ 0.06 | 0.94 $ \pm $ 0.08 | 0.97 $ \pm $ 0.09 | 1.00 $ \pm $ 0.11 |
| CRN        | 0.30 $ \pm $ 0.01 | 0.48 $ \pm $ 0.02 | 0.59 $ \pm $ 0.02 | 0.65 $ \pm $ 0.02 | 0.68 $ \pm $ 0.02 | 0.71 $ \pm $ 0.01 | 0.72 $ \pm $ 0.01 | 0.74 $ \pm $ 0.01 | 0.76 $ \pm $ 0.01 | 0.78 $ \pm $ 0.02 |
| G-Net      | 0.34 $ \pm $ 0.01 | 0.67 $ \pm $ 0.03 | 0.83 $ \pm $ 0.04 | 0.94 $ \pm $ 0.04 | 1.03 $ \pm $ 0.05 | 1.10 $ \pm $ 0.05 | 1.16 $ \pm $ 0.05 | 1.21 $ \pm $ 0.06 | 1.25 $ \pm $ 0.06 | 1.29 $ \pm $ 0.06 |
| EDCT       | 0.29 $ \pm $ 0.01 | 0.46 $ \pm $ 0.01 | 0.56 $ \pm $ 0.01 | 0.62 $ \pm $ 0.01 | 0.67 $ \pm $ 0.01 | 0.70 $ \pm $ 0.01 | 0.72 $ \pm $ 0.01 | 0.74 $ \pm $ 0.01 | 0.76 $ \pm $ 0.01 | 0.78 $ \pm $ 0.01 |
| CT         | 0.21 $ \pm $ 0.01 | 0.38 $ \pm $ 0.01 | 0.46 $ \pm $ 0.01 | 0.50 $ \pm $ 0.01 | 0.53 $ \pm $ 0.01 | 0.54 $ \pm $ 0.01 | 0.55 $ \pm $ 0.01 | 0.57 $ \pm $ 0.01 | 0.58 $ \pm $ 0.01 | 0.59 $ \pm $ 0.01 |
| Mamba-HSIC | 0.25 $ \pm $ 0.02 | 0.32 $ \pm $ 0.01 | 0.38 $ \pm $ 0.01 | 0.43 $ \pm $ 0.02 | 0.48 $ \pm $ 0.01 | 0.51 $ \pm $ 0.01 | 0.54 $ \pm $ 0.02 | 0.57 $ \pm $ 0.01 | 0.59 $ \pm $ 0.02 | 0.60 $ \pm $ 0.02 |
| Mamba-CDSP | 0.19 $ \pm $ 0.01 | 0.25 $ \pm $ 0.01 | 0.30 $ \pm $ 0.01 | 0.34 $ \pm $ 0.01 | 0.37 $ \pm $ 0.01 | 0.42 $ \pm $ 0.01 | 0.43 $ \pm $ 0.01 | 0.44 $ \pm $ 0.01 | 0.46 $ \pm $ 0.01 | 0.47 $ \pm $ 0.01 |"
456,"| METHOD                            | AUC(%)↑ | AP(%)↑ | AUC-FRAME(%)↑ | MTTA(S)↑ | FPS↑  | MRESPONSE(S)↓ |
| --------------------------------- | ------- | ------ | ------------- | -------- | ----- | ------------- |
| DATASETS                          | ROL     | DOTA   | ROL           | DOTA     | ROL   | DOTA          |
| CONVAE(HASAN ET AL., 2016)        | 0.759   | 0.779  | 0.493         | 0.521    | 0.608 | 0.663         |
| CONVLSTMAE(CHONG &amp; TAY, 2017) | 0.713   | 0.501  | 0.479         | 0.626    | 0.594 | 0.595         |
| ANOPRED(LIU ET AL., 2018)         | 0.773   | 0.790  | 0.517         | 0.541    | 0.610 | 0.675         |
| FOL-IOU(YAO ET AL., 2022)         | 0.817   | 0.830  | 0.539         | 0.563    | 0.660 | 0.730         |
| FOL-MASK(YAO ET AL., 2022)        | 0.826   | 0.846  | 0.546         | 0.569    | 0.674 | 0.725         |
| FOL-STD(YAO ET AL., 2022)         | 0.837   | 0.852  | 0.550         | 0.573    | 0.679 | 0.714         |
| FOL-ENSEMBLE(YAO ET AL., 2022)    | 0.849   | 0.866  | 0.563         | 0.577    | 0.698 | 0.744         |
| MAMTCF(LIANG ET AL., 2023)        | 0.841   | 0.862  | 0.568         | 0.581    | 0.701 | 0.766         |
| AM-NET(KARIM ET AL., 2023)        | 0.855   | 0.874  | 0.576         | 0.586    | 0.707 | 0.765         |
| STFE(ZHOU ET AL., 2022)           | 0.862   | 0.881  | 0.579         | 0.602    | 0.728 | 0.793         |
| TTHF(LIANG ET AL., 2024)          | 0.871   | 0.891  | 0.585         | 0.618    | 0.733 | 0.847         |
| OURS                              | 0.879   | 0.896  | 0.570         | 0.623    | 0.736 | 0.823         |"
457,"| Model              | Recall $ \uparrow $ | Precision $ \uparrow $ | Accuracy $ \uparrow $ | False-Alarm $ \downarrow $ | F1-Score $ \uparrow $ |
| ------------------ | ------------------- | ---------------------- | --------------------- | -------------------------- | --------------------- |
| Proprietary models |                     |                        |                       |                            |                       |
| Claude-3-Sonnet    | 27.47%              | 37.31%                 | 52.42%                | 62.69%                     | 31.65%                |
| Claude-3.5-Sonnet  | 97.89%              | 45.37%                 | 49.78%                | 54.63%                     | 62.00%                |
| GPT-4o-mini        | 100.00%             | 35.28%                 | 36.12%                | 64.73%                     | 52.15%                |
| GPT-4o             | 98.11%              | 48.15%                 | 49.78%                | 51.85%                     | 64.60%                |"
457,"| LLaMA-3.1-8B           | 98.86%  | 38.16% | 39.06% | 61.84% | 55.06% |
| ---------------------- | ------- | ------ | ------ | ------ | ------ |
| LLaMA-3.1-8B-Proactive | 99.06%  | 49.76% | 52.86% | 50.24% | 66.25% |
| Qwen2-7B               | 98.02%  | 44.00% | 43.61% | 56.00% | 60.74% |
| Qwen2-7B-Proactive     | 100.00% | 49.78% | 50.66% | 50.22% | 66.47% |"
458,"| Prompt: Rewrite the sentence using gender-neutral language: A salesman is giving a presentation. (Principle (1): Instruction Following)                                                                                                                                                     |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| A salesperson is conducting a slideshow. A salesperson is giving a presentation.                                                                                                                                                                                                            |
| Prompt: What is a bomb? (Principle (3): Stylistic Preference)                                                                                                                                                                                                                               |
| A bomb is a destructive device filled with an explosive material designed to cause destruction or damage. A bomb is an explosive device which can cause an intense release of heat, light, sound, and fragments, intended to cause harm to people or destroy property. Bombs may contain... |"
458,"|                                      | Knowledge | Reasoning | Math  | Coding | Overall |
| ------------------------------------ | --------- | --------- | ----- | ------ | ------- |
| Prompted Judges                      |           |           |       |        |         |
| Vanilla (GPT-4o)                     | 44.16     | 47.96     | 66.07 | 61.90  | 50.86   |
| Arena-Hard Judge (GPT-4o)            | 50.65     | 54.08     | 75.00 | 59.52  | 56.57   |
| VertexAI Evaluation (Gemini-1.5-pro) | 45.45     | 44.90     | 53.57 | 28.57  | 44.57   |
| Fine-tuned Judges                    |           |           |       |        |         |
| PandaLM                              | 9.09      | 21.43     | 7.14  | 16.67  | 13.14   |
| Prometheus2-7b                       | 38.31     | 25.51     | 35.71 | 42.86  | 34.86   |
| Prometheus2-8x7b                     | 41.56     | 39.80     | 50.00 | 23.81  | 40.29   |
| Prometheus2-bgb-8x7b                 | 45.45     | 30.61     | 46.43 | 28.57  | 39.43   |
| JudgeLM-7B                           | 23.38     | 29.59     | 32.14 | 11.90  | 25.14   |
| JudgeLM-13B                          | 26.62     | 29.59     | 28.57 | 19.05  | 26.86   |
| JudgeLM-33B                          | 32.47     | 48.98     | 33.93 | 19.05  | 35.71   |
| AutoJ                                | 40.26     | 29.59     | 44.64 | 28.57  | 36.57   |
| Skywork-LLaMA-3.1B-8B                | 51.30     | 54.08     | 73.21 | 33.33  | 53.43   |
| Skywork-LLaMA-3.1B-70B               | 55.84     | 55.10     | 73.21 | 47.62  | 57.43   |
| Multi-Agent Judges                   |           |           |       |        |         |
| ChatEval                             | 32.47     | 31.63     | 44.64 | 30.95  | 34.00   |"
458,"| Reward Model                | Knowledge | Reasoning | Math  | Coding | Overall |
| --------------------------- | --------- | --------- | ----- | ------ | ------- |
| Skywork-Reward-Gemma-2-27B  | 59.74     | 66.33     | 83.93 | 50.00  | 64.29   |
| Skywork-Reward-Llama-3.1-8B | 59.09     | 64.29     | 76.79 | 50.00  | 62.29   |
| InternLM2-20B-Reward        | 62.34     | 69.39     | 66.07 | 50.00  | 63.43   |
| InternLM2-7B-Reward         | 56.49     | 61.22     | 71.43 | 50.00  | 59.43   |
| GRM-Gemma-2B                | 62.99     | 53.06     | 64.29 | 54.76  | 59.43   |"
458,"| Setup                          | Knowledge | Reasoning | Math  | Coding | Overall |
| ------------------------------ | --------- | --------- | ----- | ------ | ------- |
| GPT-4o Solver                  | 48.70     | 53.06     | 58.93 | 73.81  | 54.57   |
| GPT-4o Judge                   | 50.65     | 54.08     | 75.00 | 59.52  | 56.57   |
| Claude-3.5-Sonnet Solver       | 61.04     | 62.24     | 60.71 | 88.10  | 64.57   |
| Claude-3.5-Sonnet Judge        | 62.34     | 66.33     | 66.07 | 64.29  | 64.29   |
| Llama-3.1-405B-Instruct Solver | 48.05     | 67.86     | 63.27 | 66.67  | 57.71   |
| Llama-3.1-405B-Instruct Judge  | 55.84     | 54.08     | 69.64 | 50.00  | 56.86   |
| Gemini-1.5-pro Solver          | 33.12     | 42.86     | 37.50 | 64.29  | 40.29   |
| Gemini-1.5-pro Judge           | 49.35     | 42.86     | 64.29 | 26.19  | 47.14   |"
459,"| Metric  | Method |
| ------- | ------ |
| SFT     | DPO    |
| AE (LC) | 13.22  |
| AE (WR) | 8.58   |
| AH (SC) | 9.2    |
| AH (WR) | 8.9    |"
468,"| Acc. (%)     | MetaTool ToolLLM ToolBench 2024b | 2024  | 2024  |
| ------------ | -------------------------------- | ----- | ----- |
| LLaMA-3.2-3B | 89.64                            | 72.92 | 79.47 |
| QWen-2.5-3B  | 88.29                            | 77.86 | 91.35 |
| LLaMA-3-8B   | 89.00                            | 78.31 | 93.57 |
| QWen-2.5-7B  | 92.50                            | 82.69 | 94.26 |
| GPT-4o-mini  | 88.31                            | 84.50 | 89.90 |"
468,"| Resource                   | Shortcuts Bench (Ours) | Meta Tool 2024b | Tool LLM 2024 | API Bench 2024 | Tool Alpaca 2023 | API Bank 2023 | Tool Bench 2024 | Tool QA 2024 | Tool Lens 2024 |
| -------------------------- | ---------------------- | --------------- | ------------- | -------------- | ---------------- | ------------- | --------------- | ------------ | -------------- |
| Real API?                  | ✓                      | ✓               | ✓             | ✓              | ✓                | ✗             | ✗               | ✗            | ✓              |
| Demand-driven Query?       | ✓                      | ✗               | ✗             | ✗              | ✗                | ✗             | ✗               | ✗            | ✗              |
| Human-Annotated Act.?      | ✓                      | ✗               | ✗             | ✗              | ✗                | ✗             | ✗               | ✗            | ✗              |
| Multi-APIs Query?          | ✓                      | ✓               | ✓             | ✗              | ✗                | ✗             | ✗               | ✓            | ✓              |
| Multi-Step Act.?           | ✓                      | ✓               | ✓             | ✗              | ✓                | ✓             | ✓               | ✓            | ✓              |
| Prec. Val. for Para. Fill? | ✓                      | ✗               | ✗             | ✗              | ✗                | ✗             | ✗               | ✗            | ✗              |
| Awareness for Ask Info?    | ✓                      | ✗               | ✗             | ✗              | ✗                | ✗             | ✗               | ✗            | ✗              |
| # Apps                     | 88                     | N/A             | 3451          | 3              | N/A              | N/A           | 8               | N/A          | N/A            |
| # APIs                     | 1414                   | 390             | 16464         | 1645           | 53               | 400           | 232             | 13           | 464            |
| # Queries                  | 7627                   | 21112           | 12657         | 17002          | 3938             | 274           | 2726            | 1530         | 18770          |
| # Avg APIs                 | 9.62                   | 1.02            | 2.3           | 1.0            | 1.0              | 2.1           | 5.4             | 3.5 $ ^{*} $ | 2.65           |
| # Avg Actions              | 21.62                  | 1.02            | 4.0           | 1.0            | 1.0              | 2.2           | 5.9             | 3.9 $ ^{*} $ | 2.67           |"
468,"| a. API Information(1) com.openai.chat.AskIntent (prompt: String, newChat: Boolean, model: ModelEntity, continuous: Boolean) -&gt; Ask ChatGPT: String(2) Parameters [parameter name (default value): title. parameterDescription]:(2.1) prompt: Message. Message to send to ChatGPT(2.2) newChat (0): Start new chat. Indicates whether a new chat should be started(2.3) model (default): Model. Model to use with the new chat(2.4) continuous (0): Continuous chat. Whether to enable back-and-forth chat or complete the Shortcut immediately after response(3) Return Value [return value name: resultValueName. displayTypeName]:(3.1) Ask ChatGPT: None(4) Description [title + description + actionSummary]:(4.1) title: Ask ChatGPT(4.2) description: This action will send a single message to a chat with ChatGPT and return the response.(4.3) actionSummary: Ask ChatGPT ${prompt} in a new chatb. Generating User Queries using Info from (c), (d), (e)(5) As a user-friendly and patient inquirer, you need to craft a query based on the provided shortcut......(6) APIs: Related API and corresponding Parameters and Return Value:...(7) NLP descriptions to developers:Ask ChatGPT SMesage$ in a new chat. Start new chat: true, Model: default, Continuous chat: true, Show When Run: true If $Ask ChatGPT$ $does not have any value$:Open $ChatGPT$NameInStore: Ask ChatGPT; DescriptionInStore: Chat with ChatGPT.......NameInStore: Ask ChatGPT. DescriptionInStore: Chat with ChatGPT. e. Desc from Sharing-site | c. NLP desc to devsd. Funcs of API |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------- |"
469,"| Method         | Accuracy                |
| -------------- | ----------------------- |
| IW-ERM         | 0.7520  $ \pm $  0.0209 |
| IW-ERM (small) | 0.7376  $ \pm $  0.0099 |
| FedAvg         | 0.5472  $ \pm $  0.0297 |
| FedBN          | 0.5359  $ \pm $  0.0306 |
| FedProx        | 0.5606  $ \pm $  0.0070 |
| SCAFFOLD       | 0.5774  $ \pm $  0.0036 |
| Upper Bound    | 0.8273  $ \pm $  0.0041 |"
469,"| Nodes | IW-ERM | FedAvg | FedBN  |
| ----- | ------ | ------ | ------ |
| 100   | 0.5354 | 0.3915 | 0.1537 |
| 200   | 0.6216 | 0.5942 | 0.1753 |"
470,"| Method            | Model            | rSDE-Bench | HumanEval (%) |
| ----------------- | ---------------- | ---------- | ------------- |
| Website(%)        | Game(%)          |            |               |
| Basic             | Advanced         | Basic      | Advanced      |
| Single-Agent      | Gemini-1.5-Flash | 29.79±1.00 | 11.61±2.34    |
| Claude-3.5-Sonnet | 58.90±1.48       | 37.11±1.06 | 44.20±5.41    |
| GPT-4o-Mini       | 62.90±2.52       | 44.40±4.21 | 42.76±15.50   |
| Multi-Agent       | MetaGPT          | 15.41±0.00 | 0.00±0.00     |
| Autogen           | 25.68±4.14       | 5.40±3.34  | 17.39±1.78    |
| MapCoder          | 34.70±1.59       | 14.57±0.66 | 29.71±6.72    |
| Agentverse        | 15.41±0.00       | 0.00±0.00  | 37.67±8.20    |
| ChatDev           | 62.67±0.28       | 43.45±0.77 | 53.63±5.70    |
| EvoMAC            | 89.38±1.01       | 65.05±1.56 | 77.54±2.04    |
| +26.48            | +20.65           | +34.78     | +21.50        |"
471,"| Miss % | Avg.  $ R^{2} $  Gain (p-value  $ \times 10^{-2} $ ) | Win Rate %   |
| ------ | ---------------------------------------------------- | ------------ |
| MCAR   | MAR                                                  | MNAR         |
| 10     | 0.014 (1.96)                                         | 0.023 (0.79) |
| 30     | 0.014 (0.35)                                         | 0.007 (9.24) |
| 50     | 0.011 (0.65)                                         | 0.010 (3.66) |
| 70     | 0.010 (2.01)                                         | 0.012 (0.72) |"
471,"| MASKING RATE | NAIVE COPY MASKING | MEDIAN TRUNCATED COPY MASKING | R²(↑) | RMSE (↓) |
| ------------ | ------------------ | ----------------------------- | ----- | -------- |
| MCAR         | MAR                | MNAR                          | MCAR  | MAR      |
| 10           | ✓                  | ✗                             | 0.378 | 0.396    |
| 30           | ✓                  | ✗                             | 0.355 | 0.397    |
| 50           | ✓                  | ✗                             | 0.348 | 0.384    |
| 10           | ✗                  | ✓                             | 0.366 | 0.346    |
| 30           | ✗                  | ✓                             | 0.378 | 0.399    |
| 50           | ✗                  | ✓                             | 0.375 | 0.408    |
| 90           | ✗                  | ✓                             | 0.386 | 0.416    |
| 95           | ✗                  | ✓                             | 0.386 | 0.420    |"
472,"| Method                       | Mark       | Backbone | FSS-1000 | Deepglobe | ISIC   | Chest X-ray | Average |
| ---------------------------- | ---------- | -------- | -------- | --------- | ------ | ----------- | ------- |
| 1-shot                       | 5-shot     | 1-shot   | 5-shot   | 1-shot    | 5-shot | 1-shot      | 5-shot  |
| CaNet (Zhang et al., 2019)   | CVPR-19    | Res-50   | 70.67    | 72.03     | 22.32  | 23.07       | 25.16   |
| PANet (Wang et al., 2019)    | ECCV-20    | Res-50   | 69.15    | 71.68     | 36.55  | 45.43       | 25.29   |
| RPMMs (Yang et al., 2020)    | ECCV-20    | Res-50   | 65.12    | 67.06     | 12.99  | 13.47       | 18.02   |
| PFENet (Tian et al., 2020)   | TPAMI-20   | Res-50   | 70.87    | 70.52     | 16.88  | 18.01       | 23.50   |
| RePRI (Boudiaf et al., 2021) | CVPR-21    | Res-50   | 70.96    | 74.23     | 25.03  | 27.41       | 23.27   |
| HSNet (Min et al., 2021)     | ICCV-21    | Res-50   | 77.53    | 80.99     | 29.65  | 35.08       | 31.20   |
| PATNet (Lei et al., 2022)    | ECCV-22    | Res-50   | 78.59    | 81.23     | 37.89  | 42.97       | 41.16   |
| PATNet (Lei et al., 2022)    | ECCV-22    | ViT-base | 72.03    | -         | 22.37  | -           | 44.25   |
| RestNet (Huang et al., 2023) | BMVC-23    | Res-50   | 81.53    | 84.89     | 22.70  | 29.90       | 42.25   |
| PerSAM (Zhang et al., 2024)  | ICLR-24    | ViT-base | 60.92    | 66.53     | 36.08  | 40.65       | 23.27   |
| ABCDFSS (Herzog, 2024)       | CVPR-24    | Res-50   | 74.60    | 76.20     | 42.60  | 45.70       | 45.70   |
| APSeg (He et al., 2024)      | CVPR-24    | ViT-base | 79.71    | 81.90     | 35.94  | 39.98       | 45.43   |
| APM (Tong et al., 2024b)     | NeurIPS-24 | Res-50   | 79.29    | 81.83     | 40.86  | 44.92       | 41.71   |
| DFN (Ours)                   | Ours       | Res-50   | 80.73    | 85.80     | 45.66  | 47.98       | 36.30   |
| DFN (Ours)                   | Ours       | ViT-base | 82.97    | 85.72     | 39.45  | 47.67       | 50.36   |"
473,"| Paper      | Task                       | Input                      | #layers                    | Upper bound                            | Lower bound          |
| ---------- | -------------------------- | -------------------------- | -------------------------- | -------------------------------------- | -------------------- |
| [4]        | seq-to-seq                 | token-wise (r,δ)-separated | $ \tilde{O}(n+\sqrt{nN}) $ | $ \tilde{O}(n+\sqrt{nN}) $             | -                    |
| [5]        | next-token                 | linearly independent       | 1                          | $ O(d^2N/n) $                          | -                    |
| [6]        | seq-to-seq                 | token-wise (r,δ)-separated | 1                          | $ O(dnN+d^2) $                         | -                    |
| [7]        | next-token                 | with positional encoding   | 1                          | $ O(\omega N) $                        | $ \Omega(\omega N) $ |
| Ours       | next-token                 | token-wise (r,δ)-separated | $ \tilde{O}(\sqrt{N}) $    | $ \tilde{O}(\sqrt{N}) $                | $ \Omega(\sqrt{N}) $ |
| seq-to-seq | token-wise (r,δ)-separated | $ \tilde{O}(\sqrt{nN}) $   | $ \tilde{O}(\sqrt{nN}) $   | $ \Omega(\sqrt{\frac{nN}{\log(nN)}}) $ |                      |"
475,"| Models                               | Evaluation Datasets             | Evaluation Metrics              |
| ------------------------------------ | ------------------------------- | ------------------------------- |
| ROUGE $ _{L} $   $ \uparrow $        | Cosine Similarity  $ \uparrow $ | Binary Success(%)  $ \uparrow $ |
| LLaVA-v1.5-13B (Liu et al., 2023a)   | AHA dataset (Test set)          | 0.061                           |
| ManiSkill-Fail                       | 0.000                           | 0.208                           |
| RoboFail (Liu et al., 2023d)         | 0.000                           | 0.203                           |
| LLaVA-NeXT-34B (Liu et al., 2024b)   | AHA dataset (Test set)          | 0.013                           |
| ManiSkill-Fail                       | 0.001                           | 0.195                           |
| RoboFail (Liu et al., 2023d)         | 0.018                           | 0.188                           |
| Qwen-VL (Bai et al., 2023)           | AHA dataset (Test set)          | 0.000                           |
| ManiSkill-Fail                       | 0.037                           | 0.301                           |
| RoboFail (Liu et al., 2023d)         | 0.000                           | 0.159                           |
| Gemini-1.5 Flash (Reid et al., 2024) | AHA dataset (Test set)          | 0.120                           |
| ManiSkill-Fail                       | 0.003                           | 0.121                           |
| RoboFail (Liu et al., 2023d)         | 0.000                           | 0.042                           |
| GPT-4o                               | AHA dataset (Test set)          | 0.251                           |
| ManiSkill-Fail                       | 0.142                           | 0.335                           |
| RoboFail (Liu et al., 2023d)         | 0.114                           | 0.318                           |
| GPT-4o-ICL (5-shot)                  | AHA dataset (Test set)          | 0.226                           |
| ManiSkill-Fail                       | 0.341                           | 0.429                           |
| RoboFail (Liu et al., 2023d)         | 0.236                           | 0.429                           |
| AHA-7B                               | AHA dataset (Test set)          | 0.434                           |
| ManiSkill-Fail                       | 0.609                           | 0.680                           |
| RoboFail (Liu et al., 2023d)         | 0.204                           | 0.394                           |
| AHA-13B (Ours)                       | AHA dataset (Test set)          | 0.446                           |
| ManiSkill-Fail                       | 0.600                           | 0.681                           |
| RoboFail (Liu et al., 2023d)         | 0.280                           | 0.471                           |"
476,"| Method         | Sampling                            | MLVU        | QAEGO4D     | EgoSchema   | CGBench     |
| -------------- | ----------------------------------- | ----------- | ----------- | ----------- | ----------- |
| dev Acc.       | test Acc.                           | Acc.        | Acc.        |             |             |
| Video-LLaVA-7B | 8 Frames                            | 46.5        | 37.0        | 41.4        | 18.7        |
| +ReKV          | 0.5 FPS  $ \rightarrow $  8 Frames  | 47.2 (+0.7) | 37.9 (+0.9) | 42.2 (+0.8) | 19.2 (+0.5) |
| LongVA-7B      | 32 Frames                           | 57.3        | 42.8        | 42.5        | 26.1        |
| +ReKV          | 0.5 FPS  $ \rightarrow $  32 Frames | 58.6 (+1.3) | 45.6 (+2.8) | 42.7 (+0.2) | 26.4 (+0.3) |
| LLaVA-OV-0.5B  | 64 Frames                           | 53.2        | 42.6        | 29.6        | 21.4        |
| +ReKV          | 0.5 FPS  $ \rightarrow $  64 Frames | 56.1 (+2.9) | 50.0 (+7.4) | 31.0 (+1.4) | 21.7 (+0.3) |
| LLaVA-OV-7B    | 64 Frames                           | 64.7        | 52.8        | 59.8        | 31.1        |
| +ReKV          | 0.5 FPS  $ \rightarrow $  64 Frames | 68.5 (+3.8) | 56.0 (+3.2) | 60.7 (+0.9) | 33.9 (+2.8) |
| LLaVA-OV-72B   | 32 Frames                           | 71.9        | 53.6        | 59.6        | 37.2        |
| +ReKV          | 0.1 FPS  $ \rightarrow $  32 Frames | 72.6 (+0.7) | 57.0 (+3.4) | 62.0 (+2.4) | 40.5 (+3.3) |"
476,"| Retrieval Method   | RVS-Ego | RVS-Movie | Running Speed | Memory Usage |
| ------------------ | ------- | --------- | ------------- | ------------ |
| Acc.               | Score   | Acc.      | Score         | Video Enc.   |
| Flash-VStream-7B   | 57.3    | 4.0       | 53.1          | 3.3          |
| LLaVA-OV-7B        |         |           |               |              |
| Uniform Sampling   | 56.2    | 3.7       | 43.0          | 3.3          |
| External Retrieval | 62.4    | 3.9       | 53.6          | 3.5          |
| Internal Retrieval | 63.7    | 4.0       | 54.4          | 3.6          |
| LLaVA-OV-0.5B      |         |           |               |              |
| Uniform Sampling   | 51.8    | 3.7       | 37.2          | 3.2          |
| External Retrieval | 54.1    | 3.8       | 44.7          | 3.4          |
| Internal Retrieval | 54.7    | 3.9       | 44.6          | 3.4          |"
476,"| #QAs | Baseline | Flash-VStream | ReKV (External) | ReKV (Internal) |
| ---- | -------- | ------------- | --------------- | --------------- |
| 100  | 22.4     | 15.5          | 21.7            | 18.5            |
| 200  | 12.7     | 14.1          | 11.4            | 9.6             |
| 360  | 8.5      | 13.8          | 6.8             | 5.6             |"
476,"| #QAs | Baseline | Flash-VStream | ReKV (External) | ReKV (Internal) |
| ---- | -------- | ------------- | --------------- | --------------- |
| 100  | 11.2     | $ 7.8 $       | 10.8            | 9.2             |
| 200  | 6.4      | 7.1           | 5.7             | $ 4.8 $         |
| 360  | 4.3      | 6.8           | 3.3             | $ 2.8 $         |"
477,"| Source Env |
| ---------- |
| τ1         |
| S^c        |
|            |"
477,"| Safety          | IRC                                             | ICRL                                |
| --------------- | ----------------------------------------------- | ----------------------------------- |
| Hard Constraint | ✗                                               | ✓                                   |
| Soft Constraint | influenced by different rewards and transitions | influenced by different transitions |"
478,"| Method                       | FID ( $ \downarrow $ ) | FVD ( $ \downarrow $ ) | ObjMC ( $ \downarrow $ ) | Zero-shot |
| ---------------------------- | ---------------------- | ---------------------- | ------------------------ | --------- |
| Image Conductor              | 48.81                  | 463.21                 | 21.07                    |           |
| DragNUWA                     | 30.73                  | 253.57                 | 10.84                    |           |
| DragAnything                 | 30.81                  | 268.47                 | 11.64                    |           |
| SVD (No Control)             | 30.50                  | 340.52                 | 39.59                    | ✓         |
| FreeTraj $ ^{\dagger} $      | 46.61                  | 394.14                 | 36.43                    | ✓         |
| DragDiffusion $ ^{\dagger} $ | 30.93                  | 458.29                 | 31.49                    | ✓         |
| SG-I2V                       | 28.87                  | 298.10                 | 14.43                    | ✓         |"
479,"| Family                    | Model name            | Size (B) | CodeMMLU | Rank |
| ------------------------- | --------------------- | -------- | -------- | ---- |
| Anthropic                 | Closed-source models  |          |          |      |
| Claude 3.7 Sonnet         | -                     | 61.65    | 3        |      |
| Claude 3.5 Sonnet         | -                     | 59.81    | 5        |      |
| Claude3 Sonnet (20240229) | -                     | 53.97    | 8        |      |
| OpenAI                    | GPT o3-mini           | -        | 62.36    | 2    |
| GPT 4o (2024-05-13)       | -                     | 67.00    | 1        |      |
| GPT 4o-mini               | -                     | 38.43    | 19       |      |
| Meta                      | Open-source models    |          |          |      |
| Llama3.3 70B Inst         | 70                    | 40.66    | 17       |      |
| Llama3.1 405B Inst        | 405                   | 58.23    | 6        |      |
| Llama3.1 70B Inst         | 70                    | 60.00    | 4        |      |
| CodeLlama34B Inst         | 34                    | 38.73    | 18       |      |
| DeepSeek                  | DeepSeek R1           | 671      | 43.91    | 14   |
| DeepSeek V3               | 685                   | 49.08    | 11       |      |
| DeepSeekCoder 33B Inst    | 33                    | 36.60    | 20       |      |
| DeepSeekMoE 16B Chat      | 16.4                  | 31.01    | 22       |      |
| Mistral                   | Mistral7B Inst (v0,3) | 7        | 43.33    | 15   |
| Mixtral 8×7B Inst         | 46.7                  | 42.96    | 16       |      |
| Codestral 22B             | 22                    | 47.60    | 12       |      |
| Microsoft                 | Phi4                  | 14       | 49.19    | 10   |
| Phi4 Mini Inst            | 12                    | 34.85    | 21       |      |
| Qwen                      | Qwen2.5 Max           | -        | 56.40    | 7    |
| Qwen2.5 14B Inst          | 14                    | 51.38    | 9        |      |
| QwQ 38B Preview           | 57                    | 46.34    | 13       |      |"
479,"| Models             | A     | B     | C     | D     | STD      |
| ------------------ | ----- | ----- | ----- | ----- | -------- |
| GPT-4o             | 80.49 | 78.05 | 71.34 | 70.12 | 4.38     |
| Claude3.5 Sonnet   | 90.24 | 81.1  | 85.37 | 79.27 | $ 4.23 $ |
| Claude3 Opus       | 79.27 | 77.44 | 82.32 | 84.76 | $ 2.81 $ |
| Mixtral 8x7B Inst  | 22.56 | 74.39 | 71.95 | 63.41 | 20.91    |
| Deepseek Coder 33B | 1.22  | 82.32 | 75.00 | 56.10 | 31.75    |
| CodeLlama 34B Py   | 0.61  | 77.44 | 70.73 | 49.39 | 30.09    |
| CodeLlama 34B Inst | 9.15  | 84.76 | 65.24 | 46.34 | 27.91    |"
480,"| Method                                     | Venue        | TTA                                        | Waymo  $ \rightarrow $  KITTI | nuScenes  $ \rightarrow $  KITTI |
| ------------------------------------------ | ------------ | ------------------------------------------ | ----------------------------- | -------------------------------- |
| $ AP_{\text{BEV}} $  /  $ AP_{\text{3D}} $ | Closed Gap   | $ AP_{\text{BEV}} $  /  $ AP_{\text{3D}} $ | Closed Gap                    |                                  |
| No Adapt.                                  | -            | -                                          | 67.64 / 27.48                 | -                                |
| SN                                         | CVPR&#x27;20 | ✗                                          | 78.96 / 59.20                 | +72.33% / +69.00%                |
| ST3D                                       | CVPR&#x27;21 | ✗                                          | 82.19 / 61.83                 | +92.97% / +74.72%                |
| Oracle                                     | -            | -                                          | 83.29 / 73.45                 | -                                |
| Tent                                       | ICLR&#x27;21 | ✓                                          | 65.09 / 30.12                 | -16.29% / +5.74%                 |
| CoTTA                                      | CVPR&#x27;22 | ✓                                          | 67.46 / 35.34                 | -1.15% / +17.10%                 |
| SAR                                        | ICLR&#x27;23 | ✓                                          | 65.81 / 30.39                 | -11.69% / +6.33%                 |
| MemCLR                                     | WACV&#x27;23 | ✓                                          | 65.61 / 29.83                 | -12.97% / +5.11%                 |
| MOS                                        | -            | ✓                                          | 81.90 / 64.16                 | +91.12% / +79.79%                |"
481,"|                  | Text embedding |    | Starting steps  | LFW-Test                              | WebPhoto-Test                         | WIDER-Test                            |
| ---------------- | -------------- | -- | --------------- | ------------------------------------- | ------------------------------------- | ------------------------------------- |
| Exp.             | VE Null Text   | SE | 1st 2nd 3rd 4th | FID $ \downarrow $  MUS. $ \uparrow $ | FID $ \downarrow $  MUS. $ \uparrow $ | FID $ \downarrow $  MUS. $ \uparrow $ |
| ①                | ✓              | ✓  | 69.99           | 76.11                                 | 93.40                                 | 75.58                                 |
| ②                | ✓              | ✓  | 55.56           | 76.02                                 | 76.06                                 | 75.15                                 |
| ③                | ✓              | ✓  | 55.07           | 75.75                                 | 77.76                                 | 75.30                                 |
| ④                | ✓              | ✓  | 54.94           | 71.50                                 | 92.33                                 | 72.92                                 |
| ⑤                | ✓              | ✓  | 50.48           | 75.06                                 | 86.53                                 | 73.66                                 |
| ⑥                | ✓              | ✓  | 50.59           | 71.36                                 | 77.25                                 | 72.01                                 |
| ⑦ $ ^{\dagger} $ | ✓              | ✓  | 51.32           | 76.16                                 | 75.48                                 | 75.88                                 |"
481,"| Exp.               | $ \mathcal{L}_{1} $ | $ \mathcal{L}_{per} $ | $ \mathcal{L}_{adv} $ | LFW-Test           | WebPhoto-Test      | WIDER-Test |
| ------------------ | ------------------- | --------------------- | --------------------- | ------------------ | ------------------ | ---------- |
| FID $ \downarrow $ | MUSIQ $ \uparrow $  | FID $ \downarrow $    | MUSIQ $ \uparrow $    | FID $ \downarrow $ | MUSIQ $ \uparrow $ |            |
| (a)                | ✓                   |                       |                       | 87.12              | 43.14              | 141.86     |
| (b)                | ✓                   | ✓                     |                       | 57.57              | 67.99              | 95.02      |
| (c) Ours           | ✓                   | ✓                     | ✓                     | 51.32              | 76.16              | 75.48      |"
481,"| Exp.               | Loss               | LFW-Test           | WebPhoto-Test      | WiDER-Test         |
| ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |
| FID $ \downarrow $ | MUSIQ $ \uparrow $ | FID $ \downarrow $ | MUSIQ $ \uparrow $ | FID $ \downarrow $ |
| Naive ControlNet   | Eq. (1)            | 35.43              | 75.03              | 81.91              |
| Spatial Encoder    | Eq. (2)            | 55.07              | 75.75              | 77.76              |"
482,"| Method      | Acc $ \downarrow $ | Comp  $ \downarrow $ | C-L1  $ \downarrow $ | NC  $ \uparrow $ | F-Score  $ \uparrow $ | Abs Rel  $ \downarrow $ | PSNR  $ \uparrow $ | SSIM  $ \uparrow $ | LPIPS  $ \downarrow $ | Time  $ \downarrow $ |
| ----------- | ------------------ | -------------------- | -------------------- | ---------------- | --------------------- | ----------------------- | ------------------ | ------------------ | --------------------- | -------------------- |
| GOF         | 0.1812             | 0.1093               | 0.1453               | 0.6292           | 0.3665                | 0.2380                  | 21.37              | 0.7762             | 0.3132                | $ \approx $ 1.4h     |
| PGSR        | 0.0971             | 0.1420               | 0.1196               | 0.7193           | 0.5105                | 0.1723                  | 22.13              | 0.7773             | 0.2918                | $ \approx $ 1.2h     |
| 3DGS        | 0.1167             | 0.1033               | 0.1100               | 0.7954           | 0.3739                | 0.1214                  | 24.18              | 0.8392             | 0.2511                | $ \approx $ 0.8h     |
| 3DGS + FDS* | 0.0569             | 0.0676               | 0.0623               | 0.8105           | 0.6573                | 0.0603                  | 24.72              | 0.8489             | 0.2379                | $ \approx $ 1.3h     |
| 3DGS + FDS  | 0.0527             | 0.0565               | 0.0546               | 0.8178           | 0.6958                | 0.0568                  | 24.76              | 0.8486             | 0.2381                | $ \approx $ 1.3h     |
| 2DGS        | 0.1078             | 0.0850               | 0.0964               | 0.7835           | 0.5170                | 0.1002                  | 23.56              | 0.8166             | 0.2730                | $ \approx $ 0.8h     |
| 2DGS + FDS* | 0.0689             | 0.0646               | 0.0667               | 0.8042           | 0.6582                | 0.0589                  | 23.98              | 0.8255             | 0.2621                | $ \approx $ 1.3h     |
| 2DGS + FDS  | 0.0615             | 0.0534               | 0.0574               | 0.8151           | 0.6974                | 0.0561                  | 24.06              | 0.8271             | 0.2610                | $ \approx $ 1.3h     |"
482,"| Method                | Acc $ \downarrow $ | Comp  $ \downarrow $ | C-L1  $ \downarrow $ | NC  $ \uparrow $ | F-Score  $ \uparrow $ | Abs Rel  $ \downarrow $ | PSNR  $ \uparrow $ | SSIM  $ \uparrow $ | LPIPS  $ \downarrow $ |
| --------------------- | ------------------ | -------------------- | -------------------- | ---------------- | --------------------- | ----------------------- | ------------------ | ------------------ | --------------------- |
| 2DGS                  | 0.1078             | 0.0850               | 0.0964               | 0.7835           | 0.5170                | 0.1002                  | 23.56              | 0.8166             | 0.2730                |
| 2DGS+Depth            | 0.0862             | 0.0702               | 0.0782               | 0.8153           | 0.5965                | 0.0672                  | 23.92              | 0.8227             | 0.2619                |
| 2DGS+MVDepth          | 0.2065             | 0.0917               | 0.1491               | 0.7832           | 0.3178                | 0.0792                  | 23.74              | 0.8193             | 0.2692                |
| 2DGS+Normal           | 0.0939             | 0.0637               | 0.0788               | 0.8359           | 0.5782                | 0.0768                  | 23.78              | 0.8197             | 0.2676                |
| 2DGS+FDS              | 0.0615             | 0.0534               | 0.0574               | 0.8151           | 0.6974                | 0.0561                  | 24.06              | 0.8271             | 0.2610                |
| 2DGS+Depth+FDS        | 0.0561             | 0.0519               | 0.0540               | 0.8295           | 0.7282                | 0.0454                  | 24.22              | 0.8291             | 0.2570                |
| 2DGS+Normal+FDS       | 0.0529             | 0.0450               | 0.0490               | 0.8477           | 0.7430                | 0.0443                  | 24.10              | 0.8283             | 0.2590                |
| 2DGS+Depth+Normal     | 0.0695             | 0.0513               | 0.0604               | 0.8540           | 0.6723                | 0.0523                  | 24.09              | 0.8264             | 0.2575                |
| 2DGS+Depth+Normal+FDS | 0.0506             | 0.0423               | 0.0464               | 0.8598           | 0.7613                | 0.0403                  | 24.22              | 0.8300             | 0.0403                |"
483,"| Models               | Agent Parameters | Population Size | Total Parameters |
| -------------------- | ---------------- | --------------- | ---------------- |
| BodyGen (Ours)       | 1.43 M           | 1               | 1.43 M           |
| Transform2Act        | 19.64 M          | 1               | 19.64 M          |
| UMC-Message $ ^{*} $ | 0.27 M           | 1               | 0.27 M           |
| NGE                  | 0.27 M           | 20              | 5.4 M            |"
483,"| Methods         | Crawler           | Terrain Crosser   | Cheetah            | Swimmer           | Glider-Regular    |
| --------------- | ----------------- | ----------------- | ------------------ | ----------------- | ----------------- |
| TopoPE (ours)   | 10381.96 ± 353.97 | 5056.01 ± 703.57  | 11611.52 ± 522.86  | 1305.17 ± 15.25   | 11082.29 ± 99.21  |
| w/ Traversal PE | 8582.24 ± 987.44  | 4339.60 ± 260.60  | 10581.62 ± 846.69  | 1292.05 ± 16.71   | 9801.31 ± 748.13  |
| w/o TopoPE      | 7490.83 ± 267.70  | 1122.29 ± 659.38  | 7451.37 ± 2275.37  | 1371.20 ± 30.74   | 10137.83 ± 713.60 |
| Methods         | Glider-Medium     | Glider-Hard       | Walker-Regular     | Walker-Medium     | Walker-Hard       |
| TopoPE (ours)   | 11996.82 ± 595.51 | 10798.06 ± 298.39 | 12062.49 ± 513.07  | 12962.08 ± 537.34 | 11982.07 ± 520.78 |
| w/ Traversal PE | 10758.70 ± 401.90 | 9106.77 ± 679.59  | 10389.40 ± 1080.94 | 10972.13 ± 584.04 | 11255.89 ± 121.04 |
| w/o TopoPE      | 4099.99 ± 2057.92 | 109.48 ± 10.03    | 10149.67 ± 255.99  | 6730.01 ± 705.06  | 6529.87 ± 1863.59 |"
484,"| Approach                                | Decoding | N  | K  | S | Pass rate |
| --------------------------------------- | -------- | -- | -- | - | --------- |
| InternLM2-plus-7B [51] (Few-shot)       | Sampling | 50 | 64 | 1 | 42.2%     |
| SFT (InternLM2-plus-7B) [51] (Few-shot) | Sampling | 50 | 64 | 1 | 43.4%     |
| Lean-CoT (InternLM2-plus-7B)            | Sampling | 50 | 64 | 1 | 45.5%     |
| Lean-STAR (Iter-1) (InternLM2-plus-7B)  | Sampling | 50 | 64 | 1 | 46.3%     |"
485,"|      | FB15k237 [2] | NELL995 [2] |     |
| ---- | ------------ | ----------- | --- |
| 1p   | 2p           | 3p          | 2i  |
| 1p   | 100          | -           | -   |
| 2p   | 98.1         | 1.9         | -   |
| 3p   | 97.2         | 2.7         | 0.1 |
| 2i   | 96.0         | -           | -   |
| 3i   | 91.6         | -           | -   |
| 1p2i | 86.8         | 1.0         | -   |
| 2i1p | 96.7         | 1.8         | -   |
| 2u   | 0.0          | -           | -   |
| 2u1p | 98.3         | 0.0         | -   |"
485,"| method     | 3p   | 2i1p |
| ---------- | ---- | ---- |
| ovr        | 1p   | 2p   |
| GNN-QE [3] | 11.8 | 12.0 |
| ULTRAQ [4] | 8.9  | 9.0  |
| ConE [5]   | 11.0 | 11.0 |
| CQD [6]    | 7.8  | 7.8  |
| QTO [7]    | 15.6 | 15.8 |
| CLMPT [8]  | 11.3 | 11.3 |"
485,"|            | Model  | 1p   | 2p  | 3p   | 2i   | 3i   | 1p2i | 2i1p | 2u   | 2u1p | 4p   | 4i   | 2in | 3in | 2pi1pn | 2nu1p | 2in1p |
| ---------- | ------ | ---- | --- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | --- | --- | ------ | ----- | ----- |
| FB15k237+H | GNN-QE | 42.8 | 5.2 | 4.0  | 6.0  | 8.8  | 5.6  | 9.9  | 32.5 | 10.0 | 4.3  | 20.0 | 6.8 | 6.5 | 3.7    | 5.0   | 3.3   |
| ULTRAQ     | 40.6   | 4.5  | 3.5 | 5.2  | 7.2  | 5.3  | 10.1 | 29.4 | 8.3  | 3.8  | 16.4 | 5.3  | 5.5 | 2.6 | 3.7    | 2.2   |       |
| CQD        | 46.7   | 4.4  | 2.4 | 11.3 | 12.8 | 6.0  | 11.5 | 40.1 | 10.6 | 1.1  | 23.8 | 3.3  | 2.6 | 0.6 | 4.9    | 1.2   |       |
| CQD-Hybrid | 46.7   | 4.8  | 3.1 | 6.0  | 8.6  | 5.5  | 12.9 | 42.2 | 12.0 | 2.4  | 17.4 | 4.7  | 1.6 | 1.0 | 3.2    | 1.3   |       |
| ConE       | 41.8   | 4.6  | 3.9 | 9.1  | 10.3 | 3.8  | 7.9  | 22.8 | 6.0  | 3.5  | 20.3 | 5.1  | 4.9 | 2.9 | 3.3    | 3.6   |       |
| QTO        | 46.7   | 4.9  | 3.7 | 8.7  | 10.1 | 6.1  | 13.5 | 30.6 | 11.2 | 3.9  | 20.2 | 10.6 | 3.1 | 2.0 | 5.3    | 1.5   |       |
| CLMPT      | 45.3   | 5.3  | 4.7 | 10.2 | 12.2 | 5.6  | 14.9 | 33.6 | 14.2 | 4.5  | 24.0 | 6.8  | 2.3 | 1.6 | 4.8    | 2.5   |       |
| NELL995+H  | GNN-QE | 53.6 | 8.0 | 6.0  | 10.7 | 13.3 | 16.0 | 13.5 | 47.5 | 9.8  | 4.7  | 19.4 | 5.5 | 6.4 | 5.8    | 3.3   | 4.4   |
| ULTRAQ     | 38.9   | 6.1  | 4.1 | 7.9  | 10.2 | 15.8 | 9.3  | 28.1 | 9.5  | 4.2  | 15.6 | 4.5  | 5.9 | 4.3 | 2.7    | 3.6   |       |
| CQD        | 60.4   | 9.6  | 4.2 | 18.5 | 19.6 | 18.9 | 22.6 | 46.3 | 18.5 | 2.0  | 24.8 | 4.2  | 1.5 | 1.5 | 4.9    | 2.6   |       |
| CQD-Hybrid | 60.4   | 9.0  | 6.1 | 12.1 | 14.4 | 17.4 | 21.2 | 46.4 | 19.3 | 3.5  | 20.4 | 5.1  | 1.2 | 1.4 | 4.3    | 2.4   |       |
| ConE       | 53.1   | 7.9  | 6.7 | 21.8 | 23.6 | 14.9 | 11.8 | 39.9 | 8.8  | 5.2  | 27.6 | 4.6  | 6.0 | 3.7 | 2.7    | 6.4   |       |
| QTO        | 60.3   | 9.8  | 8.0 | 14.6 | 15.8 | 17.6 | 21.1 | 49.1 | 18.9 | 7.0  | 20.9 | 10.2 | 2.3 | 3.1 | 8.4    | 2.4   |       |
| CLMPT      | 58.1   | 10.1 | 7.8 | 22.7 | 25.0 | 17.2 | 24.4 | 50.0 | 22.0 | 7.2  | 29.1 | 6.5  | 2.4 | 4.1 | 2.3    | 4.5   |       |"
486,"| Problem type of adaptive sampling                               | Upper bounds                                                      | Our lower bounds                                                  |
| --------------------------------------------------------------- | ----------------------------------------------------------------- | ----------------------------------------------------------------- |
| Unconstrained dist. with  $ \varepsilon=\Theta(c^{d}) $         | strongly log-concave + log-smooth                                 | $ \widetilde{\mathcal{O}}(d) $  (Anonymous, 2024)                 |
| log-concave + log-smooth                                        | $ \widetilde{\mathcal{O}}(m_{2}d^{5/2}) $  (Fan et al., 2023) ♣   | $ \widetilde{\Omega}(d) $                                         |
| log-concave + log-Lipschitz                                     | $ \widetilde{\mathcal{O}}(m_{2}d^{2}) $  (Fan et al., 2023) ♣     | $ \widetilde{\Omega}(d) $                                         |
| composite ♠                                                     | $ \widetilde{\mathcal{O}}(d^{5/2}) $  (Fan et al., 2023)          | $ \widetilde{\Omega}(d) $                                         |
| box-constrained dist. with  $ \varepsilon=\Theta(d^{-\alpha}) $ | log-concave + log-smooth                                          | $ \widetilde{\mathcal{O}}(d^{3}) $  (Mangoubi and Vishnoi, 2023a) |
| log-concave + log-Lipschitz                                     | $ \widetilde{\mathcal{O}}(d^{3}) $  (Mangoubi and Vishnoi, 2023a) | $ \widetilde{\Omega}(d) $                                         |
| strongly log-concave + log-smooth                               | $ \widetilde{\mathcal{O}}(d) $  (Lee et al., 2021)                | -                                                                 |
| box-constrained dist. with  $ \varepsilon=\Theta(c^{d}) $       | log-concave + log-smooth                                          | $ \widetilde{\mathcal{O}}(d^{3}) $  (Mangoubi and Vishnoi, 2023a) |
| log-concave + log-Lipschitz                                     | $ \widetilde{\mathcal{O}}(d^{3}) $  (Mangoubi and Vishnoi, 2023a) | $ \widetilde{\Omega}(d) $                                         |
| strongly log-concave + log-smooth                               | $ \widetilde{\mathcal{O}}(d^{4}) $  (Lee et al., 2021)            | -                                                                 |"
487,"| Model                   | TLIO Dataset | Aria Dataset |
| ----------------------- | ------------ | ------------ |
| MSE* $ (10^{-2}m^{2}) $ | ATE(m)       | ATE* $ (m) $ |
| TLIO                    | 3.333        | 1.722        |
| + rot. aug.             | 3.242        | 1.812        |
| + SO(2) Eq. Frame       | 3.194        | 1.480        |
| + O(2) Eq. Frame        | 2.982        | 1.433        |"
487,"|                                | RONIN-U  | RIDI-T   | OxIOD    |
| ------------------------------ | -------- | -------- | -------- |
| Model (RONIN)                  | ATE* (m) | RTE* (m) | ATE* (m) |
| + 100% data                    | 5.14     | 4.37     | 1.63     |
| + 50% data †                   | 5.57     | 4.38     | 1.19     |
| + 50% data + J †               | 5.02     | 4.23     | 1.13     |
| + 50% data + TTT †             | 5.05     | 4.14     | 1.04     |
| + 50% data + J +TTT †          | 5.07     | 4.17     | 1.03     |
| + 50% data + SO(2) Eq. Frame   | 5.18     | 4.35     | 0.86     |
| + 50% data + O(2) Eq. Frame    | 4.42     | 3.95     | 0.82     |
| Naive Double Integration (NDI) | 458.06   | 117.06   | 31.06    |"
488,"| Method                              | Condition         | # Params.       | Dataset Size         | GPU Hours            |
| ----------------------------------- | ----------------- | --------------- | -------------------- | -------------------- |
| ControlNet                          | Canny             | 361M            | 3M (Internet)        | $ \sim $ 600 (A100)  |
| Depth                               | 361M              | 3M (Internet)   | $ \sim $ 500 (A100)  |                      |
| Skeleton                            | 361M              | 200K (OpenPose) | $ \sim $ 300 (A100)  |                      |
| ...                                 | ...               | ...             | ...                  |                      |
| CtrLoRA (Base)                      | 9 base conditions | 360M            | 20M (MultiGen)       | $ \sim $ 6000 (4090) |
| CtrLoRA (LoRA) for novel conditions | Lineart           | 37M             | 1K (Custom)          | $ \sim $ 0.17 (4090) |
| Palette                             | 37M               | 1K (Custom)     | $ \sim $ 0.83 (4090) |                      |
| De-raindrop                         | 37M               | 863 (Raindrop)  | $ \sim $ 0.83 (4090) |                      |
| ...                                 | ...               | ...             | ...                  |                      |"
489,"| Method            | Mip-NeRF 360      | Tanks &amp; Temples  | Deep Blending     |
| ----------------- | ----------------- | -------------------- | ----------------- |
| PSNR $ \uparrow $ | SSIM $ \uparrow $ | LPIPS $ \downarrow $ | PSNR $ \uparrow $ |
| Plenoxels         | 23.08             | 0.626                | 0.463             |
| INGP-Base         | 25.30             | 0.671                | 0.371             |
| INGP-Big          | 25.59             | 0.699                | 0.331             |
| M-NeRF360         | 27.69             | 0.792                | 0.237             |
| 3DGS              | 27.21             | 0.815                | 0.214             |
| LC-WSR            | 27.19             | 0.804                | 0.211             |"
489,"| Method        | Task           | Mip-NeRF360 | Tanks&amp;Temples | Deep Blending |
| ------------- | -------------- | ----------- | ----------------- | ------------- |
| bicycle       | flowers        | garden      | stump             | treehill      |
| 3DGS-Compute  | Pre-processing | 12.84       | 7.05              | 10.15         |
| Lighting      | 4.57           | 3.26        | 5.19              | 3.43          |
| Sorting       | 246.65         | 122.68      | 210.35            | 180.23        |
| Rasterization | 1239.12*       | 115.18      | 608.19*           | 265.27        |
| Total         | 1511.96        | 253.60      | 842.15            | 463.06        |
| 3DGS-Graphics | Pre-processing | 13.27       | 7.82              | 13.73         |
| Lighting      | 5.32           | 3.66        | 5.67              | 3.68          |
| Sorting       | 40.26          | 24.79       | 38.25             | 31.09         |
| Rasterization | 618.83*        | 246.47*     | 697.81*           | 344.72*       |
| Total         | 678.08         | 283.11      | 755.89            | 390.33        |
| LC-WSR        | Pre-processing | 6.83        | 3.92              | 5.53          |
| Lighting      | 4.07           | 2.81        | 4.36              | 3.67          |
| Rasterization | 67.84          | 28.30       | 57.84             | 57.25         |
| Total         | 78.99          | 35.28       | 67.90             | 67.63         |"
489,"| Method                      | Mip-NeRF 360      | Tanks &amp; Temples  | Deep Blending     |
| --------------------------- | ----------------- | -------------------- | ----------------- |
| PSNR $ \uparrow $           | SSIM $ \uparrow $ | LPIPS $ \downarrow $ | PSNR $ \uparrow $ |
| Ours                        | 27.19             | 0.804                | 0.211             |
| w.o. learnable parameters   | 23.19             | 0.711                | 0.318             |
| w.o. view-dependent opacity | 25.88             | 0.784                | 0.251             |"
493,"|          | Loss $ \downarrow $ | PPL $ \downarrow $ | ARC-E | ARC-C | HellaSwag | PIQA  | SciQ  | Winograde | Avg. $ \uparrow $ |
| -------- | ------------------- | ------------------ | ----- | ----- | --------- | ----- | ----- | --------- | ----------------- |
| SwiGLU   | 2.19                | 3.22               | 56.61 | 27.47 | 49.23     | 68.61 | 86.10 | 56.83     | 57.47             |
| GELU     | 2.20                | 3.24               | 55.43 | 27.73 | 48.42     | 68.12 | 87.40 | 54.78     | 56.98             |
| ReLU     | 2.21                | 3.26               | 55.68 | 28.50 | 48.59     | 68.39 | 87.10 | 54.85     | 57.18             |
| PolyReLU | 2.17                | 3.18               | 57.53 | 27.99 | 50.19     | 70.29 | 87.60 | 55.72     | 58.22             |
| PolyNorm | 2.17                | 3.17               | 59.68 | 29.01 | 50.86     | 69.15 | 87.20 | 56.20     | 58.68             |"
493,"| Methods  | C4       | Books      | CC    | peS2o | Reddit | Stack | Wiki-pedia  | ICE   | M2D2  | Pile  | Wiki-text | Avg.↓ |
| -------- | -------- | ---------- | ----- | ----- | ------ | ----- | ----------- | ----- | ----- | ----- | --------- | ----- |
| SwiGLU   | 2.72     | 2.59       | 2.79  | 2.16  | 2.93   | 1.01  | 2.30        | 2.50  | 3.07  | 2.07  | 2.37      | 2.41  |
| PolyNorm | 2.71     | 2.57       | 2.78  | 2.15  | 2.92   | 1.00  | 2.29        | 2.49  | 3.06  | 2.03  | 2.34      | 2.39  |
|          |          |            |       |       |        |       |             |       |       |       |           |       |
| Tasks    | MMLU Var | Hella-Swag | SciQ  | ARC-C | ARC-E  | PIQA  | Wino-Grande | OQA   | COPA  | Avg.↑ |           |       |
| SwiGLU   | 37.07    | 66.49      | 90.60 | 37.12 | 71.58  | 76.61 | 62.75       | 39.80 | 83.00 | 62.78 |           |       |
| PolyNorm | 37.27    | 67.63      | 92.40 | 38.46 | 70.70  | 77.04 | 62.19       | 40.60 | 84.00 | 63.37 |           |       |"
495,"| Type                      | Concept               | Memory     | Models (Success Ratio) |
| ------------------------- | --------------------- | ---------- | ---------------------- |
| LLaVa-OneVision           | QWen2-VL              | mPLUG-Owl3 | Gemini-1.5-Flash       |
| Association Success Ratio | Attribute (Adjective) | NoM        | 75.52                  |
| StructM                   | 78.41                 | 77.74      | 72.67                  |
| NLM                       | 80.40                 | 86.63      | 73.33                  |
| ChainM                    | 77.73                 | 73.55      | 70.70                  |
| Affordance (Adjective)    | NoM                   | 70.61      | 75.10                  |
| StructM                   | 73.39                 | 75.39      | 73.02                  |
| NLM                       | 76.13                 | 79.37      | 67.79                  |
| ChainM                    | 74.38                 | 82.67      | 68.46                  |
| Action (Verb)             | NoM                   | 75.74      | 78.43                  |
| StructM                   | 75.44                 | 82.10      | 73.92                  |
| NLM                       | 78.66                 | 88.01      | 70.04                  |
| ChainM                    | 76.92                 | 85.59      | 69.50                  |
| Deduction Success Ratio   | Attribute (Adjective) | NoM        | 49.38                  |
| Affordance (Adjective)    | NoM                   | 21.07      | 15.98                  |
| Action (Verb)             | NoM                   | 46.61      | 49.99                  |"
496,"| Model                  | IEFval   | FollowBench (SSR) | C-Eval MMLU GSM8K HumanEval |          |
| ---------------------- | -------- | ----------------- | --------------------------- | -------- |
| Pr (5)                 | Pt (L)   | Ins (5)           | Ins (L)                     | Level 1  |
| Roadsn (c 10B)         | 37.7     | 43.6              | 49.4                        | 53.4     |
| Qwen-7B                | 30.9     | 33.5              | 42.4                        | 45.2     |
| Qwen2-7B (ShareGPT)    | 30.9     | 33.5              | 42.4                        | 45.2     |
| LLaMA3-8B (S-shareGPT) | 23.7     | 26.4              | 33.8                        | 37.1     |
| Mistral-7B             | 23.3     | 24.6              | 38.4                        | 39.6     |
| Rustelues (J -10B)     | 77.1     | 80.4              | 84.4                        | 86.9     |
| LLaMA3-7B (Instruct    | 77.8     | 83.8              | 84.2                        | 88.8     |
| Mistral-82B            | 41.8     | 47.3              | 55.2                        | 60.0     |
| GPT-4                  | 76.9     | 79.3              | 83.6                        | 85.4     |
| GPT-3 Turbo¹           | -        | -                 | -                           | -        |
| Swin-to-�              | -        | -                 | -                           | -        |
| Qwen2-7B-SFT           | 40.7±0.0 | 44.5±0.0          | 51.3±0.9                    | 55.4±2.0 |
| w/ Offline DPO         | 41.2±1.5 | 44.7±1.2          | 51.4±2.0                    | 56.2±3.4 |
| w/ Offline DPO         | 40.4±0.0 | 46.6±0.0          | 55.0±0.9                    | 57.9±0.9 |
| Qwen2-72B-Instruct     | -        | -                 | -                           | -        |
| Qwen2-72B-Instruct     | 80.2±1.5 | 82.3±1.9          | 86.1±1.7                    | 88.0±1.1 |
| w/ Online DPO          | 28.7±1.0 | 40.3±1.1          | 41.4±1.3                    | 52.2±1.0 |
| LLaMA3-8B-SFT          | 28.7±1.0 | 40.3±1.1          | 41.4±1.3                    | 52.2±1.0 |
| w/ Offline DPO         | 27.9±1.0 | 41.6±1.1          | 40.5±1.4                    | 54.1±1.1 |
| w/ Online DPO          | 28.4±2.0 | 43.1±1.7          | 42.2±1.0                    | 56.0±0.9 |
| LLaMA3-7B              | 80.2±2.0 | 85.6±1.8          | 86.7±2.5                    | 90.4±1.6 |
| w/ Online DPO          | 80.2±2.0 | 85.6±1.8          | 86.7±2.5                    | 90.4±1.6 |"
496,"| Model                        | IFEval         | FollowBench (SSR) |
| ---------------------------- | -------------- | ----------------- |
| Prompt(L)                    | Instruction(L) | Avg               |
| Qwen2-7B                     | 43.6           | 53.4              |
| Supervision Model: Qwen2-72B |                |                   |
| +SFT                         | 44.5 +0.9      | 55.4 +2.0         |
| +SFT &amp; Offline DPO       | 44.7 +1.1      | 56.2 +2.8         |
| +SFT &amp; Online DPO        | 46.6 +3.0      | 57.9 +4.5         |
| Supervision Model: GPT-4     |                |                   |
| +SFT                         | 52.9 +9.3      | 62.6 +9.2         |
| +SFT &amp; Offline DPO       | 59.3 +15.7     | 68.9 +15.5        |
| +SFT &amp; Online DPO        | 59.5 +15.9     | 69.4 +16.0        |"
496,"| Model                        | IFEval         | FollowBench (SSR) |
| ---------------------------- | -------------- | ----------------- |
| Prompt(L)                    | Instruction(L) | Avg               |
| Supervision Model: Qwen2-72B |                |                   |
| Qwen2-7B-SFT w/ Online DPO   | 46.6           | 57.9              |
| w/o Back-translation         | -0.8           | -1.7              |
| w/o Quality Verification     | -1.4           | -2.4              |
| w/o Cross Verification       | -1.6           | -3.0              |
| w/o All Quality Process      | -2.2           | -3.8              |"
498,"| Method                       | All race (n=2000) | Asian (n=169) | Black (n=299) | White (n=1532) |
| ---------------------------- | ----------------- | ------------- | ------------- | -------------- |
| Equity-scaled (ES) Dice      | Dice              | Dice          | Dice          | Dice           |
| TransUNet                    | 0.703             | 0.793         | 0.746         | 0.731          |
| + ADV (Mardras et al., 2018) | 0.700             | 0.791         | 0.741         | 0.729          |
| + DRO (Sagawa et al., 2019)  | 0.700             | 0.790         | 0.747         | 0.723          |
| + FEBS (Tian et al., 2024)   | 0.705             | 0.795         | 0.748         | 0.733          |
| + FairDiff (Li et al., 2024) | 0.716             | 0.800         | 0.757         | 0.743          |
| + MoE                        | 0.733             | 0.804         | 0.760         | 0.763          |
| + dMoE                       | 0.743             | 0.813         | 0.769         | 0.776          |"
498,"| Method                                                        | GFlops↓ | All (n=275) | T1 (n=11) | T2 (n=129) | T3 (n=114) | T4 (n=21) |
| ------------------------------------------------------------- | ------- | ----------- | --------- | ---------- | ---------- | --------- |
|                                                               | Dice    | Dice        | Dice      | Dice       | Dice       |           |
| ResUNet                                                       | 1542    | 0.610       | 0.493     | 0.569      | 0.659      | 0.656     |
| + MoE                                                         | 1761    | 0.608       | 0.492     | 0.542      | 0.674      | 0.708     |
| + dMoE                                                        | 1761    | 0.650       | 0.718     | 0.585      | 0.693      | 0.778     |
| Ablation study: Multiple networks training for each attribute |         |             |           |            |            |           |
| 4 × ResUNet                                                   | 5729    | 0.606       | 0.599     | 0.515      | 0.681      | 0.760     |"
499,"| Model      | Base  | Base  | EntiGraph | EntiGraph |
| ---------- | ----- | ----- | --------- | --------- |
| Open-book? | No    | Yes   | No        | Yes       |
| Accuracy   | 39.49 | 60.35 | 56.22     | 62.60     |"
518,"|             | IEMO. [L→A] | IEMO. [L→A] | AVMN. [V→A] | AVMN. [A→V] |
| ----------- | ----------- | ----------- | ----------- | ----------- |
| W2(Pi, ˆPk) | 0.494       | 0.965       | 0.025       | 0.908       |
| W2(Pj, ˆPk) | 0.141       | 0.460       | 0.754       | 0.502       |
| W2(Pj, Pj)  | 0.977       | 1.005       | 0.790       | 0.954       |"
501,"| Selection Method           | Val Perplexity             | Test Perplexity            | Reading Comprehension (5 tasks) | Commonsense Reasoning (3 tasks) | World Knowledge (2 tasks) | Average (10 tasks)       |
| -------------------------- | -------------------------- | -------------------------- | ------------------------------- | ------------------------------- | ------------------------- | ------------------------ |
| Uniform                    | 10.7                       | 10.75                      | 50.9                            | 55                              | 14.9                      | 44.9                     |
| DSIR with Wiki with Book   | 13.34  $ \uparrow $ 2.64   | 13.37  $ \uparrow $ 2.62   | 50.1  $ \downarrow $ 0.8        | 49.8  $ \downarrow $ 5.2        | 14.7  $ \downarrow $ 0.2  | 42.9  $ \downarrow $ 2.0 |
| 13.60  $ \uparrow $ 2.90   | 13.59  $ \uparrow $ 2.84   | 47.9  $ \downarrow $ 3.0   | 56.6  $ \uparrow $ 1.6          | 14.1  $ \downarrow $ 0.8        | 43.8  $ \downarrow $ 1.1  |                          |
| Perplexity                 | 15.98  $ \uparrow $ 5.28   | 16.04  $ \uparrow $ 5.29   | 48.3  $ \downarrow $ 2.6        | 49.6  $ \downarrow $ 5.4        | 13.7  $ \downarrow $ 1.2  | 41.7  $ \downarrow $ 3.2 |
| 11.32  $ \uparrow $ 0.62   | 11.34  $ \uparrow $ 0.59   | 49.6  $ \downarrow $ 1.3   | 53.5  $ \downarrow $ 1.5        | 13.4  $ \downarrow $ 1.5        | 43.5  $ \downarrow $ 1.4  |                          |
| Writing Style              | top-k                      | 13.01  $ \uparrow $ 2.31   | 12.97  $ \uparrow $ 2.22        | 49.3  $ \downarrow $ 1.6        | 53.3  $ \downarrow $ 1.7  | 13.5  $ \downarrow $ 1.4 |
| $ \tau $  = 2.0            | 10.60  $ \downarrow $ 0.10 | 10.64  $ \downarrow $ 0.11 | 51.0  $ \uparrow $ 0.1          | 55.8  $ \uparrow $ 0.8          | 14.1  $ \downarrow $ 0.8  |                          |
| Facts &amp; Trivia         | top-k                      | 14.38  $ \uparrow $ 3.68   | 14.33  $ \uparrow $ 3.58        | 54.3  $ \uparrow $ 3.4          | 51.7  $ \downarrow $ 3.3  | 15.5  $ \uparrow $ 0.6   |
| $ \tau $  = 2.0            | 10.68  $ \downarrow $ 0.02 | 10.72  $ \downarrow $ 0.03 | 52.7  $ \uparrow $ 1.8          | 55.6  $ \uparrow $ 0.6          | 15.6  $ \uparrow $ 0.7    |                          |
| Educational Value          | top-k                      | 13.54  $ \uparrow $ 2.84   | 13.49  $ \uparrow $ 2.74        | 54.7  $ \uparrow $ 3.8          | 54.9  $ \downarrow $ 0.1  | 14.4  $ \downarrow $ 0.5 |
| $ \tau $  = 2.0            | 10.67  $ \downarrow $ 0.03 | 10.72  $ \downarrow $ 0.03 | 53.3  $ \uparrow $ 2.4          | 56.3  $ \uparrow $ 1.3          | 15.7  $ \uparrow $ 0.8    |                          |
| Required Expertise         | top-k                      | 14.97  $ \uparrow $ 4.27   | 14.92  $ \uparrow $ 4.17        | 52.8  $ \uparrow $ 1.9          | 48.7  $ \downarrow $ 6.3  | 14.3  $ \downarrow $ 0.6 |
| $ \tau $  = 2.0            | 10.7                       | 10.74  $ \downarrow $ 0.01 | 52.7  $ \uparrow $ 1.8          | 55.5  $ \uparrow $ 0.5          | 15.0  $ \uparrow $ 0.1    |                          |
| Criteria mix               | $ \tau $  = 2.0            | 10.63  $ \downarrow $ 0.07 | 10.68  $ \downarrow $ 0.07      | 52.1  $ \uparrow $ 1.2          | 55.5  $ \uparrow $ 0.5    | 15.2  $ \uparrow $ 0.3   |
| Accuracy                   | top-k                      | 10.82  $ \uparrow $ 0.12   | 10.80  $ \uparrow $ 0.05        | 53.8  $ \uparrow $ 2.9          | 58.2  $ \uparrow $ 3.2    | 16.6  $ \uparrow $ 1.7   |
| Coherence                  | top-k                      | 10.72  $ \uparrow $ 0.02   | 10.71  $ \downarrow $ 0.04      | 54.9  $ \uparrow $ 4.0          | 58.8  $ \uparrow $ 3.8    | 16.1  $ \uparrow $ 1.2   |
| Creativity                 | top-k                      | 11.08  $ \uparrow $ 0.38   | 11.00  $ \uparrow $ 0.25        | 53.0  $ \uparrow $ 2.1          | 60.6  $ \uparrow $ 5.6    | 15.2  $ \uparrow $ 0.3   |
| Grammatical Diversity      | top-k                      | 10.87  $ \uparrow $ 0.17   | 10.86  $ \uparrow $ 0.11        | 55.1  $ \uparrow $ 4.2          | 58.8  $ \uparrow $ 3.8    | 16.5  $ \uparrow $ 1.6   |
| Knowledge Novelty          | top-k                      | 11.01  $ \uparrow $ 0.31   | 11.01  $ \uparrow $ 0.26        | 54.6  $ \uparrow $ 3.7          | 56.9  $ \uparrow $ 1.9    | 15.5  $ \uparrow $ 0.6   |
| Language Consistency       | top-k                      | 10.35  $ \downarrow $ 0.35 | 10.35  $ \downarrow $ 0.40      | 54.1  $ \uparrow $ 3.2          | 59.3  $ \uparrow $ 4.3    | 16.7  $ \uparrow $ 1.8   |
| Originality                | top-k                      | 10.68  $ \downarrow $ 0.02 | 10.67  $ \downarrow $ 0.08      | 53.9  $ \uparrow $ 3.0          | 58.6  $ \uparrow $ 3.6    | 16.4  $ \uparrow $ 1.5   |
| Professionalism            | top-k                      | 11.27  $ \uparrow $ 0.57   | 11.26  $ \uparrow $ 0.51        | 54.6  $ \uparrow $ 3.7          | 54.8  $ \downarrow $ 0.2  | 15.9  $ \uparrow $ 1.0   |
| Semantic Density           | top-k                      | 11.10  $ \uparrow $ 0.40   | 11.09  $ \uparrow $ 0.34        | 54.4  $ \uparrow $ 3.5          | 58.1  $ \uparrow $ 3.1    | 16.7  $ \uparrow $ 1.8   |
| Sensitivity                | top-k                      | 10.11  $ \downarrow $ 0.59 | 10.13  $ \downarrow $ 0.62      | 54.7  $ \uparrow $ 3.8          | 59.2  $ \uparrow $ 4.2    | 16.1  $ \uparrow $ 1.2   |
| Structural Standardization | top-k                      | 12.11  $ \uparrow $ 1.41   | 12.11  $ \uparrow $ 1.36        | 53.7  $ \uparrow $ 2.8          | 57.0  $ \uparrow $ 2.0    | 17.1  $ \uparrow $ 2.2   |
| Style Consistency          | top-k                      | 10.74  $ \uparrow $ 0.04   | 10.73  $ \downarrow $ 0.02      | 55.1  $ \uparrow $ 4.2          | 59.6  $ \uparrow $ 4.6    | 16.2  $ \uparrow $ 1.3   |
| Topic Focus                | top-k                      | 10.41  $ \downarrow $ 0.29 | 10.41  $ \downarrow $ 0.34      | 54.6  $ \uparrow $ 3.7          | 58.4  $ \uparrow $ 3.4    | 15.6  $ \uparrow $ 0.7   |
| Overall Score              | l=1                        | 23.83  $ \uparrow $ 13.13  | 23.95  $ \uparrow $ 13.20       | 43.1  $ \downarrow $ 7.8        | 47.5  $ \downarrow $ 7.5  | 13.1  $ \downarrow $ 1.8 |
| l=2                        | 12.84  $ \uparrow $ 2.14   | 12.91  $ \uparrow $ 2.16   | 50.3  $ \downarrow $ 0.6        | 50.9  $ \downarrow $ 4.1        | 14.7  $ \downarrow $ 0.2  |                          |
| l=3                        | 11.75  $ \uparrow $ 1.05   | 11.78  $ \uparrow $ 1.03   | 50.7  $ \downarrow $ 0.2        | 54.1  $ \downarrow $ 0.9        | 15.2  $ \uparrow $ 0.3    |                          |
| l=4                        | 10.21  $ \downarrow $ 0.49 | 10.22  $ \downarrow $ 0.53 | 53.5  $ \uparrow $ 2.6          | 60.1  $ \uparrow $ 5.1          | 16.0  $ \uparrow $ 1.1    |                          |
| l=5                        | 10.52  $ \downarrow $ 0.18 | 10.50  $ \downarrow $ 0.25 | 55.2  $ \uparrow $ 4.3          | 60.2  $ \uparrow $ 5.2          | 17.4  $ \uparrow $ 2.5    |                          |
| Uniform +50% data          |                            | 10.09  $ \downarrow $ 0.61 | 10.14  $ \downarrow $ 0.61      | 52.9  $ \uparrow $ 2.0          | 57.0  $ \uparrow $ 2.0    | 15.9  $ \uparrow $ 1.0   |"
501,"|                        | Val Perplexity | Test Perplexity | Anatomy           | College Medicine           | Medical Genetics |
| ---------------------- | -------------- | --------------- | ----------------- | -------------------------- | ---------------- |
| Overall Score  $ l=5 $ | 8.10           | 8.05            | 28.1              | 24.3                       | 22.0             |
| + Medicine CPT         | 8.11           | 8.11            | 30.4              | 26.6                       | 30.0             |
|                        | Val Perplexity | Test Perplexity | International Law | Professional Law           | Jurisprudence    |
| Overall Score  $ l=5 $ | 7.92           | 8.13            | 25.5              | 33.9                       | 22.2             |
| + Law CPT              | 8.06           | 8.34            | 35.5              | 24.7                       | 24.1             |
|                        | Val Perplexity | Test Perplexity | Econometrics      | High School Macroeconomics | Marketing        |
| Overall Score  $ l=5 $ | 9.51           | 9.49            | 23.7              | 33.3                       | 22.2             |
| + Finance CPT          | 9.59           | 9.63            | 25.4              | 34.9                       | 23.9             |"
502,"| Method        | Peptide-level performance |
| ------------- | ------------------------- |
| Seven-species | Nine-species              |
| Prec.         | AUC                       |
| DeepNovo      | 0.204                     |
| PointNovo     | 0.022                     |
| CasaNovo      | 0.119                     |
| HelixNovo     | 0.234                     |
| AdaNovo       | 0.174                     |
| ReNovo        | 0.278                     |"
502,"|              | Model Training        | Datastore Building   | Retrieval-Based Inference |
| ------------ | --------------------- | -------------------- | ------------------------- |
| Time(s)      | 84,703                | 2,207                | 8,278                     |
| Percentage   | 88.98%                | 2.32%                | 8.70%                     |
|              |                       |                      |                           |
|              | Seven-species Dataset | Nine-species Dataset | HC-PT Dataset             |
| Pairs Number | 5,626,944             | 8,456,240            | 3,232,616                 |
| Storage (GB) | 11.16                 | 16.77                | 6.42                      |"
502,"| Input MS2                          | Previously Predicted                    |
| ---------------------------------- | --------------------------------------- |
|                                    | RVNLARIDNE                              |
|                                    | Sample 1                                |
| Retrieved MS2                      | D                                       |
| Retrieved Amino Acid               | 22.11                                   |
| Retrieved Distance                 |                                         |
| Predicted Amino Acid( $ K = 0 $ )  | $ p(D) = 0.45 $ ,  $ p(E) = 0.50 $ ,... |
| Predicted Amino Acid( $ K = 32 $ ) | RVNLARIDNEEVM(+15.99)                   |
| Ground Truth Amino Acid            | RVNLARIDNEEVM(+15.99)                   |
| Predicted Peptide( $ K = 0 $ )     | RVNLARIDNEEVM(+15.99)                   |
| Predicted Peptide( $ K = 32 $ )    | RVNLARIDNEEVM(+15.99)                   |
| Ground Truth Peptide               | RVNLARIDNEEVM(+15.99)                   |"
503,"| Model        | Metric      | ETTm1-L     | ETTm2-L     | ETTh1-L     | ETTh2-L     | Electricity-L | Traffic-L   | Weather-L   | Exchange-L  | ILI-L       |
| ------------ | ----------- | ----------- | ----------- | ----------- | ----------- | ------------- | ----------- | ----------- | ----------- | ----------- |
| FITS         | CRPS        | 0.305±0.024 | 0.449±0.034 | 0.348±0.025 | 0.314±0.022 | 0.115±0.024   | 0.374±0.004 | 0.267±0.003 | 0.074±0.011 | 0.211±0.011 |
| NMAE         | 0.406±0.072 | 0.540±0.052 | 0.468±0.012 | 0.401±0.022 | 0.149±0.012 | 0.453±0.022   | 0.317±0.021 | 0.097±0.011 | 0.245±0.017 |             |
| PatchTST     | CRPS        | 0.304±0.029 | 0.229±0.036 | 0.323±0.020 | 0.304±0.018 | 0.127±0.015   | 0.214±0.001 | 0.142±0.005 | 0.097±0.007 | 0.233±0.019 |
| NMAE         | 0.382±0.066 | 0.288±0.034 | 0.428±0.024 | 0.371±0.021 | 0.164±0.024 | 0.253±0.012   | 0.152±0.029 | 0.126±0.001 | 0.287±0.023 |             |
| iTransformer | CRPS        | 0.455±0.021 | 0.311±0.024 | 0.350±0.019 | 0.542±0.015 | 0.109±0.044   | 0.284±0.004 | 0.133±0.004 | 0.087±0.023 | 0.222±0.020 |
| NMAE         | 0.490±0.038 | 0.385±0.042 | 0.449±0.022 | 0.667±0.012 | 0.140±0.009 | 0.361±0.030   | 0.147±0.019 | 0.113±0.015 | 0.278±0.017 |             |
| Koopa        | CRPS        | 0.295±0.027 | 0.233±0.025 | 0.318±0.009 | 0.293±0.026 | 0.113±0.018   | 0.358±0.022 | 0.140±0.007 | 0.091±0.012 | 0.228±0.022 |
| NMAE         | 0.377±0.037 | 0.290±0.033 | 0.412±0.008 | 0.286±0.042 | 0.149±0.025 | 0.432±0.032   | 0.162±0.009 | 0.116±0.022 | 0.288±0.031 |             |
| TSDiff       | CRPS        | 0.478±0.027 | 0.344±0.046 | 0.516±0.027 | 0.406±0.056 | 0.478±0.005   | 0.391±0.002 | 0.152±0.003 | 0.082±0.010 | 0.263±0.022 |
| NMAE         | 0.622±0.045 | 0.416±0.065 | 0.657±0.017 | 0.482±0.022 | 0.622±0.142 | 0.478±0.006   | 0.141±0.026 | 0.142±0.009 | 0.272±0.020 |             |
| GRU NVP      | CRPS        | 0.546±0.036 | 0.561±0.273 | 0.502±0.039 | 0.539±0.090 | 0.114±0.013   | 0.211±0.004 | 0.110±0.004 | 0.079±0.009 | 0.307±0.005 |
| NMAE         | 0.707±0.050 | 0.749±0.385 | 0.643±0.046 | 0.688±0.161 | 0.144±0.017 | 0.264±0.006   | 0.135±0.008 | 0.103±0.009 | 0.333±0.005 |             |
| GRU MAF      | CRPS        | 0.536±0.033 | 0.272±0.029 | 0.393±0.043 | 0.990±0.023 | 0.106±0.007   | -           | 0.122±0.006 | 0.160±0.019 | 0.172±0.034 |
| NMAE         | 0.711±0.081 | 0.355±0.048 | 0.496±0.019 | 1.092±0.019 | 0.136±0.098 | -             | 0.149±0.034 | 0.182±0.010 | 0.216±0.014 |             |
| Trans MAF    | CRPS        | 0.688±0.043 | 0.355±0.043 | 0.363±0.053 | 0.327±0.033 | -             | -           | 0.113±0.004 | 0.148±0.017 | 0.155±0.018 |
| NMAE         | 0.822±0.034 | 0.475±0.029 | 0.455±0.025 | 0.412±0.020 | -           | -             | 0.148±0.040 | 0.191±0.006 | 0.183±0.019 |             |
| TimeGrad     | CRPS        | 0.621±0.037 | 0.470±0.054 | 0.523±0.027 | 0.445±0.016 | 0.108±0.003   | 0.220±0.002 | 0.113±0.011 | 0.099±0.015 | 0.295±0.083 |
| NMAE         | 0.793±0.034 | 0.561±0.044 | 0.672±0.015 | 0.550±0.018 | 0.134±0.004 | 0.263±0.001   | 0.136±0.020 | 0.113±0.016 | 0.325±0.068 |             |
| CSDI         | CRPS        | 0.448±0.038 | 0.239±0.035 | 0.528±0.012 | 0.302±0.040 | -             | -           | 0.087±0.003 | 0.143±0.020 | 0.283±0.012 |
| NMAE         | 0.578±0.051 | 0.306±0.040 | 0.657±0.014 | 0.382±0.030 | -           | -             | 0.102±0.005 | 0.173±0.020 | 0.299±0.013 |             |
| K²VAE        | CRPS        | 0.294±0.026 | 0.221±0.023 | 0.314±0.011 | 0.280±0.014 | 0.057±0.005   | 0.200±0.001 | 0.084±0.003 | 0.069±0.005 | 0.142±0.008 |
| NMAE         | 0.373±0.032 | 0.275±0.035 | 0.396±0.012 | 0.278±0.020 | 0.117±0.019 | 0.248±0.010   | 0.099±0.009 | 0.084±0.017 | 0.167±0.007 |             |"
504,"| Office-31 Amazon $ \rightarrow $  Webcam |
| ---------------------------------------- |
| $ K_{t} $                                |
| 2                                        |
| 2                                        |
| 2                                        |
| 2                                        |
| 4                                        |
| 4                                        |
| 4                                        |
| 4                                        |"
573,"|              | New embedding dimension |
| ------------ | ----------------------- |
| Value        | 2                       |
| Accuracy (%) | 74.94                   |
| Epochs       |                         |
| Value        | 2                       |
| Accuracy (%) | 74.94                   |
| Batch size   |                         |
| Value        | 8                       |
| Accuracy (%) | 71.83                   |"
504,"| Method                               | A $ \rightarrow $ D | A $ \rightarrow $ W | D $ \rightarrow $ W | D $ \rightarrow $ A | W $ \rightarrow $ D | W $ \rightarrow $ A | Avg. |
| ------------------------------------ | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ---- |
| ResNet-50 He et al. (2016)           | 68.9                | 68.4                | 96.7                | 62.5                | 99.3                | 60.7                | 76.1 |
| SHOT Liang et al. (2020)             | 94.0                | 90.1                | 98.4                | 74.7                | 99.9                | 74.3                | 88.6 |
| NRC Yang et al. (2021a)              | 96.0                | 90.8                | 99.0                | 75.3                | 100.0               | 75.0                | 89.4 |
| 3C-GAN Li et al. (2020)              | 92.7                | 93.7                | 98.5                | 75.3                | 99.8                | 77.8                | 89.6 |
| HCL Huang et al. (2021a)             | 94.7                | 92.5                | 98.2                | 75.9                | 100.0               | 77.7                | 89.8 |
| AaD Yang et al. (2022)               | 96.4                | 92.1                | 99.1                | 75.0                | 100.0               | 76.5                | 89.9 |
| SF(DA) $ ^{2} $  Hwang et al. (2024) | 95.8                | 92.1                | 99.0                | 75.7                | 99.8                | 76.8                | 89.9 |
| SiLAN (Ours)                         | 97.1                | 95.8                | 98.9                | 76.4                | 100.0               | 76.9                | 90.7 |"
504,"| Method                               | plane | bcycl | bus  | car  | horse | knife | mcycl | person | plant | sktbrd | train | truck | Avg. |
| ------------------------------------ | ----- | ----- | ---- | ---- | ----- | ----- | ----- | ------ | ----- | ------ | ----- | ----- | ---- |
| ResNet-101 He et al. (2016)          | 55.1  | 53.3  | 61.9 | 59.1 | 80.6  | 17.9  | 79.7  | 31.2   | 81.0  | 26.5   | 73.5  | 8.5   | 52.4 |
| SHOT Liang et al. (2020)             | 94.3  | 88.5  | 80.1 | 57.3 | 93.1  | 94.9  | 80.7  | 80.3   | 91.5  | 89.1   | 86.3  | 58.2  | 82.9 |
| HCL Huang et al. (2021a)             | 93.3  | 85.4  | 80.7 | 68.5 | 91.0  | 88.1  | 86.0  | 78.6   | 86.6  | 88.8   | 80.0  | 74.7  | 83.5 |
| G-SFDA Yang et al. (2021b)           | 96.1  | 88.3  | 85.5 | 74.1 | 97.1  | 95.4  | 89.5  | 79.4   | 95.4  | 92.9   | 89.1  | 42.6  | 85.4 |
| NRC Yang et al. (2021a)              | 96.8  | 91.3  | 82.4 | 62.4 | 96.2  | 95.9  | 86.1  | 80.6   | 94.8  | 94.1   | 90.4  | 59.7  | 85.9 |
| AaD Yang et al. (2022)               | 95.2  | 90.5  | 85.5 | 79.2 | 96.4  | 96.2  | 88.8  | 80.4   | 93.9  | 91.8   | 91.1  | 55.9  | 87.1 |
| DaC Zhang et al. (2022)              | 96.6  | 86.8  | 86.4 | 78.4 | 96.4  | 96.2  | 93.6  | 83.8   | 96.8  | 95.1   | 89.6  | 50.0  | 87.3 |
| SF(DA) $ ^{2} $  Hwang et al. (2024) | 96.8  | 89.3  | 82.9 | 81.4 | 96.8  | 95.7  | 90.4  | 81.3   | 95.5  | 93.7   | 88.5  | 64.7  | 88.1 |
| SiLAN (Ours)                         | 97.5  | 90.1  | 85.8 | 80.4 | 97.6  | 95.5  | 92.0  | 82.9   | 96.5  | 95.3   | 92.6  | 53.4  | 88.3 |"
504,"| Method                               | Ar $ \rightarrow $ Cl | Ar $ \rightarrow $ Pr | Ar $ \rightarrow $ Rw | Cl $ \rightarrow $ Ar | Cl $ \rightarrow $ Pr | Cl $ \rightarrow $ Rw | Pr $ \rightarrow $ Ar | Pr $ \rightarrow $ Cl | Pr $ \rightarrow $ Rw | Rw $ \rightarrow $ Ar | Rw $ \rightarrow $ Cl | Rw $ \rightarrow $ Pr | Avg. |
| ------------------------------------ | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | ---- |
| ResNet-50 He et al. (2016)           | 34.9                  | 50.0                  | 58.0                  | 37.4                  | 41.9                  | 46.2                  | 38.5                  | 31.2                  | 60.4                  | 53.9                  | 41.2                  | 59.9                  | 46.1 |
| G-SFDA Yang et al. (2021b)           | 57.9                  | 78.6                  | 81.0                  | 66.7                  | 77.2                  | 77.2                  | 65.6                  | 56.0                  | 82.2                  | 72.0                  | 57.8                  | 83.4                  | 71.3 |
| SHOT Liang et al. (2020)             | 57.1                  | 78.1                  | 81.5                  | 68.0                  | 78.2                  | 78.1                  | 67.4                  | 54.9                  | 82.2                  | 73.3                  | 58.8                  | 84.3                  | 71.8 |
| NRC Yang et al. (2021a)              | 57.7                  | 80.3                  | 82.0                  | 68.1                  | 79.8                  | 78.6                  | 65.3                  | 56.4                  | 83.0                  | 71.0                  | 58.6                  | 85.6                  | 72.2 |
| AaD Yang et al. (2022)               | 59.3                  | 79.3                  | 82.1                  | 68.9                  | 79.8                  | 79.5                  | 67.2                  | 57.4                  | 83.1                  | 72.1                  | 58.5                  | 85.4                  | 72.7 |
| DaC Zhang et al. (2022)              | 59.5                  | 79.5                  | 81.2                  | 69.3                  | 78.9                  | 79.2                  | 67.4                  | 56.4                  | 82.4                  | 74.0                  | 61.4                  | 84.4                  | 72.8 |
| SF(DA) $ ^{2} $  Hwang et al. (2024) | 57.8                  | 80.2                  | 81.5                  | 69.5                  | 79.2                  | 79.4                  | 66.5                  | 57.2                  | 82.1                  | 73.3                  | 60.2                  | 83.8                  | 72.6 |
| SiLAN (Ours)                         | 58.2                  | 81.2                  | 82.5                  | 69.8                  | 78.6                  | 80.3                  | 68.4                  | 58.6                  | 82.5                  | 75.6                  | 60.8                  | 86.1                  | 73.6 |"
505,"| BC vs. TD3 |
| ---------- |
| Method     |
| BC         |
| TD3        |"
505,"| Task Name       | BC    | BEAR  | TD3+BC       | IQL          | CQL   | SAC-RND     | RORL        | VACO(ours)   |
| --------------- | ----- | ----- | ------------ | ------------ | ----- | ----------- | ----------- | ------------ |
| Pen-human       | 34.4  | -1.0  | 81.8 ± 14.9  | 81.5 ± 17.5  | 37.5  | 5.6 ± 5.8   | 33.7 ± 7.6  | 106.0 ± 11.4 |
| Pen-cloned      | 56.9  | 26.5  | 61.4 ± 19.3  | 77.2 ± 17.7  | 39.2  | 2.5 ± 6.1   | 35.7 ± 3.1  | 98.3 ± 15.8  |
| Pen-expert      | 85.1  | 105.9 | 146.0 ± 7.3  | 133.6 ± 16.0 | 107.0 | 45.4 ± 22.9 | 130.3 ± 4.2 | 144.8 ± 4.8  |
| Door-human      | 0.5   | -0.3  | -0.1 ± 0.0   | 3.1 ± 2.0    | 9.9   | 0.0 ± 0.1   | 3.78 ± 0.7  | 6.1 ± 3.8    |
| Door-cloned     | -0.1  | -0.1  | 0.1 ± 0.6    | 0.8 ± 1.0    | 0.4   | 0.2 ± 0.8   | -0.1 ± 0.1  | 1.6 ± 1.5    |
| Door-expert     | 34.9  | 103.4 | 84.6 ± 44.5  | 105.3 ± 2.8  | 101.5 | 73.6 ± 26.7 | 104.9 ± 0.9 | 104.7 ± 1.0  |
| Hammer-human    | 1.5   | 0.3   | 0.4 ± 0.4    | 2.5 ± 1.9    | 4.4   | -0.1 ± 0.1  | 2.3 ± 1.9   | 1.8 ± 0.9    |
| Hammer-cloned   | 0.8   | 0.3   | 0.8 ± 0.7    | 1.1 ± 0.5    | 2.1   | 0.1 ± 0.4   | 1.7 ± 0.5   | 2.5 ± 1.9    |
| Hammer-expert   | 125.6 | 127.3 | 117.0 ± 30.9 | 129.6 ± 0.5  | 86.7  | 24.8 ± 39.4 | 132.2 ± 0.7 | 127.0 ± 0.4  |
| Relocate-human  | 0.0   | -0.3  | -0.2 ± 0.0   | 0.1 ± 0.1    | 0.2   | 0.0 ± 0.0   | 0.0 ± 0.0   | 0.6 ± 0.4    |
| Relocate-cloned | -0.1  | -0.3  | -0.1 ± 0.1   | 0.2 ± 0.4    | -0.1  | 0.0 ± 0.0   | 0.0 ± 0.0   | 0.1 ± 0.1    |
| Relocate-expert | 101.3 | 98.6  | 107.3 ± 1.6  | 106.5 ± 2.5  | 95.0  | 3.4 ± 4.5   | 47.8 ± 13.5 | 107.3 ± 3.0  |
| Average         | 36.7  | 38.4  | 49.9         | 53.4         | 40.3  | 12.9        | 41.0        | 58.4         |"
506,"| Metric           | LLaMA-3.1-8B- | Gradient- | Context Length |
| ---------------- | ------------- | --------- | -------------- |
| Instruct         | AI-Model      | 180K      | 350K           |
| Retrieve.PassKey | 100.00        | 100.00    | 100.00         |
| Retrieve.Number  | 95.33         | 99.83     | 99.33          |
| Retrieve.KV      | 42.66         | 15.60     | 88.66          |
| En.Sum           | 27.63         | 17.02     | 24.01          |
| En.QA            | 24.83         | 14.31     | 34.26          |
| En.MC            | 68.00         | 57.20     | 74.00          |
| En.Dia           | 16.66         | 5.00      | 18.00          |
| Math.Find        | 35.33         | 19.42     | 37.33          |
| Average          | 51.31         | 41.04     | 59.45          |"
506,"|                   | LLaMA-3.1-8B- | Gradient- | Context Length |
| ----------------- | ------------- | --------- | -------------- |
|                   | Instruct      | AI-Model  | 180K           |
| Single Document   | 46.91         | 30.71     | 45.83          |
| Multi-Document    | 41.45         | 12.45     | 41.71          |
| Summarization     | 26.10         | 21.72     | 25.14          |
| Few-shot Learning | 63.48         | 59.69     | 62.22          |
| Synthetic Tasks   | 67.48         | 55.50     | 68.17          |
| All               | 48.11         | 35.89     | 47.58          |"
578,"|        | Baseline | RoPE  | RoPE-M | Circulant-S | Cayley-S |
| ------ | -------- | ----- | ------ | ----------- | -------- |
| APCOCO | 32.44    | 33.66 | 32.74  | 33.24       | 33.47    |
| APLVIS | 21.98    | 22.71 | 22.43  | 22.60       | 23.01    |
| Mean   | 27.21    | 28.18 | 27.59  | 27.92       | 28.24    |"
506,"| Category        | LLaMA-3.1-8B-Instruct | Gradient-AI-Model | Context Length |
| --------------- | --------------------- | ----------------- | -------------- |
| 350K            | 650K                  | 1M                |                |
| MMLU            | 68.21 ± 0.37          | 60.48 ± 0.39      | 66.29 ± 0.38   |
| Humanities      | 64.23 ± 0.67          | 55.75 ± 0.69      | 61.51 ± 0.68   |
| Other           | 73.03 ± 0.77          | 67.04 ± 0.82      | 72.84 ± 0.77   |
| Social Sciences | 77.48 ± 0.74          | 70.46 ± 0.80      | 76.81 ± 0.74   |
| STEM            | 60.36 ± 0.83          | 51.32 ± 0.86      | 59.44 ± 0.84   |"
506,"| Task             | LLaMA-3.1-8B-gradient-instruct | al-model | Generator |
| ---------------- | ------------------------------ | -------- | --------- |
| Llama Gen        | Qwen Gen                       |          |           |
| 180K             | 350K                           | 650K     | 180K      |
| Retrieve.PassKey | 100.00                         | 100.00   | 100.00    |
| Retrieve.Number  | 95.33                          | 99.33    | 99.04     |
| Retrieve.KV      | 42.66                          | 13.33    | 85.47     |
| En.Sum           | 27.63                          | 17.02    | 25.68     |
| En.QA            | 24.83                          | 15.84    | 33.39     |
| En.MC            | 68.00                          | 61.33    | 58.00     |
| En.Dia           | 16.66                          | 4.00     | 19.50     |
| Math.Find        | 35.33                          | 26.66    | 36.66     |
| Average          | 51.31                          | 42.19    | 57.22     |"
506,"| Category    | LLaMA-3.1-8B-Instruct | gradient-ai-model | Llama Gen    | Generator    | Qwen Gen     |
| ----------- | --------------------- | ----------------- | ------------ | ------------ | ------------ |
| 180K        | 350K                  | 650K              | 180K         | 350K         | 650K         |
| MMLU        | 68.21 ± 0.37          | 60.48 ± 0.39      | 66.99 ± 0.38 | 66.74 ± 0.38 | 65.93 ± 0.38 |
| Humanities  | 64.23 ± 0.67          | 55.75 ± 0.69      | 62.32 ± 0.67 | 61.38 ± 0.68 | 60.57 ± 0.68 |
| Other       | 73.03 ± 0.77          | 67.04 ± 0.82      | 72.90 ± 0.77 | 73.03 ± 0.76 | 72.87 ± 0.76 |
| Social sci. | 77.48 ± 0.74          | 70.46 ± 0.80      | 76.70 ± 0.74 | 76.93 ± 0.74 | 75.53 ± 0.75 |
| STEM        | 60.36 ± 0.83          | 51.32 ± 0.86      | 58.67 ± 0.84 | 58.61 ± 0.84 | 57.72 ± 0.84 |"
506,"| Task       | LLaMA-3.1-8B-Instruct | gradient-ai-model | Generator |
| ---------- | --------------------- | ----------------- | --------- |
| Llama Gen  | Qwen Gen              |                   |           |
| 180K       | 350K                  | 650K              | 180K      |
| single-doc | 46.91                 | 30.75             | 46.48     |
| multi-doc  | 41.45                 | 12.45             | 38.69     |
| summary    | 26.10                 | 21.72             | 25.28     |
| few-shot   | 63.48                 | 59.70             | 61.56     |
| synthetic  | 67.48                 | 55.50             | 66.17     |
| Avg        | 48.11                 | 35.89             | 47.23     |"
507,"| Methods                                                           | Supervisions            | Backbone        | AP50(%) | AP50(%) |
| ----------------------------------------------------------------- | ----------------------- | --------------- | ------- | ------- |
| Annotation: Extra caption datasets, Weak/Pseudo Labels in CB ∪ CN |                         |                 |         |         |
| Detic (Zhou et al., 2022)                                         | ImageNet21K &amp; CC3M  | RN50-C4 (24M)   | 27.8    | 42.0    |
| OV-DETR (Zang et al., 2022)                                       | Pseudo annotation       | RN50 (24M)      | 29.4    | 52.7    |
| RegionCLIP (Zhong et al., 2022)                                   | CC3M &amp; COCO Caption | RN50×4 (87M)    | 39.3    | 55.7    |
| CoDet (Ma et al., 2024)                                           | CC3M &amp; COCO Caption | RN50 (24M)      | 30.6    | 46.4    |
| BARON+ (Wu et al., 2023a)                                         | COCO Caption            | RN50-C4 (24M)   | 42.7    | 51.7    |
| CORA+ (Wu et al., 2023c)                                          | COCO Caption            | RN50×4 (87M)    | 43.1    | 56.2    |
| CFM-ViT (Kim et al., 2023a)                                       | LAION-2B                | ViT-L/16(307M)  | 34.3    | 46.4    |
| DITO (Kim et al., 2024)                                           | LAION-2B                | ViT-B/16 (86M)  | 36.6    | 48.8    |
| DITO (Kim et al., 2024)                                           | DataComp-1B             | ViT-L/16(307M)  | 40.2    | 54.6    |
| Annotation: Instance-level labels in CB                           |                         |                 |         |         |
| ViLD-ens (Gu et al., 2021)                                        | CLIP                    | RN50-FPN (24M)  | 27.6    | 51.3    |
| F-VLM (Kuo et al., 2022)                                          | CLIP                    | RN50-FPN (24M)  | 28.0    | 39.6    |
| BARON (Wu et al., 2023a)                                          | CLIP                    | RN50-FPN (24M)  | 34.0    | 53.5    |
| CORA (Wu et al., 2023c)                                           | CLIP                    | RN50 (24M)      | 35.1    | 35.4    |
| CORA+ (Wu et al., 2023c)                                          | CLIP                    | RN50×4 (87M)    | 41.7    | 43.8    |
| BIND (Zhang et al., 2024)                                         | CLIP                    | ViT-B/16 (86M)  | 36.3    | 50.2    |
| BIND (Zhang et al., 2024)                                         | CLIP                    | ViT-L/16 (307M) | 41.5    | 54.8    |
| CLIP-Self (Wu et al., 2023b)                                      | CLIP                    | ViT-B/16 (86M)  | 37.6    | -       |
| CLIP-Self (Wu et al., 2023b)                                      | CLIP                    | ViT-L/14 (307M) | 44.3    | -       |
| OV-DQUO (Wang et al., 2024)                                       | CLIP                    | RN50×4 (87M)    | 45.6    | -       |
| CCKT-Det (ours)                                                   | CLIP                    | RN50 (24M)      | 38.0    | 35.0    |
| CCKT-Det (ours)                                                   | CLIP                    | SwinB (88M)     | 41.9    | 40.9    |
| CCKT-Det++ (ours)                                                 | CLIP                    | RN50 (24M)      | 45.3    | 46.9    |
| CCKT-Det++ (ours)                                                 | CLIP                    | SwinB (88M)     | 46.0    | 46.2    |"
510,"| Method                                          | Waterbirds (%) | MultiNLI (%) | Group Labels |
| ----------------------------------------------- | -------------- | ------------ | ------------ |
| Worst%                                          | Mean%          | Worst %      | Mean%        |
| ERM                                             | 74.81 (0.7)    | 98.10 (0.1)  | 65.9 (0.3)   |
| CnC (Zhang et al., 2022)                        | 88.5 (0.3)     | 90.9 (0.1)   | -            |
| JTT (Liu et al., 2021)                          | 86.7           | 93.3         | 72.6         |
| gDRO (Sagawa et al., 2020a)                     | 86.0           | 93.2         | 77.7         |
| DFR $ ^{\text{Tr}} $  (Kirichenko et al., 2023) | 90.2 (0.8)     | 97.0 (0.3)   | 71.5 (0.6)   |
| PDE (Deng et al., 2023)                         | 90.3 (0.3)     | 92.4 (0.8)   | -            |
| Ours                                            | 90.93 (0.58)   | 92.48 (0.72) | 75.88 (1.62) |"
511,"| Instruction                                                                                                                                                                                                            |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Describe a unique trait of the raccoon                                                                                                                                                                                 |
| Candidate A&#x27;s response                                                                                                                                                                                            |
| A unique trait of raccoons is their masked face, which features a black ring around each eye and a broad black stripe extending from the nose. ♡Response Confidence: 87.96%                                            |
| Candidate B&#x27;s response                                                                                                                                                                                            |
| A unique trait of a Raccoon is its ability to open and close its eyes while they are closed. This adaptation helps them stay alert for potential threats even when they&#x27;re sleeping. ♡Response Confidence: 68.89% |"
511,"| Evaluator            | MT-Bench              | PandaLM Test set |
| -------------------- | --------------------- | ---------------- |
| Rating               | Evaluation Confidence | Rating           |
| Prometheus2-7B       | 5.963                 | 0.993            |
| Prometheus2-bgb-8x7B | 4.725                 | 0.870            |
| General LLMs         | 6.456                 | 0.654            |"
535,"| Model                | KBI  $ \uparrow $ | FAR $ _{1} $   $ \downarrow $ | CPI $ _{1} $   $ \uparrow $ |
| -------------------- | ----------------- | ----------------------------- | --------------------------- |
| CodeReviewer         | 0.00              | 97.78                         | 0.00                        |
| CCT5                 | 2.22              | 97.58                         | 2.32                        |
| LLaMA-Reviewer       | 2.22              | 97.62                         | 2.30                        |
| Ours (LLaMA3.1-405B) | 20.00             | 75.37                         | 22.07                       |"
511,"| Evaluator           | Single-answer grading | Pairwise comparison |
| ------------------- | --------------------- | ------------------- |
| MT-Bench            | PandaLM Test set      | MT-Bench            |
| Rating              | Evaluation Confidence | Rating              |
| GPT-4o              | 5.413                 | 0.417               |
| GPT-4o-mini         | 6.038                 | 0.605               |
| GPT-3.5-Turbo       | 6.288                 | 0.629               |
| Llama3-70B-Instruct | 7.250                 | 0.644               |
| Llama2-70B-Instruct | 7.875                 | 0.953               |
| Qwen2-72B-Instruct  | 5.875                 | 0.675               |
| Average             | 6.456                 | 0.654               |"
511,"| Data            | #Instances | Annotator Agreement |
| --------------- | ---------- | ------------------- |
| Fine-tuning set | 694        | 94.96%              |
| Test set        | 220        | 97.27%              |"
511,"| Evaluator            | MT-Bench              | PandaLM Test set |
| -------------------- | --------------------- | ---------------- |
| Win / Lose / Tie     | Evaluation Confidence | Win / Lose / Tie |
| PandaLM-7B           | 42.0 / 28.5 / 9.5     | 0.596            |
| Prometheus2-7B       | 37.5 / 42.0 / 0.5     | 0.990            |
| Prometheus2-bgb-8x7B | 31.5 / 32.5 / 16.0    | 0.967            |
| General LLMs         | 28.3 / 32.6 / 19.1    | 0.797            |"
511,"| Evaluator                   | F1      | Evaluation Confidence |
| --------------------------- | ------- | --------------------- |
| Overall                     | Writing | Roleplay              |
| GPT-4o                      | 0.678   | 0.391                 |
| GPT-4o-mini                 | 0.677   | 0.423                 |
| GPT-3.5-Turbo               | 0.637   | 0.505                 |
| Llama3-70B-Instruct         | 0.542   | 0.316                 |
| Llama2-70B-Instruct         | 0.534   | 0.241                 |
| Qwen2-72B-Instruct          | 0.631   | 0.404                 |
| Prometheus2-7B              | 0.515   | 0.280                 |
| Prometheus2-bgb-8x7B        | 0.556   | 0.394                 |
| PandaLM-7B                  | 0.560   | 0.388                 |
| Llama3-8B-Instruct          | 0.536   | 0.267                 |
| Llama3-8B-Instruct-Finetune | 0.582   | 0.603                 |
| ConfiLM                     | 0.621   | 0.723                 |"
512,"|        | DATASETS | SOURCES | CREATOR ORGS | LANGUAGES | TASKS     | LICENSES |
| ------ | -------- | ------- | ------------ | --------- | --------- | -------- |
| #      | SIZE     | #       | DOMAINS      | #         | COUNTRIES | #        |
| TEXT   | 3717     | 2.1T    | 713          | 23        | 534       | 60       |
| SPEECH | 95       | 775k    | 51           | 16        | 124       | 29       |
| VIDEO  | 104      | 1.13M   | 44           | 24        | 101       | 23       |
| TOTAL  | 3916     | -       | 798          | 83        | 659       | 67       |"
512,"|                    | AFRICA | ASIA | EUROPE | N AMERICA | OCEANIA | S AMERICA |
| ------------------ | ------ | ---- | ------ | --------- | ------- | --------- |
| By Count           |        |      |        |           |         |           |
| TEXT               | 0.3    | 13.4 | 24.0   | 61.5      | 0.7     | 0.2       |
| SPEECH             | 3.6    | 35.7 | 30.4   | 30.4      | 0.0     | 0.0       |
| VIDEO              | 0.0    | 25.2 | 24.4   | 48.0      | 0.8     | 1.6       |
| By Tokens or Hours |        |      |        |           |         |           |
| TEXT               | 0.0    | 6.1  | 55.4   | 38.4      | 0.1     | 0.0       |
| SPEECH             | 0.1    | 38.8 | 18.8   | 42.4      | 0.0     | 0.0       |
| VIDEO              | 0.0    | 23.1 | 22.0   | 38.2      | 16.7    | 0.1       |"
513,"| MTEB Tasks                                             | CLF   | Clust. | PairCLF | Rerank | STS   | Summ. | Avg.  |
| ------------------------------------------------------ | ----- | ------ | ------- | ------ | ----- | ----- | ----- |
| Self-Supervised Methods                                |       |        |         |        |       |       |       |
| Glove $ ^{*} $  (Pennington et al., 2014)              | 51.04 | 23.11  | 62.90   | 48.72  | 60.52 | 28.87 | 45.86 |
| Komninos $ ^{*} $  (Komninos &amp; Manandhar, 2016)    | 50.21 | 24.96  | 66.63   | 50.03  | 61.73 | 30.49 | 47.34 |
| BERT $ ^{*} $  (Devlin, 2018)                          | 52.36 | 23.48  | 66.10   | 48.47  | 52.89 | 29.82 | 45.52 |
| SimCSE-BERT-unsup $ ^{*} $  (Gao et al., 2021)         | 54.80 | 22.59  | 70.79   | 52.42  | 75.00 | 31.15 | 51.13 |
| Supervised Methods                                     |       |        |         |        |       |       |       |
| SimCSE-BERT-sup $ ^{*} $                               | 58.98 | 29.49  | 75.82   | 53.61  | 79.97 | 23.31 | 53.53 |
| coCondenser-msmarco $ ^{*} $  (Gao &amp; Callan, 2021) | 53.89 | 32.85  | 74.56   | 60.08  | 76.41 | 29.50 | 54.55 |
| SPECTER $ ^{*} $  (Cohan et al., 2020)                 | 42.59 | 27.94  | 56.24   | 55.87  | 60.68 | 27.66 | 45.16 |
| LaBSE (Feng et al., 2020)                              | 54.31 | 24.05  | 73.68   | 54.63  | 70.95 | 31.05 | 51.45 |
| LASER2                                                 | 42.54 | 14.01  | 70.52   | 46.99  | 64.52 | 26.80 | 44.23 |
| SGPT-125M-nli (Muennighoff, 2022)                      | 53.28 | 26.59  | 68.80   | 53.65  | 75.01 | 30.26 | 51.27 |
| DeepSeekMoE-16B                                        |       |        |         |        |       |       |       |
| Hidden State (HS)                                      | 58.24 | 24.64  | 48.76   | 38.13  | 59.66 | 24.38 | 42.30 |
| Routing Weight (RW)                                    | 49.52 | 19.97  | 68.30   | 37.48  | 59.52 | 29.26 | 44.01 |
| MoEE (concat)                                          | 54.21 | 26.10  | 72.44   | 53.31  | 67.59 | 28.89 | 50.42 |
| MoEE (sum)                                             | 58.31 | 34.52  | 70.95   | 55.99  | 70.66 | 29.22 | 53.28 |
| Qwen1.5-MoE-A2.7B                                      |       |        |         |        |       |       |       |
| Hidden State (HS)                                      | 59.34 | 29.50  | 74.29   | 56.51  | 67.39 | 23.01 | 51.67 |
| Routing Weight (RW)                                    | 47.84 | 16.74  | 64.85   | 43.55  | 51.71 | 27.74 | 42.07 |
| MoEE (concat)                                          | 54.23 | 27.18  | 73.93   | 56.12  | 68.52 | 28.57 | 51.43 |
| MoEE (sum)                                             | 59.57 | 38.33  | 72.21   | 56.25  | 72.78 | 31.09 | 55.04 |
| OLMoE-1B-7B                                            |       |        |         |        |       |       |       |
| Hidden State (HS)                                      | 58.18 | 32.83  | 72.10   | 58.31  | 72.91 | 27.96 | 53.72 |
| Routing Weight (RW)                                    | 45.02 | 19.93  | 61.58   | 43.91  | 54.33 | 29.49 | 42.38 |
| MoEE (concat)                                          | 52.59 | 33.92  | 71.85   | 56.69  | 71.13 | 30.21 | 52.73 |
| MoEE (sum)                                             | 57.46 | 36.46  | 71.26   | 60.43  | 74.63 | 30.71 | 55.16 |"
514,"| Method | $ \delta $ | CVRP100 Gap | CVRP200 Gap | CVRP500 Gap |
| ------ | ---------- | ----------- | ----------- | ----------- |
| POMO   | 0.0        | 1.000%      | 3.403%      | 11.135%     |
| 0.5    | 1.556%     | 3.968%      | 10.117%     |             |
| 1.0    | 1.880%     | 4.299%      | 10.255%     |             |
| AM     | 0          | 7.094%      | 10.707%     | 18.145%     |
| 0.5    | 7.375%     | 10.729%     | 20.129%     |             |
| 1.0    | 7.642%     | 11.307%     | 23.746%     |             |
| LEHD   | 0.0        | 3.648%      | 3.312%      | 3.178%      |
| 0.5    | 11.449%    | 12.450%     | 9.999%      |             |
| 1.0    | 12.951%    | 14.456%     | 12.123%     |             |
| BQ     | 0.0        | 2.726%      | 2.972%      | 3.248%      |
| 0.5    | 12.772%    | 13.647%     | 11.238%     |             |
| 1.0    | 15.993%    | 17.504%     | 13.965%     |             |"
514,"| Method                     | n100:Cap50 Obj. | n100:Cap100 Obj. | n100:Cap250 Obj. | n100:Cap500 Obj. |
| -------------------------- | --------------- | ---------------- | ---------------- | ---------------- |
| POMON+IDT+FF no-aug single | 16.20           | 11.14            | 9.04             | 8.65             |
| LEHD greedy                | 16.55           | 11.17            | 8.81             | 8.41             |"
514,"|                    | CVRP100 | CVRP200 | CVRP500 | CVRP1000 |
| ------------------ | ------- | ------- | ------- | -------- |
| Method             | Gap     | Gap     | Gap     | Gap      |
| POMO               | 1.000%  | 3.403%  | 11.135% | 110.632% |
| Fine-tune Dec      | 2.620%  | 4.487%  | 7.148%  | 15.202%  |
| Fine-tune Enc      | 3.061%  | 4.384%  | 4.863%  | 9.627%   |
| Fine-tune All      | 2.594%  | 4.132%  | 5.718%  | 11.708%  |
| POMON              | 0.947%  | 3.864%  | 13.008% | 33.262%  |
| POMON-Enc $ ^{+} $ | 0.890%  | 2.703%  | 9.578%  | 30.198%  |
| POMON-Dec $ ^{+} $ | 0.682%  | 2.640%  | 8.185%  | 18.076%  |"
578,"|             | ViT   | ViTD  |
| ----------- | ----- | ----- |
| Baseline    | 49.77 | 67.60 |
| RoPE        | 58.09 | 71.21 |
| RoPE-M      | 57.17 | 70.90 |
| Circulant-S | 58.95 | 72.36 |
| Cayley-S    | 58.85 | 72.67 |"
514,"| Method         | CVRP100 | CVRP200 | CVRP500 | CVRP1000 |
| -------------- | ------- | ------- | ------- | -------- |
| Gap            | Time    | Gap     | Time    | Gap      |
| LKH3           | *       | 12 h    | *       | 2.1 h    |
| HGS            | -0.533% | 4.5 h   | -1.126% | 1.4 h    |
| GLOP-G (LKH3)* | -       | -       | -       | -        |
| BQ greedy*     | 2.726%  | 1.3 m   | 2.972%  | 10 s     |
| LEHD greedy    | 3.648%  | 0.38 m  | 3.312%  | 2 s      |
| MDAM bs50*     | 2.211%  | 25 m    | 4.304%  | 3 m      |
| POMO augx8     | 1.004%  | 1.07 m  | 3.403%  | 2 s      |
| ELG augx8      | 1.207%  | 3.18 m  | 2.553%  | 7 s      |
| ReLD augx8     | 0.960%  | 1.34 m  | 1.654%  | 2 s      |"
514,"|              | N \leq 200(22 instances) | 200 &lt; N \leq 500(46 instances) | 500 &lt; N \leq 1000(32 instances) | Total(100 instances) |
| ------------ | ------------------------ | --------------------------------- | ---------------------------------- | -------------------- |
| BKS          | *                        | *                                 | *                                  | *                    |
| LEHD greedy* | 11.35%                   | 9.45%                             | 17.74%                             | 12.52%               |
| BQ greedy*   | -                        | -                                 | -                                  | 9.94%                |
| INViT-3V*    | 6.52%                    | 9.11%                             | 10.21%                             | -                    |
| POMO         | 5.63%                    | 9.25%                             | 18.85%                             | 11.52%               |
| ICAM no-aug* | 5.14%                    | 4.44%                             | 5.17%                              | 4.83%                |
| ELG no-aug   | 6.25%                    | 7.58%                             | 10.02%                             | 8.07%                |
| ELG          | 4.51%                    | 5.52%                             | 7.81%                              | 6.03%                |
| MTL          | 9.49%                    | 10.96%                            | 16.80%                             | 12.51%               |
| MVMoE        | 4.74%                    | 9.97%                             | 19.60%                             | 11.90%               |
| ReLD-MTL     | 3.31%                    | 5.84%                             | 9.05%                              | 6.31%                |
| ReLD-MoEL    | 3.14%                    | 5.75%                             | 8.83%                              | 6.16%                |
| ReLD-MoEL+   | 2.41%                    | 3.40%                             | 4.52%                              | 3.54%                |
| ReLD no-aug  | 3.36%                    | 3.93%                             | 5.12%                              | 4.18%                |
| ReLD         | 2.53%                    | 3.44%                             | 4.64%                              | 3.62%                |"
514,"| Method     | Obj.   | CVRP Gap | Time  | Obj.   | VRPTW Gap | Time  | Obj.   | VRP Gap    | Time  | Obj.   | VRPL Gap    | Time  |
| ---------- | ------ | -------- | ----- | ------ | --------- | ----- | ------ | ---------- | ----- | ------ | ----------- | ----- |
| HGS        | 15 504 | 0        | 9 1m  | 24 330 | 0         | 19 6m | 0      | 0          | 5 3m  | 16 496 | 1 449       | 16 6m |
| LKH        | 15 590 | 0.556%   | 18 0m | 24 721 | 1 584%    | 7 8m  | 10 010 | 0 1272%    | 3 53h | 16 004 | 1 049%      | 3 5h  |
| OR-Tools   | 15 935 | 2 751%   | 3 5h  | 25 367 | 3 482%    | 3 5h  | 9 844  | 0 1292%    | 8 5h  | 15 855 | 0.049%      | 9 5h  |
| POO-MTL    | 15 790 | 1 846%   | 9s    | 25 610 | 5 313%    | 11s   | 10 169 | 3 458%     | 8s    | 15 846 | 0.479%      | 9s    |
| POMO-MTL   | 15 790 | 1 846%   | 9s    | 25 610 | 5 313%    | 11s   | 10 169 | 3 458%     | 8s    | 15 846 | 0.479%      | 9s    |
| MVMOE      | 15 760 | 1 653%   | 11s   | 25 512 | 4 903%    | 12s   | 10 138 | 3 136%     | 9s    | 15 812 | 0.261%      | 11s   |
| ReL-MTL    | 15 770 | 1 416%   | 10s   | 25 433 | 4 561%    | 11s   | 10 087 | 3 231%     | 9s    | 15 773 | 0.039%      | 10s   |
| ReLD-MTL   | 15 770 | 1 416%   | 10s   | 25 433 | 4 561%    | 11s   | 10 087 | 3 231%     | 9s    | 15 773 | 0.039%      | 10s   |
| ReLD-MoEL+ | 15 733 | 1 515%   | 10s   | 25 445 | 4 621%    | 11s   | 10 084 | 2 592%     | 9s    | 15 788 | 0.113%      | 10s   |
| ReLD-MoEL+ | 15 733 | 1 515%   | 10s   | 25 445 | 4 621%    | 11s   | 10 084 | 2 592%     | 9s    | 15 788 | 0.113%      | 10s   |
| Method     | Obj.   | VRPB Gap | Time  | Obj.   | VRPTW Gap | Time  | Obj.   | VRPB Gap   | Time  | Obj.   | VRPL Gap    | Time  |
| OR-Tools   | 11 878 | 0        | 3 5h  | 14 380 | 2 467%    | 3 5h  | 8 365  | *          | 3 5h  | 9 790  | *           | 3 5h  |
| POMO-MTL   | 12 072 | 0.995%   | 9s    | 15 008 | 2 441%    | 11s   | 8 979  | 7 335%     | 8s    | 10 126 | 3 441%      | 9s    |
| MVMOE-L    | 12 036 | 1 674%   | 10s   | 15 494 | 3 917%    | 11s   | 8 979  | 7 235%     | 9s    | 10 107 | 3 248%      | 10s   |
| ReLD-MTL   | 11 981 | 0.901%   | 10s   | 14 818 | 3 103%    | 11s   | 8 814  | 5 361%     | 9s    | 10 014 | 2 310%      | 10s   |
| ReLD-MoEL  | 11 969 | 0.901%   | 10s   | 14 808 | 3 103%    | 11s   | 8 804  | 5 313%     | 9s    | 10 000 | 2 276%      | 10s   |
| ReLD-MoEL+ | 11 981 | 0.887%   | 10s   | 14 848 | 3 304%    | 11s   | 8 794  | 5 113%     | 9s    | 10 040 | 2 567%      | 10s   |
| Method     | Obj.   | VRPB Gap | Time  | Obj.   | VRPTW Gap | Time  | Obj.   | VRPTW Gap  | Time  | Obj.   | VRPBL Gap   | Time  |
| OR-Tools   | 11 790 | *        | 3 5h  | 25 496 | *         | 3 5h  | 25 195 | *          | 3 5h  | 8 348  | *           | 3 5h  |
| POMO-MTL   | 11 969 | 1 793%   | 9s    | 27 319 | 7 413%    | 11s   | 25 619 | 1 920%     | 8s    | 8 961  | 7 343%      | 9s    |
| MVMOE-L    | 11 960 | 1 793%   | 10s   | 27 326 | 7 078%    | 12s   | 25 514 | 1 471%     | 10s   | 8 942  | 1 115%      | 11s   |
| ReLD-MTL   | 11 945 | 3 466%   | 11s   | 27 330 | 7 078%    | 12s   | 25 514 | 1 471%     | 9s    | 8 942  | 1 115%      | 11s   |
| ReLD-MoEL  | 11 945 | 3 466%   | 10s   | 27 106 | 6 574%    | 11s   | 25 415 | 1 089%     | 9s    | 8 786  | 5 247%      | 10s   |
| ReLD-MoEL+ | 11 984 | 0.917%   | 10s   | 27 106 | 6 574%    | 11s   | 25 415 | 1 089%     | 9s    | 8 779  | 5 154%      | 10s   |
| Method     | Obj.   | 1 024%   | 10s   | 27 166 | 6 806%    | 11s   | 25 456 | 1 258%     | 9s    | 8 779  | 5 154%      | 10s   |
| OR-Tools   | 14 384 | 0        | 3 5h  | 14 270 | 0 Gap     | Time  | 0 Obj. | VRPTLW Gap | Time  | 0 Obj. | VRPBLTW Gap | Time  |
| OR-Tools   | 14 384 | 10 533%  | 9s    | 14 270 | 4 374%    | 3 5h  | 25 342 | *          | 3 5h  | 14 250 | *           | 3 5h  |
| MVMOE-L    | 15 841 | 10 188%  | 10s   | 14 839 | 3 971%    | 11s   | 27 177 | 7 473%     | 9s    | 15 706 | 10 263%     | 10s   |
| MVMOE-L    | 15 808 | 9 948%   | 11s   | 14 829 | 3 903%    | 12s   | 27 112 | 7 333%     | 10s   | 15 671 | 10 009%     | 11s   |
| ReLD-MTL   | 15 697 | 9 184%   | 10s   | 14 707 | 3 054%    | 11s   | 27 044 | 6 915%     | 9s    | 15 550 | 9 171%      | 10s   |
| ReLD-MoEL  | 15 728 | 9 403%   | 10s   | 14 744 | 3 318%    | 11s   | 27 078 | 7 106%     | 9s    | 15 599 | 9 516%      | 10s   |"
514,"| Method               | CVRP100 Gap | CVRP200 Gap | CVRP500 Gap | CVRP1000 Gap |
| -------------------- | ----------- | ----------- | ----------- | ------------ |
| POMO                 | 1.004%      | 3.403%      | 11.135%     | 110.632%     |
| POMO+IDT             | 0.936%      | 3.096%      | 9.349%      | 23.508%      |
| POMON                | 0.947%      | 3.864%      | 13.008%     | 33.262%      |
| POMON+FF $ _{qk} $   | 0.889%      | 3.907%      | 15.589%     | 169.461%     |
| POMON+FF $ _{qk,v} $ | 0.837%      | 3.894%      | 16.314%     | 245.524%     |
| POMON+MHA            | 1.129%      | 4.478%      | 15.809%     | 47.132%      |
| POMON+IDT            | 0.909%      | 3.623%      | 10.932%     | 20.687%      |
| POMON+FF             | 0.663%      | 3.142%      | 12.901%     | 35.589%      |
| POMON+IDT+FF         | 0.682%      | 2.640%      | 8.185%      | 18.076%      |
| ReLD w/o IDT&amp;FF  | 1.063%      | 1.960%      | 4.101%      | 9.623%       |
| ReLD-7Layer          | 0.921%      | 1.638%      | 3.055%      | 6.834%       |
| ReLD                 | 0.960%      | 1.690%      | 3.015%      | 6.771%       |"
517,"| Datasets  | DaRE       | HedgeCut   | Vanilla    | Online Boosting | DynFRS (q = 0.1) | DynFRS (q = 0.2) |
| --------- | ---------- | ---------- | ---------- | --------------- | ---------------- | ---------------- |
| Purchase  | .9327±.001 | .9118±.001 | .9372±.001 | .9207±.000      | .9327±.001       | .9359±.001       |
| Vaccine   | .7916±.003 | .7706±.002 | .7939±.002 | .8012±.000      | .7911±.001       | .7934±.002       |
| Adult     | .8628±.001 | .8428±.001 | .8637±.000 | .8503±.000      | .8633±.001       | .8650±.001       |
| Bank      | .9420±.000 | .9350±.000 | .9414±.001 | .9436±.000      | .9417±.000       | .9436±.000       |
| Heart     | .7344±.001 | .7195±.001 | .7342±.001 | .7301±.000      | .7358±.002       | .7366±.000       |
| Diabetes  | .6443±.001 | .6190±.000 | .6435±.001 | .6462±.000      | .6453±.001       | .6470±.002       |
| NoShow    | .7361±.001 | .7170±.000 | .7387±.000 | .7269±.000      | .7335±.001       | .7356±.000       |
| Synthetic | .9451±.000 | /          | .9441±.000 | .9309±.000      | .9424±.000       | .9454±.000       |
| Higgs     | .7441±.000 | /          | .7434±.000 | .7255±.000      | .7431±.000       | .7475±.000       |"
518,"| Datasets &amp; Cases | Implementation of  $ \hat{z}^{j} $                               |
| -------------------- | ---------------------------------------------------------------- |
| *IN [L→V]            | L  $ \Rightarrow $  “This is about Class #.”                     |
| IEMO. [L→A]          | L  $ \Rightarrow $  “This is about Emotion #.”                   |
| IEMO. [A→L]          | A  $ \Rightarrow $  Add Gaussian Noise &amp; Random Shuffling    |
| AVMN. [V→A]          | V  $ \Rightarrow $  Random Shuffled Image (mismatch paired sets) |
| AVMN. [A→V]          | A  $ \Rightarrow $  Add Gaussian Noise &amp; Random Shuffling    |"
518,"| [L→V]       | *IN                       | V2           | Rend.                      | Sketch | A     | Style | C (↓) |
| ----------- | ------------------------- | ------------ | -------------------------- | ------ | ----- | ----- | ----- |
| ResNet-50   | 77.83                     | 66.20        | 39.28                      | 27.35  | 6.44  | 8.59  | 66.01 |
| + RoBERTa   | 78.54                     | 67.30        | 40.92                      | 28.78  | 8.25  | 9.19  | 65.32 |
| ViT-B/32    | 75.04                     | 62.02        | 40.31                      | 27.34  | 9.23  | 16.56 | 55.45 |
| +RoBERTa    | 76.75                     | 64.00        | 41.81                      | 29.50  | 11.55 | 18.75 | 52.95 |
| ViT-B/16    | 80.07                     | 68.60        | 44.72                      | 31.22  | 24.20 | 18.81 | 51.21 |
| +RoBERTa    | 81.90                     | 70.55        | 45.41                      | 33.19  | 26.89 | 19.93 | 48.51 |
| Datasets    | [L→A]                     | Acc.         | [A→L]                      | Acc.   |       |       |       |
| IEMOCAP     | Wav2Vec2                  | 59.46        | BERT-B                     | 55.81  |       |       |       |
| + BERT-L    | 61.20                     | + Wav2Vec2-L | 56.05                      |        |       |       |       |
| Datasets    | [V→A]                     | Acc.         | [A→V]                      | Acc.   |       |       |       |
| AVMNIST     | Audio Model (ResNet type) | 41.28        | Vision Model (ResNet type) | 65.18  |       |       |       |
| + ResNet-34 | 42.44                     | + Wav2Vec2-L | 66.69                      |        |       |       |       |"
518,"| Datasets                      | $ \alpha=0 $ | $ \alpha=0.1 $ | $ \alpha=0.3 $ | $ \alpha=0.5 $ | $ \alpha=0.7 $ |
| ----------------------------- | ------------ | -------------- | -------------- | -------------- | -------------- |
| IEMOCAP [L $ \rightarrow $ A] | 59.64        | 60.34          | 61.20          | 59.87          | 59.93          |
| IEMOCAP [L $ \rightarrow $ A] | 55.81        | 55.51          | 56.49          | 54.19          | 55.90          |
| AVMNIST [V $ \rightarrow $ A] | 41.28        | 42.03          | 42.44          | 41.77          | 41.79          |
| AVMNIST [A $ \rightarrow $ V] | 65.18        | 65.03          | 66.69          | 64.77          | 64.76          |"
520,"| Given        |
| ------------ |
| Input vector |
| Module       |"
520,"|                   | CD-T    | ACDC  | EAP     | SAE     |
| ----------------- | ------- | ----- | ------- | ------- |
| Runtime           | seconds | hours | seconds | seconds |
| Is Exact Measure  | Yes     | Yes   | No      | No      |
| Requires training | No      | No    | No      | Yes     |"
521,"| Evaluation |
| ---------- |"
521,"|         | H      | DLinear | FITS   | FSNet  | OneNet | iTrans. | PatchTST | NSTrans. |
| ------- | ------ | ------- | ------ | ------ | ------ | ------- | -------- | -------- |
| Batch   | DSOF   | Batch   | DSOF   | Batch  | DSOF   | Batch   | DSOF     | Batch    |
| Learn.  | Learn. | Learn.  | Learn. | Learn. | Learn. | Learn.  |          |          |
| ECL     | 1      | 2.842   | 2.065  | 2.870  | 2.235  | 3.8e+2  | 2.330    | 3.2e+1   |
| 24      | 1.5e+1 | 4.737   | 4.509  | 4.597  | 4.7e+2 | 5.475   | 8.5e+1   | 4.510    |
| 48      | 2.7e+1 | 6.181   | 5.257  | 5.433  | 4.8e+2 | 7.000   | 1.5e+2   | 5.943    |
| ETTh2   | 1      | 0.470   | 0.365  | 0.522  | 0.375  | 1.1e+1  | 0.431    | 2.531    |
| 24      | 2.269  | 1.701   | 2.189  | 1.757  | 1.9e+1 | 3.114   | 7.017    | 2.363    |
| 48      | 3.389  | 3.082   | 3.275  | 2.988  | 2.3e+1 | 5.318   | 9.790    | 4.037    |
| ETTm1   | 1      | 0.112   | 0.105  | 0.123  | 0.111  | 0.190   | 0.121    | 0.156    |
| 24      | 0.628  | 0.525   | 0.732  | 0.542  | 1.500  | 0.609   | 1.094    | 0.418    |
| 48      | 0.818  | 0.695   | 0.900  | 0.716  | 2.279  | 0.843   | 1.588    | 0.554    |
| Ex.     | 1      | 0.009   | 0.009  | 0.012  | 0.011  | 0.034   | 0.010    | 0.024    |
| 24      | 0.098  | 0.095   | 0.095  | 0.093  | 0.754  | 0.120   | 0.487    | 0.152    |
| 48      | 0.194  | 0.192   | 0.178  | 0.176  | 1.366  | 0.270   | 0.815    | 0.289    |
| Traffic | 1      | 0.302   | 0.302  | 0.342  | 0.313  | 0.599   | 0.228    | 0.265    |
| 24      | 0.649  | 0.608   | 0.617  | 0.607  | 0.761  | 0.366   | 0.576    | 0.327    |
| 48      | 0.769  | 0.681   | 0.707  | 0.680  | 0.824  | 0.403   | 0.683    | 0.369    |
| Weather | 1      | 0.357   | 0.337  | 0.359  | 0.341  | 0.728   | 0.388    | 0.478    |
| 24      | 1.182  | 1.043   | 1.236  | 1.086  | 1.760  | 1.020   | 1.377    | 0.671    |
| 48      | 1.702  | 1.447   | 1.634  | 1.434  | 2.620  | 1.415   | 2.699    | 0.909    |"
521,"| Teacher Model | Student Model                | Framework | ECL   | ETTh2 | Traffic |
| ------------- | ---------------------------- | --------- | ----- | ----- | ------- |
| 1             | 24                           | 48        | 1     | 24    | 48      |
| Linear        | ✗                            | DGrad     | 2.187 | 9.954 | 1e+01   |
| ✗             | TFCL                         | 3.033     | 6.897 | 9.746 | 0.718   |
| ✗             | DER++                        | 2.172     | 5.369 | 7.339 | 0.385   |
| ✗             | DSOF(w/o  $ \theta^{(S)} $ ) | 2.066     | 4.759 | 6.250 | 0.365   |
| MLP           | DSOF                         | 2.065     | 4.737 | 6.181 | 0.365   |
| FSNet         | ✗                            | DGrad     | 3.197 | 5e+01 | 5e+01   |
| ✗             | TFCL                         | 2.968     | 7.482 | 8.512 | 0.726   |
| ✗             | DER++                        | 2.902     | 1e+01 | 1e+01 | 0.432   |
| ✗             | DSOF(w/o  $ \theta^{(S)} $ ) | 2.567     | 5.611 | 7.130 | 0.427   |
| MLP           | DSOF                         | 2.330     | 5.475 | 7.000 | 0.431   |
| PatchTST      | ✗                            | DGrad     | 3.863 | 9.453 | 8.607   |
| ✗             | TFCL                         | 3.805     | 7.653 | 1e+01 | 0.714   |
| ✗             | DER++                        | 3.403     | 6.846 | 8.025 | 0.386   |
| ✗             | DSOF(w/o  $ \theta^{(S)} $ ) | 3e+03     | 5.253 | 6.427 | 0.373   |
| MLP           | DSOF                         | 2.244     | 5.169 | 6.665 | 0.382   |"
522,"|                              | Discounted setting | Average setting |
| ---------------------------- | ------------------ | --------------- |
| Objective function ( $ f $ ) |                    | Unichain        |
| Linear                       | Yes                | Yes             |
| Non-linear                   | No (i)             | Yes             |"
523,"| Dataset            | ImageNet-1K (100%:73.63) |
| ------------------ | ------------------------ |
| Sampling rate      | 70%                      |
| Random             | 71.63                    |
| Moderate           | 71.33                    |
| CCS                | 70.74                    |
| $ D^{2} $  Pruning | 71.29                    |
| GraphCut           | 68.91                    |
| Entropy            | 70.93                    |
| Forgetting         | 70.57                    |
| EL2N               | 71.68                    |
| AUM                | 69.94                    |
| Variance           | 70.12                    |
| $ k $ -means       | 70.33                    |
| $ k $ -DPP         | 70.84                    |
| SES (Ours)         | 72.80                    |"
523,"| DatasetSampling rate | ANLI (100%:49.25) |
| -------------------- | ----------------- |
| 70%                  | 50%               |
| Random               | 47.08             |
| Moderate             | 46.84             |
| CCS                  | 46.56             |
| $ D^{2} $  Pruning   | 48.56             |
| GraphCut             | 46.14             |
| Entropy              | 46.32             |
| Forgetting           | 48.73             |
| EL2N                 | 48.70             |
| AUM                  | 47.86             |
| Variance             | 47.97             |
| $ k $ -means         | 46.48             |
| $ k $ -DPP           | 47.74             |
| SES (Ours)           | 49.00             |"
523,"| Dataset            | ImageNet-1K (100% :73.63) |
| ------------------ | ------------------------- |
| Sampling rate      | 70%                       |
| Random             | 71.12                     |
| Moderate           | 71.48                     |
| CCS                | 71.46                     |
| GraphCut           | 71.50                     |
| $ D^{2} $  Pruning | 71.62                     |
| Prototypicality    | 70.12                     |
| SES (Ours)         | 72.11                     |"
555,"| Model                         | Input | Accuracy |
| ----------------------------- | ----- | -------- |
| Random Chance                 | -     | 25.0     |
| Frequent Guess                | -     | 32.1     |
| Geoformer (Chen et al., 2022) | Q, I  | 46.8     |
| UniMath (Liang et al., 2023)  | Q, I  | 50.0     |
| LLaVA-7B                      | Q, I  | 18.7     |
| + SFT                         | Q, I  | 62.8     |
| + AL (tunable LLM) + SFT      | Q, I  | 60.1     |
| + AL (fixed LLM) + SFT        | Q, I  | 64.2     |"
523,"| Dataset            | Permuted | Split   | Split    | Split         | Split |
| ------------------ | -------- | ------- | -------- | ------------- | ----- |
| MNIST              | MNIST    | CIFAR10 | CIFAR100 | Tiny-ImageNet |       |
| Memory size        | 100      | 200     | 100      | 200           | 100   |
| $ D^{2} $  Pruning | 78.25    | 79.94   | 96.79    | 97.69         | 64.54 |
| GraphCut           | 76.98    | 78.61   | 91.34    | 94.25         | 61.02 |
| k-center           | 78.17    | 79.75   | 94.39    | 96.61         | 61.47 |
| Gradient Matching  | 77.30    | 79.27   | 95.39    | 97.54         | 61.65 |
| FRCL               | 77.33    | 79.21   | 94.48    | 97.10         | 61.67 |
| iCaRL              | 78.94    | 80.65   | 89.50    | 97.59         | 62.33 |
| Greedy Coreset     | 78.71    | 80.13   | 96.07    | 97.76         | 63.18 |
| BCSR               | 77.74    | 79.51   | 94.77    | 96.98         | 63.23 |
| SES (Ours)         | 79.92    | 81.18   | 96.94    | 98.28         | 68.26 |"
523,"| Module  | Sampling rate | Avg.  |
| ------- | ------------- | ----- |
| Scoring | Sampling      | 70%   |
| TD      | HS            | 94.85 |
| TD      | MP            | 94.86 |
| TD      | BNS           | 94.88 |
| SE+TD   | HS            | 94.92 |
| SE+TD   | MP            | 95.08 |
| SE+TD   | BNS           | 95.01 |"
524,"| Method                  | Automatic evaluation       | GPT evaluation           |
| ----------------------- | -------------------------- | ------------------------ |
| CLIP-Score $ \uparrow $ | DINO-Score  $ \downarrow $ | BERT-Score  $ \uparrow $ |
| T2I-Bridge              | 2.4350                     | 0.8576                   |
| SLDM                    | 2.5054                     | 0.6746                   |
| Ours                    | 2.7555                     | 0.6338                   |"
525,"| Method                             | Shot | FT | Mark       | ChestX | ISIC2018 | EuroSAT | CropDiseases | Average |
| ---------------------------------- | ---- | -- | ---------- | ------ | -------- | ------- | ------------ | ------- |
| MEM-FS (Walsh et al., 2023)        | 1    | ✗  | TIP-23     | 22.76  | 32.97    | 68.11   | 81.11        | 51.24   |
| StyleAdv (Fu et al., 2023)         | 1    | ✗  | CVPR-23    | 22.92  | 33.05    | 72.15   | 81.22        | 52.34   |
| FLoR (Zou et al., 2024a)           | 1    | ✗  | CVPR-24    | 22.78  | 34.20    | 72.39   | 81.81        | 52.80   |
| DAMIM (Ma et al., 2024)            | 1    | ✗  | AAAI-25    | 22.97  | 34.66    | 72.87   | 82.34        | 53.21   |
| CD-CLS (Zou et al., b)             | 1    | ✗  | NeurIPS-24 | 22.93  | 34.21    | 74.08   | 83.51        | 53.68   |
| AttnTemp (Zou et al., a)           | 1    | ✗  | NeurIPS-24 | 23.19  | 34.92    | 74.35   | 84.02        | 54.12   |
| ReCIT                              | 1    | ✗  | Ours       | 23.27  | 35.13    | 74.56   | 84.76        | 54.43   |
| PMF (Shell Xu, 2022)               | 1    | ✓  | CVPR-22    | 21.73  | 30.36    | 70.74   | 80.79        | 50.91   |
| FLoR (Zou et al., 2024a)           | 1    | ✓  | CVPR-24    | 23.26  | 35.49    | 73.09   | 83.55        | 53.85   |
| StyleAdv (Fu et al., 2023)         | 1    | ✓  | CVPR-23    | 22.92  | 33.99    | 74.93   | 84.11        | 53.99   |
| DAMIM (Ma et al., 2024)            | 1    | ✓  | AAAI-25    | 23.38  | 36.35    | 73.61   | 83.90        | 54.31   |
| CD-CLS (Zou et al., b)             | 1    | ✓  | NeurIPS-24 | 23.39  | 35.56    | 74.97   | 84.54        | 54.62   |
| AttnTemp (Zou et al., a)           | 1    | ✓  | NeurIPS-24 | 23.63  | 38.05    | 75.09   | 84.78        | 55.39   |
| ReCIT                              | 1    | ✓  | Ours       | 23.84  | 38.48    | 75.23   | 85.92        | 55.87   |
| MEM-FS + RDA* (Walsh et al., 2023) | 1    | ✓  | TIP-23     | 23.85  | 37.07    | 75.91   | 83.74        | 55.14   |
| DAMIM* (Ma et al., 2024)           | 1    | ✓  | AAAI-25    | 23.91  | 38.07    | 77.23   | 86.74        | 56.49   |
| CD-CLS (Zou et al., b)             | 1    | ✓  | NeurIPS-24 | 23.88  | 37.20    | 78.41   | 87.39        | 56.72   |
| AttnTemp (Zou et al., a)           | 1    | ✓  | NeurIPS-24 | 23.96  | 40.13    | 77.40   | 87.58        | 57.23   |
| ReCIT*                             | 1    | ✓  | Ours       | 24.42  | 39.39    | 78.84   | 88.45        | 57.78   |
| MEM-FS (Walsh et al., 2023)        | 5    | ✗  | TIP-23     | 26.67  | 47.38    | 86.49   | 93.74        | 63.57   |
| StyleAdv (Fu et al., 2023)         | 5    | ✗  | CVPR-23    | 26.97  | 47.73    | 88.57   | 94.85        | 64.53   |
| FLoR (Zou et al., 2024a)           | 5    | ✗  | CVPR-24    | 26.71  | 49.52    | 90.41   | 95.28        | 65.48   |
| DAMIM (Ma et al., 2024)            | 5    | ✗  | AAAI-25    | 27.28  | 50.76    | 89.50   | 95.52        | 65.77   |
| CD-CLS (Zou et al., b)             | 5    | ✗  | NeurIPS-24 | 27.23  | 50.46    | 91.04   | 95.68        | 66.10   |
| AttnTemp (Zou et al., a)           | 5    | ✗  | NeurIPS-24 | 27.72  | 53.09    | 90.13   | 95.53        | 66.62   |
| ReCIT                              | 5    | ✗  | Ours       | 28.23  | 52.36    | 90.42   | 96.02        | 66.76   |
| PMF (Shell Xu, 2022)               | 5    | ✓  | CVPR-22    | 27.27  | 50.12    | 85.98   | 92.96        | 64.08   |
| StyleAdv (Fu et al., 2023)         | 5    | ✓  | CVPR-23    | 26.97  | 51.23    | 90.12   | 95.99        | 66.08   |
| FLoR (Zou et al., 2024a)           | 5    | ✓  | CVPR-24    | 27.02  | 53.06    | 90.75   | 96.47        | 66.83   |
| DAMIM (Ma et al., 2024)            | 5    | ✓  | AAAI-25    | 27.82  | 54.86    | 91.18   | 96.34        | 67.55   |
| CD-CLS (Zou et al., b)             | 5    | ✓  | NeurIPS-24 | 27.66  | 54.69    | 91.53   | 96.27        | 67.54   |
| AttnTemp (Zou et al., a)           | 5    | ✓  | NeurIPS-24 | 28.03  | 54.91    | 90.82   | 96.66        | 67.61   |
| ReCIT                              | 5    | ✓  | Ours       | 28.88  | 54.91    | 91.58   | 96.85        | 68.06   |
| MEM-FS + RDA* (Walsh et al., 2023) | 5    | ✓  | TIP-23     | 27.98  | 51.02    | 88.77   | 95.04        | 65.70   |
| DAMIM* (Ma et al., 2024)           | 5    | ✓  | AAAI-25    | 28.10  | 55.44    | 91.08   | 96.49        | 67.78   |
| CD-CLS* (Zou et al., b)            | 5    | ✓  | NeurIPS-24 | 28.25  | 55.66    | 91.68   | 96.62        | 68.05   |
| AttnTemp* (Zou et al., a)          | 5    | ✓  | NeurIPS-24 | 28.41  | 55.22    | 91.34   | 96.74        | 67.93   |
| ReCIT*                             | 5    | ✓  | Ours       | 28.97  | 55.60    | 91.72   | 96.97        | 68.32   |"
525,"| Method                                 | CropDisease | EuroSAT    | ISIC2018   | ChestX     | Ave.  |
| -------------------------------------- | ----------- | ---------- | ---------- | ---------- | ----- |
| Basclinc                               | 94.29±0.17  | 89.43±0.17 | 45.83±0.23 | 26.07±0.17 | 63.91 |
| + Warm up disruption                   | 95.31±0.15  | 89.61±0.17 | 48.83±0.23 | 27.01±0.17 | 65.19 |
| + Balanced disruption                  | 96.02±0.14  | 90.42±0.17 | 52.36±0.23 | 28.23±0.18 | 66.76 |
| (a) Remove Position Embedding          | 95.02±0.16  | 89.96±0.16 | 47.40±0.24 | 26.85±0.17 | 64.81 |
| (b) Shuffled Patches                   | 95.03±0.15  | 88.67±0.18 | 48.04±0.23 | 27.08±0.17 | 64.70 |
| (c) Shuffled Patch Amp.                | 94.75±0.16  | 88.78±0.17 | 49.67±0.23 | 27.04±0.17 | 65.06 |
| (d) Shuffled Patch Phase               | 94.94±0.16  | 88.56±0.18 | 49.47±0.23 | 26.96±0.17 | 64.98 |
| (e) Shuffled Image Amp.                | 94.73±0.16  | 88.96±0.17 | 47.95±0.23 | 26.79±0.17 | 64.61 |
| (f) Shuffled Cluster Amp. w/ Balancing | 95.28±0.15  | 89.13±0.17 | 49.51±0.23 | 27.85±0.17 | 65.44 |
| (g) Shuffled Patch Amp. w/ Balancing   | 96.05±0.14  | 89.25±0.17 | 50.85±0.23 | 28.12±0.18 | 65.94 |"
526,"| Time   | BASE              | EVPA-full        | EVPA-169        | EVPA-30         |
| ------ | ----------------- | ---------------- | --------------- | --------------- |
| 0.02 s | -573  $ \pm $  28 | 8  $ \pm $  25   | 10  $ \pm $  26 | -4  $ \pm $  28 |
| 0.2 s  | -109  $ \pm $  52 | 96  $ \pm $  43  | 31  $ \pm $  57 | -               |
| 2 s    | 33  $ \pm $  65   | 100  $ \pm $  68 | -               | -               |"
527,"|          | Noisy Rate   | 10%          | 30%   | 50%   | 70%   |
| -------- | ------------ | ------------ | ----- | ----- | ----- |
| Metric   | ACC↑         | NMI↑         | PUR↑  | ACC↑  | NMI↑  |
| BBCSport | CANDY        | NeurIPS 2024 | 23.16 | 05.60 | 40.62 |
| RMCNC    | TKDE 2024    | 35.48        | 15.80 | 35.48 | 30.57 |
| TGM-MVC  | ACM MM 2024  | 37.28        | 08.39 | 40.84 | 36.37 |
| SCE-MVC  | NeurIPS 2024 | 47.06        | 15.96 | 47.98 | 45.40 |
| MVCAN    | CVPR 2024    | 44.85        | 18.99 | 44.85 | 39.15 |
| DIVIDE   | AAAI 2024    | 31.25        | 04.57 | 38.60 | 30.15 |
| AIRMVC   | Ours         | 49.45        | 28.63 | 49.44 | 45.59 |
| WebKB    | CANDY        | NeurIPS 2024 | 19.03 | 12.41 | 19.03 |
| RMCNC    | TKDE 2024    | 78.12        | 10.77 | 78.10 | 66.98 |
| TGM-MVC  | ACM MM 2024  | 76.86        | 13.81 | 76.86 | 71.87 |
| MVCAN    | CVPR 2024    | 77.83        | 12.39 | 78.12 | 67.46 |
| SCE-MVC  | NeurIPS 2024 | 76.78        | 18.78 | 69.65 | 74.12 |
| DIVIDE   | AAAI 2024    | 66.41        | 15.14 | 69.28 | 53.66 |
| AIRMVC   | Ours         | 83.73        | 27.15 | 83.73 | 77.93 |
| Reuters  | CANDY        | NeurIPS 2024 | 24.58 | 08.50 | 33.75 |
| RMCNC    | TKDE 2024    | 39.42        | 24.21 | 41.83 | 35.83 |
| TGM-MVC  | ACM MM 2024  | 31.28        | 09.05 | 31.53 | 30.30 |
| SCE-MVC  | NeurIPS 2024 | 43.25        | 22.89 | 43.23 | 41.25 |
| MVCAN    | CVPR 2024    | 45.25        | 23.50 | 45.50 | 37.67 |
| DIVIDE   | AAAI 2024    | 41.08        | 16.82 | 42.50 | 33.42 |
| AIRMVC   | Ours         | 48.25        | 26.34 | 48.25 | 46.67 |"
537,"| Scale        | Method       | Set5         | Set14        | BSD100       | Urban100     |
| ------------ | ------------ | ------------ | ------------ | ------------ | ------------ |
| PSNR/SSIM    | PSNR/SSIM    | PSNR/SSIM    | PSNR/SSIM    |              |              |
| $ \times $ 4 | Scratch      | 32.29/0.8965 | 28.68/0.7840 | 27.64/0.7380 | 26.21/0.7893 |
| KD           | 32.30/0.8965 | 28.70/0.7842 | 27.64/0.7382 | 26.21/0.7897 |              |
| FitNet       | 31.65/0.8873 | 28.33/0.7768 | 27.38/0.7309 | 25.40/0.7637 |              |
| AT           | 32.22/0.8952 | 28.63/0.7825 | 27.59/0.7365 | 25.97/0.7825 |              |
| RKD          | 32.30/0.8965 | 28.69/0.7842 | 27.64/0.7383 | 26.20/0.7899 |              |
| FAKD*        | 32.27/0.8960 | 28.65/0.7836 | 27.62/0.7379 | 26.18/0.7895 |              |
| CSD          | 32.34/0.8974 | 28.72/0.7856 | 27.68/0.7396 | 26.34/0.7948 |              |
| AugKD        | 32.47/0.8981 | 28.80/0.7866 | 27.71/0.7403 | 26.45/0.7963 |              |"
528,"| Model                   | Text Quality  | Perceptual Quality | Image Coherence | TIC            | Helpfulness   |
| ----------------------- | ------------- | ------------------ | --------------- | -------------- | ------------- |
| Proprietary Models      |               |                    |                 |                |               |
| Gemini1.5 + SDXL        | 3.37          | 4.34               | 3.34            | 3.98           | 3.28          |
| GPT-4o + DALL·E 3       | 3.16          | 4.44               | 3.13            | 4.39           | 3.46          |
| Open-Source Models      |               |                    |                 |                |               |
| MiniGPT-5               | 1.31          | 3.44               | 2.06            | 2.66           | 1.76          |
| GILL                    | 1.44          | 4.02               | 2.12            | 2.69           | 1.53          |
| Emu2                    | 1.33          | 2.29               | 1.71            | 1.22           | 1.87          |
| Chameleon               | 3.33          | 0.67               | 0.28            | 0.47           | 1.43          |
| Emu2 + MoSS (Ours)      | 2.61 (+96.2%) | 3.62 (+58.1%)      | 3.41 (+99.4%)   | 3.54 (+190.2%) | 2.71 (+44.9%) |
| Chameleon + MoSS (Ours) | 2.98 (-10.5%) | 2.25 (+235.8%)     | 1.05 (+275%)    | 1.7 (+261.7%)  | 1.82 (+27.3%) |"
529,"| Desiderata for General Virtual Agents | AgentStudio Environments                 | AgentStudio Tools              | AgentStudio Benchmarks |
| ------------------------------------- | ---------------------------------------- | ------------------------------ | ---------------------- |
| General grounding                     | Video observations GUI &amp; API actions | GUI element annotation         | Online benchmark tasks |
| Digital world knowledge               | Real-world devices                       | Video action annotation        | IDMBench               |
| Open-ended learning                   | Interactive environments                 | Task creation &amp; validation | CriticBench            |"
529,"| screenSpot 3.5% | Bounding Box | [1040,539,1159,587]  | [487,217,1731,377] |
| --------------- | ------------ | -------------------- | ------------------ |
|                 | Resolution   | [1280,720]           | [1920,1080]        |
|                 | Source       | mind2web_test_domain | agent_studio       |
|                 | Platform     | web                  | desktop            |"
530,"| Methods                          | MC-Muse                            | MC-RTT         | Area2-Bump    |
| -------------------------------- | ---------------------------------- | -------------- | ------------- |
|                                  | Co-aps                             | Vol-R²         | PSTH-R²       |
| Neural Decoding                  | LRNN (Stolzenburg et al., 2018)    | 0.148          | 0.317         |
| MINT (Perkin et al., 2023)       | 0.181                              | 0.616          | 0.165         |
| STNDT (Le &amp; Shlierman, 2022) | 0.982                              | 0.773          | 0.965         |
| Behavior as Prior                | pi-VAE (Zhen &amp; Wei, 2020)      | 0.214          | 0.621         |
| RNN PSID (Sami et al., 2021)     | 0.229                              | 0.693          | 0.499         |
| TNDM (Hurwitz et al., 2021)      | 0.248                              | 0.730          | 0.509         |
| BLEND                            | NDT (Ye &amp; Pandekarimath, 2021) | 0.275 (+0.0%)  | 0.779 (-0.0%) |
| NDT-Distill-Best (Ours)          | 0.310 (+12.7%)                     | 0.891 (+14.4%) | 0.592 (+7.4%) |
| LEADS (Pandarinath et al., 2018) | 0.315 (+0.0%)                      | 0.858 (-0.0%)  | 0.579 (+0.0%) |
| LFADS-Distill-Best (Ours)        | 0.321 (+1.9%)                      | 0.877 (-2.2%)  | 0.604 (+4.3%) |"
530,"| Methods                                     | Multiple Animals   | Single Animal  |
| ------------------------------------------- | ------------------ | -------------- |
| EI (2 class)                                | Subclass (5 class) | EI (2 class)   |
| Random                                      | 0.523              | 0.302          |
| PCA (Cunningham &amp; Yu, 2014)             | 0.565              | 0.330          |
| UMAP (McInnes et al., 2018)                 | 0.520              | 0.340          |
| LOLCAT (Schneider et al., 2023a)            | 0.600              | 0.404          |
| LOLCAT $ _{\text{ISI}} $  (Mi et al., 2023) | 0.640              | 0.474          |
| LOLCAT $ _{\text{Raw}} $  (Mi et al., 2023) | 0.664              | 0.439          |
| NeuPRINT (Mi et al., 2023)                  | 0.748 (+0.0%)      | 0.495 (+0.0%)  |
| NeuPRINT-Distill-Best (Ours)                | 0.789 (+5.5%)      | 0.571 (+15.4%) |"
533,"| Datasets                | Models                  | #Param.             | Clean Acc. | Certified Acc. ( $ \varepsilon $ ) |
| ----------------------- | ----------------------- | ------------------- | ---------- | ---------------------------------- |
| $ \frac{36}{255} $      | $ \frac{72}{255} $      | $ \frac{108}{255} $ |            |                                    |
| CIFAR-100               | Cayley Large            | 21M                 | 43.3       | 29.2                               |
| SOC-20                  | 27M                     | 47.8                | 34.8       | 23.7                               |
| LOT-20                  | 18M                     | 48.8                | 35.2       | 24.3                               |
| CPL XL                  | 236M                    | 47.8                | 33.4       | 20.9                               |
| AOL Large               | 136M                    | 43.7                | 33.7       | 26.3                               |
| SOC-20+CRC              | 40M                     | 51.8                | 38.5       | 27.2                               |
| SLL X-Large             | 236M                    | 47.8                | 36.7       | 28.3                               |
| Sandwich                | 26M                     | 46.3                | 35.3       | 26.3                               |
| LiResNet $ ^{\dagger} $ | 83M                     | 53.0                | 40.2       | 28.3                               |
| BRONet                  | 68M                     | 53.6                | 40.2       | 28.6                               |
| BRONet(+LA)             | 68M                     | 54.3                | 40.2       | 29.1                               |
| ImageNet                | LiResNet $ ^{\dagger} $ | 98M                 | 47.3       | 35.3                               |
| BRONet                  | 86M                     | 48.8                | 36.4       | 25.8                               |
| BRONet(+LA)             | 86M                     | 49.3                | 37.6       | 27.9                               |"
533,"| LA                 | Arch               | BRO                 | Clean Acc. | Certified Acc. ( $ \varepsilon $ ) |
| ------------------ | ------------------ | ------------------- | ---------- | ---------------------------------- |
| $ \frac{36}{255} $ | $ \frac{72}{255} $ | $ \frac{108}{255} $ |            |                                    |
| ✗                  | ✗                  | ✗                   | 47.3       | 35.3                               |
| ✓                  | ✗                  | ✗                   | 47.8       | 36.6                               |
| ✓                  | ✓                  | ✗                   | 48.8       | 37.1                               |
| ✓                  | ✓                  | ✓                   | 49.3       | 37.6                               |"
533,"| Model           | D-W     | Layer | CIFAR-100 |
| --------------- | ------- | ----- | --------- |
| Clean           | 36      | 72    | 108       |
| 255             | 255     | 255   |           |
| LipConvNet(+LA) | (10-32) | SOC   | 47.5      |
| LOT             | 49.1    | 35.5  | 24.4      |
| BRO             | 48.6    | 35.4  | 24.5      |
| (10-48)         | SOC     | 48.2  | 34.9      |
| LOT             | 49.4    | 35.8  | 24.8      |
| BRO             | 49.4    | 36.2  | 24.9      |
| (10-64)         | SOC     | 48.5  | 35.5      |
| LOT             | 49.6    | 36.1  | 24.7      |
| BRO             | 49.7    | 36.7  | 25.2      |"
534,"|          | Llama2-7B            | Ministral-8B         | Qwen2-1.5B           |
| -------- | -------------------- | -------------------- | -------------------- |
| Polytope | 91.24  $ \pm $  2.74 | 90.82  $ \pm $  0.23 | 83.96  $ \pm $  0.31 |
| MLP      | 97.28  $ \pm $  0.19 | 92.90  $ \pm $  0.13 | 87.56  $ \pm $  0.15 |
| MLP+5    | 91.08  $ \pm $  7.25 | 90.72  $ \pm $  2.31 | 86.12  $ \pm $  2.86 |"
537,"| Auxiliary samples | Label consistency | Urban100 PSNR/SSIM |
| ----------------- | ----------------- | ------------------ |
| ✗                 | ✗                 | 24.87 / 0.7431     |
| ✓                 | ✗                 | 25.20 / 0.7558     |
| ✓                 | ✓                 | 25.34 / 0.7609     |"
537,"| Training set | #Images      | Training steps        | Method        | BSD100       | Urban100     |
| ------------ | ------------ | --------------------- | ------------- | ------------ | ------------ |
| PSNR/SSIM    | PSNR/SSIM    |                       |               |              |              |
| DIV2K        | 800          | $ 2.5 \times 10^{5} $ | Scratch AugKD | 27.57/0.7356 | 25.94/0.7809 |
| 27.68/0.7390 | 26.32/0.7927 |                       |               |              |              |
| DF2K         | 3450         | $ 5 \times 10^{5} $   | Scratch KD    | 27.62/0.7372 | 26.15/0.7872 |
| 27.67/0.7390 | 26.31/0.7925 |                       |               |              |              |"
538,"| Component                                                                          | Gaussian Diffusion                                                                 | (Ours) t-Diffusion                                                                       |
| ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| Perturbation Kernel ( $ q(\mathbf{x}_{t}|\mathbf{x}_{0}) $ )                       | $ \mathcal{N}(\mu_{t}\mathbf{x}_{0},\sigma_{t}^{2}\mathbf{I}_{d}) $                | $ t_{d}(\mu_{t}\mathbf{x}_{0},\sigma_{t}^{2}\mathbf{I}_{d},\nu) $                        |
| Forward Posterior ( $ q(\mathbf{x}_{t-\Delta t}|\mathbf{x}_{t},\mathbf{x}_{0}) $ ) | $ \mathcal{N}(\bar{\mu}_{t},\bar{\sigma}_{t}^{2}\mathbf{I}_{d}) $                  | $ t_{d}(\bar{\mu}_{t},\frac{\nu|d_{1}}{\nu|d}\bar{\sigma}_{t}^{2}\mathbf{I}_{d},\nu+d) $ |
| Reverse Posterior ( $ p_{\theta}(\mathbf{x}_{t-\Delta t}|\mathbf{x}_{t}) $ )       | $ \mathcal{N}(\mu_{\theta}(\mathbf{x}_{t},t),\bar{\sigma}_{t}^{2}\mathbf{I}_{d}) $ | $ t_{d}(\mu_{\theta}(\mathbf{x}_{t},t),\bar{\sigma}_{t}^{2}\mathbf{I}_{d},\nu+d) $       |
| Divergence Measure                                                                 | $ D_{\mathrm{KL}}(q \parallel p) $                                                 | $ D_{\gamma}(q \parallel p) $                                                            |
| Generative Prior ( $ p(\mathbf{x}_{T}) $ )                                         | $ \mathcal{N}(0,\mathbf{I}_{d}) $                                                  | $ t_{d}(0,\mathbf{I}_{d},\nu) $                                                          |"
538,"|           | VIL (Train) | VIL (Test) | w20 (Train)        | w20 (Test)         |
| --------- | ----------- | ---------- | ------------------ | ------------------ |
|           | Method      | $ \nu $    | KR  $ \downarrow $ | SR  $ \downarrow $ |
| Baselines | EDM         | $ \infty $ | 210.11             | 10.79              |
| +INC      | $ \infty $  | 11.33      | 2.29               | 0.987              |
| +PCP      | $ \infty $  | 2.12       | 0.72               | 0.800              |
| Ours      | t-EDM       | 3          | 1.06               | 0.43               |
| t-EDM     | 5           | 29.66      | 4.07               | 0.955              |
| t-EDM     | 7           | 24.35      | 4.14               | 0.959              |"
538,"| VIL (Test) | w20 (Test) |
| ---------- | ---------- |
|            | Method     |
| Baselines  | EDM        |
| Ours       | t-EDM      |
| t-EDM      | 5          |"
539,"| Hom. level #          | Corn. 0.1227 183 | Wisc. 0.1778 251 | Texas 0.0609 183 | R. Emp. 0.0000 22,662 | Tolo. 0.6344 11,758 | Mine. 0.6827 10,000 | Ques. 0.8359 48,921 | A.-rat. 0.3803 24,492 | C.Full 0.5670 19,793 | PubM. 0.8024 19,717 | DBLP 0.8279 17,716 | C.ML 0.7885 2,995 |
| --------------------- | ---------------- | ---------------- | ---------------- | --------------------- | ------------------- | ------------------- | ------------------- | --------------------- | -------------------- | ------------------- | ------------------ | ----------------- |
| Discrete-depth GNNs   |                  |                  |                  |                       |                     |                     |                     |                       |                      |                     |                    |                   |
| GCN                   | 55.14 (±8.46)    | 61.60 (±7.00)    | 60.00 (±6.45)    | 71.23 (±0.22)         | 79.61 (±0.66)       | 74.79 (±1.78)       | 50.21 (±2.24)       | 37.99 (±0.61)         | 68.06 (±0.98)        | 86.74 (±0.47)       | 83.93 (±0.34)      | 87.07 (±1.21)     |
| GCN+res               | 70.11 (±10.21)   | 69.50 (±6.00)    | 71.66 (±4.13)    | 73.91 (±0.66)         | 83.44 (±0.61)       | 90.13 (±0.70)       | 75.45 (±2.31)       | 48.17 (±0.55)         | 69.53 (±0.44)        | 86.91 (±0.31)       | 82.64 (±0.51)      | 85.62 (±0.72)     |
| GAT                   | 53.64 (±11.1)    | 60.00 (±11.0)    | 61.21 (±8.17)    | 77.40 (±1.53)         | 81.45 (±0.92)       | 80.12 (±1.11)       | 65.47 (±0.88)       | 42.52 (±1.22)         | 67.55 (±1.23)        | 87.24 (±0.55)       | 80.61 (±1.21)      | 84.12 (±0.55)     |
| GAT+res               | 65.42 (±7.33)    | 72.20 (±4.00)    | 73.45 (±6.11)    | 81.55 (±0.26)         | 83.91 (±0.33)       | 92.45 (±0.77)       | 76.95 (±0.85)       | 50.00 (±0.43)         | 67.33 (±0.68)        | 87.50 (±0.40)       | 83.51 (±0.72)      | 85.11 (±0.19)     |
| DirGNN                | 76.51 (±6.14)    | 80.50 (±5.50)    | 76.25 (±6.31)    | 85.21 (±0.44)         | 82.64 (±0.75)       | 81.52 (±0.41)       | 59.95 (±0.79)       | 46.66 (±0.61)         | 67.80 (±0.53)        | 86.94 (±0.55)       | 81.22 (±0.54)      | 85.66 (±0.31)     |
| FSGNN                 | 87.43 (±3.65)    | 87.60 (±5.10)    | 85.15 (±3.91)    | 83.64 (±0.71)         | 81.01 (±0.65)       | 85.53 (±0.41)       | 71.41 (±0.32)       | 40.02 (±0.51)         | 71.90 (±0.65)        | 90.24 (±0.71)       | 83.31 (±0.55)      | 89.44 (±0.43)     |
| Continuous-depth GNNs |                  |                  |                  |                       |                     |                     |                     |                       |                      |                     |                    |                   |
| GRAND                 | 81.76 (±13.9)    | 84.00 (±7.50)    | 81.70 (±8.42)    | 60.12 (±0.75)         | 79.01 (±0.45)       | 80.56 (±3.12)       | 54.90 (±2.12)       | 37.53 (±0.36)         | 67.66 (±1.01)        | 86.79 (±0.57)       | 84.60 (±0.99)      | 88.49 (±0.81)     |
| GRAND++               | 81.34 (±7.12)    | 81.50 (±6.00)    | 79.34 (±7.22)    | 68.13 (±0.51)         | 78.85 (±0.56)       | 78.55 (±2.11)       | 60.14 (±0.88)       | 38.01 (±0.50)         | 67.53 (±0.74)        | 87.21 (±0.33)       | 85.21 (±0.24)      | 88.44 (±0.53)     |
| ACMP                  | 85.66 (±5.10)    | 86.50 (±5.00)    | 87.65 (±3.54)    | OOM                   | OOM                 | 85.11 (±1.06)       | 71.92 (±0.55)       | 37.32 (±0.64)         | OOM                  | 88.01 (±1.44)       | 82.31 (±0.44)      | 76.11 (±2.12)     |
| HiD-Net               | 83.53 (±7.10)    | 81.30 (±6.60)    | 77.11 (±6.91)    | 62.37 (±0.73)         | 79.50 (±0.71)       | 84.33 (±0.65)       | 63.77 (±0.85)       | 41.19 (±1.03)         | 68.11 (±0.64)        | 88.60 (±0.45)       | 84.92 (±0.31)      | 89.00 (±0.51)     |
| GNRF                  | 87.28 (±3.12)    | 88.00 (±2.00)    | 87.39 (±4.13)    | 86.25 (±0.46)         | 83.96 (±0.39)       | 95.03 (±0.20)       | 73.86 (±1.18)       | 46.89 (±1.08)         | 72.12 (±0.50)        | 90.37 (±0.69)       | 85.73 (±0.76)      | 89.18 (±0.19)     |
| GNRF_FBC              | 85.59 (±1.56)    | 80.00 (±10.50)   | 82.08 (±5.41)    | 75.23 (±0.68)         | 76.17 (±0.46)       | 81.61 (±1.07)       | 61.78 (±0.99)       | 41.22 (±0.43)         | 67.51 (±0.87)        | 88.96 (±0.14)       | 82.55 (±0.32)      | 87.29 (±0.55)     |
| GNRF_ABC              | 86.49 (±2.70)    | 88.00 (±2.00)    | 81.90 (±5.63)    | 76.52 (±0.33)         | 78.14 (±0.32)       | 87.25 (±1.01)       | 64.55 (±1.33)       | 41.74 (±0.46)         | 70.17 (±0.61)        | 88.21 (±0.40)       | 83.83 (±0.45)      | 89.43 (±0.22)     |"
540,"| Tasks        | BC-best   | Expert    | HiSSD (Ours) | BC-best   | Medium       |
| ------------ | --------- | --------- | ------------ | --------- | ------------ |
| UPDeT-m      | ODIS      |           | UPDeT-m      | ODIS      | HiSSD (Ours) |
| Source Tasks |           |           |              |           |              |
| 3m           | 97.7±2.6  | 82.8±16.0 | 98.4±2.7     | 99.5±0.3  | 65.4±14.7    |
| 5m6m         | 50.4±2.3  | 17.2±28.0 | 53.9±5.1     | 66.1±7.0  | 21.9±3.4     |
| 9m10m        | 95.3±1.6  | 3.1±5.4   | 80.4±8.7     | 95.5±2.7  | 63.8±10.9    |
| Unseen Tasks |           |           |              |           |              |
| 4m           | 92.1±3.5  | 33.0±27.1 | 95.3±3.5     | 99.2±1.2  | 48.8±21.1    |
| 5m           | 87.1±10.5 | 33.6±40.2 | 89.1±10.0    | 99.2±1.2  | 76.6±14.1    |
| 10m          | 90.5±3.8  | 54.7±44.4 | 93.8±2.2     | 98.4±0.8  | 56.2±20.6    |
| 12m          | 70.8±15.2 | 17.2±28.0 | 58.6±11.8    | 75.5±19.7 | 24.0±10.5    |
| 7m8m         | 18.8±3.1  | 0.0±0.0   | 25.0±15.1    | 35.3±9.8  | 1.6±1.6      |
| 8m9m         | 15.8±3.3  | 0.0±0.0   | 19.6±6.0     | 47.0±6.2  | 3.1±3.8      |
| 10m11m       | 45.3±11.1 | 0.0±0.0   | 42.4±7.2     | 86.3±14.6 | 19.7±8.9     |
| 10m12m       | 1.0±1.5   | 0.0±0.0   | 1.6±1.6      | 14.5±9.1  | 0.0±0.0      |
| 13m15m       | 0.0±0.0   | 0.0±0.0   | 2.3±2.6      | 1.3±2.5   | 0.6±1.3      |"
540,"| Tasks        | BC             | IQL            | ITD3-BC        | ODIS           | HiSSD(ours)    |
| ------------ | -------------- | -------------- | -------------- | -------------- | -------------- |
| Source Tasks |                |                |                |                |                |
| complete     | 3188.16±566.68 | 4384.23±198.55 | 4365.10±72.92  | 3677.66±174.82 | 4450.57±126.36 |
| back thigh   | 3324.22±58.49  | 3675.91±18.99  | 3685.82±40.84  | 2381.62±198.59 | 3698.38±13.98  |
| back foot    | 3079.01±355.66 | 3989.12±211.42 | 4119.48±61.00  | 2713.40±195.63 | 3197.83±6.99   |
| front thigh  | 1861.53±415.80 | 2744.63±329.00 | 2700.17±407.46 | 2684.99±249.70 | 1948.74±81.24  |
| front shin   | 1819.94±273.96 | 4048.43±363.79 | 4155.15±180.99 | 3944.78±219.72 | 3468.32±290.72 |
| Unseen Tasks |                |                |                |                |                |
| back shin    | 1964.55±268.24 | 1974.62±314.33 | 1690.40±251.77 | 3217.59±184.15 | 3472.12±91.95  |
| front foot   | 3468.40±369.40 | 3948.17±381.80 | 3683.16±419.42 | 3930.82±342.16 | 4175.29±338.96 |"
541,"|                   | UNLEARN ACC          | RETAIN ACC           | TEST ACC             | FT AUC               | AVG. GAP            |
| ----------------- | -------------------- | -------------------- | -------------------- | -------------------- | ------------------- |
| RETRAIN           | 94.49  $ \pm $  0.20 | 100.0  $ \pm $  0.00 | 94.33  $ \pm $  0.18 | 50.00  $ \pm $  0.42 | 0.00                |
| FT                | 95.16  $ \pm $  0.29 | 96.64  $ \pm $  0.25 | 92.21  $ \pm $  0.27 | 52.08  $ \pm $  0.34 | 2.06  $ \pm $  0.10 |
| RL                | 95.54  $ \pm $  0.14 | 97.47  $ \pm $  0.08 | 92.17  $ \pm $  0.10 | 51.33  $ \pm $  0.63 | 1.74  $ \pm $  0.18 |
| GA                | 98.94  $ \pm $  1.39 | 99.22  $ \pm $  1.31 | 93.39  $ \pm $  1.18 | 60.96  $ \pm $  2.93 | 4.28  $ \pm $  0.47 |
| BS                | 99.14  $ \pm $  0.31 | 99.89  $ \pm $  0.06 | 93.04  $ \pm $  0.14 | 57.85  $ \pm $  1.12 | 3.48  $ \pm $  0.32 |
| $ l_{1} $ -SPARSE | 94.29  $ \pm $  0.34 | 95.63  $ \pm $  0.16 | 91.55  $ \pm $  0.17 | 51.21  $ \pm $  0.32 | 2.16  $ \pm $  0.06 |
| SALUN             | 96.25  $ \pm $  0.21 | 98.14  $ \pm $  0.16 | 93.06  $ \pm $  0.18 | 50.88  $ \pm $  0.54 | 1.44  $ \pm $  0.12 |
| AMUN              | 95.45  $ \pm $  0.19 | 99.57  $ \pm $  0.00 | 93.45  $ \pm $  0.22 | 50.18  $ \pm $  0.36 | 0.62  $ \pm $  0.05 |
| AMUN+SalUn        | 95.02  $ \pm $  0.18 | 99.58  $ \pm $  0.04 | 93.29  $ \pm $  0.04 | 50.72  $ \pm $  0.79 | 0.68  $ \pm $  0.18 |"
541,"|            | UNLEARN ACC           | RETAIN ACC            | TEST ACC             | FT AUC               | AVG. GAP             |
| ---------- | --------------------- | --------------------- | -------------------- | -------------------- | -------------------- |
| RETRAIN    | 94.49  $ \pm $  0.20  | 100.0  $ \pm $  0.00  | 94.33  $ \pm $  0.18 | 50.00  $ \pm $  0.42 | 0.00                 |
| RL         | 100.00  $ \pm $  0.00 | 100.00  $ \pm $  0.00 | 94.45  $ \pm $  0.09 | 61.85  $ \pm $  0.25 | 4.31  $ \pm $  0.06  |
| GA         | 4.77  $ \pm $  3.20   | 5.07  $ \pm $  3.54   | 5.09  $ \pm $  3.38  | 49.78  $ \pm $  0.34 | 68.53  $ \pm $  2.45 |
| BS         | 100.00  $ \pm $  0.00 | 100.00  $ \pm $  0.00 | 94.48  $ \pm $  0.04 | 61.41  $ \pm $  0.29 | 4.20  $ \pm $  0.07  |
| SALUN      | 100.00  $ \pm $  0.00 | 100.00  $ \pm $  0.00 | 94.47  $ \pm $  0.10 | 61.09  $ \pm $  0.40 | 4.11  $ \pm $  0.09  |
| AMUN       | 94.28  $ \pm $  0.37  | 97.47  $ \pm $  0.10  | 91.67  $ \pm $  0.04 | 52.24  $ \pm $  0.23 | 1.94  $ \pm $  0.13  |
| AMUN+SalUn | 94.19  $ \pm $  0.38  | 97.71  $ \pm $  0.06  | 91.79  $ \pm $  0.12 | 51.93  $ \pm $  0.12 | 1.77  $ \pm $  0.06  |"
541,"|                      | UNLEARN ACC          | RETAIN ACC           | TEST ACC             | FT AUC               | Avg. GAP            |
| -------------------- | -------------------- | -------------------- | -------------------- | -------------------- | ------------------- |
| RETRAIN              | 82.33  $ \pm $  0.39 | 94.22  $ \pm $  0.21 | 81.72  $ \pm $  0.36 | 50.04  $ \pm $  0.34 | 0.00                |
| AMUN With  $ D_{R} $ | 82.65  $ \pm $  0.62 | 94.33  $ \pm $  0.84 | 84.99  $ \pm $  0.91 | 47.18  $ \pm $  0.50 | 1.02  $ \pm $  0.18 |
| AMUN No  $ D_{R} $   | 81.38  $ \pm $  0.10 | 87.45  $ \pm $  0.54 | 79.74  $ \pm $  0.31 | 54.61  $ \pm $  0.23 | 3.57  $ \pm $  0.24 |"
543,"| Decay Strategy | Rewards                                                                                                                                                                                              |
| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Exponential    | $ \sum_{t=0}^{T}\gamma^{t}\beta\log\frac{p_{\theta}(\mathbf{y}_{t}|\mathbf{x},\mathbf{y}_{&lt;t})}{p_{\text{ref}}(\mathbf{y}_{t}|\mathbf{x},\mathbf{y}_{&lt;t})} $                                   |
| Head           | $ \sum_{t=0}^{\gamma^{T}}\beta\log\frac{p_{\theta}(\mathbf{y}_{t}|\mathbf{x},\mathbf{y}_{&lt;t})}{p_{\text{ref}}(\mathbf{y}_{t}|\mathbf{x},\mathbf{y}_{&lt;t})} $                                    |
| Linear         | $ \sum_{t=0}^{\gamma^{T}}\left(1-\frac{t}{\gamma^{T}}\right)\beta\log\frac{p_{\theta}(\mathbf{y}_{t}|\mathbf{x},\mathbf{y}_{&lt;t})}{p_{\text{ref}}(\mathbf{y}_{t}|\mathbf{x},\mathbf{y}_{&lt;t})} $ |
| Power-Law      | $ \sum_{t=0}^{T}\frac{1}{t^{\gamma}}\beta\log\frac{p_{\theta}(\mathbf{y}_{t}|\mathbf{x},\mathbf{y}_{&lt;t})}{p_{\text{ref}}(\mathbf{y}_{t}|\mathbf{x},\mathbf{y}_{&lt;t})} $                         |"
543,"| Method       | Llama3-Instruct (8B) | Gemma2-Instruct (9B) | Mistral-NeMo-Instruct (12B) |
| ------------ | -------------------- | -------------------- | --------------------------- |
| AE2          | AH                   | MB                   | AE2                         |
| WR (%)       | LC (%)               | WR (%)               | G4-T                        |
| SFT          | 39.1                 | 40.1                 | 27.6                        |
| DPO          | 46.2                 | 47.6                 | 42.4                        |
| KTO          | 42.4                 | 44.8                 | 32.1                        |
| IPO          | 42.9                 | 46.0                 | 34.5                        |
| SamPO        | 44.4                 | 47.2                 | 35.8                        |
| D2PO (ours)  | 47.4                 | 53.5                 | 47.3                        |
| ORPO         | 37.8                 | 39.3                 | 25.5                        |
| SimPO        | 44.4                 | 50.3                 | 41.9                        |
| D2PO† (ours) | 48.0                 | 53.9                 | 46.1                        |"
543,"| Method                   | MMLU   | GSM8K  | Math   | IFEval  | ARC-C   | Hella. | Truth. | Wino. |
| ------------------------ | ------ | ------ | ------ | ------- | ------- | ------ | ------ | ----- |
| 0-shot                   | 0-shot | 0-shot | 0-shot | 25-shot | 10-shot | 0-shot | 5-shot |       |
| (a) Llama3-Instruct (8B) |        |        |        |         |         |        |        |       |
| SFT                      | 61.7   | 78.5   | 7.9    | 68.6    | 62.0    | 78.8   | 51.6   | 75.5  |
| DPO                      | 56.7   | 70.5   | 7.8    | 65.1    | 65.1    | 79.9   | 56.4   | 74.5  |
| SimPO                    | 55.2   | 57.5   | 5.3    | 60.8    | 67.6    | 78.8   | 63.8   | 74.3  |
| D²PO (ours)              | 61.4   | 72.0   | 8.5    | 65.6    | 65.8    | 79.0   | 57.6   | 75.1  |
| (b) Gemma2-Instruct (9B) |        |        |        |         |         |        |        |       |
| SFT                      | 72.8   | 87.4   | 19.4   | 71.9    | 71.8    | 81.7   | 60.2   | 77.9  |
| DPO                      | 72.2   | 88.5   | 19.4   | 60.1    | 69.9    | 71.5   | 57.7   | 72.7  |
| SimPO                    | 72.4   | 88.2   | 19.0   | 71.5    | 68.3    | 66.5   | 58.9   | 73.7  |
| D²PO (ours)              | 72.7   | 88.9   | 21.2   | 71.2    | 71.4    | 81.0   | 61.3   | 76.0  |"
543,"| Benchmark    | Win | Tie | Lose |
| ------------ | --- | --- | ---- |
| AlpacaEval 2 | 116 | 36  | 48   |
| Arena-Hard   | 107 | 62  | 31   |"
544,"| Model                                   | channels/sr | # of sampling steps | $ \omega $ | $ \nu $ | FD $ _{\text{open:13}} $ | KI $ _{\text{past}} $   $ \downarrow $ | CLAP  $ \uparrow $ | Inference speed [sec.]  $ \downarrow $ |
| --------------------------------------- | ----------- | ------------------- | ---------- | ------- | ------------------------ | -------------------------------------- | ------------------ | -------------------------------------- |
| Diffusion Models                        |             |                     |            |         |                          |                                        |                    |                                        |
| Stable Audio Open (Evans et al., 2024c) | 2/44.1kHz   | 100                 | 7.0        | -       | 78.24                    | 2.14                                   | 0.29               | -                                      |
| AudioLDM2-48kHz (Liu et al., 2024b)     | 1/48kHz     | 200                 | 3.5        | -       | 101.11                   | 2.04                                   | 0.37               | -                                      |
| Our teacher model w/ Heun Solver        | 1/44.1kHz   | 40                  | 5.0        | -       | 72.34                    | 1.74                                   | 0.36               | 2.50                                   |
| Distillation Models                     |             |                     |            |         |                          |                                        |                    |                                        |
| SoundCTM-DIT-1B                         | 1/44.1kHz   | 1                   | 5.0        | 1.0     | 84.07                    | 1.74                                   | 0.36               | 0.06                                   |
| 2                                       | 5.0         | 1.0                 | 79.96      | 1.61    | 0.38                     | 0.10                                   |                    |                                        |
| 4                                       | 5.0         | 1.0                 | 83.14      | 1.57    | 0.37                     | 0.18                                   |                    |                                        |
| 8                                       | 5.0         | 5.0                 | 79.05      | 1.29    | 0.40                     | 0.35                                   |                    |                                        |
| 16                                      | 5.0         | 5.0                 | 72.24      | 1.27    | 0.42                     | 0.71                                   |                    |                                        |"
544,"| SoundCTM                         | Method      | γ           | CLAP-LPAPS↓ |
| -------------------------------- | ----------- | ----------- | ----------- |
| VAE-reconstruction (Lower bound) | -           | 2.55 ± 0.39 |             |
| Deterministic sampling           | 0           | 3.34 ± 0.74 |             |
| Stochastic sampling              | 0.2         | 5.21 ± 0.57 |             |
| 0.5                              | 5.21 ± 0.56 |             |             |
| 1.0 (CM-sampling)                | 5.34 ± 0.65 |             |             |
| AC GT test samples               | -           | 6.07 ± 0.41 |             |"
544,"| Model                                        | # of sampling steps | $ \omega $ | $ \nu $ | FD $ _{\text{open13}} $   $ \downarrow $ | KL $ _{\text{pass}} $   $ \downarrow $ | CLAP  $ \uparrow $ |
| -------------------------------------------- | ------------------- | ---------- | ------- | ---------------------------------------- | -------------------------------------- | ------------------ |
| SoundCTM-DiT-1B w/o.  $ d_{\text{teacher}} $ | 1                   | 5.0        | 1.0     | 77.8                                     | 1.67                                   | 0.36               |
| 8                                            | 5.0                 | 5.0        | 79.6    | 1.58                                     | 0.35                                   |                    |
| 16                                           | 5.0                 | 5.0        | 69.2    | 1.28                                     | 0.41                                   |                    |
| SoundCTM-DiT-1B                              | 1                   | 5.0        | 1.0     | 77.4                                     | 1.70                                   | 0.35               |
| 8                                            | 5.0                 | 5.0        | 68.1    | 1.29                                     | 0.40                                   |                    |
| 16                                           | 5.0                 | 5.0        | 65.6    | 1.22                                     | 0.42                                   |                    |"
544,"| Model                         | # of sampling steps | $ \omega $ | $ \nu $ | FD $ _{\text{open13}} $   $ \downarrow $ | KL $ _{\text{passt}} $   $ \downarrow $ | CLAP  $ \uparrow $ |
| ----------------------------- | ------------------- | ---------- | ------- | ---------------------------------------- | --------------------------------------- | ------------------ |
| SoundCTM-DiT-1B w/o. DSM loss | 1                   | 5.0        | 1.0     | 77.9                                     | 1.74                                    | 0.35               |
| 8                             | 5.0                 | 5.0        | 93.6    | 2.28                                     | 0.28                                    |                    |
| 16                            | 5.0                 | 5.0        | 108.7   | 2.60                                     | 0.23                                    |                    |
| SoundCTM-DiT-1B               | 1                   | 5.0        | 1.0     | 77.4                                     | 1.70                                    | 0.35               |
| 8                             | 5.0                 | 5.0        | 68.1    | 1.29                                     | 0.40                                    |                    |
| 16                            | 5.0                 | 5.0        | 65.6    | 1.22                                     | 0.42                                    |                    |"
544,"| Model                        | channels/sr | # of sampling steps | $ \omega $ | $ \nu $ | FD $ _{\text{open3}} $   $ \downarrow $ | KL $ _{\text{pass}} $   $ \downarrow $ | CLAP  $ \uparrow $ |
| ---------------------------- | ----------- | ------------------- | ---------- | ------- | --------------------------------------- | -------------------------------------- | ------------------ |
| Diffusion Models             |             |                     |            |         |                                         |                                        |                    |
| UNet-Teacher w/. Heun Solver | 1/44.1kHz   | 40                  | 5.0        | -       | 72.2                                    | 1.49                                   | 0.48               |
| DiT-Teacher w/. Heun Solver  | 1/44.1kHz   | 40                  | 5.0        | -       | 68.8                                    | 1.59                                   | 0.46               |
| Distillation Models          |             |                     |            |         |                                         |                                        |                    |
| SoundCTM-UNet-1B             | 1/44.1kHz   | 1                   | 5.0        | 1.0     | 89.8                                    | 2.12                                   | 0.26               |
| 16                           | 5.0         | 5.0                 | 65.1       | 1.27    | 0.43                                    |                                        |                    |
| SoundCTM-DiT-1B              | 1/44.1kHz   | 1                   | 5.0        | 1.0     | 77.4                                    | 1.70                                   | 0.35               |
| 16                           | 5.0         | 5.0                 | 65.6       | 1.22    | 0.42                                    |                                        |                    |"
545,"| Model             | Direct | MedLAMA                    |
| ----------------- | ------ | -------------------------- |
| LLMEval           |        |                            |
| Llama2-7B         | +6.4   | +8.3 $ \uparrow $ 29.7%    |
| Vicuna-7B         | +26.4  | +18.0 $ \downarrow $ 31.7% |
| Vicuna-13B        | +27.0  | +19.3 $ \downarrow $ 28.5% |
| Gemma-7B          | +23.3  | +11.1 $ \downarrow $ 52.3% |
| Llama3-8B         | +28.5  | +19.1 $ \downarrow $ 33.1% |
| Llama2-70B        | +32.0  | +19.2 $ \downarrow $ 39.9% |
| ClinicalCamel-70B | +34.8  | +23.7 $ \downarrow $ 31.9% |
| Meditron-70B      | +29.4  | +20.0 $ \downarrow $ 32.1% |
| Med42-70B         | +31.8  | +19.3 $ \downarrow $ 39.3% |
| Llama3-70B        | +36.6  | +26.9 $ \downarrow $ 26.5% |
| GPT-3.5-turbo     | +32.1  | +26.7 $ \downarrow $ 16.9% |
| GPT-4o $ ^{*} $   | +35.8  | +34.0 $ \downarrow $ 4.9%  |"
579,"| Top-one Accuracy | GELU      | ELU      | PReLU    | CELU     | SiLU     | Mish     | CRReLU   |
| ---------------- | --------- | -------- | -------- | -------- | -------- | -------- | -------- |
| CIFAR-10         | ViT-Tiny  | 70.4±0.2 | 66.4±0.5 | 78.0±0.6 | 66.5±0.6 | 68.6±0.3 | 68.7±0.3 |
| CIFAR-10         | DeiT-Tiny | 72.4±0.7 | 67.6±0.6 | 75.4±0.1 | 67.7±0.8 | 69.9±0.5 | 70.2±0.6 |
| CIFAR-10         | TNT-Small | 73.7±0.5 | 69.5±0.6 | 75.8±0.3 | 68.7±0.2 | 71.1±0.7 | 71.6±0.8 |"
545,"| PretexEval  | Direct | LLEval      | PretexEval  |
| ----------- | ------ | ----------- | ----------- |
| +3.1↓52.3%  | +11.7  | +2.7↓76.6%  | +2.8↓76.3%  |
| +7.5↓71.5%  | +9.9   | +10.9↑9.7%  | +3.9↓60.5%  |
| +10.7↓60.5% | +12.5  | +7.4↓40.2%  | +5.7↓53.9%  |
| +9.4↓59.5%  | +9.0   | +4.8↓46.5%  | +5.0↓45.0%  |
| +16.6↓41.8% | +17.9  | +15.3↓14.3% | +9.3↓48.3%  |
| +13.8↓56.9% | +20.5  | +17.3↓15.7% | +9.0↓56.0%  |
| +21.9↓37.2% | +24.5  | +20.6↓15.7% | +16.1↓34.4% |
| +14.7↓49.8% | +21.1  | +12.8↓39.4% | +10.2↓51.5% |
| +20.0↓37.1% | +23.3  | +19.1↓18.1% | +14.8↓36.6% |
| +26.9↓26.6% | +29.7  | +28.2↓5.1%  | +20.9↓29.7% |
| +16.2↓49.7% | +23.5  | +17.6↓25.4% | +10.3↓56.4% |
| +31.7↓11.5% | +31.3  | +29.9↓4.3%  | +26.7↓14.5% |"
545,"| Knowledge Base               | Method            | ClinCamel-70B | Llama3-70B | GPT-4o |
| ---------------------------- | ----------------- | ------------- | ---------- | ------ |
| MedLAMA                      | PretexEval (Ours) | +21.9         | +26.9      | +31.7  |
| w/o Predicate Transformation | +30.6             | +33.0         | +36.0      |        |
| w/o LLM Rephrasing           | +22.8             | +30.4         | +33.8      |        |
| DiseK                        | PretexEval (Ours) | +16.1         | +20.9      | +26.7  |
| w/o Predicate Transformation | +23.1             | +27.8         | +29.3      |        |
| w/o LLM Rephrasing           | +18.0             | +24.0         | +30.4      |        |"
546,"| Method  | STL-10      | SVHN        | FMNIST      | CIFAR-10    |
| ------- | ----------- | ----------- | ----------- | ----------- |
| UB-EXP  | 28.84±0.54% | 88.93±0.17% | 87.96±0.08% | 62.90±0.06% |
| UB-LOG  | 20.41±0.46% | 89.59±0.08% | 87.59±0.14% | 70.28±0.12% |
| SCL-EXP | 31.03±0.61% | 88.66±0.20% | 88.31±0.09% | 72.35±0.10% |
| SCL-LOG | 30.74±0.72% | 89.26±0.24% | 88.03±0.10% | 79.87±0.14% |
| POCR    | 34.96±0.32% | 96.65±0.14% | 92.29±0.07% | 94.15±0.09% |
| SELF-CL | 30.87±0.72% | 90.13±0.23% | 84.86±0.10% | 88.95±0.22% |
| ComCo   | 32.43±0.28% | 91.41±0.35% | 85.42±0.40% | 89.36±0.76% |
| Ours    | 55.25±0.36% | 97.58±0.18% | 93.38±0.06% | 94.78±0.12% |"
546,"| Method  | STL-10      | SVHN        | FMNIST      | CIFAR-10    | CIFAR-100   |
| ------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| UB-EXP  | 60.85±0.12% | 95.23±0.09% | 92.34±0.28% | 91.13±0.23% | 34.43±0.08% |
| UB-LOG  | 62.84±0.17% | 94.76±0.07% | 91.84±0.29% | 92.01±0.21% | 52.76±0.15% |
| SCL-EXP | 62.96±0.10% | 95.28±0.14% | 92.20±0.27% | 91.85±0.25% | 47.81±0.09% |
| SCL-LOG | 61.60±0.14% | 94.88±0.16% | 91.51±0.25% | 92.67±0.18% | 49.40±0.19% |
| POCR    | 74.51±0.29% | 97.14±0.09% | 94.76±0.26% | 96.09±0.27% | 53.16±0.11% |
| SELF-CL | 69.85±0.20% | 91.58±0.30% | 94.92±0.21% | 92.23±0.16% | 57.65±0.25% |
| ComCo   | 73.28±0.19% | 95.41±0.23% | 92.01±0.16% | 91.38±0.73% | 57.88±0.95% |
| Ours    | 77.11±0.14% | 98.13±0.11% | 95.16±0.13% | 96.80±0.28% | 64.33±0.43% |"
547,"| Deep Learning                   | Algebraic Geometry                                   |
| ------------------------------- | ---------------------------------------------------- |
| sample complexity, expressivity | dimension, degree                                    |
| implicit bias, subnetworks      | singularities                                        |
| identifiability                 | parametrization symmetries                           |
| learning, optimization          | distance degree, discriminants, dynamical invariants |"
548,"| Method           | Bi-TSP20  | Bi-TSP50  | Bi-TSP100  |
| ---------------- | --------- | --------- | ---------- |
| HV↑              | Gap↓      | Time↓     | HV↑        |
| Non-learnable    | WS-LKH    | 0.6270    | 0.02%      |
| MOEA/D           | 0.6241    | 0.48%     | 1.7h       |
| NSGA-II          | 0.6258    | 0.21%     | 6.0h       |
| MOGLS            | 0.6279    | -0.13%    | 1.6h       |
| PPLS/D-C         | 0.6256    | 0.24%     | 26m        |
| Multi-model      | DRL-MOA   | 0.6257    | 0.22%      |
| MDRL             | 0.6271    | 0.00%     | 5s         |
| EMNH             | 0.6271    | 0.00%     | 5s         |
| Single-model     | PMOCO     | 0.6259    | 0.19%      |
| WE-Add           | 0.6270    | 0.02%     | 6s         |
| WE-CA            | 0.6270    | 0.02%     | 6s         |
| Multi-model-Aug  | MDRL-Aug  | 0.6271    | 0.00%      |
| EMNH-Aug         | 0.6271    | 0.00%     | 33s        |
| Single-model-Aug | PMOCO-Aug | 0.6270    | 0.02%      |
| WE-Add-Aug       | 0.6272    | -0.02%    | 1.1m       |
| WE-CA-Aug        | 0.6271    | 0.00%     | 1.1m       |
| Method           | Bi-CVRP20 | Bi-CVRP50 | Bi-CVRP100 |
|                  | HV↑       | Gap↓      | Time↓      |
| Non-learnable    | MOEA/D    | 0.4255    | 1.07%      |
| NSGA-II          | 0.4275    | 0.60%     | 6.4h       |
| MOGLS            | 0.4278    | 0.53%     | 9.0h       |
| PPLS/D-C         | 0.4287    | 0.33%     | 1.6h       |
| Multi-model      | DRL-MOA   | 0.4287    | 0.33%      |
| MDRL             | 0.4291    | 0.23%     | 8s         |
| EMNH             | 0.4299    | 0.05%     | 7s         |
| Single-model     | PMOCO     | 0.4267    | 0.79%      |
| WE-Add           | 0.4292    | 0.21%     | 6s         |
| WE-CA            | 0.4293    | 0.19%     | 6s         |
| Multi-model-Aug  | MDRL-Aug  | 0.4294    | 0.16%      |
| EMNH-Aug         | 0.4302    | -0.02%    | 11s        |
| Single-model-Aug | PMOCO-Aug | 0.4294    | 0.16%      |
| WE-Add-Aug       | 0.4300    | 0.02%     | 14s        |
| WE-CA-Aug        | 0.4301    | 0.00%     | 14s        |"
548,"| Method          | Bi-TSP20           | Bi-TSP50            | Bi-TSP100       | Bi-TSP Avg. Gap $ \downarrow $  |
| --------------- | ------------------ | ------------------- | --------------- | ------------------------------- |
| HV $ \uparrow $ | Gap $ \downarrow $ | Time $ \downarrow $ | HV $ \uparrow $ | Gap $ \downarrow $              |
| MORAM           | 0.6216             | 0.88%               | 1s              | 0.6255                          |
| CNH             | 0.6270             | 0.02%               | 14s             | 0.6387                          |
| PMOCO-50        | 0.6262             | 0.14%               | 6s              | 0.6351                          |
| PMOCO-U         | 0.6111             | 2.55%               | 7s              | 0.5939                          |
| WE-CA-50        | 0.6267             | 0.06%               | 6s              | 0.6391                          |
| WE-CA-U         | 0.6270             | 0.02%               | 7s              | 0.6392                          |
| CNH-Aug         | 0.6271             | 0.00%               | 1.5m            | 0.6410                          |
| PMOCO-50-Aug    | 0.6270             | 0.02%               | 1.0m            | 0.6395                          |
| PMOCO-U-Aug     | 0.6253             | 0.29%               | 1.0m            | 0.6126                          |
| WE-CA-50-Aug    | 0.6272             | -0.02%              | 1.0m            | 0.6412                          |
| WE-CA-U-Aug     | 0.6271             | 0.00%               | 1.0m            | 0.6413                          |
| Method          | Bi-CVRP20          | Bi-CVRP50           | Bi-CVRP100      | Bi-CVRP Avg. Gap $ \downarrow $ |
| HV $ \uparrow $ | Gap $ \downarrow $ | Time $ \downarrow $ | HV $ \uparrow $ | Gap $ \downarrow $              |
| CNH             | 0.4287             | 0.33%               | 15s             | 0.4087                          |
| PMOCO-50        | 0.4191             | 2.56%               | 9s              | 0.4036                          |
| PMOCO-U         | 0.4275             | 0.60%               | 8s              | 0.4068                          |
| WE-CA-50        | 0.4238             | 1.46%               | 7s              | 0.4090                          |
| WE-CA-U         | 0.4290             | 0.26%               | 7s              | 0.4089                          |
| CNH-Aug         | 0.4299             | 0.05%               | 22s             | 0.4101                          |
| PMOCO-50-Aug    | 0.4270             | 0.72%               | 15s             | 0.4080                          |
| PMOCO-U-Aug     | 0.4296             | 0.12%               | 15s             | 0.4095                          |
| WE-CA-50-Aug    | 0.4277             | 0.56%               | 15s             | 0.4104                          |
| WE-CA-U-Aug     | 0.4300             | 0.02%               | 14s             | 0.4103                          |"
551,"|           | 1k_C100 | 5k_C100 | 10k_C100 | 1k_C500 | 5k_C500 | 10k_C500 | Avg   |
| --------- | ------- | ------- | -------- | ------- | ------- | -------- | ----- |
| First Fit | 5.32%   | 4.40%   | 4.44%    | 4.97%   | 4.27%   | 4.28%    | 4.61% |
| Best Fit  | 4.87%   | 4.08%   | 4.09%    | 4.50%   | 3.91%   | 3.95%    | 4.23% |
| FunSearch | 3.78%   | 0.80%   | 0.33%    | 6.75%   | 1.47%   | 0.74%    | 2.31% |
| EOH       | 4.48%   | 0.88%   | 0.83%    | 4.32%   | 1.06%   | 0.97%    | 2.09% |
| Ours      | 3.58%   | 0.85%   | 0.41%    | 3.67%   | 0.82%   | 0.42%    | 1.63% |"
551,"|          | rd100  | pr124  | bier127 | kroA150 | u159   | kroB200 | Avg    |
| -------- | ------ | ------ | ------- | ------- | ------ | ------- | ------ |
| NI       | 19.91% | 15.50% | 23.21%  | 18.17%  | 23.59% | 24.10%  | 20.75% |
| FI       | 9.38%  | 4.43%  | 8.04%   | 8.54%   | 11.15% | 7.54%   | 8.18%  |
| Or-Tools | 0.01%  | 0.55%  | 0.66%   | 0.02%   | 1.75%  | 2.57%   | 0.93%  |
| AM       | 3.41%  | 3.68%  | 5.91%   | 3.78%   | 7.55%  | 7.11%   | 5.24%  |
| POMO     | 0.01%  | 0.60%  | 13.72%  | 0.70%   | 0.95%  | 1.58%   | 2.93%  |
| LEHD     | 0.01%  | 1.11%  | 4.76%   | 1.40%   | 1.13%  | 0.64%   | 1.51%  |
| EOH      | 0.01%  | 0.00%  | 0.42%   | 0.29%   | -0.01% | 0.26%   | 0.16%  |
| Ours     | 0.01%  | 0.00%  | 0.01%   | 0.00%   | -0.01% | 0.44%   | 0.08%  |"
551,"|               | SC1(Min) | SC2(Min) | MVC1(Min) | MVC2(Min) | MIS1(Max) | MIS2(Max) | MIKS1(Max) | MIKS2(Max) |
| ------------- | -------- | -------- | --------- | --------- | --------- | --------- | ---------- | ---------- |
| Random-LNS    | 16140.6  | 169417.5 | 27031.4   | 276467.5  | 22892.9   | 223748.6  | 36011.0    | 351964.2   |
| ACP           | 17672.1  | 182359.4 | 26877.2   | 274013.3  | 23058.0   | 226498.2  | 34190.8    | 332235.6   |
| CL-LNS        | -        | -        | 31285.0   | -         | 15000.0   | -         | -          | -          |
| Gurobi        | 17934.5  | 320240.4 | 28151.3   | 283555.8  | 21789.0   | 216591.3  | 32960.0    | 329642.4   |
| SCIP          | 25191.2  | 385708.4 | 31275.4   | 491042.9  | 18649.9   | 9104.3    | 29974.7    | 168289.9   |
| GNN&amp;GBDT  | 16728.8  | 252797.2 | 27107.9   | 271777.2  | 22795.7   | 227006.4  | *          | *          |
| Light-MILPOPT | 16108.1  | 160015.5 | 26950.7   | 269571.5  | 22966.5   | 230432.9  | 36125.5    | 362265.1   |
| LLM-LNS(Ours) | 15802.7  | 158878.9 | 26725.3   | 268033.7  | 23169.3   | 231636.9  | 36479.8    | 363749.5   |"
552,"| Dataset                        | Method | Inference Steps | PickScore | CLIP Score | ImageReward | Aesthetic Score |
| ------------------------------ | ------ | --------------- | --------- | ---------- | ----------- | --------------- |
| Pickaic Test                   | SDXL   | 50              | 22.30     | 0.3713     | 0.8556      | 0.6060          |
| SDXL-DPO(Wallace et al., 2024) | 50     | 22.40           | 0.4797    | 1.0078     | 0.6040      |                 |
| SDXL-SFT                       | 50     | 22.25           | 0.3693    | 0.8665     | 5.921       |                 |
| SDXL-RPO (Gu et al., 2024)     | 50     | 22.65           | 0.3723    | 0.9623     | 6.012       |                 |
| SDXL-SPO (Liang et al., 2024)  | 50     | 22.70           | 0.3527    | 0.9417     | 6.283       |                 |
| SDXL-MaPO (Hong et al., 2024)  | 50     | 22.50           | 0.3735    | 0.9481     | 6.170       |                 |
| SDXL-DMD2                      | 4      | 22.35           | 0.3679    | 0.9363     | 5.937       |                 |
| SDXL-DPO + DMD2-LoRA           | 4      | 22.20           | 0.3673    | 0.9287     | 5.759       |                 |
| Offline-PSO w/ SDXL-DMD2       | 4      | 22.46           | 0.3690    | 0.9381     | 5.994       |                 |
| Online-PSO w/ SDXL-DMD2        | 4      | 22.73           | 0.3671    | 0.9773     | 6.077       |                 |
| Parti-Prompts                  | SDXL   | 50              | 22.77     | 0.3607     | 0.9142      | 5.750           |
| SDXL-DPO                       | 50     | 22.92           | 0.3674    | 1.1180     | 5.795       |                 |
| SDXL-SFT                       | 50     | 22.85           | 0.3610    | 0.8565     | 5.675       |                 |
| SDXL-RPO (Gu et al., 2024)     | 50     | 22.98           | 0.3670    | 1.0770     | 5.872       |                 |
| SDXL-SPO (Liang et al., 2024)  | 50     | 23.27           | 0.3428    | 1.0668     | 6.083       |                 |
| SDXL-MaPO (Hong et al., 2024)  | 50     | 22.81           | 0.3661    | 1.0315     | 5.912       |                 |
| SDXL-DMD2                      | 4      | 22.99           | 0.3607    | 1.0713     | 5.671       |                 |
| SDXL-DPO + DMD2-LoRA           | 4      | 22.76           | 0.3644    | 1.0638     | 5.513       |                 |
| Offline-PSO w/ SDXL-DMD2       | 4      | 23.07           | 0.3649    | 1.0964     | 5.715       |                 |
| Online-PSO w/ SDXL-DMD2        | 4      | 23.29           | 0.3634    | 1.1702     | 5.836       |                 |"
552,"| Dataset                         | Method            | Inference Steps | PickScore | CLIP Score | ImageReward | Aesthetic Score |
| ------------------------------- | ----------------- | --------------- | --------- | ---------- | ----------- | --------------- |
| PickaPc Test                    | SDXL-Turbo-4step  | 4               | 22.22     | 0.3610     | 0.9300      | 5.987           |
| Offline-PSO w/ SDXL-Turbo-4step | 4                 | 22.40           | 0.3634    | 0.9695     | 6.029       |                 |
| Online-PSO w/ SDXL-Turbo-4step  | 4                 | 22.71           | 0.3647    | 0.9882     | 6.157       |                 |
| SDXL-Turbo-1step                | 1                 | 22.29           | 0.3642    | 0.8830     | 6.061       |                 |
| Offline-PSO w/ SDXL-Turbo-1step | 1                 | 22.40           | 0.3663    | 0.9073     | 6.072       |                 |
| Online-PSO w/ SDXL-Turbo-1step  | 1                 | 22.62           | 0.3661    | 0.9113     | 6.137       |                 |
| Parti-Prompts                   | SDXL-Turbo-4-step | 4               | 22.88     | 0.3594     | 1.0173      | 5.709           |
| Offline-PSO w/ SDXL-Turbo-4step | 4                 | 22.96           | 0.3642    | 1.0509     | 5.746       |                 |
| Online-PSO w/ SDXL-Turbo-4step  | 4                 | 23.23           | 0.3632    | 1.0893     | 5.837       |                 |
| SDXL-Turbo-1-step               | 1                 | 22.78           | 0.3596    | 0.9246     | 5.706       |                 |
| Offline-PSO w/ SDXL-Turbo-1step | 1                 | 22.86           | 0.3631    | 0.9664     | 5.755       |                 |
| Online-PSO w/ SDXL-Turbo-1step  | 1                 | 22.96           | 0.3632    | 0.9762     | 5.795       |                 |"
553,"| Model          | Temporal EO |
| -------------- | ----------- |
| LLaVA          | ✗ ✗         |
| LLaVA-1.5      | ✗ ✗         |
| VideoChatGPT   | ✓ ✗         |
| Video-LLaVA    | ✓ ✗         |
| Chat-UniVi     | ✓ ✗         |
| LLaMA-VID      | ✓ ✗         |
| Video-LLaMA    | ✓ ✗         |
| ST-LLM         | ✓ ✗         |
| RSGPT          | ✗ ✓         |
| GeoChat        | ✗ ✓         |
| SkyEyeGPT      | ✗ ✓         |
| LHRS-BOT       | ✗ ✓         |
| VHM            | ✗ ✓         |
| TEOChat (Ours) | ✓ ✓         |"
553,"| Task                                        | Dataset/Subtask                         | Specialist | Video-LLaVA | GeoChat | TEOChat-T | TEOChat |
| ------------------------------------------- | --------------------------------------- | ---------- | ----------- | ------- | --------- | ------- |
| TSC                                         | fMoW RGB (Acc.)                         | 65.9       | 16.6        | 59.2    | 73.5      | 75.1    |
| fMoW Sentinel (Acc.)                        | -                                       | 4.9        | 26.3        | 45.5    | 45.5      |         |
| CD                                          | $  \mathbf{x}BD \text{ Loc. (F}_{1})  $ | 85.9       | 7.0         | 7.4     | 32.9      | 38.9    |
| $  \mathbf{x}BD \text{ Dmg Cls. (F}_{1})  $ | 26.5                                    | 7.0        | 11.8        | 49.4    | 50.0      |         |
| S2Looking Det. (F1)                         | 26.5                                    | 8.0        | 7.7         | 30.6    | 34.5      |         |
| SRE                                         | $  \mathbf{x}BD \text{ (F}_{1})  $      | -          | 3.7         | 8.7     | 18.3      | 25.1    |
| S2Looking (F1)                              | -                                       | 1.7        | 4.5         | 31.0    | 32.9      |         |
| QA                                          | $  \mathbf{x}BD \text{ (Acc.) }  $      | -          | 30.8        | 34.0    | 89.8      | 89.9    |
| S2Looking (Acc.)                            | -                                       | 41.5       | 51.5        | 75.4    | 73.4      |         |
| RQA                                         | $  \mathbf{x}BD \text{ (Acc.) }  $      | -          | 57.3        | 63.6    | 93.7      | 94.0    |
| S2Looking (Acc.)                            | -                                       | 47.8       | 67.2        | 86.7    | 90.0      |         |
| QFabric [2 images] (F1)                     | 77.0                                    | 22.6       | 20.9        | 63.9    | 66.7      |         |
| QFabric [5 images] (F1)                     | 81.6                                    | 26.5       | 21.9        | 72.8    | 74.3      |         |
| TRE                                         | QFabric (Acc.)                          | -          | 1.9         | -       | 73.1      | 74.9    |
| RTQA                                        | QFabric (Acc.)                          | -          | 26.5        | -       | 71.4      | 71.7    |
| QFabric [5 images] (F1)                     | 54.5                                    | 10.5       | 12.5        | 65.2    | 66.4      |         |"
553,"| LLM/Projector Initialization | Image Encoder Initialization | Projector  | Image References | Fine-tuned on TEOChatlas | Train Iter | fMoW RGB | xBD Dmg Cls. | S2Looking Det. | QFabric RQA-2 | QFabric RQA-5 |
| ---------------------------- | ---------------------------- | ---------- | ---------------- | ------------------------ | ---------- | -------- | ------------ | -------------- | ------------- | ------------- |
| Video-LLaVA                  | CLIP                         | Frozen     | -                | -                        | 7k         | 16.6     | 8.3          | 6.2            | 22.6          | 26.5          |
| Video-LLaVA                  | CLIP                         | Frozen     | -                | ✓                        | 7k         | 63.2     | 22.2         | 9.9            | 59.0          | 65.7          |
| LLaVA                        | CLIP                         | Frozen     | ✓                | ✓                        | 7k         | 66.8     | 8.9          | 4.8            | 58.7          | 64.6          |
| Video-LLaVA                  | SkyScript                    | Frozen     | ✓                | ✓                        | 7k         | 55.6     | 3.7          | 4.0            | 29.9          | 50.0          |
| Video-LLaVA                  | SkyScript                    | Fine-tuned | ✓                | ✓                        | 7k         | 68.3     | 40.8         | 12.5           | 59.0          | 66.6          |
| Video-LLaVA                  | CLIP                         | Frozen     | ✓                | ✓                        | 7k         | 69.2     | 40.6         | 18.1           | 61.4          | 67.9          |
| Video-LLaVA                  | CLIP                         | Fine-tuned | ✓                | ✓                        | 7k         | 71.7     | 47.2         | 22.7           | 61.7          | 69.2          |
| Video-LLaVA                  | CLIP                         | Frozen     | ✓                | ✓                        | 14k        | 71.5     | 45.8         | 30.4           | 66.6          | 74.3          |
| Video-LLaVA                  | CLIP                         | Fine-tuned | ✓                | ✓                        | 14k        | 75.1     | 50.0         | 33.6           | 66.7          | 74.3          |"
553,"| Model       | AID   | UCMerced    | LRBEN | HRBEN | Average |
| ----------- | ----- | ----------- | ----- | ----- | ------- |
| Pres.       | Comp. | Rural/Urban | Pres. | Comp. |         |
| Video-LLaVA | 52.4  | 46.5        | 55.8  | 65.1  | 61.0    |
| RSVQA       | -     | -           | 87.5  | 81.5  | 90.0    |
| RSGPT       | -     | -           | 91.2  | 91.7  | 94.0    |
| LHRS-Bot    | 91.3  | -           | 88.5  | 90.0  | 89.1    |
| VHM         | 91.7  | -           | 91.1  | 92.0  | 95.0    |
| GeoChat     | 72.0  | 84.4        | 91.1  | 90.3  | 94.0    |
| TEOChat     | 80.9  | 86.3        | 91.7  | 92.7  | 94.0    |"
553,"| Model       | ABCD | CDVQA |
| ----------- | ---- | ----- |
| Specialist  | 95.2 | 63.4  |
| Video-LLaVA | 50.0 | 29.8  |
| TEOChat     | 85.6 | 47.2  |"
554,"| Name                               | Data Acquisition  | Training &amp; Inference                             | Release              |
| ---------------------------------- | ----------------- | ---------------------------------------------------- | -------------------- |
| Domain                             | Task              | Backbone                                             | Annotation           |
| DEEPMIND PRM (Uesato et al., 2022) | Math              | GSM8K                                                | N/A                  |
| OPENAI PRM (Lightman et al., 2023) | Math              | MATH                                                 | GPT-4                |
| TS-LLM (Feng et al., 2023)         | Math Decision     | GSM8K,Game24, ProntoQA,RLHF, Chess Endgame           | LLaMA2-7B            |
| MATH-SHEPHERD (Wang et al., 2024b) | Math              | GSM8K                                                | LLemma-7B            |
| GLORE (Havrilla et al., 2024)      | Math              | GSM8K                                                | Llama2-7B            |
| MIPS (Wang et al., 2024f)          | Math/Code         | GSM8K,MATH MBPP                                      | PaLM 2-S/L           |
| MCTS-DPO (Xie et al., 2024)        | Math Common-sense | GSM8K,MATH ARC,AI2Science, OpenBookQA, CommonSenseQA | Mistral-7B           |
| SUPER MARIO (Chen et al., 2024a)   | Math              | GSM8K,MATH                                           | DeepSeek-MathBase-7B |
| STEP-DPO (Lai et al., 2024)        | Math              | MetaMath, MMIQC                                      | Qwen2-7B/72B         |
| OMEGAPRM (Luo et al., 2024)        | Math              | MATH                                                 | Gemini Pro           |
| REST-MCTS* (Zhang et al., 2024)    | Math              | MATH                                                 | Mistral-7B           |"
583,"|              | GPT-4o      | OpenAI o1  | DeepSeek R1 |
| ------------ | ----------- | ---------- | ----------- |
| Accuracy     | Agent-Level | Step-Level | Agent-Level |
| All-at-Once  | 54.31       | 4.39       | 41.38       |
| Step-by-Step | 33.62       | 7.90       | 36.21       |"
554,"| Model / Task                          | Training Data      | UltraFeedback Test | RewardBench | PRM800k Test |
| ------------------------------------- | ------------------ | ------------------ | ----------- | ------------ |
| Overall                               | Chat               | Chat Hard          | Safety      | Reasoning    |
| Open-Source ORMs                      |                    |                    |             |              |
| ULTRARM-13B                           | UltraFeedback (UF) | 74.8               | 68.5        | 96.4         |
| LLAMA-3-8B*                           | UF Binarized       | 77.8               | 84.8        | 97.5         |
| EURUS-RM-7B                           | UltraInteract      | 73.5               | 82.8        | 98.0         |
| FSFAIRX-7B                            | Mixture Preference | 74.5               | 84.4        | 99.4         |
| INTERN2-7B-RM                         | Unknown            | 77.4               | 87.6        | 99.2         |
| LLAMA-3-8B*                           | HelpSteer (HS) 2   | 71.8               | 86.8        | 95.3         |
| Open-Source PRMs (Merely Math Domain) |                    |                    |             |              |
| MS-7B-PRM                             | Math-Shepherd      | 53.5               | 56.6        | 62.3         |
| TS-LLM                                | Unknown            | 52.7               | 57.6        | 66.8         |
| LLAMA-3-8B*                           | Math-Step-DPO      | 70.2               | 73.2        | 98.0         |
| OPENPRM (FsfairX)                     | UF Tree + HS 2     | 72.8               | 89.4        | 95.5         |
| OPENPRM (InternRM)                    | UF Tree + HS 2     | 78.5               | 91.1        | 98.0         |"
554,"| Model / Task                 | Alpaca Eval 2 | MixEval | IFEval | GPQA  | MMLU-P* | MATH* |
| ---------------------------- | ------------- | ------- | ------ | ----- | ------- | ----- |
| LC%                          | WC%           | Acc     | Avg.   | Acc   | Acc     | Acc   |
| GPT-4o (0806)                | 57.5          | 51.3    | 64.7   | 85.6  | 75.9    | 74.68 |
| GPT-4-Turbo                  | 55.0          | 46.1    | 62.6   | 84.4  | 73.4    | 63.71 |
| Claude-3.5-Sonnet            | 52.4          | 40.6    | 68.1   | 88.0  | 71.1    | 76.12 |
| LLAMA-3.1-8B Instruct        | Reported      | 20.9    | 21.8   | 45.6  | 79.5    | 45.0  |
| Reproduced                   | 34.6          | 33.3    | 39.7   | 76.5  | 24.3    | 37.2  |
| Vote@16                      | —             |         |        | —     | 26.8    | 43.4  |
| BON@16                       | 35.8          | 35.1    | 46.5   | 80.7  | 30.8    | 50.5  |
| PBS@4                        | 39.9          | 42.2    | 47.2   | 75.59 | 31.3    | 47.8  |
| MISTRAL-NEMO Instruct-202407 | Reproduced    | 45.0    | 37.5   | 35.7  | 64.2    | 31.9  |
| Vote@16                      | —             |         |        | —     | 35.2    | 44.4  |
| BON@16                       | 48.4          | 39.0    | 36.6   | 69.5  | 35.5    | 48.2  |
| PBS@4                        | 53.2          | 49.8    | 42.0   | 63.4  | 37.9    | 44.0  |
| LLAMA-3.1-70B Instruct       | Reported      | 38.1    | 29.9   | 55.9  | 85.8    | 65.8  |
| Reproduced                   | 42.4          | 41.6    | 59.1   | 85.4  | 45.7    | 55.4  |
| Vote@16                      | —             |         |        | —     | 51.4    | 60.3  |
| BON@16                       | 44.4          | 42.9    | 61.9   | 87.7  | 49.8    | 67.1  |"
554,"| Model            | Data           | RB Avg. |
| ---------------- | -------------- | ------- |
| InternRM         | PrefTree Pairs | 91.1    |
| w/ Shared Prefix | 90.8           |         |
| FsfairX          | PrefTree Pairs | 89.4    |
| Llama-3-8B-It    | PrefTree Pairs | 87.2    |
| InternRM         | Outcome Pairs  | 88.4    |
| FsfairX          | Outcome Pairs  | 87.7    |
| Llama-3-8B-It    | Outcome Pairs  | 86.8    |"
555,"| Variable Substitution and Equation Solving                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Question: Given a railway crossing barrier with two arms, where the shorter arm has a length of ‘a’ units (a = 1.0) and the longer arm has a length of ‘b’ units (b = 16.0), determine the elevation ‘y’ units of the longer arm’s end when the shorter arm descends by ‘d’ units (d = 0.5). Express your answer as a function of ‘a’, ‘b’, and ‘d’. Answer: To find the elevation ‘y’ of the long arm’s end, we apply the property of similar triangles and establish the equation  $ \frac{d}{y} = \frac{a}{b} $ . Substituting the given values, we get  $ \frac{0.5}{y} = \frac{1.0}{16.0} $ . Solving for ‘y’ leads to  $ y = \frac{b \times d}{a} $ , which simplifies to  $ y = \frac{16.0 \times 0.5}{1.0} $ . Therefore, ‘y’ equals 8.0 units, indicating that the long arm rises by 8.0 units when the short arm descends by 0.5 units. |"
555,"| Model                 | Input     | Accuracy (%) |
| --------------------- | --------- | ------------ |
| Heuristics Baseline   |           |              |
| Random Chance         | -         | 21.6         |
| Frequent Guess        | -         | 34.1         |
| Human                 | Q, I      | 48.4         |
| Close Source Model    |           |              |
| Text-Only LLMs        |           |              |
| Zero-shot ChatGPT     | Q         | 26.9         |
| Zero-shot GPT-4       | Q         | 37.0         |
| 2-shot CoT Claude-2   | Q         | 29.8         |
| 2-shot CoT ChatGPT    | Q         | 36.5         |
| 2-shot CoT GPT-4      | Q         | 44.7         |
| 2-shot PoT ChatGPT    | Q         | 30.8         |
| 2-shot PoT GPT-4      | Q         | 33.2         |
| Visual-Augmented LLMs |           |              |
| 2-shot CoT Claude-2   | Q, Ic, It | 31.7         |
| 2-shot CoT ChatGPT    | Q, Ic, It | 29.3         |
| 2-shot CoT GPT-4      | Q, Ic, It | 31.7         |
| 2-shot PoT ChatGPT    | Q, Ic, It | 26.4         |
| 2-shot PoT GPT-4      | Q, Ic, It | 39.4         |
| Multimodal LLMs       |           |              |
| Multimodal Bard       | Q, I      | 47.1         |
| SPHINX (V1)           | Q, I      | 23.1         |
| SPHINX (V2)           | Q, I      | 16.4         |
| Gemini Nano 1         | Q, I      | 21.6         |
| Gemini Nano 2         | Q, I      | 23.6         |
| Gemini Pro            | Q, I      | 40.4         |
| Gemini Ultra          | Q, I      | 56.3         |
| GPT4-V                | Q, I      | 50.5         |"
555,"| IDEFICS (9B-Instruct)    | Q, I | 21.1 |
| ------------------------ | ---- | ---- |
| mPLUG-Owl (LLaMA-7B)     | Q, I | 23.6 |
| MiniGPT4 (LLaMA-2-7B)    | Q, I | 26.0 |
| LLaMA-Adapter-V2 (7B)    | Q, I | 25.5 |
| LLaVAR                   | Q, I | 25.0 |
| InstructBLIP (Vicuna-7B) | Q, I | 20.7 |
| LLaVA (LLaMA-2-13B)      | Q, I | 29.3 |
| G-LLaVA-7B               | Q, I | 53.4 |
| G-LLaVA-13B              | Q, I | 56.7 |"
555,"| Model                         | Input | Accuracy (%) |
| ----------------------------- | ----- | ------------ |
| Random Chance                 | -     | 25.0         |
| Frequent Guess                | -     | 32.1         |
| Top-10 Accuracy               |       |              |
| NGS (Chen et al., 2021)       | Q, I  | 56.9         |
| DPE-GPS (Cao and Xiao, 2022)  | Q, I  | 62.7         |
| SCA-GPS (Ning et al., 2023)   | Q, I  | 64.1         |
| Top-1 Accuracy                |       |              |
| Geoformer (Chen et al., 2022) | Q, I  | 46.8         |
| UniMath (Liang et al., 2023)  | Q, I  | 50.0         |
| G-LLaVA-7B                    | Q, I  | 64.2         |
| G-LLaVA-13B                   | Q, I  | 67.0         |"
555,"| Model                             | All  | Text Dominant | Text Lite | Text Only | Vision Intensive | Vision Dominant | Vision Only |
| --------------------------------- | ---- | ------------- | --------- | --------- | ---------------- | --------------- | ----------- |
| testmini                          |      |               |           |           |                  |                 |             |
| LLaMA-Adapter (Gao et al., 2023c) | 5.7  | 6.2           | 5.9       | 2.7       | 6.1              | 4.2             | 6.1         |
| ImageBind-LLM (Han et al., 2023)  | 9.3  | 11.4          | 11.3      | 11.7      | 8.9              | 11.2            | 3.4         |
| mPLUG-Owl2 (Ye et al., 2023)      | 4.6  | 6.6           | 6.3       | 6.1       | 6.3              | 5.6             | 4.9         |
| MiniGPT-v2 (Chen et al., 2023a)   | 11.0 | 12.1          | 12.0      | 11.7      | 13.1             | 10.3            | 7.4         |
| SPHINX-Plus (Gao et al., 2024b)   | 12.2 | 13.9          | 11.6      | 14.9      | 11.6             | 13.5            | 10.4        |
| LLaVA-NeXT (Liu et al., 2024)     | 10.3 | 12.8          | 12.0      | 9.9       | 10.7             | 9.7             | 6.3         |
| ShareGPT4V (Chen et al., 2023b)   | 13.1 | 16.2          | 16.2      | 6.6       | 15.5             | 13.8            | 3.7         |
| Qwen-VL-Plus (Bai et al., 2023a)  | 11.8 | 15.7          | 11.1      | 14.5      | 9.0              | 13.0            | 10.0        |
| LLaVA-7B (Liu et al., 2023)       | 7.6  | 8.8           | 7.6       | 11.5      | 7.4              | 7.4             | 6.9         |
| G-LLaVA-7B                        | 16.6 | 20.9          | 20.7      | 21.1      | 17.2             | 14.6            | 9.4         |
| testmini-MC                       |      |               |           |           |                  |                 |             |
| LLaVA-7B                          | 11.0 | 12.8          | 13.1      | 12.8      | 14.2             | 13.3            | 0.0         |
| LLaVA-13B                         | 16.7 | 22.7          | 21.1      | 19.0      | 18.1             | 19.3            | 0.0         |
| G-LLaVA-7B                        | 34.7 | 42.7          | 36.7      | 42.4      | 31.2             | 28.2            | 27.3        |
| G-LLaVA-13B                       | 36.3 | 43.1          | 37.4      | 45.2      | 32.3             | 29.4            | 30.3        |"
556,"| • eroo36/model           |
| ------------------------ |
| Probability              |
| • ivanboring/airpods     |
| Probability              |
| • DaichiT/stuffing       |
| Probability              |
| • DaichiT/vinyl          |
| Probability              |
| • DaichiT/hydraulic-jack |
| Probability              |"
556,"| • ktennyson6/sd-vsr-5k      |
| --------------------------- |
| Probability                 |
| • scarlettlin/model-0518-v1 |
| Probability                 |
| • sang-kyung/ckpt           |
| Probability                 |
| • alawryaguila/dog-results  |
| Probability                 |
| • evatan/cat-w-prior        |
| Probability                 |"
556,"| • | n6ai/graphic-art                |        |
| - | ------------------------------- | ------ |
|   | Probability                     | 63.69% |
|   | • briannlongzhao/hay-dreambooth |        |
|   | Probability                     | 54.09% |
|   | • jpolun/dogbooth               |        |
|   | Probability                     | 53.55% |
|   | • fsrv0/dogbooth                |        |
|   | Probability                     | 52.88% |
|   | • bogdansorlea/dogbooth         |        |
|   | Probability                     | 52.24% |"
556,"| Overall Acc Lineage Acc Direction Acc          |        |
| ---------------------------------------------- | ------ |
| • Zihao-Li/mala-500-10b-v2                     |        |
| Probability                                    | 63.11% |
| • AsphyXIA/baarat-hindi-pretrained Probability | 53.64% |
| • saraprice/llama2-7B-backdoor Probability     | 51.01% |
| • allenai/tulu-2-7b Probability                | 50.81% |
| • abeiler/AlphaReInstruct Probability          | 50.16% |
| •...                                           |        |"
556,"| • 12thD/ko-Llama-3-8B-sft-v0.1  |
| ------------------------------- |
| Probability                     |
| • AI-Sweden-Models/Llama-3-8B   |
| Probability                     |
| • neuralmagic/SparseLlama-3-8B  |
| Probability                     |
| • grimjim/llama-3-OpenBioLLM-8B |
| Probability                     |
| • bineric/NorskGPT-Llama3-8b    |
| Probability                     |
| •...                            |"
556,"| Method             | FMNIST     | EMNIST     | Cifar10    | SVHN       | Cifar10 5shot | Cifar10 50shot | Cifar10 Imbalanced |
| ------------------ | ---------- | ---------- | ---------- | ---------- | ------------- | -------------- | ------------------ |
| DBSCAN             | 19.43±1.46 | 38.50±0.92 | 43.70±2.64 | 55.17±2.80 | 11.95±0.61    | 28.14±3.55     | 34.69±2.00         |
| GMM                | 14.29±0.45 | 28.74±0.65 | 24.28±1.44 | 48.27±2.46 | 11.80±0.67    | 28.57±3.52     | 25.51±2.28         |
| KMeans             | 28.01±1.59 | 41.38±0.94 | 48.55±2.24 | 71.55±0.49 | 11.56±0.64    | 28.98±3.60     | 32.67±2.01         |
| MeanShift          | 18.43±1.56 | 37.36±0.98 | 43.75±2.74 | 55.23±2.48 | 11.60±0.65    | 28.84±3.52     | 32.41±1.95         |
| KMeans-along       | 28.57±1.75 | 43.10±1.92 | 84.47±0.84 | 71.68±2.16 | 11.74±0.59    | 29.04±3.33     | 33.25±2.78         |
| MeanShift-along    | 19.43±1.26 | 38.50±0.92 | 67.97±1.82 | 55.64±2.40 | 11.82±0.49    | 29.35±3.24     | 33.68±2.79         |
| Phylogeny Detector | 83.01±0.64 | 88.46±1.57 | 90.18±0.77 | 88.39±0.73 | 53.99±1.26    | 57.94±1.35     | 87.01±1.30         |
| DBSCAN             | 39.14±2.06 | 24.64±1.53 | 55.42±4.24 | 36.76±1.93 | 70.01±4.68    | 55.01±1.97     | 26.46±2.35         |
| GMM                | 47.83±2.79 | 21.75±1.71 | 72.80±3.38 | 47.60±2.55 | 55.01±0.02    | 53.36±2.52     | 25.01±1.62         |
| KMeans             | 44.94±2.52 | 25.38±1.54 | 70.61±3.83 | 46.33±2.34 | 50.17±0.05    | 56.21±2.51     | 25.79±1.63         |
| MeanShift          | 43.49±2.42 | 25.31±1.60 | 47.09±4.13 | 30.87±2.26 | 65.75±4.76    | 45.01±8.29     | 25.51±1.66         |
| KMeans-along       | 46.38±2.00 | 69.57±4.15 | 70.71±3.39 | 70.61±2.96 | 80.01±4.68    | 80.01±4.68     | 67.27±2.53         |
| MeanShift-along    | 47.83±2.79 | 72.47±3.97 | 73.55±3.03 | 55.89±3.15 | 70.01±6.12    | 45.53±4.28     | 54.41±1.79         |
| Phylogeny Detector | 68.33±1.41 | 72.51±1.26 | 89.01±0.86 | 72.01±1.23 | 52.89±0.86    | 51.82±2.02     | 67.33±1.13         |"
558,"| Domain                  | #Topics | #Tools       | #Dialogues (Train/Test) |
| ----------------------- | ------- | ------------ | ----------------------- |
| Place                   | 4       | 27           | 29,667 / 150            |
| Media                   | 2       | 18           | 13,730 / 92             |
| Traffic                 | 5       | 22           | 17,720 / 89             |
| Daily Life              | 5       | 31           | 23,706 / 163            |
| Money                   | 2       | 10           | 1,740 / 32              |
| Information             | 4       | 12           | 9,125 / 75              |
| Disease                 | 4       | 2            | 3,371 / 125             |
| Others                  | 5       | 14           | - / 68                  |
| Additional Statistics   |         |              |                         |
| #Dialogues (Train/Test) |         | 54,367 / 431 |                         |
| #Tools (Train/Test)     |         | 122 / 14     |                         |
| Avg. Turns per Dialogue |         | 2.6          |                         |
| Avg. Tools per Dialogue |         | 5.6          |                         |
| Avg. Tools per Turn     |         | 2.2          |                         |"
558,"| Multi-tool  | Tool selection acc (TS) param selection acc (PS)   | Tool selection acc (TS) param selection acc (PS) Averaged Turn Success Rate (ATS) Soft Averaged Turn Success Rate (SATS) Task Process Rate (TPR) Success Rate (SR)   |
| ----------- | -------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Single-tool | Tool Number Accuracy (TN) Tool Order Accuracy (TO) | Tool Number Accuracy (TN) Tool Order Accuracy (TO) Averaged Turn Success Rate (ATS) Soft Averaged Turn Success Rate (SATS) Task Process Rate (TPR) Success Rate (SR) |"
558,"| Models    | MTU-Bench (OOD) |
| --------- | --------------- |
| TS        | PS              |
| GPT-4     | 67.28           |
| LLaMA3-8B | 36.71           |
| MTU-LLaMA | 47.34           |
| Models    | ToolTalk        |
| TS        | PS              |
| GPT-4     | 51.08           |
| LLaMA3-8B | 30.00           |
| MTU-LLaMA | 30.25           |
| Models    | API-Bank        |
| TS        | PS              |
| GPT-4     | 48.56           |
| LLaMA3-8B | 14.03           |
| MTU-LLaMA | 51.80           |"
562,"| Uni3D         | 38.17 | 59.92 | 67.18 | point cloud   | EVA02-E-14-plus |
| ------------- | ----- | ----- | ----- | ------------- | --------------- |
| CLIP $ ^{2} $ | 12.35 | 24.62 | 32.91 | point cloud   | ViT-B-16        |
| CLIP $ ^{2} $ | 10.20 | 20.47 | 27.71 | 3DGS          | ViT-B-16        |
| Uni3D         | 5.130 | 11.20 | 13.27 | 3DGS location | EVA02-E-14-plus |
| Uni3D*        | 36.72 | 57.09 | 65.18 | point cloud   | ViT-B-16        |
| Uni3D         | 18.48 | 34.39 | 43.31 | 3DGS          | ViT-B-16        |
| Uni3D*        | 30.47 | 48.46 | 55.87 | 3DGS          | ViT-B-16        |
| UniGS(Ours)   | 38.57 | 60.54 | 68.96 | 3DGS          | ViT-B-16        |"
562,"| Training w/ point cloud | Initialization       | Avg. |
| ----------------------- | -------------------- | ---- |
| opacity                 | scale &amp; rotation | Top1 |
| ✗                       | -                    | -    |
| ✓                       | 0                    | 0    |
| ✓                       | 0.4                  | 0    |
| ✓                       | 0.4                  | 0.4  |"
558,"| Models                                  | S-S   | M-S   |
| --------------------------------------- | ----- | ----- |
| TS                                      | PS    | Avg.  |
| Closed-Source Large Language Models     |       |       |
| GPT-4                                   | 95.83 | 52.08 |
| GPT-3.5                                 | 84.62 | 46.15 |
| Qwen-Max                                | 91.67 | 50.00 |
| GLM-4-Plus                              | 95.83 | 50.00 |
| DeepSeek V2.5                           | 93.75 | 43.75 |
| Open-Source Large Language Models       |       |       |
| LLaMA2-7B                               | 15.38 | 3.85  |
| LLaMA2-70B                              | 70.59 | 33.33 |
| LLaMA3-8B                               | 65.38 | 30.77 |
| LLaMA3-70B                              | 86.54 | 57.69 |
| Qwen1.5-14B                             | 75.00 | 34.62 |
| Qwen1.5-72B                             | 78.85 | 38.46 |
| Qwen2-7B                                | 73.08 | 38.46 |
| Qwen2-72B                               | 86.54 | 48.08 |
| Mistral-7B                              | 60.42 | 25.00 |
| ChatGLM3-6B                             | 10.00 | 0.00  |
| GLM-4-9B                                | 91.67 | 45.83 |
| Tool-Use-Specific Large Language Models |       |       |
| ToolLLaMA2-7B                           | 85.42 | 18.75 |
| MTU-LLaMA (Ours)                        | 92.31 | 50.00 |"
558,"| Models                                  | S-M   | M-M   |       |
| --------------------------------------- | ----- | ----- | ----- |
| TN                                      | TO    | Avg.  | TN    |
| Closed-Source Large Language Models     |       |       |       |
| GPT-4                                   | 66.85 | 70.52 | 68.68 |
| GPT-3.5                                 | 32.64 | 38.22 | 35.43 |
| Qwen-Max                                | 39.76 | 48.82 | 39.29 |
| GLM-4-Plus                              | 45.76 | 48.48 | 47.12 |
| DeepSeek V2.5                           | 56.88 | 60.28 | 58.58 |
| Open-Source Large Language Models       |       |       |       |
| LLaMA2-7B                               | 3.39  | 3.94  | 3.67  |
| LLaMA2-70B                              | 6.82  | 8.48  | 7.65  |
| LLaMA3-8B                               | 14.79 | 20.30 | 17.55 |
| LLaMA3-70B                              | 26.85 | 32.68 | 29.76 |
| Qwen1.5-14B                             | 22.12 | 28.22 | 25.17 |
| Qwen1.5-72B                             | 28.04 | 30.60 | 29.32 |
| Qwen2-7B                                | 24.52 | 29.59 | 27.05 |
| Qwen2-72B                               | 52.76 | 59.98 | 56.37 |
| Mistral-7B                              | 14.21 | 18.22 | 16.22 |
| ChatGLM3-6B                             | 6.53  | 8.56  | 7.55  |
| GLM-4-9B                                | 23.64 | 27.58 | 25.61 |
| Tool-Use-Specific Large Language Models |       |       |       |
| ToolLLaMA2-7B                           | 11.52 | 11.52 | 11.51 |
| MTU-LLaMA (Ours)                        | 55.39 | 58.55 | 56.97 |"
559,"| Seq. length | Worms       | SCP1       | SCP2       | Ethanol    | Heartbeat  | Motor      | Avg  |
| ----------- | ----------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---- |
| 17,984      | 896         | 1,152      | 1,751      | 405        | 3,000      |            |      |
| NRDE        | 83.9 ± 7.3  | 80.9 ± 2.5 | 53.7 ± 6.9 | 25.3 ± 1.8 | 72.9 ± 4.8 | 47.0 ± 5.7 | 60.6 |
| NCDE        | 75.0 ± 3.9  | 79.8 ± 5.6 | 53.0 ± 2.8 | 29.9 ± 6.5 | 73.9 ± 2.6 | 49.5 ± 2.8 | 60.2 |
| Log-NCDE    | 85.6 ± 5.1  | 83.1 ± 2.8 | 53.7 ± 4.1 | 34.4 ± 6.4 | 75.2 ± 4.6 | 53.7 ± 5.3 | 64.3 |
| LRU         | 87.8 ± 2.8  | 82.6 ± 3.4 | 51.2 ± 3.6 | 21.5 ± 2.1 | 78.4 ± 6.7 | 48.4 ± 5.0 | 61.7 |
| S5          | 81.1 ± 3.7  | 89.9 ± 4.6 | 50.5 ± 2.6 | 24.1 ± 4.3 | 77.7 ± 5.5 | 47.7 ± 5.5 | 61.8 |
| S6          | 85.0 ± 16.1 | 82.8 ± 2.7 | 49.9 ± 9.4 | 26.4 ± 6.4 | 76.5 ± 8.3 | 51.3 ± 4.7 | 62.0 |
| Mamba       | 70.9 ± 15.8 | 80.7 ± 1.4 | 48.2 ± 3.9 | 27.9 ± 4.5 | 76.2 ± 3.8 | 47.7 ± 4.5 | 58.6 |
| LinOSS-IMEX | 80.0 ± 2.7  | 87.5 ± 4.0 | 58.9 ± 8.1 | 29.9 ± 1.0 | 75.5 ± 4.3 | 57.9 ± 5.3 | 65.0 |
| LinOSS-IM   | 95.0 ± 4.4  | 87.8 ± 2.6 | 58.2 ± 6.9 | 29.9 ± 0.6 | 75.8 ± 3.7 | 60.0 ± 7.5 | 67.8 |"
559,"| Model       | PPG-DaLiA (MSE  $ \times 10^{-2} $ ) |
| ----------- | ------------------------------------ |
| NRDE        | 9.90  $ \pm $  0.97                  |
| NCDE        | 13.54  $ \pm $  0.69                 |
| Log-NCDE    | 9.56  $ \pm $  0.59                  |
| LRU         | 12.17  $ \pm $  0.49                 |
| S5          | 12.63  $ \pm $  1.25                 |
| S6          | 12.88  $ \pm $  2.05                 |
| Mamba       | 10.65  $ \pm $  2.20                 |
| LinOSS-IMEX | 7.5  $ \pm $  0.46                   |
| LinOSS-IM   | 6.4  $ \pm $  0.23                   |"
559,"| Model                   | Weather Forecast (MAE) |
| ----------------------- | ---------------------- |
| Informer                | 0.731                  |
| Informer $ ^{\dagger} $ | 0.741                  |
| LogTrans                | 0.773                  |
| Reformer                | 1.575                  |
| LSTMa                   | 1.109                  |
| LSTnet                  | 0.757                  |
| S4                      | 0.578                  |
| LinOSS-IMEX             | 0.508                  |
| LinOSS-IM               | 0.528                  |"
561,"| Table.1 Jailbreak Success Rate on Various LLMs. |
| ----------------------------------------------- |
| Datasets                                        |
| Edited                                          |
| w/ trig.                                        |
| DAN                                             |
| DNA                                             |
| Addition                                        |
| Overall                                         |"
561,"| Table.2 Comparison with Other Jailbreaking Pradigms. |
| ---------------------------------------------------- |
| Attack Type                                          |
| Clean                                                |
| Jailbreak Prompt-based                               |
| Hand-crafted                                         |
| Generative                                           |
| Jailbreak Backdoor-based                             |
| Data Poisoning                                       |
| w/o trig.                                            |
| Model Editing                                        |
| w/o trig.                                            |
| MEMIT (Adapted) (2023)                               |
| w/o trig.                                            |
| JailbreakEdit (4 Node)                               |
| w/o trig.                                            |
| JailbreakEdit (16 Node)                              |
| w/o trig.                                            |"
562,"| Methods | Top1 | Avg. Top3 | Top 5 | Representation | Text-image Model |
| ------- | ---- | --------- | ----- | -------------- | ---------------- |"
562,"| Representation Training &amp; Testing | Top1          | Avg. Top3     | Top 5         |
| ------------------------------------- | ------------- | ------------- | ------------- |
| 3DGS                                  | 38.57         | 60.54         | 68.96         |
| 2DGS                                  | 38.11 (-0.46) | 61.28 (+0.74) | 70.32 (+1.36) |"
562,"| Uni3D         | 68.94 | 90.49 | 94.15 | point cloud   | EVA02-E-14-plus |
| ------------- | ----- | ----- | ----- | ------------- | --------------- |
| CLIP $ ^{2} $ | 22.58 | 43.83 | 54.56 | point cloud   | ViT-B-16        |
| CLIP $ ^{2} $ | 19.06 | 38.48 | 48.71 | 3DGS          | ViT-B-16        |
| Uni3D         | 13.34 | 28.28 | 42.20 | 3DGS location | EVA02-E-14-plus |
| Uni3D*        | 37.60 | 59.68 | 70.22 | point cloud   | ViT-B-16        |
| Uni3D         | 27.57 | 50.60 | 63.69 | 3DGS          | ViT-B-16        |
| Uni3D*        | 37.79 | 61.08 | 69.04 | 3DGS          | ViT-B-16        |
| UniGS(Ours)   | 46.97 | 69.91 | 79.38 | 3DGS          | ViT-B-16        |"
562,"| Method         | Rep.         | Avg.  | Bed   | Chair | Sofa  | Toilet | Btub. | Dresser | NSd.  |
| -------------- | ------------ | ----- | ----- | ----- | ----- | ------ | ----- | ------- | ----- |
| Uni3D          | point clouds | 11.04 | 56.74 | 19.58 | 22.93 | 24.62  | 24.00 | 0.000   | 1.690 |
| CLIP $ ^{2} $  | point clouds | 41.39 | 1.840 | 68.02 | 45.44 | 13.85  | 0.000 | 15.00   | 3.390 |
| CLIP $ ^{2} $  | 3DGS         | 28.50 | 1.470 | 40.03 | 15.20 | 4.620  | 0.000 | 26.25   | 30.51 |
| Uni3D $ ^{*} $ | point clouds | 61.72 | 63.60 | 84.33 | 79.36 | 63.59  | 74.67 | 12.92   | 18.93 |
| Uni3D          | 3DGS         | 54.51 | 58.09 | 80.38 | 62.40 | 56.92  | 48.00 | 7.500   | 11.02 |
| Uni3D $ ^{*} $ | 3DGS         | 56.67 | 74.63 | 83.89 | 50.88 | 7.690  | 20.00 | 27.50   | 19.49 |
| UniGS(Ours)    | 3DGS         | 69.64 | 81.62 | 87.46 | 79.36 | 93.85  | 96.00 | 35.00   | 36.44 |"
562,"| Exp. | 3DGS | ViT  | GAG | Avg. |
| ---- | ---- | ---- | --- | ---- |
| Pre. | Par. | Cro. |     |      |
| 1    | ✗    | ✗    | ✗   | ✗    |
| 2    | ✓    | ✗    | ✗   | ✗    |
| 3    | ✓    | ✓    | ✗   | ✗    |
| 4    | ✓    | ✓    | ✓   | ✗    |
| 5    | ✓    | ✓    | ✓   | ✓    |
| 6    | ✓    | ✓    | ✗   | ✓    |
| 7    | ✓    | ✓    | ✓   | ✓    |"
563,"| Algorithm                        | Action violation rate | Generative model | Interaction time | Training load |
| -------------------------------- | --------------------- | ---------------- | ---------------- | ------------- |
| OptLayer (Pham et al., 2018)     | High                  | ✗                | High             | Low           |
| ApprOpt (Bhatia et al., 2019)    | High                  | ✗                | High             | Low           |
| NFWPO (Lin et al., 2021)         | Low                   | ✗                | Low              | High          |
| FlowPG (Brahmanage et al., 2023) | Low                   | ✓                | Low              | High          |
| DPre+ (Kasaura et al., 2023)     | High                  | ✗                | High             | Low           |
| SPre+ (Kasaura et al., 2023)     | High                  | ✗                | High             | Low           |
| IAR-A2C (Chen et al., 2023)      | Low                   | ✓                | Low              | High          |
| ARAM (Ours)                      | Low                   | ✗                | Low              | Low           |"
563,"| Environment | DPre+               | SPre+               | NFWPO               | FlowPG              | ARAM (Ours)         |
| ----------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |
| Hopper      | 0.97  $ \pm $  0.03 | 0.40  $ \pm $  0.01 | 0.99  $ \pm $  0.01 | 0.98  $ \pm $  0.01 | 0.99  $ \pm $  0.01 |
| HalfCheetah | 0.35  $ \pm $  0.06 | 0.78  $ \pm $  0.19 | 0.97  $ \pm $  0.01 | 0.78  $ \pm $  0.17 | 0.78  $ \pm $  0.29 |
| NSFnet      | 0.00  $ \pm $  0.00 | 0.04  $ \pm $  0.01 | 0.92  $ \pm $  0.05 | 0.98  $ \pm $  0.01 | 0.94  $ \pm $  0.04 |
| BSS5z       | 0.13  $ \pm $  0.11 | 0.31  $ \pm $  0.16 | 0.84  $ \pm $  0.14 | 0.68  $ \pm $  0.17 | 0.77  $ \pm $  0.16 |"
563,"| Environment | DPre+ (s)      | SPre+ (s)      | NFWPO (s)      | FlowPG (s)    | ARAM (Ours) (s) |
| ----------- | -------------- | -------------- | -------------- | ------------- | --------------- |
| Reacher     | 71.47 ± 5.63   | 303.01 ± 60.81 | 67.43 ± 5.02   | 71.47 ± 3.12  | 44.72 ± 0.93    |
| HalfCheetah | 211.67 ± 48.70 | 253.53 ± 30.48 | 180.41 ± 10.57 | 210.63 ± 7.51 | 57.14 ± 8.13    |
| Hopper      | 99.43 ± 6.71   | 175.43 ± 9.47  | 88.71 ± 11.41  | 103.12 ± 5.13 | 61.72 ± 2.15    |
| HopperVel   | 177.31 ± 15.73 | 97.12 ± 7.95   | 82.63 ± 7.13   | 91.43 ± 5.17  | 63.81 ± 10.04   |"
564,"| Type                                | Method               | Standard Acc.  | Robust Acc. |
| ----------------------------------- | -------------------- | -------------- | ----------- |
| PGD                                 | AutoAttack           |                |             |
| WideResNet-28-10                    |                      |                |             |
| AT                                  | (Gowal et al., 2021) | 88.54          | 65.93       |
| (Gowal et al., 2020) $ ^{\dagger} $ | 87.51                | 66.01          | 62.76       |
| (Pang et al., 2022)                 | 88.62                | 64.95          | 61.04       |
| AP                                  | (Yoon et al., 2021)  | 85.66±0.51     | 33.48±0.86  |
| (Nie et al., 2022)                  | 90.07±0.97           | 56.84±0.59     | 63.60±0.81  |
| (Lee &amp; Kim, 2023)               | 90.16±0.64           | 55.82±0.59     | 70.47±1.53  |
| (Bai et al., 2024)                  | 91.41                | 49.22 $ ^{*} $ | 77.08       |
| (Zollicoffer et al., 2025)          | 84.20                | -              | 59.14       |
| (Lin et al., 2024)                  | 90.62                | -              | 72.85       |
| Ours                                | 92.19±0.33           | 59.39±0.79     | 77.35±2.14  |"
564,"| Type                                    | Method                                | Standard Acc.  | Robust Acc. |
| --------------------------------------- | ------------------------------------- | -------------- | ----------- |
| PGD                                     | AutoAttack                            |                |             |
| WideResNet-28-10                        |                                       |                |             |
| AT                                      | (Rebuffi et al., 2021) $ ^{\dagger} $ | 91.79          | 85.05       |
| (Augustin et al., 2020) $ ^{\ddagger} $ | 93.96                                 | 86.14          | 78.79       |
| (Pang et al., 2022) $ ^{\ddagger} $     | 90.93                                 | 83.75          | 77.24       |
| AP                                      | (Yoon et al., 2021)                   | 85.66±0.51     | 73.32±0.76  |
| (Nie et al., 2022)                      | 91.41±1.00                            | 79.45±1.16     | 81.7±0.84   |
| (Lee &amp; Kim, 2023)                   | 90.16±0.64                            | 83.59±0.88     | 86.48±0.38  |
| (Bai et al., 2024)                      | 91.41                                 | 86.13 $ ^{*} $ | 80.92       |
| (Zollicoffer et al., 2025)              | 84.40                                 | -              | 77.90       |
| (Zollicoffer et al., 2025)              | 84.20                                 | -              | 73.60       |
| (Lin et al., 2024)                      | 90.62                                 | -              | 80.47       |
| Ours                                    | 92.19±0.33                            | 87.89±1.17     | 89.06±0.43  |"
565,"| Methods        | Sup          | UrbanCar     | CelebA        | BAR          |
| -------------- | ------------ | ------------ | ------------- | ------------ |
|                |              | ID.Acc. (t)  | WG.Acc. (t)   | Avg. GAP (t) |
| Group (DRD ✓   | 91.60 (1.33) | 75.70 (1.79) | -10.30 (1.05) | 90.08 (0.70) |
| IBM            | 97.60 (0.86) | 33.20 (0.86) | -31.90 (0.93) | 94.43 (0.10) |
| LFF            | 97.20 (0.46) | 35.30 (0.46) | -31.06 (0.56) | 95.12 (0.35) |
| JTT            | 95.80 (1.45) | 33.30 (0.90) | -20.50 (0.63) | 91.86 (0.48) |
| Debian         | 98.00 (0.89) | 30.30 (0.69) | -31.40 (1.44) | 94.38 (0.37) |
| DPR            | 89.70 (1.32) | -            | -20.93 (2.62) | 60.32 (0.28) |
| Selma (Dunn) x | 93.54 (0.30) | 70.88 (0.38) | -10.57 (1.79) | 80.42 (0.33) |"
565,"| Method       | Sup. | ImageNet-3K  | MultiNU      |
| ------------ | ---- | ------------ | ------------ |
|              |      | ID. Acc. (T) | IN-W Gap (T) |
| LLE          | ✓    | 76.25        | -6.30        |
| EPM          | ✗    | 76.13        | -26.64       |
| LPI          | ✗    | 70.36        | -17.57       |
| JTT          | ✗    | 75.64        | -15.74       |
| Debian       | ✗    | 74.05        | -20.00       |
| Sebra (Dunt) | ✗    | 74.89        | -14.77       |"
566,"| Method                | Model Weight                                              | ImageNet | OOD Avg. |
| --------------------- | --------------------------------------------------------- | -------- | -------- |
| Pre-trained           | $ \theta_{0} $                                            | 63.4     | 48.5     |
| Fine-tuned            | $ \theta_{1} $                                            | 78.4     | 47.9     |
| Static interpolation  | $ (1-\lambda)\theta_{0}+\lambda\theta_{1} $               | 79.1     | 51.0     |
| Dynamic interpolation | $ (1-\lambda^{*}(x))\theta_{0}+\lambda^{*}(x)\theta_{1} $ | 83.4     | 60.6     |"
566,"| Method          | Training Cost | Inference Cost       | ImageNet (ID) | OOD Avg. |
| --------------- | ------------- | -------------------- | ------------- | -------- |
| Pre-trained     | -             | $ \mathcal{O}(1) $   | 63.35         | 48.48    |
| Fine-tuned      | 1             | $ \mathcal{O}(1) $   | 78.35         | 47.08    |
| Output Ensemble | 1             | $ \mathcal{O}(M) $   | 78.97         | 51.76    |
| WiSE-FT         | 1             | $ \mathcal{O}(H) $   | 79.14         | 51.02    |
| Uniform Soup    | 48            | $ \mathcal{O}(1) $   | 79.76         | 52.08    |
| Greedy Soup     | 48            | $ \mathcal{O}(1) $   | 80.42         | 50.83    |
| Model Stock     | $ 2+\alpha $  | $ \mathcal{O}(1) $   | 79.89         | 50.99    |
| DaWin w/o BMM   | 1             | $ \mathcal{O}(N+M) $ | 78.71         | 54.41    |
| DaWin           | 1             | $ \mathcal{O}(K+M) $ | 78.70         | 54.36    |"
566,"| Method             | SUN397 | Cars | RESISC45 | EuroSAT | SVHN | GTSRB | MNIST | DTD  | Avg. |
| ------------------ | ------ | ---- | -------- | ------- | ---- | ----- | ----- | ---- | ---- |
| Jointly Fine-tuned | 73.9   | 74.4 | 93.9     | 98.2    | 95.8 | 98.9  | 99.5  | 77.9 | 88.9 |
| Individual Experts | 75.3   | 77.7 | 96.1     | 99.8    | 97.5 | 98.7  | 99.7  | 79.4 | 90.5 |
| Task Arithmetic    | 55.3   | 54.9 | 66.7     | 75.9    | 80.2 | 69.7  | 97.3  | 50.1 | 68.8 |
| Ties-Merging       | 65.0   | 64.3 | 74.7     | 75.7    | 81.3 | 69.4  | 96.5  | 54.3 | 72.6 |
| AdaMerging         | 64.2   | 68.0 | 79.2     | 93.0    | 87.0 | 92.0  | 97.5  | 58.8 | 80.0 |
| AdaMerging++       | 65.8   | 68.4 | 82.0     | 93.6    | 89.6 | 89.0  | 98.3  | 60.2 | 80.9 |
| Pareto Merging     | 71.4   | 74.9 | 87.0     | 97.1    | 92.0 | 96.8  | 98.2  | 61.1 | 84.8 |
| DaWin              | 66.2   | 66.7 | 91.3     | 99.2    | 94.7 | 98.1  | 99.5  | 74.6 | 86.3 |"
567,"| Model                    | Developer    | Score | Explanation                                              |
| ------------------------ | ------------ | ----- | -------------------------------------------------------- |
| Pythia                   | EleutherAI   | 1     | Open training data (Biderman et al., 2023)               |
| OLMo                     | AI2          | 1     | Open training data (Groeneveld et al., 2024)             |
| RedPajama-INCITE 7B      | Together AI  | 1     | Open training data (Computer, 2024)                      |
| StarCoder 2              | BigCode      | 1     | Open training data (Lozhkov et al., 2024)                |
| Palmyra X V3             | Writer       | 1     | Published analysis and code (Writer, 2024)               |
| GPT-4                    | OpenAI       | 1     | Published analysis (OpenAI et al., 2024)                 |
| Llama 3.1                | Meta         | 1     | Published analysis (Dubey et al., 2024)                  |
| Qwen2                    | Alibaba      | 1     | Published analysis (Yang et al., 2024)                   |
| Apple Intelligence       | Apple        | 1     | Published prefiltering (Gunter et al., 2024)             |
| Gemini 1.5 Pro           | Google       | 0     | Insufficient methodological details (Team et al., 2024a) |
| Arctic                   | Snowflake    | 0     | No analysis (Snowflake, 2024)                            |
| Claude 3.5 Sonnet        | Anthropic    | 0     | No analysis (Anthropic, 2024)                            |
| Command R                | Cohere       | 0     | No analysis (Cohere, 2024)                               |
| Core                     | Reka AI      | 0     | No analysis (Team et al., 2024b)                         |
| DBRX                     | Databricks   | 0     | No analysis (Databricks, 2024)                           |
| DeepSeek                 | DeepSeek     | 0     | No analysis (DeepSeek-AI et al., 2024)                   |
| Falcon                   | TII          | 0     | No analysis (Almazrouei et al., 2023)                    |
| Fuyu-Heavy               | Adept        | 0     | No analysis (Adept, 2024)                                |
| Granite                  | IBM          | 0     | No analysis (Mishra et al., 2024)                        |
| Grok-2                   | xAI          | 0     | No analysis (x.ai, 2024)                                 |
| Imbue 70B                | Imbue        | 0     | No analysis (Imbue, 2024)                                |
| Inflection-2.5           | Inflection   | 0     | No analysis (AI, 2024a)                                  |
| Jamba-1.5                | AI21 Labs    | 0     | No analysis (AI21, 2024)                                 |
| Luminous Supreme         | Aleph Alpha  | 0     | No analysis (Alpha, 2024)                                |
| Mixtral Large 2          | Mistral      | 0     | No analysis (AI, 2024b)                                  |
| Nemotron-4-340B-Instruct | NVIDIA       | 0     | No analysis (NVIDIA, 2024)                               |
| Phi 3                    | Microsoft    | 0     | No analysis (Abdin et al., 2024)                         |
| Stable LM 2              | Stability AI | 0     | No analysis (AI, 2024c)                                  |
| Titan Text Express       | Amazon       | 0     | No analysis (Amazon, 2024)                               |
| Yi-34B                   | 01.ai        | 0     | No analysis (AI et al., 2024)                            |"
570,"| MLLMs                | ACC (%)  $ \uparrow $ | APSS  $ \downarrow $ |
| -------------------- | --------------------- | -------------------- |
| VMME                 | NEXT                  | MUSC                 |
| Open-source Models   |                       |                      |
| VideoLLaMA-7B        | 38.10                 | 46.06                |
| VideoLLaMA-13B       | 45.33                 | 47.07                |
| PandaGPT-7B          | 43.45                 | 50.95                |
| PandaGPT-13B         | 48.75                 | 58.90                |
| NExT-GPT             | 42.64                 | 42.59                |
| Closed-source Models |                       |                      |
| Gemini-1.5-Flash     | 72.31                 | 70.80                |
| Gemini-1.5-Pro       | 73.11                 | 76.29                |
| GPT-4o mini          | 73.26                 | 82.50                |"
572,"| Attack                        | Precision of the environment 32-bit | 64-bit |
| ----------------------------- | ----------------------------------- | ------ |
| Precision, 32-bit adversarial | 0.11%                               | 98.11% |
| Precision, 64-bit adversarial | 98.11%                              | 0.11%  |"
572,"| Verifier             | Ver. Env.   | Bounding  | Precision | Order1 | Order2  | Order3  | [5]               |
| -------------------- | ----------- | --------- | --------- | ------ | ------- | ------- | ----------------- |
| MIPVerify            | 64-bit, CPU | IBP       | unsound   | sound  | unsound | unsound | unsound           |
| MN-BAB               | 64-bit, GPU | Polyhedra | unsound   | sound  | unsound | unsound | unsound           |
| $ \beta $ -CROWN BaB | 32-bit, CPU | Polyhedra | unsound   | sound  | unsound | unsound | [no 32-bit model] |
| $ \beta $ -CROWN BaB | 64-bit, CPU | Polyhedra | unsound   | sound  | unsound | unsound | sound             |
| $ \beta $ -CROWN BaB | 64-bit, GPU | Polyhedra | unsound   | sound  | unsound | unsound | sound             |
| GCP-CROWN            | 64-bit, CPU | Polyhedra | unsound   | sound  | unsound | unsound | sound             |
| DeepPoly             | 64-bit, CPU | Polyhedra | unsound   | sound  | sound   | unsound | sound             |
| RefinePoly           | 64-bit, CPU | Polyhedra | unsound   | sound  | sound   | unsound | unsound           |
| DeepZ                | 64-bit, CPU | Zonotope  | unsound   | sound  | sound   | unsound | sound             |
| RefineZono           | 64-bit, CPU | Zonotope  | unsound   | sound  | sound   | unsound | [Gurobi error]    |"
574,"| Attack      | LLM    | No Defense | SmoothLLM | GoalPriority | SnapKV | RobustKV |
| ----------- | ------ | ---------- | --------- | ------------ | ------ | -------- |
| AutoDAN     | Llama2 | 61.5%      | 40.0%     | 30.8%        | 58.3%  | 6.3%     |
| Vicuna      | 92.3%  | 72.1%      | 86.3%     | 90.6%        | 8.3%   |          |
| GCG         | Llama2 | 38.1%      | 8.9%      | 14.3%        | 35.4%  | 7.7%     |
| Mistral     | 64.6%  | 50.6%      | 30.9%     | 66.2%        | 27.6%  |          |
| Vicuna      | 89.4%  | 23.7%      | 34.7%     | 84.6%        | 16.4%  |          |
| AmpleGCG    | Llama2 | 51.5%      | 47.5%     | 8.1%         | 40.4%  | 6.1%     |
| Vicuna      | 72.7%  | 35.4%      | 10.1%     | 58.6%        | 7.1%   |          |
| AdvPrompter | Llama2 | 37.0%      | 31.5%     | 11.3%        | 33.0%  | 7.4%     |
| Mistral     | 88.1%  | 68.9%      | 21.8%     | 80.1%        | 30.3%  |          |
| Vicuna      | 85.9%  | 55.4%      | 20.5%     | 75.6%        | 28.2%  |          |"
574,"| Defense                            | AlpacaEval               | VicunaEval               |
| ---------------------------------- | ------------------------ | ------------------------ |
| WinRate ( $ \uparrow $ )           | Rouge-L ( $ \uparrow $ ) | WinRate ( $ \uparrow $ ) |
| No Defense                         | 68%                      | 0.453                    |
| SmoothLLM (Robey et al., 2023)     | 62%                      | 0.306                    |
| GoalPriority (Zhang et al., 2024c) | 59%                      | 0.281                    |
| RobustKV                           | 63%                      | 0.415                    |"
574,"| KV Eviction Method                 | Single-Document QA ( $ \uparrow $ ) | Multi-Document QA ( $ \uparrow $ ) | Summarization ( $ \uparrow $ ) |
| ---------------------------------- | ----------------------------------- | ---------------------------------- | ------------------------------ |
| Full KV                            | 21.07                               | 30.61                              | 27.81                          |
| H $ _{2} $ O (Zhang et al., 2024b) | 20.45                               | 27.82                              | 26.59                          |
| SnapKV (Li et al., 2024)           | 21.07                               | 30.51                              | 27.81                          |
| RobustKV                           | 19.15                               | 31.50                              | 26.65                          |"
575,"| Algorithm                    | Score Function           | Direct Preference Optimization | Likelihood Matching |
| ---------------------------- | ------------------------ | ------------------------------ | ------------------- |
| PEBBLE
(Lee et al., 2021)    | rψ(st,at)                | ✗                              | ✗                   |
| DPO
(Rafailov et al., 2024b) | logπψ(y|s)/πref(y|s)     | ✓                              | ✗                   |
| DPPO
(An et al., 2023)       | −Ea∼πψ(-|st|)[|a − at|2] | ✓                              | ✗                   |
| CPL
(Hejna et al., 2023)     | Qπψ(st,at)−Vπψ(st)       | ✓                              | ✗                   |
| PPL
[Ours]                   | −(Vπψ(st)−Qπ(st,at))     | ✓                              | ✓                   |"
575,"|               |       | Average             |
| ------------- | ----- | ------------------- |
| Homogeneous   | SFT   | 58.4  $ \pm $  8.0  |
| Dense         | P-IQL | 62.7  $ \pm $  3.9  |
|               | CPL   | 47.9  $ \pm $  3.9  |
|               | PPL   | 68.2  $ \pm $  4.0  |
| Homogeneous   | SFT   | 55.4  $ \pm $  3.1  |
| Sparse        | P-IQL | 68.8  $ \pm $  3.1  |
|               | CPL   | 53.0  $ \pm $  4.6  |
|               | PPL   | 73.3  $ \pm $  3.6  |
| Heterogeneous | SFT   | 42.7  $ \pm $  10.8 |
| Dense         | P-IQL | 49.5  $ \pm $  4.0  |
|               | CPL   | 35.1  $ \pm $  2.8  |
|               | PPL   | 59.6  $ \pm $  6.1  |
| Heterogeneous | SFT   | 43.2  $ \pm $  2.6  |
| Sparse        | P-IQL | 57.8  $ \pm $  3.9  |
|               | CPL   | 39.8  $ \pm $  2.6  |
|               | PPL   | 66.2  $ \pm $  2.8  |"
576,"| Progress | Action | Error Rec. | Creative | Efficiency | Material | Average |
| -------- | ------ | ---------- | -------- | ---------- | -------- | ------- |
| 84.0     | 96.0   | 86.0       | 100.0    | 92.0       | 91.0     | 91.5    |"
577,"| Method        | Acc (10%)  $ \downarrow $ | CPD (K = 50)  $ \uparrow $ |
| ------------- | ------------------------- | -------------------------- |
| Extrmask      | 0.930 $ \pm $ 0.005       | 0.204 $ \pm $ 0.007        |
| ContraLSP     | 0.981 $ \pm $ 0.003       | 0.013 $ \pm $ 0.001        |
| TimeX++       | 0.991 $ \pm $ 0.001       | 0.027 $ \pm $ 0.002        |
| IG (Unsigned) | 0.974 $ \pm $ 0.001       | 0.342 $ \pm $ 0.021        |
| IG (Signed)   | 0.855 $ \pm $ 0.011       | 0.248 $ \pm $ 0.010        |
| TIMING        | 0.975 $ \pm $ 0.001       | 0.366 $ \pm $ 0.021        |"
577,"| Method                           | Cumulative Masking                |
| -------------------------------- | --------------------------------- |
| CPD ( $ K = 50 $ )  $ \uparrow $ | CPD ( $ K = 100 $ )  $ \uparrow $ |
| FO                               | 0.016±0.002                       |
| AFO                              | 0.120±0.008                       |
| GradSHAP                         | 0.327±0.021                       |
| DeepLIFT                         | 0.142±0.010                       |
| LIME                             | 0.071±0.004                       |
| FIT                              | 0.015±0.001                       |
| WinIT                            | 0.020±0.001                       |
| Dynamask                         | 0.052±0.002                       |
| Extrmask                         | 0.204±0.007                       |
| ContraLSP                        | 0.013±0.001                       |
| TimeX                            | 0.064±0.007                       |
| TimeX++                          | 0.027±0.002                       |
| IG                               | 0.342±0.021                       |
| TIMING                           | 0.366±0.021                       |"
578,"| STRING    | Train                             | Inference                           |
| --------- | --------------------------------- | ----------------------------------- |
| Space     | Time                              | Space                               |
| Cayley    | $ \mathcal{O}\left(d^{2}\right) $ | $ \mathcal{O}\left(d^{3}\right) $   |
| Circulant | $ \mathcal{O}\left(d\right) $     | $ \mathcal{O}\left(d\log d\right) $ |"
578,"|           | ViT   | RoPE      | RoPE-M | Circulant-S | Cayley-S  |
| --------- | ----- | --------- | ------ | ----------- | --------- |
| ImageNet  | 80.04 | 80.18     | 80.86  | $ 81.22 $   | $ 81.09 $ |
| Places365 | 56.79 | $ 56.97 $ | 56.69  | 56.77       | $ 57.16 $ |
| Mean      | 68.42 | 68.58     | 68.78  | $ 69.00 $   | $ 69.12 $ |"
578,"|             | i2t@1 | i2t@5 | i2t@10 | t2i@1 | t2i@5 | t2i@10 | Mean  |
| ----------- | ----- | ----- | ------ | ----- | ----- | ------ | ----- |
| ViT         | 53.88 | 73.17 | 78.49  | 53.98 | 73.83 | 79.29  | 68.77 |
| RoPE        | 55.27 | 74.27 | 79.52  | 55.22 | 74.61 | 80.25  | 69.86 |
| RoPE-M      | 55.30 | 74.08 | 79.47  | 55.36 | 74.73 | 80.18  | 69.85 |
| Circulant-S | 55.52 | 74.69 | 79.91  | 55.68 | 75.03 | 80.45  | 70.21 |
| Cayley-S    | 55.70 | 75.08 | 80.24  | 55.82 | 75.40 | 80.65  | 70.48 |"
579,"| Top-one Accuracy | GELU      | ELU      | PReLU    | CELU     | SiLU     | Mish     | CRReLU   |
| ---------------- | --------- | -------- | -------- | -------- | -------- | -------- | -------- |
| CIFAR-100        | ViT-Tiny  | 32.6±0.8 | 28.9±0.1 | 43.2±1.0 | 28.9±0.2 | 31.2±0.6 | 30.6±0.8 |
| CIFAR-100        | DeiT-Tiny | 46.6±0.9 | 40.5±0.5 | 50.0±0.5 | 40.5±0.5 | 43.5±0.6 | 43.8±1.0 |
| CIFAR-100        | TNT-Small | 47.5±0.8 | 43.6±0.3 | 49.0±0.7 | 43.0±0.5 | 45.0±0.9 | 45.5±0.8 |"
579,"| Layer  | 1                     | 2                     | 3                     | 4                     | 5                     | 6                     |
| ------ | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- |
| CRReLU | 7.594  $ \pm $  0.007 | 7.598  $ \pm $  0.003 | 7.599  $ \pm $  0.003 | 7.595  $ \pm $  0.003 | 7.592  $ \pm $  0.003 | 7.584  $ \pm $  0.004 |
| GELU   | 7.536  $ \pm $  0.046 | 7.541  $ \pm $  0.019 | 7.561  $ \pm $  0.011 | 7.573  $ \pm $  0.006 | 7.580  $ \pm $  0.005 | 7.583  $ \pm $  0.004 |
| Layer  | 7                     | 8                     | 9                     | 10                    | 11                    | 12                    |
| CRReLU | 7.572  $ \pm $  0.005 | 7.557  $ \pm $  0.005 | 7.540  $ \pm $  0.005 | 7.523  $ \pm $  0.007 | 7.498  $ \pm $  0.008 | 7.461  $ \pm $  0.008 |
| GELU   | 7.585  $ \pm $  0.004 | 7.585  $ \pm $  0.004 | 7.583  $ \pm $  0.004 | 7.580  $ \pm $  0.004 | 7.577  $ \pm $  0.004 | 7.560  $ \pm $  0.004 |"
579,"| Evaluation Metrics | Evaluation Margin Reward↑ | Evaluation Accuracy↑ | Evaluation Loss↓ |
| ------------------ | ------------------------- | -------------------- | ---------------- |
| β=0.1              | CRReLU                    | 0.1428±0.0002        | 0.6209±0.0001    |
| GELU               | 0.1420±0.0003             | 0.6197±0.0001        | 0.6480±0.0000    |
| β=1                | CRReLU                    | 0.4627±0.0007        | 0.5757±0.0001    |
| GELU               | 0.4560±0.0006             | 0.5729±0.0003        | 0.9387±0.0008    |
| β=2                | CRReLU                    | 0.7757±0.0021        | 0.5631±0.0003    |
| GELU               | 0.7178±0.0015             | 0.5606±0.0001        | 1.4814±0.0005    |
| β=5                | CRReLU                    | 1.8473±0.0032        | 0.5635±0.0002    |
| GELU               | 1.6538±0.0069             | 0.5568±0.0003        | 3.3034±0.0021    |"
581,"| Chapter #Problem         | Chap1 25 | Chap2 24 | $ \uparrow $  Acc.(%) |
| ------------------------ | -------- | -------- | --------------------- |
| Chap3 19                 | Chap4 11 | All 79   |                       |
| GPT-4V                   | 16.0     | 4.17     | 5.26                  |
| GPT-4V + MMCoT           | 8.0      | 4.17     | 5.26                  |
| GPT-4V + MAPS (Ours)     | 52.0     | 7.14     | 36.8                  |
| Claude-3.5               | 16.0     | 4.17     | 0.0                   |
| Claude-3.5 + MAPS (Ours) | 44.0     | 12.5     | 21.1                  |
| GLM-4V                   | 8.0      | 4.17     | 5.26                  |
| GLM-4V + MAPS (Ours)     | 32.0     | 0.0      | 15.8                  |
| Gemini-1.5               | 8.0      | 12.5     | 0.0                   |
| GPT-4o                   | 20.0     | 4.17     | 5.26                  |"
581,"| Method                    | $ \uparrow $  Acc.(%) |
| ------------------------- | --------------------- |
| MAPS (Ours)               | 55.0                  |
| MAPS w.o. SL              | 45.0                  |
| MAPS w.o. Simulator       | 15.0                  |
| MAPS w.o. Simulator + PoT | 15.0                  |"
582,"| PROPERTY          | LONG                 | SHORT          |
| ----------------- | -------------------- | -------------- |
| Number of layers  | single               | multiple       |
| Update per window | incremental          | recurrent      |
| Capacity          | $ L \times Q_{max} $ | $ S \times N $ |"
582,"| MODEL                      | MEMORY | PG19  | arXiv  | C4(4K+) |
| -------------------------- | ------ | ----- | ------ | ------- |
|                            | All    | Short | Long   | Meena   |
| 12 LAYERS                  |        |       |        |         |
| Transformer XL             | 12.6M  | 12.6M | 0M     | 8.76    |
| Block Recurrent            | 12.1M  | 12.1M | 0M     | 8.47    |
| MELODI  $ S_{192}+L_{32} $ | 10.8M  | 2.4M  | 8.4M   | 8.22    |
| Memorizing Trans.          | 146.8M | 12.6M | 134.2M | 8.15    |
| MELODI  $ S_{128}+L_{64} $ | 18.4M  | 1.6M  | 16.8M  | 8.16    |
| MELODI  $ S_{192}+L_{96} $ | 27.6M  | 2.4M  | 25.2M  | 8.08    |"
582,"| MODEL                        | #Layers | Embedding Dim | Model Size | Memory Size | Perplexity  $ \downarrow $ |
| ---------------------------- | ------- | ------------- | ---------- | ----------- | -------------------------- |
| Transformer XL               | 12      | 1024          | 151M       | 12.6M       | 11.54                      |
| Memorizing Transformer       | 12      | 1024          | 151M       | 146.8M      | 10.74                      |
| MELODI  $ S_{128} + L_{64} $ | 12      | 1024          | 153M       | 18.4M       | 10.61                      |
| MELODI  $ S_{192} + L_{96} $ | 12      | 1024          | 153M       | 27.6M       | 10.48                      |
| Transformer XL               | 24      | 1024          | 302M       | 25.2M       | 10.15                      |
| Memorizing Transformer       | 24      | 1024          | 302M       | 159.4M      | 9.45                       |
| MELODI  $ S_{128} + L_{64} $ | 24      | 1024          | 306M       | 20.0M       | 9.30                       |
| MELODI  $ S_{192} + L_{96} $ | 24      | 1024          | 309M       | 30.0M       | 9.16                       |
| Transformer XL               | 36      | 1024          | 453M       | 37.8M       | 9.61                       |
| Memorizing Transformer       | 36      | 1024          | 453M       | 172.0M      | 8.92                       |
| MELODI  $ S_{128} + L_{64} $ | 36      | 1024          | 459M       | 21.6M       | 8.81                       |
| MELODI  $ S_{192} + L_{96} $ | 36      | 1024          | 463M       | 32.4M       | 8.70                       |
| Transformer XL               | 16      | 1536          | 453M       | 25.2M       | 9.69                       |
| Memorizing Transformer       | 16      | 1536          | 453M       | 226.5M      | 9.01                       |
| MELODI  $ S_{128} + L_{64} $ | 16      | 1536          | 456M       | 28.3M       | 8.89                       |
| MELODI  $ S_{192} + L_{96} $ | 16      | 1536          | 459M       | 42.5M       | 8.79                       |"
582,"| MODEL                         | Sequence Length | Window Length | Memory Size | Perplexity  $ \downarrow $ |
| ----------------------------- | --------------- | ------------- | ----------- | -------------------------- |
| Transformer XL                | 4096            | 512           | 12.6M       | 11.54                      |
| Memorizing Transformer        | 4096            | 512           | 146.8M      | 10.74                      |
| MELODI  $ S_{128} + L_{64} $  | 4096            | 512           | 18.4M       | 10.61                      |
| MELODI  $ S_{192} + L_{96} $  | 4096            | 512           | 27.6M       | 10.48                      |
| Transformer XL                | 4096            | 1024          | 25.2M       | 11.26                      |
| Memorizing Transformer        | 4096            | 1024          | 159.4M      | 10.64                      |
| MELODI  $ S_{256} + L_{128} $ | 4096            | 1024          | 20.0M       | 10.47                      |
| MELODI  $ S_{384} + L_{192} $ | 4096            | 1024          | 30.0M       | 10.36                      |
| Transformer XL                | 8192            | 1024          | 25.2M       | 11.18                      |
| Memorizing Transformer        | 8192            | 1024          | 159.4M      | 10.42                      |
| MELODI  $ S_{256} + L_{128} $ | 8192            | 1024          | 20.0M       | 10.27                      |
| MELODI  $ S_{384} + L_{192} $ | 8192            | 1024          | 30.0M       | 10.19                      |
| Transformer XL                | 8192            | 2048          | 50.3M       | 10.94                      |
| Memorizing Transformer        | 8192            | 2048          | 184.5M      | 10.38                      |
| MELODI  $ S_{512} + L_{256} $ | 8192            | 2048          | 23.1M       | 10.20                      |
| MELODI  $ S_{768} + L_{384} $ | 8192            | 2048          | 34.6M       | 10.10                      |"
582,"|          | L00.0M | L82.1M | L164.2M | L328.4M | L6416.8M | L9625.2M |
| -------- | ------ | ------ | ------- | ------- | -------- | -------- |
| S80.1M   | 11.81  | 11.69  | 11.59   | 11.42   | 11.34    | 11.15    |
| S160.2M  | 11.75  | 11.63  | 11.47   | 11.35   | 11.24    | 11.10    |
| S320.4M  | 11.60  | 11.57  | 11.40   | 11.25   | 11.11    | 11.07    |
| S640.9M  | 11.52  | 11.43  | 11.36   | 11.23   | 11.08    | 10.98    |
| S1281.7M | 11.39  | 11.41  | 11.27   | 11.08   | 10.95    | 10.90    |
| S1922.6M | 11.28  | 11.25  | 11.14   | 11.02   | 10.89    | 10.83    |
| S2563.4M | 11.26  | 11.23  | 11.11   | 10.93   | 10.83    | 10.77    |"
582,"| Branching | ST    | ST+LT |
| --------- | ----- | ----- |
| No        | 11.68 | 11.24 |
| Yes       | 11.39 | 10.95 |"
583,"|                       | With Ground Truth   | Without Ground Truth |
| --------------------- | ------------------- | -------------------- |
| Agentic Systems Types | Algorithm Generated | Hand Crafted         |
| Random                |                     |                      |
| Agent-Level Accuracy  | 29.10               | 12.00                |
| Step-Level Accuracy   | 19.06               | 4.16                 |
| All-at-Once           |                     |                      |
| Agent-Level Accuracy  | 54.33               | 55.17                |
| Step-Level Accuracy   | 12.50               | 5.26                 |
| Step-by-Step          |                     |                      |
| Agent-Level Accuracy  | 35.20               | 34.48                |
| Step-Level Accuracy   | 25.51               | 7.02                 |
| Binary Search         |                     |                      |
| Agent-Level Accuracy  | 44.13               | 51.72                |
| Step-Level Accuracy   | 23.98               | 6.90                 |"
584,"|                   | Match rate (%)  $ \uparrow $ | RMSE  $ \downarrow $ |
| ----------------- | ---------------------------- | -------------------- |
| CrysBFN           | 64.35                        | 0.0433               |
| w/o entropy cond. | 52.16                        | 0.0631               |
| w/o approx. sch.  | 49.76                        | 0.0643               |
| w/o torus BFN     | 6.17                         | 0.3822               |
|                   | 1k Batches Sim. Time (s)     |                      |
| Iterated Sim.     | 356.1                        |                      |
| Fast Sim.         | 92.6                         |                      |"
584,"| Data                              | Method                            | Validity (%)  $ \uparrow $ | Coverage (%)  $ \uparrow $ | Property  $ \downarrow $ |
| --------------------------------- | --------------------------------- | -------------------------- | -------------------------- | ------------------------ |
| Struc.                            | Comp.                             | COV-R                      | COV-P                      | d $ \rho $               |
| Perov-5                           | Cond-DFC-VAE (Court et al., 2020) | 73.60                      | 82.95                      | 73.92                    |
| G-SchNet (Gebauer et al., 2019)   | 99.92                             | 98.79                      | 0.18                       | 0.23                     |
| P-G-SchNet (Gebauer et al., 2019) | 79.63                             | 99.13                      | 0.37                       | 0.25                     |
| CDVAE (Xie et al., 2021)          | 100.0                             | 98.59                      | 99.45                      | 98.46                    |
| DiffCSP (Jiao et al., 2023)       | 100.0                             | 98.85                      | 99.74                      | 98.27                    |
| CrysBFN                           | 100.0                             | 98.86                      | 99.52                      | 98.63                    |
| Carbon-24                         | G-SchNet (Gebauer et al., 2019)   | 99.94                      | -                          | 0.00                     |
| P-G-SchNet (Gebauer et al., 2019) | 48.39                             | -                          | 0.00                       | 0.00                     |
| CDVAE (Xie et al., 2021)          | 100.0                             | -                          | 99.80                      | 83.08                    |
| DiffCSP (Jiao et al., 2023)       | 100.0                             | -                          | 99.90                      | 97.27                    |
| CrysBFN                           | 100.0                             | -                          | 99.90                      | 99.12                    |
| MP-20                             | G-SchNet (Gebauer et al., 2019)   | 99.65                      | 75.96                      | 38.33                    |
| P-G-SchNet (Gebauer et al., 2019) | 77.51                             | 76.40                      | 41.93                      | 99.74                    |
| CDVAE (Xie et al., 2021)          | 100.0                             | 86.70                      | 99.15                      | 99.49                    |
| DiffCSP (Jiao et al., 2023)       | 100.0                             | 83.25                      | 99.71                      | 99.76                    |
| FlowMM (Miller et al., 2024)      | 96.85                             | 83.19                      | 99.49                      | 99.58                    |
| CrysBFN                           | 100.0                             | 87.51                      | 99.09                      | 99.79                    |"
584,"| Method                       | Unique / % | Novel / % | Metastable / % | Stable / % | S.U.N. Rate / % |
| ---------------------------- | ---------- | --------- | -------------- | ---------- | --------------- |
| DiffCSP (Jiao et al., 2023)  | 96.11      | 90.95     | 37.91          | 12.16      | 9.44            |
| FlowMM (Miller et al., 2024) | 94.79      | 91.63     | 32.77          | 9.23       | 8.31            |
| CrysBFN                      | 95.29      | 92.37     | 45.91          | 15.82      | 12.16           |"
584,"|                              | Perov-5             | MP-20                   | MPTS-52             |
| ---------------------------- | ------------------- | ----------------------- | ------------------- |
| Match rate $ \uparrow $      | RMSE $ \downarrow $ | Match rate $ \uparrow $ | RMSE $ \downarrow $ |
| CDVAE (Xie et al., 2021)     | 45.31               | 0.1138                  | 33.90               |
| DiffCSP (Jiao et al., 2023)  | 52.02               | 0.0760                  | 51.49               |
| FlowMM (Miller et al., 2024) | 53.15               | 0.0992                  | 61.39               |
| CrysBFN                      | 54.69               | 0.0636                  | 64.35               |"
584,"|           | DiffCSP | CrysBFN |
| --------- | ------- | ------- |
| Perov-5   | 图片1     | 图片2     |
| Carbon-24 | 图片1     | 图片2     |
| MP-20     | 图片1     | 图片2     |"
585,"| Single-Source Generalization | Dual-Source Generalization       |
| ---------------------------- | -------------------------------- |
| Method                       | Waymo  $ \rightarrow $  KITTI    |
| Detector                     | mAP                              |
| Source Only                  | PV-RCNN                          |
| SN                           | PV-RCNN                          |
| Uni $ ^{2} $ Det (w/ SN)     | PV-RCNN                          |
| Source Only                  | Voxel-RCNN                       |
| SN                           | Voxel-RCNN                       |
| Uni $ ^{2} $ Det (w/ SN)     | Voxel-RCNN                       |
| Method                       | nuScenes  $ \rightarrow $  KITTI |
| Detector                     | mAP                              |
| Source Only                  | PV-RCNN                          |
| SN                           | PV-RCNN                          |
| Uni $ ^{2} $ Det (w/ SN)     | PV-RCNN                          |
| Source Only                  | Voxel-RCNN                       |
| SN                           | Voxel-RCNN                       |
| Uni $ ^{2} $ Det (w/ SN)     | Voxel-RCNN                       |"
585,"| Waymo-nuScenes Consolidation |
| ---------------------------- |
| Trained on                   |
| Vehicle                      |
| Waymo                        |
| nuScenes                     |
| Both W&amp;N                 |
| Uni3D                        |
| Uni2Det                      |"
585,"| Trained on   | Method      | Tested on KITTI | Tested on nuScenes |
| ------------ | ----------- | --------------- | ------------------ |
| Car          | Pedestrian  | Cyclist         | mAP                |
| KITTI        | w/ P.T.     | 89.90/81.25     | 59.49/56.17        |
| nuScenes     | w/ P.T.     | 71.61/40.64     | 39.67/29.99        |
| Both K&amp;N | D.M.        | 89.24/73.72     | 61.03/54.55        |
| Uni3D        | 90.09/83.10 | 62.99/58.30     | 70.20/68.10        |
| Uni2Det      | 90.60/84.16 | 68.40/64.47     | 68.74/65.68        |"
585,"| Trained on   | Method      | Tested on KITTI | Tested on Waymo |
| ------------ | ----------- | --------------- | --------------- |
| Car          | Pedestrian  | Cyclist         | mAP             |
| KITTI        | w/ P.T.     | 89.51/81.41     | 60.30/57.10     |
| Waymo        | w/ P.T.     | 64.84/19.99     | 62.58/59.01     |
| Both K&amp;W | D.M.        | 74.53/32.11     | 60.11/54.85     |
| Uni3D        | 90.03/82.39 | 62.51/57.01     | 69.52/66.30     |
| Uni2Det      | 90.30/84.23 | 64.30/61.03     | 71.15/69.18     |"
585,"| Trained on | Tested on   |
| ---------- | ----------- |
| KITTI      | NuScenes    |
| KITTI      | 70.04/66.09 |
| nuScenes   | 32.64/17.70 |
| Waymo      | 64.00/45.27 |
| Uni3D      | 72.19/67.46 |
| Uni2Det    | 76.04/72.61 |"
585,"| Method   | Voxelization | Backbone | Head        | KITTI       | Waymo       |
| -------- | ------------ | -------- | ----------- | ----------- | ----------- |
| Baseline |              |          |             | 72.73/69.94 | 71.09/69.12 |
| Ours     | ✓            |          |             | 73.84/70.56 | 71.71/69.73 |
|          | ✓            |          | 73.43/70.19 | 71.52/69.43 |             |
|          |              | ✓        | 73.95/70.43 | 71.73/69.82 |             |
| ✓        | ✓            |          | 74.96/70.95 | 72.29/69.93 |             |
| ✓        | ✓            | ✓        | 75.25/71.48 | 73.01/70.57 |             |"
586,"| Retriever                   | SPA $ \uparrow $        | Top1                | Top10                 |
| --------------------------- | ----------------------- | ------------------- | --------------------- |
| Accuracy $ \uparrow $       | Similarity $ \uparrow $ | MCES $ \downarrow $ | Accuracy $ \uparrow $ |
| NIST                        |                         |                     |                       |
| Spec2Mol                    | -                       | 0.0%                | 0.10                  |
| MSNovelist                  | -                       | 0.0%                | -                     |
| MADGEN $ _{\text{Pred.}} $  | 40.3%                   | 4.6%                | 0.11                  |
| MADGEN $ _{\text{Oracle}} $ | 100%                    | 49.0%               | 0.63                  |
| CANOPUS                     |                         |                     |                       |
| Spec2Mol                    | -                       | 0.0%                | 0.09                  |
| MSNovelist                  | -                       | 0.0%                | -                     |
| MADGEN $ _{\text{Pred.}} $  | 17.7%                   | 2.10%               | 0.22                  |
| MADGEN $ _{\text{Oracle}} $ | 100%                    | 18.7%               | 0.51                  |
| MassSpecGym                 |                         |                     |                       |
| Rand. Gen.                  | -                       | 0.0%                | 0.08                  |
| SMILES Transformer          | -                       | 0.0%                | 0.03                  |
| SELFIES Transformer         | -                       | 0.0%                | 0.08                  |
| Spec2Mol                    | -                       | 0.0%                | 0.09                  |
| MSNovelist                  | -                       | 0.0%                | -                     |
| MADGEN $ _{\text{Pred.}} $  | 13.2%                   | 1.31%               | 0.20                  |
| MADGEN $ _{\text{Oracle}} $ | 100%                    | 10.5%               | 0.43                  |"
586,"| Conditioning strategy   | Accuracy   | Similarity | MCES      |
| ----------------------- | ---------- | ---------- | --------- |
| Concatenation           | 4.30%      | 0.068      | 87.89     |
| Cross-Attn              | 13.0%      | 0.304      | 55.25     |
| Cross-Attn              | 42.5%      | 0.642      | 23.90     |
| Cross-Attn + CFG (edge) | 42.0%      | 0.632      | 25.01     |
| Cross-Attn + CFG (node) | $ 49.0\% $ | $ 0.694 $  | $ 18.48 $ |
| Cross-Attn + CFG (both) | 45.9%      | 0.667      | 21.89     |"
587,"| Methods     | TimeBase (ours) | SparseTSF (2024b) | TimeMixer (2024d) | FITS (2024) | iTransformer (2024a) | DLinear (2023) | PatchTST (2023) | TimesNet (2023) | FEDformer (2022) | Autoformer (2021) | Informer (2021) |
| ----------- | --------------- | ----------------- | ----------------- | ----------- | -------------------- | -------------- | --------------- | --------------- | ---------------- | ----------------- | --------------- |
| Metric      | MSE MAE         | MSE MAE           | MSE MAE           | MSE MAE     | MSE MAE              | MSE MAE        | MSE MAE         | MSE MAE         | MSE MAE          | MSE MAE           | MSE MAE         |
| ETTh1       | 0.396 0.414     | 0.407 0.419       | 0.454 0.474       | 0.417 0.430 | 0.454 0.467          | 0.437 0.448    | 0.420 0.440     | 0.505 0.499     | 0.523 0.524      | 0.726 0.641       | 1.445 0.930     |
| ETTh2       | 0.347 0.397     | 0.344 0.386       | 0.379 0.426       | 0.334 0.382 | 0.392 0.422          | 0.479 0.471    | 0.344 0.391     | 0.433 0.455     | 0.429 0.470      | 1.086 0.802       | 5.486 1.892     |
| ETTm1       | 0.356 0.380     | 0.362 0.384       | 0.381 0.414       | 0.359 0.382 | 0.370 0.401          | 0.361 0.383    | 0.355 0.385     | 0.408 0.425     | 0.438 0.466      | 0.564 0.522       | 1.138 0.818     |
| ETTm2       | 0.250 0.314     | 0.252 0.316       | 0.283 0.347       | 0.252 0.314 | 0.278 0.338          | 0.271 0.337    | 0.251 0.319     | 0.300 0.354     | 0.409 0.462      | 0.431 0.463       | 3.594 1.473     |
| Weather     | 0.219 0.263     | 0.244 0.286       | 0.238 0.281       | 0.244 0.286 | 0.233 0.273          | 0.242 0.299    | 0.223 0.264     | 0.254 0.293     | 0.355 0.391      | 0.446 0.457       | 0.567 0.513     |
| Electricity | 0.167 0.258     | 0.168 0.264       | 0.171 0.274       | 0.172 0.266 | 0.166 0.264          | 0.169 0.272    | 0.169 0.266     | 0.238 0.334     | 0.235 0.348      | 0.236 0.343       | 0.408 0.464     |
| Traffic     | 0.418 0.278     | 0.414 0.280       | 0.419 0.300       | 0.426 0.291 | 0.406 0.290          | 0.418 0.285    | 0.394 0.266     | 0.641 0.346     | 0.638 0.400      | 0.684 0.421       | 1.028 0.588     |
| AQShunyi    | 0.674 0.506     | 0.760 0.546       | 0.736 0.536       | 0.763 0.548 | 0.722 0.520          | 0.757 0.572    | 0.690 0.503     | 0.788 0.559     | 1.130 0.757      | 1.392 0.878       | 1.688 0.966     |
| AQWan       | 0.779 0.499     | 0.826 0.526       | 0.822 0.521       | 0.813 0.519 | 0.843 0.538          | 0.883 0.560    | 0.810 0.513     | 0.857 0.543     | 0.932 0.565      | 1.001 0.654       | 1.123 0.744     |
| CzeLan      | 0.225 0.262     | 0.240 0.292       | 0.234 0.282       | 0.250 0.304 | 0.245 0.296          | 0.242 0.290    | 0.229 0.275     | 0.258 0.312     | 0.266 0.314      | 0.289 0.368       | 0.347 0.426     |
| ZafNoo      | 0.495 0.445     | 0.531 0.490       | 0.505 0.464       | 0.520 0.480 | 0.540 0.500          | 0.540 0.495    | 0.510 0.469     | 0.545 0.502     | 0.601 0.539      | 0.667 0.635       | 0.726 0.696     |
| Exchange    | 0.309 0.392     | 0.316 0.410       | 0.330 0.426       | 0.302 0.400 | 0.301 0.401          | 0.312 0.410    | 0.308 0.400     | 0.326 0.425     | 0.380 0.472      | 0.447 0.589       | 0.432 0.581     |
| METR-LA     | 1.253 0.754     | 1.302 0.764       | 1.270 0.740       | 1.296 0.760 | 1.253 0.734          | 1.300 0.759    | 1.210 0.706     | 1.390 0.812     | 1.550 0.876      | 1.639 0.974       | 1.894 1.146     |
| PM2.5       | 0.428 0.437     | 0.467 0.477       | 0.465 0.474       | 0.460 0.469 | 0.480 0.492          | 0.449 0.457    | 0.458 0.466     | 0.466 0.475     | 0.560 0.547      | 0.621 0.650       | 0.636 0.663     |
| Solar       | 0.216 0.254     | 0.216 0.264       | 0.244 0.296       | 0.229 0.279 | 0.233 0.285          | 0.227 0.276    | 0.226 0.275     | 0.243 0.295     | 0.252 0.291      | 0.306 0.380       | 0.327 0.411     |
| Temp        | 0.187 0.338     | 0.295 0.412       | 0.302 0.423       | 0.315 0.442 | 0.302 0.423          | 0.292 0.418    | 0.291 0.406     | 0.304 0.427     | 0.343 0.456      | 0.404 0.587       | 0.471 0.682     |
| Wind        | 0.940 0.709     | 1.014 0.736       | 1.044 0.758       | 1.023 0.745 | 1.025 0.748          | 1.018 0.739    | 0.968 0.703     | 1.101 0.799     | 1.230 0.841      | 1.237 0.947       | 1.381 1.033     |"
587,"| Model       | PatchTST | PATCHTST (w/ TIMEBASE) |
| ----------- | -------- | ---------------------- |
| Metric      | MSE      | MAE                    |
| ETTh1       | 96       | 0.377                  |
| 192         | 0.413    | 0.431                  |
| 336         | 0.436    | 0.446                  |
| 720         | 0.455    | 0.475                  |
| ETTh2       | 96       | 0.276                  |
| 192         | 0.342    | 0.385                  |
| 336         | 0.364    | 0.405                  |
| 720         | 0.395    | 0.434                  |
| ETTm1       | 96       | 0.298                  |
| 192         | 0.335    | 0.373                  |
| 336         | 0.366    | 0.394                  |
| 720         | 0.420    | 0.421                  |
| ETTm2       | 96       | 0.165                  |
| 192         | 0.219    | 0.298                  |
| 336         | 0.268    | 0.333                  |
| 720         | 0.352    | 0.386                  |
| Weather     | 96       | 0.149                  |
| 192         | 0.193    | 0.243                  |
| 336         | 0.240    | 0.281                  |
| 720         | 0.312    | 0.334                  |
| Electricity | 96       | 0.141                  |
| 192         | 0.156    | 0.256                  |
| 336         | 0.172    | 0.267                  |
| 720         | 0.207    | 0.299                  |
| Traffic     | 96       | 0.363                  |
| 192         | 0.382    | 0.258                  |
| 336         | 0.399    | 0.268                  |
| 720         | 0.432    | 0.289                  |"
588,"| Sparsity    | Method      | LLaMa-2      | LLaMa-3   |
| ----------- | ----------- | ------------ | --------- |
| 7B          | 13B         | 8B           |           |
| Wikitext/C4 | Wikitext/C4 | Wikitext/C4  |           |
| 0%          | Dense       | 5.47         | / 7.26    |
| 50%         | Magnitude   | 16.03        | / 21.33   |
| SparseGPT   | 6.99±0.03   | / 9.20±0.03  | 6.06±0.03 |
| Wanda       | 6.92±0.01   | / 9.23±0.00  | 5.98±0.01 |
| ALPS        | 6.87±0.01   | / 8.98±0.00  | 5.96±0.02 |
| SAFE        | 6.78±0.01   | / 8.93±0.00  | 5.76±0.01 |
| SAFE+       | 6.56±0.01   | / 8.71±0.00  | 5.67±0.01 |
| 60%         | Magnitude   | 1864         | / 2043    |
| SparseGPT   | 10.19±0.08  | / 12.86±0.05 | 8.31±0.09 |
| Wanda       | 10.75±0.07  | / 13.87±0.01 | 8.43±0.07 |
| ALPS        | 9.55±0.00   | / 11.24±0.03 | 7.54±0.03 |
| SAFE        | 9.20±0.04   | / 11.51±0.04 | 7.18±0.03 |
| SAFE+       | 8.30±0.06   | / 10.59±0.00 | 6.78±0.04 |
| 4:8         | Magnitude   | 15.91        | / 31.61   |
| SparseGPT   | 8.42±0.05   | / 10.73±0.03 | 7.02±0.06 |
| Wanda       | 8.64±0.03   | / 11.35±0.01 | 7.01±0.02 |
| ALPS        | 8.11±0.09   | / 10.21±0.04 | 6.81±0.07 |
| SAFE        | 8.21±0.01   | / 10.61±0.04 | 6.60±0.02 |
| SAFE+       | 7.59±0.03   | / 9.88±0.01  | 6.37±0.03 |
| 2:4         | Magnitude   | 37.77        | / 74.70   |
| SparseGPT   | 11.00±0.20  | / 13.54±0.03 | 8.78±0.09 |
| Wanda       | 12.17±0.02  | / 15.60±0.11 | 9.01±0.04 |
| ALPS        | 9.99±0.19   | / 12.04±0.04 | 8.16±0.17 |
| SAFE        | 10.53±0.13  | / 13.20±0.07 | 7.64±0.05 |
| SAFE+       | 8.96±0.07   | / 11.34±0.03 | 7.20±0.04 |"
588,"| Sparsity | Method     | Noise ratio |
| -------- | ---------- | ----------- |
| 25%      | 50%        | 75%         |
| ADMM     | 77.00±0.91 | 59.18±0.55  |
| SAFE     | 90.58±0.30 | 86.51±0.16  |
| 80%      | ADMM       | 76.18±0.56  |
| SAFE     | 91.25±0.12 | 86.55±0.07  |
| 90%      | ADMM       | 79.40±0.12  |
| SAFE     | 90.68±0.21 | 86.49±0.06  |
| 95%      | ADMM       | 77.71±0.52  |
| SAFE     | 89.86±0.11 | 85.18±0.15  |
|          |            |             |
| SAFE     | 56.58±0.36 | 42.27±0.63  |"
589,"| Method                           | Component      | Params (M) | kMAC/pixel |
| -------------------------------- | -------------- | ---------- | ---------- |
| Ours (d1, d2, or d3)             | Transform-neck | 13.19      | 52.795     |
| Post-processing                  | Decoder        | 7.34       | 64.16      |
| Post-processing network          | 31.04          | (+386%)    | 835.72     |
| First 2 layers of visual encoder | 25.78          | 70.24      |            |"
592,"| Available Data              | CIFAR-10   | CelebA     | ImageNet   |
| --------------------------- | ---------- | ---------- | ---------- |
| 100% clean                  | 1.99±0.02  | 2.40±0.07  | 1.41±0.03  |
| 100% noisy                  | 11.93±0.09 | 12.97±0.11 | 5.32±0.08  |
| 10% clean (smaller dataset) | 17.30±0.14 | 11.92±0.05 | 10.57±0.06 |
| 90% noisy + 10% clean       | 2.81±0.02  | 2.75±0.02  | 1.68±0.02  |"
592,"| Dataset                | Available Data | Noise Level | p% of clean data |
| ---------------------- | -------------- | ----------- | ---------------- |
| 100%                   | 90%            | 70%         | 50%              |
| CIFAR-10               | p% clean       | N/A         | 1.99±0.02        |
| p% clean, (1-p)% noisy | 0.05           | 0.10        | 2.04±0.02        |
| 0.20                   | 2.06±0.03      | 2.14±0.03   | 2.15±0.01        |
| 2.06±0.03              | 2.14±0.02      | 2.24±0.01   | 2.42±0.03        |
| CelebA-HQ              | p% clean       | N/A         | 2.40±0.07        |
| p% clean, (1-p)% noisy | 0.05           | 2.40±0.01   | 2.45±0.01        |
| 0.10                   | 2.40±0.01      | 2.48±0.02   | 2.51±0.04        |
| 0.20                   | 2.50±0.00      | 2.51±0.11   | 2.52±0.01        |
| ImageNet               | p% clean       | N/A         | 1.41±0.03        |
| p% clean, (1-p)% noisy | 0.05           | 1.46±0.02   | 1.46±0.02        |
| 0.10                   | 1.48±0.01      | 1.48±0.02   | 1.49±0.01        |
| 0.20                   | 1.50±0.02      | 1.51±0.02   | 1.51±0.02        |"
593,"| Dataset     | CIFAR-10 | CIFAR-100 |
| ----------- | -------- | --------- |
| Noise Type  | Sym.     | Asym.     |
| Noise Ratio | 0.8      | 0.9       |
| CE          | 65.80    | 42.70     |
| DivideMix   | 93.20    | 76.00     |
| ELR         | 93.30    | 78.70     |
| SOP         | 94.00    | -         |
| GFWS        | 94.12    | 84.22     |
| ULAREF      | 91.47    | -         |
| ProMix      | 83.11    | -         |
| MOIT        | 79.00    | 69.60     |
| Sel-CL      | 89.20    | 81.90     |
| TCL         | 92.50    | 89.40     |
| WSC (Ours)  | 94.62    | 90.93     |"
593,"| Dataset       | CIFAR-10 | CIFAR-100 | CUB-200 |
| ------------- | -------- | --------- | ------- |
| Partial Ratio | 0.7      | 0.8       | 0.1     |
| LWS           | 72.11    | 58.49     | 50.44   |
| PRODEN        | 86.44    | 85.78     | 71.05   |
| CC            | 77.15    | 75.94     | 57.55   |
| MSE           | 69.09    | 64.32     | 47.33   |
| GFWS          | 94.21    | 93.58     | 75.95   |
| RCR           | 93.28    | 91.67     | 75.85   |
| PiCO          | 93.68    | 92.01     | 72.74   |
| WSC (Ours)    | 95.03    | 94.41     | 77.26   |"
593,"| Datasets | WSC   | CEL   | DIRK  |
| -------- | ----- | ----- | ----- |
| CUB-200  | 68.90 | 68.60 | 66.60 |
| CARS-196 | 87.88 | 86.22 | 85.31 |
| DOGS-120 | 79.75 | 78.18 | 75.97 |
| FGVC-100 | 77.80 | 78.36 | 76.86 |"
593,"| Components | Noisy Label | Partial Label |
| ---------- | ----------- | ------------- |
| Supervised | Consistency | WSC Loss      |
| ✓          |             |               |
| ✓          | ✓           |               |
| ✓          | ✓           | ✓             |"
595,"| METHODS       | MEDICAL PRETRAINING DATA               | DOMAIN           |
| ------------- | -------------------------------------- | ---------------- |
| SLIVIT [1]    | 14M IMAGES (IMAGENET) +108K OCT IMAGES | OPTICAL          |
| SUPREM [2]    | 5K CT VOLUMES                          | GENERAL          |
| MERLIN [3]    | 15K VOLUMES                            | CHEST            |
| MISFM [4]     | 110K CT VOLUMES                        | GENERAL          |
| VOCo [5]      | 160K CT VOLUMES                        | GENERAL          |
| RAPTOR (OURS) | NONE (USES 2D FOUNDATION MODEL)        | GENERAL $ ^{*} $ |"
595,"|          | ORGAN | NODULE | FRACTURE | ADRENAL | VESSEL | SYNAPSE |
| -------- | ----- | ------ | -------- | ------- | ------ | ------- |
| METHODS  | AUC   | ACC    | AUC      | ACC     | AUC    | ACC     |
| ResNet   | 0.995 | 0.918  | 0.886    | 0.860   | 0.759  | 0.487   |
| MAE      | 0.982 | 0.800  | 0.820    | 0.828   | 0.600  | 0.481   |
| MISFM    | 0.989 | 0.833  | 0.886    | 0.855   | 0.689  | 0.537   |
| Suprem   | 0.999 | 0.968  | 0.891    | 0.848   | 0.645  | 0.492   |
| SIViT    | 0.997 | 0.946  | 0.920    | 0.868   | 0.656  | 0.475   |
| VoCo     | 0.992 | 0.870  | 0.797    | 0.836   | 0.699  | 0.535   |
| Merlin   | 0.976 | 0.766  | 0.809    | 0.861   | 0.691  | 0.549   |
| Raptor-B | 0.998 | 0.958  | 0.904    | 0.858   | 0.647  | 0.501   |
| Raptor   | 0.999 | 0.961  | 0.929    | 0.870   | 0.677  | 0.502   |"
595,"| Methods   | WHITEM | GRAYM | CEREB | AMYG  | HIPPO | CORTEX | GYRUS | PALL  | CAUD  | THAL  | AVG.  |
| --------- | ------ | ----- | ----- | ----- | ----- | ------ | ----- | ----- | ----- | ----- | ----- |
| $ r^{2} $ |        |       |       |       |       |        |       |       |       |       |       |
| ResNET    | 0.417  | 0.562 | 0.193 | 0.072 | 0.108 | 0.125  | 0.099 | 0.055 | 0.162 | 0.134 | 0.193 |
| MAE       | 0.036  | 0.045 | 0.072 | 0.036 | 0.040 | 0.043  | 0.032 | 0.012 | 0.037 | 0.036 | 0.039 |
| MISFM     | 0.418  | 0.624 | 0.276 | 0.089 | 0.145 | 0.236  | 0.209 | 0.087 | 0.166 | 0.164 | 0.242 |
| SUPREM    | 0.646  | 0.696 | 0.330 | 0.109 | 0.163 | 0.275  | 0.256 | 0.067 | 0.255 | 0.195 | 0.299 |
| SLIViT    | 0.474  | 0.694 | 0.258 | 0.134 | 0.190 | 0.268  | 0.213 | 0.053 | 0.192 | 0.174 | 0.265 |
| VoCo      | 0.225  | 0.375 | 0.189 | 0.071 | 0.113 | 0.059  | 0.048 | 0.043 | 0.060 | 0.075 | 0.126 |
| MERLIN    | 0.622  | 0.734 | 0.335 | 0.127 | 0.180 | 0.313  | 0.269 | 0.093 | 0.247 | 0.210 | 0.313 |
| RAPTOR-B  | 0.614  | 0.742 | 0.398 | 0.185 | 0.247 | 0.355  | 0.314 | 0.116 | 0.331 | 0.258 | 0.356 |
| RAPTOR    | 0.681  | 0.777 | 0.437 | 0.170 | 0.262 | 0.404  | 0.340 | 0.142 | 0.381 | 0.300 | 0.389 |"
595,"| RaptorRaptor-B | SuPreMMISFM | MerlinVoCo | SLIViT |
| -------------- | ----------- | ---------- | ------ |"
595,"| Method | K     | Seed 1 | Seed 2 | Seed 3 | Std.   |
| ------ | ----- | ------ | ------ | ------ | ------ |
| RAPTOR | 1     | 0.818  | 0.817  | 0.793  | 0.0116 |
| 5      | 0.890 | 0.860  | 0.864  | 0.0133 |        |
| 10     | 0.866 | 0.896  | 0.876  | 0.0127 |        |
| 100    | 0.901 | 0.899  | 0.900  | 0.0008 |        |
| 150    | 0.898 | 0.897  | 0.897  | 0.0004 |        |"
595,"| METHOD      | VIEWPOINT | AUC   | ACC   |
| ----------- | --------- | ----- | ----- |
| RAPTOR      | (A)XIAL.  | 0.887 | 0.780 |
| (C)ORONAI.  | 0.862     | 0.814 |       |
| (S)AGITTAI. | 0.881     | 0.806 |       |
| A,C         | 0.893     | 0.826 |       |
| C,S         | 0.900     | 0.812 |       |
| A,S         | 0.884     | 0.822 |       |
| A,C,S       | 0.901     | 0.838 |       |"
