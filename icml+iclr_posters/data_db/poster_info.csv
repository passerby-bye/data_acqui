id,poster_id,title,authors,source_url,page_url,text_content,local_path_md,local_path_json
1,29917,HelpSteer2-Preference: Complementing Ratings with Preferences,"{""Zhilin Wang"",""Alexander Bukharin"",""Olivier Delalleau"",""Daniel Egert"",""Gerald Shen"",""Jiaqi Zeng"",""Oleksii Kuchaiev"",""Yi Dong""}",https://iclr.cc/media/PosterPDFs/ICLR%202025/29917.png?t=1743613361.8600876,https://iclr.cc/virtual/2025/poster/29917,"# HelpSteer2-Preference: Complementing Ratings with Preferences ## Why do we need HelpSteer2-Preference? It's unclear what the best approach for Reward Modelling is Bradley-Terry models: OpenAI InstructGPT, Anthropic HH-RLHF, Meta Llama 3 ○ Regression models: Nemotron 4 340B Reward, RLHFlow ArmoRM 8B • We need matched data for both approaches to find out ○ Identical set of prompts and responses ○ Collected for Purpose: Retrofitting Regression data for preference is not sufficient o High Quality: Garbage in; Garbage out – need to ensure high signal to noise ratio Open-source dataset to support open science ○ Data Gap: There’s currently no open-source dataset that fulfills all of the criteria above The community needs a high-quality, matched and open-source Preference data to accompany Regression data Regression and Bradley-Terry perform similarly but can complement each other to reach No. 1 on Reward Bench with Overall 94.1 (on 1 Oct 2024). ## Reward Modelling Results Scaled Bradley-Terry Optimal form of Bradley-Terry is Scaled Bradley-Terry which uses preference strength to scale loss proportionally $$ \mathcal{L}_{\mathcal{B T}}=-\log\left(\sigma(r_{\theta}(x,y_{c})-r_{\theta}(x,y_{r}))\right) $$ Regular BT [1] $$ \mathcal{L}_{\mathcal{M}\mathcal{B}\mathcal{T}}=-\log\left(\sigma(r_{\theta}(x,y_{c})-r_{\theta}(x,y_{r})-m)\right) $$ Margin BT [2] $$ \mathcal{L}_{\mathcal{S}\mathcal{B}\mathcal{T}}=-m\log\left(\sigma(r_{\theta}(x,y_{c})-r_{\theta}(x,y_{r}))\right) $$ Scaled BT (ours) ## HelpSteer2-Preference: Overview reward ( $ r_{\theta} $ ) for the chosen response ( $ y_{c} $ ) and the rejected response ( $ y_{r} $ ) with the same prompt (x) magnitude (m) of this preference (1 - slightly better, 2 - better, 3 - much better) Perspective 1 – Data Utilization: Repeated sampling of response pairs with higher preference magnitude Perspective 2 – Model Training: Larger gradient updates from response pairs with greater preference strength Difference from Margin BT: Does not assume that the chosen and rejected reward difference >= margin term HelpSteer2-Preference is an open-source, CC-BY-4.0 licensed, and rich dataset for top-performing and efficient reward modelling. - Top-performing: Used for Llama-3.1-Nemotron-70B-Reward, No. 1 on Reward Bench (94.1) at time of release (Oct 2024). - Complementary to Ratings: Prompts and responses are identical with HelpSteer2 (which contains Likert-5 ratings of Helpfulness and other attributes), permitting fair comparison. - Rich data: Each of 10k samples contains preference between two responses, preference strengths (slightly better, better and much better) and human-written preference justifications. https://huggingface.co/datasets/nvidia/HelpSteer2#preferences-new---1-oct-2024 >300k downloads Model Alignment Results Trained Reward Model can be used with REINFORCE algorithm during RLHF to produce top-performing model on MT-Bench, AlpacaEval 2 LC and Arena Hard Reward and REINFORCE models openly accessible (Llama 3.1 licensed) at https://huggingface.co/collections/nvidia/llama-31-nemotron-70b-670e93cd366feea16abc13d8 ## HelpSteer2-Preference Data Analysis - Preference and Helpfulness generally correlates: Larger helpfulness difference likely suggests stronger preference - Correlation not perfect: Some samples have responses with identical helpfulness but show preference for one response over the other - Position bias is weak: Humans show slight preference for latter response, possibly because of recency effect. Much lower than automated evals (e.g. GPT4/Claude in MT Bench [3]) ## Reference Links [1] https://arxiv.org/abs/2203.02155 [2] https://arxiv.org/abs/2307.09288 [3] https://arxiv.org/abs/2306.05685",/Users/sunxt/Desktop/M2-IASD/S3/Data acqui/小组作业-icml_posters/processed_data/29917/result.md,/Users/sunxt/Desktop/M2-IASD/S3/Data acqui/小组作业-icml_posters/processed_data/29917/result.json
2,28329,GlycanML: A Multi-Task and Multi-Structure Benchmark for Glycan Machine Learning,"{""Minghao Xu"",""Yunteng Geng"",""Yihang Zhang"",""Ling Yang"",""Jian Tang"",""Wentao Zhang""}",https://iclr.cc/media/PosterPDFs/ICLR%202025/28329.png?t=1741534541.6324637,https://iclr.cc/virtual/2025/poster/28329,## GlycanML: A Multi-Task and Multi-Structure Benchmark for Glycan Machine Learning Minghao Xu Yuteng Geng* Yihang Zhang* Ling Yang Jian Tang Wentao Zhang ## Covering Diverse Types of Glycan Understanding Tasks Paper Project Page Code ## Supporting Glycan Sequence and Graph Representations ## Maintaining A Leaderboard of Glycan Machine Learning Models,/Users/sunxt/Desktop/M2-IASD/S3/Data acqui/小组作业-icml_posters/processed_data/28329/result.md,/Users/sunxt/Desktop/M2-IASD/S3/Data acqui/小组作业-icml_posters/processed_data/28329/result.json
3,40170,Position: In-House Evaluation Is Not Enough. Towards Robust Third-Party Evaluation and Flaw Disclosure for General-Purpose AI,"{""Shayne Longpre"",""Kevin Klyman"",""Ruth Elisabeth Appel"",""Sayash Kapoor"",""Rishi Bommasani"",""Michelle Sahar"",""Sean McGregor"",""Avijit Ghosh"",""Borhane Blili-Hamelin"",""Nathan Butters"",""Alondra Nelson"",""Amit Elazari"",""Andrew Sellars"",""Casey Ellis"",""Dane Sherrets"",""Dawn Song"",""Harley Geiger"",""Ilona Cohen"",""Lauren McIlvenny"",""Madhulika Srikumar"",""Mark Jaycox"",""Markus Anderljung"",""Nadine Johnson"",""Nicholas Carlini"",""Nicolas Miailhe"",""Nik Marda"",""Peter Henderson"",""Rebecca Portnoff"",""Rebecca Weiss"",""Victoria Westerhoff"",""Yacine Jernite"",""Rumman Chowdhury"",""Percy Liang"",""Arvind Narayanan""}",https://icml.cc/media/PosterPDFs/ICML%202025/40170.png?t=1751933980.6469772,https://icml.cc/virtual/2025/poster/40170,"## I n-House Evaluation Is Not Enough: ## Towards Robust Third-Party Flaw Disclosure for General-Purpose AI Shayne Longpre*, Kevin Klyman*, Ruth E Appel*, Sayash Kapoor, Rishi Bommasani, Michelle Sahar, Sean McGregor, Avijit Ghosh, Borhane Bili-Hamelin, Nathan Butters, Alondra Nelson, Amit Elazari, Andrew Sellars, Casey John Ellis, Dane Sherrets, Dawn Song, Harley Geiger, Ilona Cohen, Lauren McIlvenny, Madhulika Srikumar, Mark M Jaycox, Markus Anderljung, Nadine Farid Johnson, Nicholas Carlini, Nicolas Mialhe, Nik Marda, Peter Henderson, Rebecca S Portnoff, Rebecca Weiss, Victoria Westerhoff, Yacine Jernite, Rumman Chowdhury, Percy Liang, Arvind Narayanan ## Problem Statement AI systems, agents, and their applications have many risks. However, there are obstacles to mitigation: 1. An absence of flaw reporting culture 2. Limited disclosure infrastructure (eg bug bounties) 3. No legal protections for third-party evaluators ## Recommendations We recommend the AI community adopt 3 conventions from the software security community: 1. Evaluators should submit flaw reports 2. AI developers should adopt flaw disclosure programs, to coordinate universally transferable flaws 3. AI developers should protect evaluators with safe harbors ## Next Steps We are building out a flaw report form, that is: A. Fast, and convenient to fill-out B. Collects information that makes it easy for developers to validate, triage, and reproduce reported flaws Coordinated Flaw Disclosure Paper Link ## Schema for a Flaw Report AI flaw reports are complex to design. The relevant information is contingent on many conditions, such as whether the flaw has caused harm (and become an “incident”), or whether there is a malicious threat actor.",/Users/sunxt/Desktop/M2-IASD/S3/Data acqui/小组作业-icml_posters/processed_data/40170/result.md,/Users/sunxt/Desktop/M2-IASD/S3/Data acqui/小组作业-icml_posters/processed_data/40170/result.json
